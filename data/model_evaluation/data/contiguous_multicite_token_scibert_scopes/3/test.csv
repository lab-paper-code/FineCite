token_context,word_context,seg_context,sent_cotext,label
"[' #TAUTHOR_TAG with and without introspection, we']","[' #TAUTHOR_TAG with and without introspection, we']","['complement predictor.', 'for both cooperative methods, i. e.  #TAUTHOR_TAG with and without introspection, we train an independent extra predictor on the unselected words from the generator, which does not affect the training of the generator - predictor framework.', 'from']","['this section, we evaluate the proposed methods on the more challenging single - aspect beer dataset.', 'similar to previous experiments, we force all the methods to have comparable highlighting ratio and continuity constraints for fair evaluation.', 'the highlighting ratio is determined from human estimation on a small set of data.', 'we report two classification results, which are the accuracy of the predictor and complement predictor.', 'for both cooperative methods, i. e.  #TAUTHOR_TAG with and without introspection, we train an independent extra predictor on the unselected words from the generator, which does not affect the training of the generator - predictor framework.', '']",5
['original model of  #TAUTHOR_TAG with'],['original model of  #TAUTHOR_TAG with'],['further conduct subjective evaluations by comparing the original model of  #TAUTHOR_TAG with'],"['further conduct subjective evaluations by comparing the original model of  #TAUTHOR_TAG with our introspective threeplayer model.', 'we mask the extracted rationales and present the unselected words only to the hu - table 5 : subjective evaluations on the task of controlling the unselected rationale words.', '']",5
"["". e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model""]","[""of su - pervised signal, i. e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model""]","["". e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model should be low."", 'however, we']","['predictive performances on the relation classification task are shown in the right part of the table 4.', 'we observe consistent results as in previous datasets.', 'clearly, the introspective generator helps the accuracy and the three - player game regularize the complement of the rationale selections.', 'examples of the extracted rationales for relation classification, it is difficult to conduct subjective evaluations because the task requires people to have sufficient knowledge of the schema of relation annotation.', 'to further demonstrate the quality of generated rationales, we provide some illustrative examples.', ""since there is a rich form of su - pervised signal, i. e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model should be low."", 'however, we still spot quite a few cases.', 'in the first example,  #TAUTHOR_TAG fails to highlight the second entity while ours does.', 'in the second example, the introspective three - player model selects more words than  #TAUTHOR_TAG.', 'in this case, the two entities themselves suffice to serve as the rationales.', 'however, our model preserves the words like "" working "".', 'this problem might due to the bias of the dataset.', 'for example, some words that are not relevant to the target entities may still correlate with the labels.', 'in the case, our model will pick these words as a part of the rationale']",5
['rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerate'],"['rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerated rationales.', 'the method of  #TAUTHOR_TAG works well in many applications.', 'however, as discussed in']",['as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerate'],"['first equality is given by lemma 1. 1 ; the second equality is given by eq. ( 3 ).', 'for the inequality, the equality holds if and only if py ( · | r ( x ) ) = py ( · | x ),', 'which is eq. ( 4 ).', 'lemma 1. 3.', 'a rationalization scheme z ( x ) that satisfies eq. ( 5 ) is the global minimizer of lg as defined in eq. ( 9 ).', 'proof.', 'according to lemma 1. 1, lg can be rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerated rationales.', 'the method of  #TAUTHOR_TAG works well in many applications.', 'however, as discussed in section 1 and 2. 2, all the cooperative rationalization approaches may suffer from the problem of degeneration.', 'in this section, we design an experiment to confirm the existence of the problem in the original  #TAUTHOR_TAG model.', 'we use the same single - beer review constructed from ( mc  #AUTHOR_TAG, as will be described in appendix c. instead of constructing a balanced binary classification task, we set the samples with scores higher than 0. 5 as positive examples.', 'on such a task, the prediction model with full inputs achieves 82. 3 % accuracy on the development set']",5
['rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerate'],"['rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerated rationales.', 'the method of  #TAUTHOR_TAG works well in many applications.', 'however, as discussed in']",['as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerate'],"['first equality is given by lemma 1. 1 ; the second equality is given by eq. ( 3 ).', 'for the inequality, the equality holds if and only if py ( · | r ( x ) ) = py ( · | x ),', 'which is eq. ( 4 ).', 'lemma 1. 3.', 'a rationalization scheme z ( x ) that satisfies eq. ( 5 ) is the global minimizer of lg as defined in eq. ( 9 ).', 'proof.', 'according to lemma 1. 1, lg can be rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerated rationales.', 'the method of  #TAUTHOR_TAG works well in many applications.', 'however, as discussed in section 1 and 2. 2, all the cooperative rationalization approaches may suffer from the problem of degeneration.', 'in this section, we design an experiment to confirm the existence of the problem in the original  #TAUTHOR_TAG model.', 'we use the same single - beer review constructed from ( mc  #AUTHOR_TAG, as will be described in appendix c. instead of constructing a balanced binary classification task, we set the samples with scores higher than 0. 5 as positive examples.', 'on such a task, the prediction model with full inputs achieves 82. 3 % accuracy on the development set']",5
['rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerate'],"['rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerated rationales.', 'the method of  #TAUTHOR_TAG works well in many applications.', 'however, as discussed in']",['as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerate'],"['first equality is given by lemma 1. 1 ; the second equality is given by eq. ( 3 ).', 'for the inequality, the equality holds if and only if py ( · | r ( x ) ) = py ( · | x ),', 'which is eq. ( 4 ).', 'lemma 1. 3.', 'a rationalization scheme z ( x ) that satisfies eq. ( 5 ) is the global minimizer of lg as defined in eq. ( 9 ).', 'proof.', 'according to lemma 1. 1, lg can be rewritten as table 1 and degeneration cases of  #TAUTHOR_TAG generates degenerated rationales.', 'the method of  #TAUTHOR_TAG works well in many applications.', 'however, as discussed in section 1 and 2. 2, all the cooperative rationalization approaches may suffer from the problem of degeneration.', 'in this section, we design an experiment to confirm the existence of the problem in the original  #TAUTHOR_TAG model.', 'we use the same single - beer review constructed from ( mc  #AUTHOR_TAG, as will be described in appendix c. instead of constructing a balanced binary classification task, we set the samples with scores higher than 0. 5 as positive examples.', 'on such a task, the prediction model with full inputs achieves 82. 3 % accuracy on the development set']",5
"['the training of  #TAUTHOR_TAG, we stipulate that the generated rationales are very concise : we punish it']","['the training of  #TAUTHOR_TAG, we stipulate that the generated rationales are very concise : we punish it']","['the training of  #TAUTHOR_TAG, we stipulate that the generated rationales are very concise : we punish it']","['the training of  #TAUTHOR_TAG, we stipulate that the generated rationales are very concise : we punish it when the rationales have more than 3 pieces or more than 20 % of the words are generated ( both with hinge losses ).', 'from the results, we can see that  #TAUTHOR_TAG tends to predict color words, like dark - brown, yellow, as rationales.', 'this is a clue of degeneration, since most of the appearance reviews start with describing colors.', 'therefore a degenerated generator can learn to split the vocabulary of colors, and communicate with the predictor by using some of the colors for the positive label and some others for the negative label.', 'such a learned generator also fails to generalize well, given the significant performance decrease ( 76. 4 % v. s. 82. 3 % ).', 'by comparison, our method with three - player game could achieve both higher accuracy and more meaningful rationales']",5
"['the training of  #TAUTHOR_TAG, we stipulate that the generated rationales are very concise : we punish it']","['the training of  #TAUTHOR_TAG, we stipulate that the generated rationales are very concise : we punish it']","['the training of  #TAUTHOR_TAG, we stipulate that the generated rationales are very concise : we punish it']","['the training of  #TAUTHOR_TAG, we stipulate that the generated rationales are very concise : we punish it when the rationales have more than 3 pieces or more than 20 % of the words are generated ( both with hinge losses ).', 'from the results, we can see that  #TAUTHOR_TAG tends to predict color words, like dark - brown, yellow, as rationales.', 'this is a clue of degeneration, since most of the appearance reviews start with describing colors.', 'therefore a degenerated generator can learn to split the vocabulary of colors, and communicate with the predictor by using some of the colors for the positive label and some others for the negative label.', 'such a learned generator also fails to generalize well, given the significant performance decrease ( 76. 4 % v. s. 82. 3 % ).', 'by comparison, our method with three - player game could achieve both higher accuracy and more meaningful rationales']",5
[' #TAUTHOR_TAG ( acc : 76'],"[' #TAUTHOR_TAG ( acc : 76. 4 % ) :', '']",[' #TAUTHOR_TAG ( acc : 76'],"['section describes how we construct the single - aspect review task from the multi - aspect beer review dataset ( mc  #AUTHOR_TAG.', 'in many multi - aspect beer reviews, we can see clear patterns indicating the aspect of the following sentences.', 'for example, the sentences starting with "" appearance : "" or "" a : "" are likely to be a review on the appearance aspect ; and the sentences original text ( positive ) : dark - brown / black color with a huge tan head that gradually collapses, leaving thick lacing.', 'rationale from  #TAUTHOR_TAG ( acc : 76. 4 % ) :', '[ "" dark - brown / black color "" ] rationale from our method ( acc : 80. 4 % ) :', '']",5
[' #TAUTHOR_TAG ( acc : 76'],"[' #TAUTHOR_TAG ( acc : 76. 4 % ) :', '']",[' #TAUTHOR_TAG ( acc : 76'],"['section describes how we construct the single - aspect review task from the multi - aspect beer review dataset ( mc  #AUTHOR_TAG.', 'in many multi - aspect beer reviews, we can see clear patterns indicating the aspect of the following sentences.', 'for example, the sentences starting with "" appearance : "" or "" a : "" are likely to be a review on the appearance aspect ; and the sentences original text ( positive ) : dark - brown / black color with a huge tan head that gradually collapses, leaving thick lacing.', 'rationale from  #TAUTHOR_TAG ( acc : 76. 4 % ) :', '[ "" dark - brown / black color "" ] rationale from our method ( acc : 80. 4 % ) :', '']",5
"['by  #TAUTHOR_TAG and our method.', '']","['by  #TAUTHOR_TAG and our method.', '']","['by  #TAUTHOR_TAG and our method.', ""each rationale word is masked with the symbol'* '."", 'the masked texts from different methods are mixed']","['section explains how we designed the human study.', 'the goal is to evaluate the unpredictable rates of the input texts after the rationales are removed.', 'to this end, we mask the original texts with the rationales generated by  #TAUTHOR_TAG and our method.', ""each rationale word is masked with the symbol'* '."", 'the masked texts from different methods are mixed and shuffled so the evaluators cannot know from which systems an input was generated.', 'we have two human evaluators who are not the authors of the paper.', 'during evaluation, an evaluator is presented with one masked text and asked to try her / his best to predict the sentiment label of it.', '']",5
"[' #TAUTHOR_TAG.', '']","['on askubuntu  #TAUTHOR_TAG.', 'askubuntu is a non - factoid']","['on askubuntu  #TAUTHOR_TAG.', '']","['following the suggestion from the reviews, we evaluate the proposed method on the question retrieval task on askubuntu  #TAUTHOR_TAG.', 'askubuntu is a non - factoid question retrieval benchmark.', 'the goal is to retrieve the most relevant questions from an input question.', 'we use the same data split provided by  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG.', '']","['on askubuntu  #TAUTHOR_TAG.', 'askubuntu is a non - factoid']","['on askubuntu  #TAUTHOR_TAG.', '']","['following the suggestion from the reviews, we evaluate the proposed method on the question retrieval task on askubuntu  #TAUTHOR_TAG.', 'askubuntu is a non - factoid question retrieval benchmark.', 'the goal is to retrieve the most relevant questions from an input question.', 'we use the same data split provided by  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG.', '']","['on askubuntu  #TAUTHOR_TAG.', 'askubuntu is a non - factoid']","['on askubuntu  #TAUTHOR_TAG.', '']","['following the suggestion from the reviews, we evaluate the proposed method on the question retrieval task on askubuntu  #TAUTHOR_TAG.', 'askubuntu is a non - factoid question retrieval benchmark.', 'the goal is to retrieve the most relevant questions from an input question.', 'we use the same data split provided by  #TAUTHOR_TAG.', '']",5
"['word embeddings as released by  #TAUTHOR_TAG.', 'results']","['word embeddings as released by  #TAUTHOR_TAG.', 'results']","['provides better performances.', '8 we use the same word embeddings as released by  #TAUTHOR_TAG.', 'results']","['consider the following three - step training strategy : 1 ) pre - train a classifier with the full text ; 2 ) fix the pre - trained classifier, which is used for both the predictor 7 https : / / github. com / taolei87 / askubuntu.', 'table 9 : testing map on the askubuntu dataset.', 'map c refers to the map score of the complement predictor.', 'the desired rationalization method will have high map and low map c. and the complement predictor in the three - player game approach, and pre - train the rationale generators ; and 3 ) fine - tune all modules end - to - end.', 'this pipeline significantly stabilizes the training and provides better performances.', '8 we use the same word embeddings as released by  #TAUTHOR_TAG.', '']",5
"['as', ' #TAUTHOR_TAG to']","['##s. moreover, we empirically show that ( 1 ) the three - player framework on its own helps cooperative games such as', ' #TAUTHOR_TAG to']","['moreover, we empirically show that ( 1 ) the three - player framework on its own helps cooperative games such as', ' #TAUTHOR_TAG to improve']","['', 'for the two cooperative methods. in order to prevent degenerate rationales', ', we propose a three - player game that renders explicit control over the unselected parts. in addition to the generator and the predictor', 'as in conventional cooperative rationale selection schemes, we add a third adversarial player, called the complement predictor, to regularize the cooperative communication between the generator and the predictor. the goal of', 'the complement predictor is to predict the correct label using only words left out of the rationale. during training,', 'the generator aims to fool the complement predictor while still maintaining high accuracy for the', 'predictor. this ensures that the selected rationale must contain all / most of the', 'information about the target label, leaving out irrelevant parts, within size constraints imposed on the rationales. we also theoretically show that the equilibrium', 'of the three - player game guarantees good properties for the extracted rationales. moreover, we empirically show that ( 1 ) the three - player framework on its own helps cooperative games such as', ' #TAUTHOR_TAG to improve both predictive accuracy and rationale quality ; (', '2 ) by combining the two solutions - introspective generator and', 'the three player game - we can achieve high predictive accuracy and non - degenerate rationales']",6
"['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05']","['only 10 % of the words are used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05 v. s. 87. 59 ).', 'with the additional third player added, the accuracy is slightly improved, which validates that controlling the unselected words improves the robustness of rationales.', 'on the other hand, our introspection models are able to maintain higher predictive accuracy ( 86. 16 v. s. 82. 05 ) compared to  #TAUTHOR_TAG, while only sacrificing a little loss on highlighting precision ( 0. 47 % drop ).', 'similar observations are made when 20 % of the words required to highlight with one exception.', 'comparing the model of  #TAUTHOR_TAG with and without the proposed mini - max module, there is a huge gap of more than 5 % on recall of generated rationales.', 'this confirms the motivation that the original cooperative game tends to generate less comprehensive rationales, where the three - player framework controls the unselected words to be less informative so the recall is significantly improved.', 'it is worth mentioning that when a classifier is trained with randomly highlighted rationales ( i. e. random dropout  #AUTHOR_TAG on the inputs ), it performs significantly worse on both predictive accuracy and highlighting qualities.', 'this confirms that extracting concise and sufficient rationales is not a trivial task.', '']",6
"['word embeddings as released by  #TAUTHOR_TAG.', 'results']","['word embeddings as released by  #TAUTHOR_TAG.', 'results']","['provides better performances.', '8 we use the same word embeddings as released by  #TAUTHOR_TAG.', 'results']","['consider the following three - step training strategy : 1 ) pre - train a classifier with the full text ; 2 ) fix the pre - trained classifier, which is used for both the predictor 7 https : / / github. com / taolei87 / askubuntu.', 'table 9 : testing map on the askubuntu dataset.', 'map c refers to the map score of the complement predictor.', 'the desired rationalization method will have high map and low map c. and the complement predictor in the three - player game approach, and pre - train the rationale generators ; and 3 ) fine - tune all modules end - to - end.', 'this pipeline significantly stabilizes the training and provides better performances.', '8 we use the same word embeddings as released by  #TAUTHOR_TAG.', '']",6
"['n is the binary mask.', 'many previous works  #TAUTHOR_TAG a ) follows the above']","['following form at each position i :', 'where zi ∈ { 0, 1 } n is the binary mask.', 'many previous works  #TAUTHOR_TAG a ) follows the above']","['0, 1 } n is the binary mask.', 'many previous works  #TAUTHOR_TAG a ) follows the above definition of rationales.', 'in this work, we further define the complement of rationale,']","['target application here is text classification on data tokens in the form of { ( x, y ) }.', 'denote x = x1 : l as a sequence of words in an input text with length l. denote y as a label.', 'our goal is to generate a rationale, denoted as r ( x ) = r1 : l ( x ), which is a selection of words in x that accounts for y.', 'formally, r ( x ) is a hard - masked version of x that takes the following form at each position i :', 'where zi ∈ { 0, 1 } n is the binary mask.', 'many previous works  #TAUTHOR_TAG a ) follows the above definition of rationales.', 'in this work, we further define the complement of rationale, denoted as r c ( x ), as', 'for notational ease,']",0
"['n is the binary mask.', 'many previous works  #TAUTHOR_TAG a ) follows the above']","['following form at each position i :', 'where zi ∈ { 0, 1 } n is the binary mask.', 'many previous works  #TAUTHOR_TAG a ) follows the above']","['0, 1 } n is the binary mask.', 'many previous works  #TAUTHOR_TAG a ) follows the above definition of rationales.', 'in this work, we further define the complement of rationale,']","['target application here is text classification on data tokens in the form of { ( x, y ) }.', 'denote x = x1 : l as a sequence of words in an input text with length l. denote y as a label.', 'our goal is to generate a rationale, denoted as r ( x ) = r1 : l ( x ), which is a selection of words in x that accounts for y.', 'formally, r ( x ) is a hard - masked version of x that takes the following form at each position i :', 'where zi ∈ { 0, 1 } n is the binary mask.', 'many previous works  #TAUTHOR_TAG a ) follows the above definition of rationales.', 'in this work, we further define the complement of rationale, denoted as r c ( x ), as', 'for notational ease,']",3
"['method in  #TAUTHOR_TAG. training during training, the three players']","['method in  #TAUTHOR_TAG. training during training, the three players']","['method in  #TAUTHOR_TAG. training during training, the three players']","['y conditioned on r c, denoted asp c (', 'y | r ). both predictors are trained using the cross entropy loss, i. e. where h ( p ; q ) denotes the', 'cross entropy between p and q. p ( · | · ) denotes the empirical distribution. it is worth emphasizing that lp and lc are both', 'functions of the generator. generator : the generator extracts r and r c by generating the rationale mask, z ( · ), as shown in eqs. ( 1 - 2 ). specifically,', 'z ( · ) is determined by minimizing the weighted combination of four losses : where lg encourages the gap between lp and lc to be', 'large, i. e. it stipulates the comprehensiveness property of the rationale ( eq. ( 5 ) ). intuitively, if the complement rationale is less informative of', 'y than the rationale, then lc should be larger than lp. ls and lc impose the sparsity and continuity respectively, which correspond to eq. ( 6 ) : from eq.', '( 7 ), we can see that the generator plays a', 'cooperative game with the predictor, because both tries to maximize', 'the predictive performance of r. on the other hand, the generator plays an adversarial game with the complement predictor, because the latter tries to maximize the', 'predictive performance of r c, but the former tries to reduce it. without the complement predictor, and thus the loss lg, the framework reduces to the method in  #TAUTHOR_TAG. training during training, the three players perform gradient descent steps with respect to their own losses. for the generator,', 'since z ( x ) is a set of binary variables, we cannot apply the regular gradient descent algorithm. instead we will use policy gradient  #AUTHOR_TAG to optimize', 'the models. we maximize the reward that is defined as the negative loss in eq. (', '8 ). in order to have bounded rewards for training stability, the negative losses lp and lc are replaced with accuracy. theoretical', 'guarantees the proposed framework is able to obtain a rationale that simultane - ously satisfies the conditions in eqs', '. ( 4 ) to ( 6 ), as stated in the following theorem : theorem 1. a rationalization', 'scheme z ( x ) that simultaneously satisfies eqs. ( 4', ') - ( 6 ) is the global optimizer of eq. ( 8 ). the proof is given in appendix a. the basic idea', 'is that there is a correspondence between each term in eq. ( 8 )', 'and each of the properties eqs. ( 4 ) - (', '6 ). the minimization of each loss term is equivalent to satisfying the', 'corresponding property']",4
"['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05']","['only 10 % of the words are used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05 v. s. 87. 59 ).', 'with the additional third player added, the accuracy is slightly improved, which validates that controlling the unselected words improves the robustness of rationales.', 'on the other hand, our introspection models are able to maintain higher predictive accuracy ( 86. 16 v. s. 82. 05 ) compared to  #TAUTHOR_TAG, while only sacrificing a little loss on highlighting precision ( 0. 47 % drop ).', 'similar observations are made when 20 % of the words required to highlight with one exception.', 'comparing the model of  #TAUTHOR_TAG with and without the proposed mini - max module, there is a huge gap of more than 5 % on recall of generated rationales.', 'this confirms the motivation that the original cooperative game tends to generate less comprehensive rationales, where the three - player framework controls the unselected words to be less informative so the recall is significantly improved.', 'it is worth mentioning that when a classifier is trained with randomly highlighted rationales ( i. e. random dropout  #AUTHOR_TAG on the inputs ), it performs significantly worse on both predictive accuracy and highlighting qualities.', 'this confirms that extracting concise and sufficient rationales is not a trivial task.', '']",4
"['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05']","['only 10 % of the words are used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05 v. s. 87. 59 ).', 'with the additional third player added, the accuracy is slightly improved, which validates that controlling the unselected words improves the robustness of rationales.', 'on the other hand, our introspection models are able to maintain higher predictive accuracy ( 86. 16 v. s. 82. 05 ) compared to  #TAUTHOR_TAG, while only sacrificing a little loss on highlighting precision ( 0. 47 % drop ).', 'similar observations are made when 20 % of the words required to highlight with one exception.', 'comparing the model of  #TAUTHOR_TAG with and without the proposed mini - max module, there is a huge gap of more than 5 % on recall of generated rationales.', 'this confirms the motivation that the original cooperative game tends to generate less comprehensive rationales, where the three - player framework controls the unselected words to be less informative so the recall is significantly improved.', 'it is worth mentioning that when a classifier is trained with randomly highlighted rationales ( i. e. random dropout  #AUTHOR_TAG on the inputs ), it performs significantly worse on both predictive accuracy and highlighting qualities.', 'this confirms that extracting concise and sufficient rationales is not a trivial task.', '']",4
"['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05']","['only 10 % of the words are used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05 v. s. 87. 59 ).', 'with the additional third player added, the accuracy is slightly improved, which validates that controlling the unselected words improves the robustness of rationales.', 'on the other hand, our introspection models are able to maintain higher predictive accuracy ( 86. 16 v. s. 82. 05 ) compared to  #TAUTHOR_TAG, while only sacrificing a little loss on highlighting precision ( 0. 47 % drop ).', 'similar observations are made when 20 % of the words required to highlight with one exception.', 'comparing the model of  #TAUTHOR_TAG with and without the proposed mini - max module, there is a huge gap of more than 5 % on recall of generated rationales.', 'this confirms the motivation that the original cooperative game tends to generate less comprehensive rationales, where the three - player framework controls the unselected words to be less informative so the recall is significantly improved.', 'it is worth mentioning that when a classifier is trained with randomly highlighted rationales ( i. e. random dropout  #AUTHOR_TAG on the inputs ), it performs significantly worse on both predictive accuracy and highlighting qualities.', 'this confirms that extracting concise and sufficient rationales is not a trivial task.', '']",4
"['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant']","['used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05']","['only 10 % of the words are used,  #TAUTHOR_TAG has a significant performance downgrade compared to the accuracy when using the whole passage ( 82. 05 v. s. 87. 59 ).', 'with the additional third player added, the accuracy is slightly improved, which validates that controlling the unselected words improves the robustness of rationales.', 'on the other hand, our introspection models are able to maintain higher predictive accuracy ( 86. 16 v. s. 82. 05 ) compared to  #TAUTHOR_TAG, while only sacrificing a little loss on highlighting precision ( 0. 47 % drop ).', 'similar observations are made when 20 % of the words required to highlight with one exception.', 'comparing the model of  #TAUTHOR_TAG with and without the proposed mini - max module, there is a huge gap of more than 5 % on recall of generated rationales.', 'this confirms the motivation that the original cooperative game tends to generate less comprehensive rationales, where the three - player framework controls the unselected words to be less informative so the recall is significantly improved.', 'it is worth mentioning that when a classifier is trained with randomly highlighted rationales ( i. e. random dropout  #AUTHOR_TAG on the inputs ), it performs significantly worse on both predictive accuracy and highlighting qualities.', 'this confirms that extracting concise and sufficient rationales is not a trivial task.', '']",4
"[' #TAUTHOR_TAG with and without introspection, we']","[' #TAUTHOR_TAG with and without introspection, we']","['complement predictor.', 'for both cooperative methods, i. e.  #TAUTHOR_TAG with and without introspection, we train an independent extra predictor on the unselected words from the generator, which does not affect the training of the generator - predictor framework.', 'from']","['this section, we evaluate the proposed methods on the more challenging single - aspect beer dataset.', 'similar to previous experiments, we force all the methods to have comparable highlighting ratio and continuity constraints for fair evaluation.', 'the highlighting ratio is determined from human estimation on a small set of data.', 'we report two classification results, which are the accuracy of the predictor and complement predictor.', 'for both cooperative methods, i. e.  #TAUTHOR_TAG with and without introspection, we train an independent extra predictor on the unselected words from the generator, which does not affect the training of the generator - predictor framework.', '']",4
"["". e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model""]","[""of su - pervised signal, i. e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model""]","["". e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model should be low."", 'however, we']","['predictive performances on the relation classification task are shown in the right part of the table 4.', 'we observe consistent results as in previous datasets.', 'clearly, the introspective generator helps the accuracy and the three - player game regularize the complement of the rationale selections.', 'examples of the extracted rationales for relation classification, it is difficult to conduct subjective evaluations because the task requires people to have sufficient knowledge of the schema of relation annotation.', 'to further demonstrate the quality of generated rationales, we provide some illustrative examples.', ""since there is a rich form of su - pervised signal, i. e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model should be low."", 'however, we still spot quite a few cases.', 'in the first example,  #TAUTHOR_TAG fails to highlight the second entity while ours does.', 'in the second example, the introspective three - player model selects more words than  #TAUTHOR_TAG.', 'in this case, the two entities themselves suffice to serve as the rationales.', 'however, our model preserves the words like "" working "".', 'this problem might due to the bias of the dataset.', 'for example, some words that are not relevant to the target entities may still correlate with the labels.', 'in the case, our model will pick these words as a part of the rationale']",4
"["". e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model""]","[""of su - pervised signal, i. e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model""]","["". e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model should be low."", 'however, we']","['predictive performances on the relation classification task are shown in the right part of the table 4.', 'we observe consistent results as in previous datasets.', 'clearly, the introspective generator helps the accuracy and the three - player game regularize the complement of the rationale selections.', 'examples of the extracted rationales for relation classification, it is difficult to conduct subjective evaluations because the task requires people to have sufficient knowledge of the schema of relation annotation.', 'to further demonstrate the quality of generated rationales, we provide some illustrative examples.', ""since there is a rich form of su - pervised signal, i. e., the number of class labels is large, the chance of any visible degeneration of the  #TAUTHOR_TAG's model should be low."", 'however, we still spot quite a few cases.', 'in the first example,  #TAUTHOR_TAG fails to highlight the second entity while ours does.', 'in the second example, the introspective three - player model selects more words than  #TAUTHOR_TAG.', 'in this case, the two entities themselves suffice to serve as the rationales.', 'however, our model preserves the words like "" working "".', 'this problem might due to the bias of the dataset.', 'for example, some words that are not relevant to the target entities may still correlate with the labels.', 'in the case, our model will pick these words as a part of the rationale']",4
[' #TAUTHOR_TAG ( acc : 76'],"[' #TAUTHOR_TAG ( acc : 76. 4 % ) :', '']",[' #TAUTHOR_TAG ( acc : 76'],"['section describes how we construct the single - aspect review task from the multi - aspect beer review dataset ( mc  #AUTHOR_TAG.', 'in many multi - aspect beer reviews, we can see clear patterns indicating the aspect of the following sentences.', 'for example, the sentences starting with "" appearance : "" or "" a : "" are likely to be a review on the appearance aspect ; and the sentences original text ( positive ) : dark - brown / black color with a huge tan head that gradually collapses, leaving thick lacing.', 'rationale from  #TAUTHOR_TAG ( acc : 76. 4 % ) :', '[ "" dark - brown / black color "" ] rationale from our method ( acc : 80. 4 % ) :', '']",4
[' #TAUTHOR_TAG ( acc : 76'],"[' #TAUTHOR_TAG ( acc : 76. 4 % ) :', '']",[' #TAUTHOR_TAG ( acc : 76'],"['section describes how we construct the single - aspect review task from the multi - aspect beer review dataset ( mc  #AUTHOR_TAG.', 'in many multi - aspect beer reviews, we can see clear patterns indicating the aspect of the following sentences.', 'for example, the sentences starting with "" appearance : "" or "" a : "" are likely to be a review on the appearance aspect ; and the sentences original text ( positive ) : dark - brown / black color with a huge tan head that gradually collapses, leaving thick lacing.', 'rationale from  #TAUTHOR_TAG ( acc : 76. 4 % ) :', '[ "" dark - brown / black color "" ] rationale from our method ( acc : 80. 4 % ) :', '']",4
"[' #TAUTHOR_TAG.', '']","['on askubuntu  #TAUTHOR_TAG.', 'askubuntu is a non - factoid']","['on askubuntu  #TAUTHOR_TAG.', '']","['following the suggestion from the reviews, we evaluate the proposed method on the question retrieval task on askubuntu  #TAUTHOR_TAG.', 'askubuntu is a non - factoid question retrieval benchmark.', 'the goal is to retrieve the most relevant questions from an input question.', 'we use the same data split provided by  #TAUTHOR_TAG.', '']",4
"['word embeddings as released by  #TAUTHOR_TAG.', 'results']","['word embeddings as released by  #TAUTHOR_TAG.', 'results']","['provides better performances.', '8 we use the same word embeddings as released by  #TAUTHOR_TAG.', 'results']","['consider the following three - step training strategy : 1 ) pre - train a classifier with the full text ; 2 ) fix the pre - trained classifier, which is used for both the predictor 7 https : / / github. com / taolei87 / askubuntu.', 'table 9 : testing map on the askubuntu dataset.', 'map c refers to the map score of the complement predictor.', 'the desired rationalization method will have high map and low map c. and the complement predictor in the three - player game approach, and pre - train the rationale generators ; and 3 ) fine - tune all modules end - to - end.', 'this pipeline significantly stabilizes the training and provides better performances.', '8 we use the same word embeddings as released by  #TAUTHOR_TAG.', '']",4
['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],['is consistent with previous findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],"['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG. meanwhile, our implicit sr']",[' #TAUTHOR_TAG'],0
['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],['is consistent with previous findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],"['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG. meanwhile, our implicit sr']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG, treating']","[' #TAUTHOR_TAG, treating']","['##l  #TAUTHOR_TAG, treating opinion expressions as the major predicates.', 'these systems can achieve good performances, indicating that srl information']","['##l aims to find the core semantic arguments for a given predicate, which is highly correlative with the orl task.', 'the semantic roles agent ( arg0 ) and patient ( arg1 ) are often corresponding to the opinion holder and target, respectively.', 'several works even directly transfer semantic roles into opinion roles for orl  #TAUTHOR_TAG, treating opinion expressions as the major predicates.', 'these systems can achieve good performances, indicating that srl information can be greatly useful for orl.', '']",0
['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],['is consistent with previous findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],"['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG. meanwhile, our implicit sr']",[' #TAUTHOR_TAG'],1
['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],['is consistent with previous findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],"['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG. meanwhile, our implicit sr']",[' #TAUTHOR_TAG'],3
"[', which is consistent with previous studies  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG.', 'the implicit sr']","['very helpful for orl, which is consistent with previous studies  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG.', 'the implicit srl - sawr method is highly effective']","[', which is consistent with previous studies  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG.', 'the implicit srl - sawr method is highly effective']","['', 'the tendencies are similar by exploiting the binary and proportional matching methods.', 'the results show that srl information is very helpful for orl, which is consistent with previous studies  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG.', 'the implicit srl - sawr method is highly effective to integrate srl information into the orl model']",3
['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],['is consistent with previous findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],"['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG. meanwhile, our implicit sr']",[' #TAUTHOR_TAG'],5
"['directly  #TAUTHOR_TAG.', '']","['orl directly  #TAUTHOR_TAG.', '']","['directly  #TAUTHOR_TAG.', '']","['', 'considering the much larger scale of annotated srl corpora, srl can benefit orl potentially.', 'according to the above findings, we design a simple system by mapping srl outputs into orl directly  #TAUTHOR_TAG.', '']",5
['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],['is consistent with previous findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG'],"['findings  #TAUTHOR_TAG marasovic and  #AUTHOR_TAG. meanwhile, our implicit sr']",[' #TAUTHOR_TAG'],4
"[' #TAUTHOR_TAG, treating']","[' #TAUTHOR_TAG, treating']","['##l  #TAUTHOR_TAG, treating opinion expressions as the major predicates.', 'these systems can achieve good performances, indicating that srl information']","['##l aims to find the core semantic arguments for a given predicate, which is highly correlative with the orl task.', 'the semantic roles agent ( arg0 ) and patient ( arg1 ) are often corresponding to the opinion holder and target, respectively.', 'several works even directly transfer semantic roles into opinion roles for orl  #TAUTHOR_TAG, treating opinion expressions as the major predicates.', 'these systems can achieve good performances, indicating that srl information can be greatly useful for orl.', '']",6
"['the final results.', 'we present a replication study of bert pretraining  #TAUTHOR_TAG that carefully measures the']","['the final results.', 'we present a replication study of bert pretraining  #TAUTHOR_TAG that carefully measures the']","['the final results.', 'we present a replication study of bert pretraining  #TAUTHOR_TAG that carefully measures the impact of many key hyperparameters and training data size.', 'we find that bert was significantly undertrained,']","['model pretraining has led to significant performance gains but careful comparison between different approaches is challenging.', 'training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results.', 'we present a replication study of bert pretraining  #TAUTHOR_TAG that carefully measures the impact of many key hyperparameters and training data size.', 'we find that bert was significantly undertrained, and can match or exceed the performance of every model published after it.', 'our best model achieves state - of - the - art results on glue, race and squad.', 'these results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements.', 'we release our models and code']",5
"['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['- training methods such as elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most.', 'training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.', 'we present a replication study of bert pretraining  #TAUTHOR_TAG, which includes a careful evaluation of the effects of hyperparmeter tuning and training set size.', 'we find that bert was significantly undertrained and propose an improved recipe for training bert models, which we call roberta, that can match or exceed the performance of all of the post - bert methods.', 'our modifications are simple, they include : ( 1 ) training the model longer, with bigger batches, over more data ; ( 2 ) removing the next sentence prediction objective ; ( 3 ) training on longer sequences ; and ( 4 ) dynamically changing the masking pattern applied to the training data.', 'we also collect a large new dataset ( cc - news ) of comparable size to other privately used datasets, to better control for training set size effects.', 'when controlling for training data, our improved training procedure improves upon the published bert results on both glue and squad.', '']",5
['of the bert  #TAUTHOR_TAG pretraining approach and some of the training choices that we will examine experimentally in the following'],['of the bert  #TAUTHOR_TAG pretraining approach and some of the training choices that we will examine experimentally in the following section'],"['this section, we give a brief overview of the bert  #TAUTHOR_TAG pretraining approach and some of the training choices that we will examine experimentally in the following section']","['this section, we give a brief overview of the bert  #TAUTHOR_TAG pretraining approach and some of the training choices that we will examine experimentally in the following section']",5
"['follows the original bert paper  #TAUTHOR_TAG.', 'in']","['follows the original bert paper  #TAUTHOR_TAG.', 'in']","['follows the original bert paper  #TAUTHOR_TAG.', 'in']","['', 'the glue organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held - out test data.', 'for the replication study in section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data ( i. e., without multi - task training or ensembling ).', 'our finetuning procedure follows the original bert paper  #TAUTHOR_TAG.', 'in section 5 we additionally report test set results obtained from the public leaderboard.', 'these results depend on a several task - specific modifications, which we describe in section 5. 1.', '']",5
"['follows the original bert paper  #TAUTHOR_TAG.', 'in']","['follows the original bert paper  #TAUTHOR_TAG.', 'in']","['follows the original bert paper  #TAUTHOR_TAG.', 'in']","['', 'the glue organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held - out test data.', 'for the replication study in section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data ( i. e., without multi - task training or ensembling ).', 'our finetuning procedure follows the original bert paper  #TAUTHOR_TAG.', 'in section 5 we additionally report test set results obtained from the public leaderboard.', 'these results depend on a several task - specific modifications, which we describe in section 5. 1.', '']",5
"['we feed a sequence to the model.', 'this becomes crucial when pretraining for more steps or with larger datasets.', 'results table 1 compares the published bert base results from  #TAUTHOR_TAG to']","['we feed a sequence to the model.', 'this becomes crucial when pretraining for more steps or with larger datasets.', 'results table 1 compares the published bert base results from  #TAUTHOR_TAG to']","['we feed a sequence to the model.', 'this becomes crucial when pretraining for more steps or with larger datasets.', 'results table 1 compares the published bert base results from  #TAUTHOR_TAG to']","['discussed in section 2, bert relies on randomly masking and predicting tokens.', 'the original bert implementation performed masking once during data preprocessing, resulting in a single static mask.', 'to avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training.', 'thus, each training sequence was seen with the same mask four times during training.', 'we compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model.', 'this becomes crucial when pretraining for more steps or with larger datasets.', 'results table 1 compares the published bert base results from  #TAUTHOR_TAG to our reimplementation with either static or dynamic masking.', 'we find that our reimplementation with static masking performs similar to the original bert model, and dynamic masking is comparable or slightly better than static masking.', 'given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments']",5
"['bert  #TAUTHOR_TAG, with']","['bert  #TAUTHOR_TAG, with']","['format used in bert  #TAUTHOR_TAG, with the nsp loss.']","['and squad 1. 1. however, some recent work has questioned the necessity of the nsp loss  #AUTHOR_TAG. to better understand this discrepancy, we compare several alternative training formats : • segment -', 'pair + nsp : this follows the original input format used in bert  #TAUTHOR_TAG, with the nsp loss. each input has a pair of segments, which can each contain multiple natural sentences, but the', 'total combined length must be less than 512 tokens. • sentence - pair + nsp :', 'each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. since these inputs are', '']",5
"['bert  #TAUTHOR_TAG, with']","['bert  #TAUTHOR_TAG, with']","['format used in bert  #TAUTHOR_TAG, with the nsp loss.']","['and squad 1. 1. however, some recent work has questioned the necessity of the nsp loss  #AUTHOR_TAG. to better understand this discrepancy, we compare several alternative training formats : • segment -', 'pair + nsp : this follows the original input format used in bert  #TAUTHOR_TAG, with the nsp loss. each input has a pair of segments, which can each contain multiple natural sentences, but the', 'total combined length must be less than 512 tokens. • sentence - pair + nsp :', 'each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. since these inputs are', '']",5
"['', 'in particular, while both bert  #TAUTHOR_TAG']","['adopt a much simpler approach for squad compared to past work.', 'in particular, while both bert  #TAUTHOR_TAG']","['', 'in particular, while both bert  #TAUTHOR_TAG']","['adopt a much simpler approach for squad compared to past work.', 'in particular, while both bert  #TAUTHOR_TAG and xlnet augment their training data with additional qa datasets, we only finetune roberta using the provided squad training data.', ' #AUTHOR_TAG also employed a custom layer - wise learning rate schedule to finetune results could potentially be improved by augmenting this with additional pronoun disambiguation datasets.', 'xlnet, while we use the same learning rate for all layers.', '']",5
"['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['- training methods such as elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most.', 'training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.', 'we present a replication study of bert pretraining  #TAUTHOR_TAG, which includes a careful evaluation of the effects of hyperparmeter tuning and training set size.', 'we find that bert was significantly undertrained and propose an improved recipe for training bert models, which we call roberta, that can match or exceed the performance of all of the post - bert methods.', 'our modifications are simple, they include : ( 1 ) training the model longer, with bigger batches, over more data ; ( 2 ) removing the next sentence prediction objective ; ( 3 ) training on longer sequences ; and ( 4 ) dynamically changing the masking pattern applied to the training data.', 'we also collect a large new dataset ( cc - news ) of comparable size to other privately used datasets, to better control for training set size effects.', 'when controlling for training data, our improved training procedure improves upon the published bert results on both glue and squad.', '']",0
['of the bert  #TAUTHOR_TAG pretraining approach and some of the training choices that we will examine experimentally in the following'],['of the bert  #TAUTHOR_TAG pretraining approach and some of the training choices that we will examine experimentally in the following section'],"['this section, we give a brief overview of the bert  #TAUTHOR_TAG pretraining approach and some of the training choices that we will examine experimentally in the following section']","['this section, we give a brief overview of the bert  #TAUTHOR_TAG pretraining approach and some of the training choices that we will examine experimentally in the following section']",0
"['is supported natively in fairseq  #AUTHOR_TAG.', 'the original bert implementation  #TAUTHOR_TAG uses a character - level bp']","['is supported natively in fairseq  #AUTHOR_TAG.', 'the original bert implementation  #TAUTHOR_TAG uses a character - level bpe vocabulary of size 30k,']","['multiple mini - batches are accumulated locally before each optimization step.', 'this functionality is supported natively in fairseq  #AUTHOR_TAG.', 'the original bert implementation  #TAUTHOR_TAG uses a character - level bpe vocabulary of size 30k,']","['', 'however, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work.', ' #AUTHOR_TAG introduce a clever implementation of bpe that uses bytes instead of unicode characters as the base subword units.', 'using bytes makes it possible to learn a subword vocabulary of a modest size ( 50k units ) that can still encode any input text without introducing any "" unknown "" tokens.', '8 large batch training can improve training efficiency even without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini - batches are accumulated locally before each optimization step.', 'this functionality is supported natively in fairseq  #AUTHOR_TAG.', 'the original bert implementation  #TAUTHOR_TAG uses a character - level bpe vocabulary of size 30k, which is learned after preprocessing the input with heuristic tokenization rules.', ' #AUTHOR_TAG, we instead consider training bert with a larger byte - level bpe vocabulary containing 50k subword units, without any additional preprocessing or tokenization of the input.', 'this adds approximately 15m and 20m additional parameters for bert base and bert large, respectively.', 'early experiments revealed only slight differences between these encodings, with the  #AUTHOR_TAG bpe achieving slightly worse end - task performance on some tasks.', 'nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments.', 'a more detailed comparison of these encodings is left to future work']",0
"['', 'in particular, while both bert  #TAUTHOR_TAG']","['adopt a much simpler approach for squad compared to past work.', 'in particular, while both bert  #TAUTHOR_TAG']","['', 'in particular, while both bert  #TAUTHOR_TAG']","['adopt a much simpler approach for squad compared to past work.', 'in particular, while both bert  #TAUTHOR_TAG and xlnet augment their training data with additional qa datasets, we only finetune roberta using the provided squad training data.', ' #AUTHOR_TAG also employed a custom layer - wise learning rate schedule to finetune results could potentially be improved by augmenting this with additional pronoun disambiguation datasets.', 'xlnet, while we use the same learning rate for all layers.', '']",0
"['masked language modeling  #TAUTHOR_TAG.', '']","['masked language modeling  #TAUTHOR_TAG.', '']","['masked language modeling  #TAUTHOR_TAG.', 'many recent papers have used a basic recipe of fine']","['##training methods have been designed with different training objectives, including language modeling  #AUTHOR_TAG, machine translation ( mc  #AUTHOR_TAG, and masked language modeling  #TAUTHOR_TAG.', 'many recent papers have used a basic recipe of finetuning models for each end task  #AUTHOR_TAG, and pretraining with some variant of a masked language model objective.', 'however, newer methods have improved performance by multi - task fine tuning  #AUTHOR_TAG, incorporating entity embeddings  #AUTHOR_TAG, span prediction  #AUTHOR_TAG, and multiple variants of autoregressive pretraining  #AUTHOR_TAG.', 'performance is also typically improved by training bigger models on more data  #TAUTHOR_TAG.', 'our goal was to replicate, simplify, and better tune the training of bert, as a reference point for better understanding the relative performance of all of these methods.', 'we carefully evaluate a number of design decisions when pretraining bert models.', 'we find that performance can be substantially improved by training the model longer, with bigger batches over more data ; removing the next sentence prediction objective ; training on longer sequences ; and dynamically changing the masking pattern applied to the training data.', 'our improved pretraining procedure, which we call roberta, achieves state - of - the - art results on glue, race and squad, without multi - task finetuning for glue or additional data for squad.', ""these results illustrate the importance of these previously overlooked design decisions and suggest that bert's pretraining objective remains competitive with recently proposed alternatives."", 'we additionally use a novel dataset, cc - news, and release our models and code for pretraining and finetuning at : https : / / github. com / pytorch / fairseq.', 'optimized bert pretraining approach']",0
"['masked language modeling  #TAUTHOR_TAG.', '']","['masked language modeling  #TAUTHOR_TAG.', '']","['masked language modeling  #TAUTHOR_TAG.', 'many recent papers have used a basic recipe of fine']","['##training methods have been designed with different training objectives, including language modeling  #AUTHOR_TAG, machine translation ( mc  #AUTHOR_TAG, and masked language modeling  #TAUTHOR_TAG.', 'many recent papers have used a basic recipe of finetuning models for each end task  #AUTHOR_TAG, and pretraining with some variant of a masked language model objective.', 'however, newer methods have improved performance by multi - task fine tuning  #AUTHOR_TAG, incorporating entity embeddings  #AUTHOR_TAG, span prediction  #AUTHOR_TAG, and multiple variants of autoregressive pretraining  #AUTHOR_TAG.', 'performance is also typically improved by training bigger models on more data  #TAUTHOR_TAG.', 'our goal was to replicate, simplify, and better tune the training of bert, as a reference point for better understanding the relative performance of all of these methods.', 'we carefully evaluate a number of design decisions when pretraining bert models.', 'we find that performance can be substantially improved by training the model longer, with bigger batches over more data ; removing the next sentence prediction objective ; training on longer sequences ; and dynamically changing the masking pattern applied to the training data.', 'our improved pretraining procedure, which we call roberta, achieves state - of - the - art results on glue, race and squad, without multi - task finetuning for glue or additional data for squad.', ""these results illustrate the importance of these previously overlooked design decisions and suggest that bert's pretraining objective remains competitive with recently proposed alternatives."", 'we additionally use a novel dataset, cc - news, and release our models and code for pretraining and finetuning at : https : / / github. com / pytorch / fairseq.', 'optimized bert pretraining approach']",0
"['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['- training methods such as elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most.', 'training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.', 'we present a replication study of bert pretraining  #TAUTHOR_TAG, which includes a careful evaluation of the effects of hyperparmeter tuning and training set size.', 'we find that bert was significantly undertrained and propose an improved recipe for training bert models, which we call roberta, that can match or exceed the performance of all of the post - bert methods.', 'our modifications are simple, they include : ( 1 ) training the model longer, with bigger batches, over more data ; ( 2 ) removing the next sentence prediction objective ; ( 3 ) training on longer sequences ; and ( 4 ) dynamically changing the masking pattern applied to the training data.', 'we also collect a large new dataset ( cc - news ) of comparable size to other privately used datasets, to better control for training set size effects.', 'when controlling for training data, our improved training procedure improves upon the published bert results on both glue and squad.', '']",1
"['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to']","['- training methods such as elmo  #AUTHOR_TAG, gpt  #AUTHOR_TAG, bert  #TAUTHOR_TAG, xlm  #AUTHOR_TAG, and xlnet have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most.', 'training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.', 'we present a replication study of bert pretraining  #TAUTHOR_TAG, which includes a careful evaluation of the effects of hyperparmeter tuning and training set size.', 'we find that bert was significantly undertrained and propose an improved recipe for training bert models, which we call roberta, that can match or exceed the performance of all of the post - bert methods.', 'our modifications are simple, they include : ( 1 ) training the model longer, with bigger batches, over more data ; ( 2 ) removing the next sentence prediction objective ; ( 3 ) training on longer sequences ; and ( 4 ) dynamically changing the masking pattern applied to the training data.', 'we also collect a large new dataset ( cc - news ) of comparable size to other privately used datasets, to better control for training set size effects.', 'when controlling for training data, our improved training procedure improves upon the published bert results on both glue and squad.', '']",6
"['t = 512 tokens.', 'unlike  #TAUTHOR_TAG,']","['t = 512 tokens.', 'unlike  #TAUTHOR_TAG,']","['most t = 512 tokens.', 'unlike  #TAUTHOR_TAG,']","['reimplement bert in fairseq  #AUTHOR_TAG.', 'we primarily follow the original bert optimization hyperparameters, given in section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting.', 'we additionally found training to be very sensitive to the adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it.', 'similarly, we found setting β 2 = 0. 98 to improve stability when training with large batch sizes.', 'we pretrain with sequences of at most t = 512 tokens.', 'unlike  #TAUTHOR_TAG, we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90 % of updates.', 'we train only with full - length sequences.', 'we train with mixed precision floating point arithmetic on dgx - 1 machines, each with 8 × 32gb nvidia v100 gpus interconnected by infiniband  #AUTHOR_TAG']",4
"['bert  #TAUTHOR_TAG, with']","['bert  #TAUTHOR_TAG, with']","['format used in bert  #TAUTHOR_TAG, with the nsp loss.']","['and squad 1. 1. however, some recent work has questioned the necessity of the nsp loss  #AUTHOR_TAG. to better understand this discrepancy, we compare several alternative training formats : • segment -', 'pair + nsp : this follows the original input format used in bert  #TAUTHOR_TAG, with the nsp loss. each input has a pair of segments, which can each contain multiple natural sentences, but the', 'total combined length must be less than 512 tokens. • sentence - pair + nsp :', 'each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. since these inputs are', '']",4
"['is supported natively in fairseq  #AUTHOR_TAG.', 'the original bert implementation  #TAUTHOR_TAG uses a character - level bp']","['is supported natively in fairseq  #AUTHOR_TAG.', 'the original bert implementation  #TAUTHOR_TAG uses a character - level bpe vocabulary of size 30k,']","['multiple mini - batches are accumulated locally before each optimization step.', 'this functionality is supported natively in fairseq  #AUTHOR_TAG.', 'the original bert implementation  #TAUTHOR_TAG uses a character - level bpe vocabulary of size 30k,']","['', 'however, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work.', ' #AUTHOR_TAG introduce a clever implementation of bpe that uses bytes instead of unicode characters as the base subword units.', 'using bytes makes it possible to learn a subword vocabulary of a modest size ( 50k units ) that can still encode any input text without introducing any "" unknown "" tokens.', '8 large batch training can improve training efficiency even without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini - batches are accumulated locally before each optimization step.', 'this functionality is supported natively in fairseq  #AUTHOR_TAG.', 'the original bert implementation  #TAUTHOR_TAG uses a character - level bpe vocabulary of size 30k, which is learned after preprocessing the input with heuristic tokenization rules.', ' #AUTHOR_TAG, we instead consider training bert with a larger byte - level bpe vocabulary containing 50k subword units, without any additional preprocessing or tokenization of the input.', 'this adds approximately 15m and 20m additional parameters for bert base and bert large, respectively.', 'early experiments revealed only slight differences between these encodings, with the  #AUTHOR_TAG bpe achieving slightly worse end - task performance on some tasks.', 'nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments.', 'a more detailed comparison of these encodings is left to future work']",4
"['original bert  #TAUTHOR_TAG.', '']","['original bert  #TAUTHOR_TAG.', '']","['10 times more data than the original bert  #TAUTHOR_TAG.', '']","['', 'for example, the recently proposed xlnet architecture ) is pretrained using nearly 10 times more data than the original bert  #TAUTHOR_TAG.', 'it is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to bert.', 'to help disentangle the importance of these factors from other modeling choices ( e. g., the pretraining objective ), we begin by training roberta following the bert large architecture ( l = 24, h = 1024, a = 16, 355m parameters ).', 'we pretrain for 100k steps over a comparable book - corpus plus wikipedia dataset as was used in  #TAUTHOR_TAG.', 'we pretrain our model using 1024 v100 gpus for approximately one day.', 'results we present our results in table 4.', 'when controlling for training data, we observe that roberta provides a large improvement over the originally reported bert large results, reaffirming the importance of the design choices we explored in section 4.', 'next, we combine this data with the three additional datasets described in section 3. 2.', 'we train roberta over the combined data with the same number of training steps as before ( 100k ).', 'in total, we pretrain over 160gb of text.', 'we observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.', '']",4
"['as positive  #AUTHOR_TAG b, a ;  #AUTHOR_TAG.', 'this formulation significantly simplifies the task, but is not directly comparable to bert  #TAUTHOR_TAG.', 'following recent']","['as positive  #AUTHOR_TAG b, a ;  #AUTHOR_TAG.', 'this formulation significantly simplifies the task, but is not directly comparable to bert  #TAUTHOR_TAG.', 'following recent work,']","['a single ( question, candidate ) pair is classified as positive  #AUTHOR_TAG b, a ;  #AUTHOR_TAG.', 'this formulation significantly simplifies the task, but is not directly comparable to bert  #TAUTHOR_TAG.', 'following recent work, we adopt the ranking approach for our test submission, but for direct comparison with bert we report development set results based on a pure classification approach.', 'wn']","['', 'task - specific modifications two of the glue tasks require task - specific finetuning approaches to achieve competitive leaderboard results.', 'qnli : recent submissions on the glue leaderboard adopt a pairwise ranking formulation for the qnli task, in which candidate answers are mined from the training set and compared to one another, and a single ( question, candidate ) pair is classified as positive  #AUTHOR_TAG b, a ;  #AUTHOR_TAG.', 'this formulation significantly simplifies the task, but is not directly comparable to bert  #TAUTHOR_TAG.', 'following recent work, we adopt the ranking approach for our test submission, but for direct comparison with bert we report development set results based on a pure classification approach.', 'wnli : we found the provided nli - format data to be challenging to work with.', 'instead we use the reformatted wnli data from super - glue  #AUTHOR_TAG a ), which indicates the span of the query pronoun and referent.', 'we finetune roberta using the margin ranking loss from  #AUTHOR_TAG.', 'for a given input sentence, we use spacy  #AUTHOR_TAG to extract additional candidate noun phrases from the sentence and finetune our model so that it assigns higher scores to positive']",4
"['', 'in particular, while both bert  #TAUTHOR_TAG']","['adopt a much simpler approach for squad compared to past work.', 'in particular, while both bert  #TAUTHOR_TAG']","['', 'in particular, while both bert  #TAUTHOR_TAG']","['adopt a much simpler approach for squad compared to past work.', 'in particular, while both bert  #TAUTHOR_TAG and xlnet augment their training data with additional qa datasets, we only finetune roberta using the provided squad training data.', ' #AUTHOR_TAG also employed a custom layer - wise learning rate schedule to finetune results could potentially be improved by augmenting this with additional pronoun disambiguation datasets.', 'xlnet, while we use the same learning rate for all layers.', '']",4
"['', 'in particular, while both bert  #TAUTHOR_TAG']","['adopt a much simpler approach for squad compared to past work.', 'in particular, while both bert  #TAUTHOR_TAG']","['', 'in particular, while both bert  #TAUTHOR_TAG']","['adopt a much simpler approach for squad compared to past work.', 'in particular, while both bert  #TAUTHOR_TAG and xlnet augment their training data with additional qa datasets, we only finetune roberta using the provided squad training data.', ' #AUTHOR_TAG also employed a custom layer - wise learning rate schedule to finetune results could potentially be improved by augmenting this with additional pronoun disambiguation datasets.', 'xlnet, while we use the same learning rate for all layers.', '']",4
"['we feed a sequence to the model.', 'this becomes crucial when pretraining for more steps or with larger datasets.', 'results table 1 compares the published bert base results from  #TAUTHOR_TAG to']","['we feed a sequence to the model.', 'this becomes crucial when pretraining for more steps or with larger datasets.', 'results table 1 compares the published bert base results from  #TAUTHOR_TAG to']","['we feed a sequence to the model.', 'this becomes crucial when pretraining for more steps or with larger datasets.', 'results table 1 compares the published bert base results from  #TAUTHOR_TAG to']","['discussed in section 2, bert relies on randomly masking and predicting tokens.', 'the original bert implementation performed masking once during data preprocessing, resulting in a single static mask.', 'to avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training.', 'thus, each training sequence was seen with the same mask four times during training.', 'we compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model.', 'this becomes crucial when pretraining for more steps or with larger datasets.', 'results table 1 compares the published bert base results from  #TAUTHOR_TAG to our reimplementation with either static or dynamic masking.', 'we find that our reimplementation with static masking performs similar to the original bert model, and dynamic masking is comparable or slightly better than static masking.', 'given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments']",3
"['original bert  #TAUTHOR_TAG.', '']","['original bert  #TAUTHOR_TAG.', '']","['10 times more data than the original bert  #TAUTHOR_TAG.', '']","['', 'for example, the recently proposed xlnet architecture ) is pretrained using nearly 10 times more data than the original bert  #TAUTHOR_TAG.', 'it is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to bert.', 'to help disentangle the importance of these factors from other modeling choices ( e. g., the pretraining objective ), we begin by training roberta following the bert large architecture ( l = 24, h = 1024, a = 16, 355m parameters ).', 'we pretrain for 100k steps over a comparable book - corpus plus wikipedia dataset as was used in  #TAUTHOR_TAG.', 'we pretrain our model using 1024 v100 gpus for approximately one day.', 'results we present our results in table 4.', 'when controlling for training data, we observe that roberta provides a large improvement over the originally reported bert large results, reaffirming the importance of the design choices we explored in section 4.', 'next, we combine this data with the three additional datasets described in section 3. 2.', 'we train roberta over the combined data with the same number of training steps as before ( 100k ).', 'in total, we pretrain over 160gb of text.', 'we observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.', '']",3
"['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited due to their strength in non - sparse representation learning and non - linear power in feature combination, which have led to advances in many nlp tasks.', 'so far, neural word segmentors have given comparable accuracies to the best statictical models.', 'with respect to non - sparse representation, character embeddings have been exploited as a foundation of neural word segmentors.', 'they serve to reduce sparsity of character ngrams, allowing, for example, "" [UNK] ( cat ) [UNK] ( lie ) [UNK] ( in ) [UNK] [UNK] ( corner ) "" to be connected with "" [UNK] ( dog ) [UNK] ( sit ) [UNK] ( in ) [UNK] * equal contribution.', '[UNK] ( corner ) ""  #AUTHOR_TAG, which is infeasible by using sparse one - hot character features.', 'in addition to character embeddings, distributed representations of character bigrams  #AUTHOR_TAG and words  #TAUTHOR_TAG have also been shown to improve segmentation accuracies.', 'with respect to non - linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi - layer perceptrons on fivecharacter windows  #AUTHOR_TAG a ), as well as lstms on characters  #AUTHOR_TAG b ;  #AUTHOR_TAG and words  #TAUTHOR_TAG.', '']",0
"['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited due to their strength in non - sparse representation learning and non - linear power in feature combination, which have led to advances in many nlp tasks.', 'so far, neural word segmentors have given comparable accuracies to the best statictical models.', 'with respect to non - sparse representation, character embeddings have been exploited as a foundation of neural word segmentors.', 'they serve to reduce sparsity of character ngrams, allowing, for example, "" [UNK] ( cat ) [UNK] ( lie ) [UNK] ( in ) [UNK] [UNK] ( corner ) "" to be connected with "" [UNK] ( dog ) [UNK] ( sit ) [UNK] ( in ) [UNK] * equal contribution.', '[UNK] ( corner ) ""  #AUTHOR_TAG, which is infeasible by using sparse one - hot character features.', 'in addition to character embeddings, distributed representations of character bigrams  #AUTHOR_TAG and words  #TAUTHOR_TAG have also been shown to improve segmentation accuracies.', 'with respect to non - linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi - layer perceptrons on fivecharacter windows  #AUTHOR_TAG a ), as well as lstms on characters  #AUTHOR_TAG b ;  #AUTHOR_TAG and words  #TAUTHOR_TAG.', '']",0
"['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited due to their strength in non - sparse representation learning and non - linear power in feature combination, which have led to advances in many nlp tasks.', 'so far, neural word segmentors have given comparable accuracies to the best statictical models.', 'with respect to non - sparse representation, character embeddings have been exploited as a foundation of neural word segmentors.', 'they serve to reduce sparsity of character ngrams, allowing, for example, "" [UNK] ( cat ) [UNK] ( lie ) [UNK] ( in ) [UNK] [UNK] ( corner ) "" to be connected with "" [UNK] ( dog ) [UNK] ( sit ) [UNK] ( in ) [UNK] * equal contribution.', '[UNK] ( corner ) ""  #AUTHOR_TAG, which is infeasible by using sparse one - hot character features.', 'in addition to character embeddings, distributed representations of character bigrams  #AUTHOR_TAG and words  #TAUTHOR_TAG have also been shown to improve segmentation accuracies.', 'with respect to non - linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi - layer perceptrons on fivecharacter windows  #AUTHOR_TAG a ), as well as lstms on characters  #AUTHOR_TAG b ;  #AUTHOR_TAG and words  #TAUTHOR_TAG.', '']",0
"['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited due to their strength in non - sparse representation learning and non - linear power in feature combination, which have led to advances in many nlp tasks.', 'so far, neural word segmentors have given comparable accuracies to the best statictical models.', 'with respect to non - sparse representation, character embeddings have been exploited as a foundation of neural word segmentors.', 'they serve to reduce sparsity of character ngrams, allowing, for example, "" [UNK] ( cat ) [UNK] ( lie ) [UNK] ( in ) [UNK] [UNK] ( corner ) "" to be connected with "" [UNK] ( dog ) [UNK] ( sit ) [UNK] ( in ) [UNK] * equal contribution.', '[UNK] ( corner ) ""  #AUTHOR_TAG, which is infeasible by using sparse one - hot character features.', 'in addition to character embeddings, distributed representations of character bigrams  #AUTHOR_TAG and words  #TAUTHOR_TAG have also been shown to improve segmentation accuracies.', 'with respect to non - linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi - layer perceptrons on fivecharacter windows  #AUTHOR_TAG a ), as well as lstms on characters  #AUTHOR_TAG b ;  #AUTHOR_TAG and words  #TAUTHOR_TAG.', '']",0
"['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited']","['has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'neural network models have been exploited due to their strength in non - sparse representation learning and non - linear power in feature combination, which have led to advances in many nlp tasks.', 'so far, neural word segmentors have given comparable accuracies to the best statictical models.', 'with respect to non - sparse representation, character embeddings have been exploited as a foundation of neural word segmentors.', 'they serve to reduce sparsity of character ngrams, allowing, for example, "" [UNK] ( cat ) [UNK] ( lie ) [UNK] ( in ) [UNK] [UNK] ( corner ) "" to be connected with "" [UNK] ( dog ) [UNK] ( sit ) [UNK] ( in ) [UNK] * equal contribution.', '[UNK] ( corner ) ""  #AUTHOR_TAG, which is infeasible by using sparse one - hot character features.', 'in addition to character embeddings, distributed representations of character bigrams  #AUTHOR_TAG and words  #TAUTHOR_TAG have also been shown to improve segmentation accuracies.', 'with respect to non - linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi - layer perceptrons on fivecharacter windows  #AUTHOR_TAG a ), as well as lstms on characters  #AUTHOR_TAG b ;  #AUTHOR_TAG and words  #TAUTHOR_TAG.', '']",5
"[' #AUTHOR_TAG b ;  #TAUTHOR_TAG, using']","[' #AUTHOR_TAG b ;  #TAUTHOR_TAG, using']","['the latter, we follow recent work  #AUTHOR_TAG b ;  #TAUTHOR_TAG, using a bidirectional lstm']","['.', 'we investigate two different approaches to encode incoming characters, namely a window approach and an lstm approach.', 'for the former, we follow prior methods  #AUTHOR_TAG, using five - character window [ c −2, c −1, c 0, c 1, c 2 ] to represent incoming characters.', 'shown in figure 3, a multi - layer perceptron ( mlp ) is employed to derive a five - character window vector d c from single - character vector rep -', 'for the latter, we follow recent work  #AUTHOR_TAG b ;  #TAUTHOR_TAG, using a bidirectional lstm to encode input character sequence.', '']",5
"[' #AUTHOR_TAG b ;  #TAUTHOR_TAG, using']","[' #AUTHOR_TAG b ;  #TAUTHOR_TAG, using']","['the latter, we follow recent work  #AUTHOR_TAG b ;  #TAUTHOR_TAG, using a bidirectional lstm']","['.', 'we investigate two different approaches to encode incoming characters, namely a window approach and an lstm approach.', 'for the former, we follow prior methods  #AUTHOR_TAG, using five - character window [ c −2, c −1, c 0, c 1, c 2 ] to represent incoming characters.', 'shown in figure 3, a multi - layer perceptron ( mlp ) is employed to derive a five - character window vector d c from single - character vector rep -', 'for the latter, we follow recent work  #AUTHOR_TAG b ;  #TAUTHOR_TAG, using a bidirectional lstm to encode input character sequence.', '']",5
"['not word embeddings, acccording to  #TAUTHOR_TAG']","['not word embeddings, acccording to  #TAUTHOR_TAG']","['not word embeddings, acccording to  #TAUTHOR_TAG']","['train the main segmentor, we adopt the global transition - based learning and beam - search strategy of  #AUTHOR_TAG.', 'for decoding, standard beam search is used, where the b best partial output hypotheses at each step are maintained in an agenda.', 'initially, the agenda contains only the start state.', 'at each step, all hypotheses in the agenda are expanded, by applying all possible actions and b highest scored resulting hypotheses are used as the agenda for the next step.', 'for training, the same decoding process is applied to each training example ( x i, y i ).', 'at step j, if the gold - standard sequence of transition actions y i j falls out of the agenda, max - margin update is performed by taking the current best hypothesis y j in the beam as a negative example, and y i j as', 'where δ ( y j, y i j ) is the number of incorrect local decisions iny j, and η controls the score margin.', 'the strategy above is early - update  #AUTHOR_TAG.', 'on the other hand, if the goldstandard hypothesis does not fall out of the agenda until the full sentence has been segmented, a final update is made between the highest scored hypothesisy ( non - gold standard ) in the agenda and the gold - standard y i, using exactly the same loss function.', 'pseudocode for the online learning algorithm is shown in algorithm 1.', 'we use adagrad  #AUTHOR_TAG to optimize model parameters, with an initial learning rate α.', 'l2 regularization and dropout  #AUTHOR_TAG on input are used to reduce overfitting, with a l2 weight λ and a dropout rate p. all the parameters in our model are randomly initialized to a value ( −r, r ), where r = 6. 0 f an in + f anout  #AUTHOR_TAG.', 'we fine - tune character and character bigram embeddings, but not word embeddings, acccording to  #TAUTHOR_TAG']",5
"['.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","['reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","[', with a 15. 0 % relative error reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","['', 'this is consistent with the observation of  #AUTHOR_TAG, who show that punctuation is more effective compared with mutual information and access variety as semisupervised data for a statistical word segmentation model.', 'with automatically - segmented data 6, heterogenous segmentation and pos information, the f - score increases to 96. 26 %, 96. 27 % and 96. 22 %, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation  #AUTHOR_TAG.', 'finally, by integrating all above information via multi - task learning, the f - score is further improved to 96. 48 %, with a 15. 0 % relative error reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with respect to sentence length on our baseline model, multitask pretraining model and  #TAUTHOR_TAG']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['on statistical word segmentation dates back to the 1990s  #AUTHOR_TAG.', 'state - of - the - art approaches include character sequence labeling models  #AUTHOR_TAG using crfs ( peng et al., 1 https : / / github. com / sutdnlp / libn3l 2004 ;  #AUTHOR_TAG and max - margin structured models leveraging word features  #AUTHOR_TAG.', 'semisupervised methods have been applied to both character - based and word - based models, exploring external training data for better segmentation  #AUTHOR_TAG.', 'our work belongs to recent neural word segmentation.', 'to our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training.', 'closest in spirit to our work,  #AUTHOR_TAG empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information.', 'their baseline is similar to ours in the sense that both character and word contexts are considered.', 'on the other hand, their model is statistical while ours is neural.', 'consequently, they integrate external knowledge as features, while we integrate it by shared network parameters.', 'our results show a similar degree of error reduction compared to theirs by using external data.', 'our model inherits from previous findings on context representations, such as character windows  #AUTHOR_TAG a ) and lstms  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'similar to  #TAUTHOR_TAG and  #AUTHOR_TAG, we use word context on top of character context.', 'however, words play a relatively less important role in our model, and we find that word lstm, which has been used by all previous neural segmentation work, is unnecessary for our model.', 'our model is conceptually simpler and more modularised compared with figure 1 : overall model.', ' #AUTHOR_TAG b ) and  #AUTHOR_TAG, allowing a central sub module, namely a fivecharacter context window, to be pretrained']",3
"['resulting in the state.', 'similar to  #TAUTHOR_TAG and  #AUTHOR_TAG, our model is a global structural model, using']","['resulting in the state.', 'similar to  #TAUTHOR_TAG and  #AUTHOR_TAG, our model is a global structural model, using']","['in the state.', 'similar to  #TAUTHOR_TAG and  #AUTHOR_TAG, our model is a global structural model, using the overall score']","['', 'in the figure, v denotes the score of a state, given by a neural network model.', 'the score of the initial state ( i. e. axiom ) is 0, and the score of a non - axiom state is the sum of scores of all incremental decisions resulting in the state.', 'similar to  #TAUTHOR_TAG and  #AUTHOR_TAG, our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter - dependent transition actions.', '']",3
"['.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","['reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","[', with a 15. 0 % relative error reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","['', 'this is consistent with the observation of  #AUTHOR_TAG, who show that punctuation is more effective compared with mutual information and access variety as semisupervised data for a statistical word segmentation model.', 'with automatically - segmented data 6, heterogenous segmentation and pos information, the f - score increases to 96. 26 %, 96. 27 % and 96. 22 %, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation  #AUTHOR_TAG.', 'finally, by integrating all above information via multi - task learning, the f - score is further improved to 96. 48 %, with a 15. 0 % relative error reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with respect to sentence length on our baseline model, multitask pretraining model and  #TAUTHOR_TAG']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['on statistical word segmentation dates back to the 1990s  #AUTHOR_TAG.', 'state - of - the - art approaches include character sequence labeling models  #AUTHOR_TAG using crfs ( peng et al., 1 https : / / github. com / sutdnlp / libn3l 2004 ;  #AUTHOR_TAG and max - margin structured models leveraging word features  #AUTHOR_TAG.', 'semisupervised methods have been applied to both character - based and word - based models, exploring external training data for better segmentation  #AUTHOR_TAG.', 'our work belongs to recent neural word segmentation.', 'to our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training.', 'closest in spirit to our work,  #AUTHOR_TAG empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information.', 'their baseline is similar to ours in the sense that both character and word contexts are considered.', 'on the other hand, their model is statistical while ours is neural.', 'consequently, they integrate external knowledge as features, while we integrate it by shared network parameters.', 'our results show a similar degree of error reduction compared to theirs by using external data.', 'our model inherits from previous findings on context representations, such as character windows  #AUTHOR_TAG a ) and lstms  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'similar to  #TAUTHOR_TAG and  #AUTHOR_TAG, we use word context on top of character context.', 'however, words play a relatively less important role in our model, and we find that word lstm, which has been used by all previous neural segmentation work, is unnecessary for our model.', 'our model is conceptually simpler and more modularised compared with figure 1 : overall model.', ' #AUTHOR_TAG b ) and  #AUTHOR_TAG, allowing a central sub module, namely a fivecharacter context window, to be pretrained']",4
"['.', 'this shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word - based segmentors  #TAUTHOR_TAG.', 'this is likely']","['−1 ( 1 - word window ), the f - measure increases to 95. 78 %.', 'this shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word - based segmentors  #TAUTHOR_TAG.', 'this is likely']","['.', 'this shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word - based segmentors  #TAUTHOR_TAG.', 'this is likely']","['', 'word context.', 'the influence of various word contexts are shown in table 5.', 'without using word information, our segmentor gives an f - score of 95. 66 % on the development data.', 'using a context of only w −1 ( 1 - word window ), the f - measure increases to 95. 78 %.', 'this shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word - based segmentors  #TAUTHOR_TAG.', 'this is likely due to the difference in our neural network structures, and that we fine - tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with  #TAUTHOR_TAG.', 'the fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word - based neural segmentors do not outperform the best character - based models by large margins.', 'given that character context is what we pretrain, our model relies more heavily with both w']",4
"['.', 'this shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word - based segmentors  #TAUTHOR_TAG.', 'this is likely']","['−1 ( 1 - word window ), the f - measure increases to 95. 78 %.', 'this shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word - based segmentors  #TAUTHOR_TAG.', 'this is likely']","['.', 'this shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word - based segmentors  #TAUTHOR_TAG.', 'this is likely']","['', 'word context.', 'the influence of various word contexts are shown in table 5.', 'without using word information, our segmentor gives an f - score of 95. 66 % on the development data.', 'using a context of only w −1 ( 1 - word window ), the f - measure increases to 95. 78 %.', 'this shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word - based segmentors  #TAUTHOR_TAG.', 'this is likely due to the difference in our neural network structures, and that we fine - tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with  #TAUTHOR_TAG.', 'the fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word - based neural segmentors do not outperform the best character - based models by large margins.', 'given that character context is what we pretrain, our model relies more heavily with both w']",4
"['.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","['reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","[', with a 15. 0 % relative error reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","['', 'this is consistent with the observation of  #AUTHOR_TAG, who show that punctuation is more effective compared with mutual information and access variety as semisupervised data for a statistical word segmentation model.', 'with automatically - segmented data 6, heterogenous segmentation and pos information, the f - score increases to 96. 26 %, 96. 27 % and 96. 22 %, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation  #AUTHOR_TAG.', 'finally, by integrating all above information via multi - task learning, the f - score is further improved to 96. 48 %, with a 15. 0 % relative error reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with respect to sentence length on our baseline model, multitask pretraining model and  #TAUTHOR_TAG']",4
"['.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","['reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","[', with a 15. 0 % relative error reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with']","['', 'this is consistent with the observation of  #AUTHOR_TAG, who show that punctuation is more effective compared with mutual information and access variety as semisupervised data for a statistical word segmentation model.', 'with automatically - segmented data 6, heterogenous segmentation and pos information, the f - score increases to 96. 26 %, 96. 27 % and 96. 22 %, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation  #AUTHOR_TAG.', 'finally, by integrating all above information via multi - task learning, the f - score is further improved to 96. 48 %, with a 15. 0 % relative error reduction.', ' #AUTHOR_TAG b ) both our model and  #TAUTHOR_TAG use global learning and beam search, but our network is different.', ' #AUTHOR_TAG b ) utilizes the action history with lstm encoder, while we use partial word rather than action information.', 'besides, the character and character bigram embeddings are fine - tuned in our model while  #TAUTHOR_TAG set the embeddings fixed during training.', 'we study the f - measure distribution with respect to sentence length on our baseline model, multitask pretraining model and  #TAUTHOR_TAG']",4
"['the sentence length compared with  #TAUTHOR_TAG.', 'their model is better on very short sentences, but worse on']","['the sentence length compared with  #TAUTHOR_TAG.', 'their model is better on very short sentences, but worse on']","['the sentence length compared with  #TAUTHOR_TAG.', 'their model is better on very short sentences, but worse on all other cases.', 'this shows the relative advantages of our model']","['particular, we cluster the sentences in the development dataset into 6 categories based on their length and evaluate their f1 - values, respectively.', 'as shown in figure 5, the models give different error distributions, with our models being more robust to the sentence length compared with  #TAUTHOR_TAG.', 'their model is better on very short sentences, but worse on all other cases.', 'this shows the relative advantages of our model']",4
['hybrid model of  #TAUTHOR_TAG by 0. 2'],['hybrid model of  #TAUTHOR_TAG by 0. 2'],"['corpora except for msr, where it underperforms the hybrid model of  #TAUTHOR_TAG by 0. 2']","['', ""all methods, as observed by  #AUTHOR_TAG. 8 we notice that both pku dataset and our heterogenous data are based on the news of people's daily. while the heterogen"", '##ous data only collect news from febuary 1998 to june 1998', ', it does not contain the sentences in the dev and test datasets of pku. hong kong corpora, respectively.', 'we map them into simplified chinese before segmentation. the weibo corpus is in a yet different genre, being social media text.  #AUTHOR_TAG achieved the best results on this dataset by using a statistical model with features learned using external', 'lexicons, the ctb7 corpus and the people daily corpus. similar to table 7, our method gives', 'the best accuracies on all corpora except for msr, where it underperforms the hybrid model of  #TAUTHOR_TAG by 0. 2 %. to our', 'knowledge, we are the first to report results for a neural segmentor on more than 3 datasets, with competitive results consistently. it verifies that knowledge learned from a certain set of resources can be used to enhance cross - domain robustness in training a neural segmentor for different datasets, which is of practical importance']",4
['hybrid model of  #TAUTHOR_TAG by 0. 2'],['hybrid model of  #TAUTHOR_TAG by 0. 2'],"['corpora except for msr, where it underperforms the hybrid model of  #TAUTHOR_TAG by 0. 2']","['', ""all methods, as observed by  #AUTHOR_TAG. 8 we notice that both pku dataset and our heterogenous data are based on the news of people's daily. while the heterogen"", '##ous data only collect news from febuary 1998 to june 1998', ', it does not contain the sentences in the dev and test datasets of pku. hong kong corpora, respectively.', 'we map them into simplified chinese before segmentation. the weibo corpus is in a yet different genre, being social media text.  #AUTHOR_TAG achieved the best results on this dataset by using a statistical model with features learned using external', 'lexicons, the ctb7 corpus and the people daily corpus. similar to table 7, our method gives', 'the best accuracies on all corpora except for msr, where it underperforms the hybrid model of  #TAUTHOR_TAG by 0. 2 %. to our', 'knowledge, we are the first to report results for a neural segmentor on more than 3 datasets, with competitive results consistently. it verifies that knowledge learned from a certain set of resources can be used to enhance cross - domain robustness in training a neural segmentor for different datasets, which is of practical importance']",4
['hybrid model of  #TAUTHOR_TAG by 0. 2'],['hybrid model of  #TAUTHOR_TAG by 0. 2'],"['corpora except for msr, where it underperforms the hybrid model of  #TAUTHOR_TAG by 0. 2']","['', ""all methods, as observed by  #AUTHOR_TAG. 8 we notice that both pku dataset and our heterogenous data are based on the news of people's daily. while the heterogen"", '##ous data only collect news from febuary 1998 to june 1998', ', it does not contain the sentences in the dev and test datasets of pku. hong kong corpora, respectively.', 'we map them into simplified chinese before segmentation. the weibo corpus is in a yet different genre, being social media text.  #AUTHOR_TAG achieved the best results on this dataset by using a statistical model with features learned using external', 'lexicons, the ctb7 corpus and the people daily corpus. similar to table 7, our method gives', 'the best accuracies on all corpora except for msr, where it underperforms the hybrid model of  #TAUTHOR_TAG by 0. 2 %. to our', 'knowledge, we are the first to report results for a neural segmentor on more than 3 datasets, with competitive results consistently. it verifies that knowledge learned from a certain set of resources can be used to enhance cross - domain robustness in training a neural segmentor for different datasets, which is of practical importance']",4
"['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['as has been commonly understood, learning local decisions for structured prediction can lead to label bias  #AUTHOR_TAG, which prevents globally optimal structures from receiving optimal', 'scores by the model. we address this potential issue by building a structural neural model for end - to - end relation extraction', ', following a recent line of efforts on globally optimized models for neural structured prediction  #AUTHOR_TAG. in particular, we follow  #AUTHOR_TAG, casting the task as an end - to -', 'end tablefilling problem. this is different from the actionbased method of  #AUTHOR_TAG, yet has shown to be more flexible and accurate  #AUTHOR_TAG. we', 'take a different approach to representation learning, addressing two potential limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction  #AUTHOR_TAG. however', ', parsing errors can lead to encoding inaccuracies of tree - lstms, thereby hurting relation extraction potentially. we take an alternative approach to integrating syntactic information, by taking the hidden lstm layers of a', 'bi - affine attention parser  #AUTHOR_TAG to augment input representations. pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly', 'represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. our method is', 'also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar', ', requiring only hidden representations on word that contain syntactic information. in contrast, the method of  #TAUTHOR_TAG must consider', 'tree lstm formulations that are specific to grammar formalisms, which can be structurally different  #AUTHOR_TAG. second,  #TAUTHOR_TAG did not explicitly learn the representation of segments when predicting', 'entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies  #AUTHOR_TAG. we take the lstm - minus method of  #AUTHOR_TAG, modelling', 'a segment as the difference between its last and first lstm hidden vectors. this method is highly efficient,', 'yet gives as accurate results as compared to more complex neural network structures to model a span of words  #AUTHOR_TAG. evaluation', 'on two benchmark datasets shows that our method outperforms previous methods of  #TAUTHOR_TAG,  #AUTHOR_TAG and', ' #AUTHOR_TAG, giving the best reported results on both benchmarks. detailed analysis shows that our integration of syntactic features is as', 'effective as traditional approaches based on discrete parser outputs. we make our code publicly as shown in figure 1, the goal of', 'relation extraction is to mine relations from raw texts. it consists of two sub - tasks, namely entity detection, which recognizes valid entities, and relation classification', ', which determines the relation categories over entity pairs. we follow recent studies and recognize entities and relations as one single task']",4
"['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['as has been commonly understood, learning local decisions for structured prediction can lead to label bias  #AUTHOR_TAG, which prevents globally optimal structures from receiving optimal', 'scores by the model. we address this potential issue by building a structural neural model for end - to - end relation extraction', ', following a recent line of efforts on globally optimized models for neural structured prediction  #AUTHOR_TAG. in particular, we follow  #AUTHOR_TAG, casting the task as an end - to -', 'end tablefilling problem. this is different from the actionbased method of  #AUTHOR_TAG, yet has shown to be more flexible and accurate  #AUTHOR_TAG. we', 'take a different approach to representation learning, addressing two potential limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction  #AUTHOR_TAG. however', ', parsing errors can lead to encoding inaccuracies of tree - lstms, thereby hurting relation extraction potentially. we take an alternative approach to integrating syntactic information, by taking the hidden lstm layers of a', 'bi - affine attention parser  #AUTHOR_TAG to augment input representations. pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly', 'represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. our method is', 'also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar', ', requiring only hidden representations on word that contain syntactic information. in contrast, the method of  #TAUTHOR_TAG must consider', 'tree lstm formulations that are specific to grammar formalisms, which can be structurally different  #AUTHOR_TAG. second,  #TAUTHOR_TAG did not explicitly learn the representation of segments when predicting', 'entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies  #AUTHOR_TAG. we take the lstm - minus method of  #AUTHOR_TAG, modelling', 'a segment as the difference between its last and first lstm hidden vectors. this method is highly efficient,', 'yet gives as accurate results as compared to more complex neural network structures to model a span of words  #AUTHOR_TAG. evaluation', 'on two benchmark datasets shows that our method outperforms previous methods of  #TAUTHOR_TAG,  #AUTHOR_TAG and', ' #AUTHOR_TAG, giving the best reported results on both benchmarks. detailed analysis shows that our integration of syntactic features is as', 'effective as traditional approaches based on discrete parser outputs. we make our code publicly as shown in figure 1, the goal of', 'relation extraction is to mine relations from raw texts. it consists of two sub - tasks, namely entity detection, which recognizes valid entities, and relation classification', ', which determines the relation categories over entity pairs. we follow recent studies and recognize entities and relations as one single task']",4
"['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['as has been commonly understood, learning local decisions for structured prediction can lead to label bias  #AUTHOR_TAG, which prevents globally optimal structures from receiving optimal', 'scores by the model. we address this potential issue by building a structural neural model for end - to - end relation extraction', ', following a recent line of efforts on globally optimized models for neural structured prediction  #AUTHOR_TAG. in particular, we follow  #AUTHOR_TAG, casting the task as an end - to -', 'end tablefilling problem. this is different from the actionbased method of  #AUTHOR_TAG, yet has shown to be more flexible and accurate  #AUTHOR_TAG. we', 'take a different approach to representation learning, addressing two potential limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction  #AUTHOR_TAG. however', ', parsing errors can lead to encoding inaccuracies of tree - lstms, thereby hurting relation extraction potentially. we take an alternative approach to integrating syntactic information, by taking the hidden lstm layers of a', 'bi - affine attention parser  #AUTHOR_TAG to augment input representations. pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly', 'represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. our method is', 'also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar', ', requiring only hidden representations on word that contain syntactic information. in contrast, the method of  #TAUTHOR_TAG must consider', 'tree lstm formulations that are specific to grammar formalisms, which can be structurally different  #AUTHOR_TAG. second,  #TAUTHOR_TAG did not explicitly learn the representation of segments when predicting', 'entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies  #AUTHOR_TAG. we take the lstm - minus method of  #AUTHOR_TAG, modelling', 'a segment as the difference between its last and first lstm hidden vectors. this method is highly efficient,', 'yet gives as accurate results as compared to more complex neural network structures to model a span of words  #AUTHOR_TAG. evaluation', 'on two benchmark datasets shows that our method outperforms previous methods of  #TAUTHOR_TAG,  #AUTHOR_TAG and', ' #AUTHOR_TAG, giving the best reported results on both benchmarks. detailed analysis shows that our integration of syntactic features is as', 'effective as traditional approaches based on discrete parser outputs. we make our code publicly as shown in figure 1, the goal of', 'relation extraction is to mine relations from raw texts. it consists of two sub - tasks, namely entity detection, which recognizes valid entities, and relation classification', ', which determines the relation categories over entity pairs. we follow recent studies and recognize entities and relations as one single task']",4
"['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG']","['as has been commonly understood, learning local decisions for structured prediction can lead to label bias  #AUTHOR_TAG, which prevents globally optimal structures from receiving optimal', 'scores by the model. we address this potential issue by building a structural neural model for end - to - end relation extraction', ', following a recent line of efforts on globally optimized models for neural structured prediction  #AUTHOR_TAG. in particular, we follow  #AUTHOR_TAG, casting the task as an end - to -', 'end tablefilling problem. this is different from the actionbased method of  #AUTHOR_TAG, yet has shown to be more flexible and accurate  #AUTHOR_TAG. we', 'take a different approach to representation learning, addressing two potential limitations of  #TAUTHOR_TAG.', 'first,  #TAUTHOR_TAG rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction  #AUTHOR_TAG. however', ', parsing errors can lead to encoding inaccuracies of tree - lstms, thereby hurting relation extraction potentially. we take an alternative approach to integrating syntactic information, by taking the hidden lstm layers of a', 'bi - affine attention parser  #AUTHOR_TAG to augment input representations. pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly', 'represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. our method is', 'also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar', ', requiring only hidden representations on word that contain syntactic information. in contrast, the method of  #TAUTHOR_TAG must consider', 'tree lstm formulations that are specific to grammar formalisms, which can be structurally different  #AUTHOR_TAG. second,  #TAUTHOR_TAG did not explicitly learn the representation of segments when predicting', 'entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies  #AUTHOR_TAG. we take the lstm - minus method of  #AUTHOR_TAG, modelling', 'a segment as the difference between its last and first lstm hidden vectors. this method is highly efficient,', 'yet gives as accurate results as compared to more complex neural network structures to model a span of words  #AUTHOR_TAG. evaluation', 'on two benchmark datasets shows that our method outperforms previous methods of  #TAUTHOR_TAG,  #AUTHOR_TAG and', ' #AUTHOR_TAG, giving the best reported results on both benchmarks. detailed analysis shows that our integration of syntactic features is as', 'effective as traditional approaches based on discrete parser outputs. we make our code publicly as shown in figure 1, the goal of', 'relation extraction is to mine relations from raw texts. it consists of two sub - tasks, namely entity detection, which recognizes valid entities, and relation classification', ', which determines the relation categories over entity pairs. we follow recent studies and recognize entities and relations as one single task']",4
"['follow  #TAUTHOR_TAG, learning global context representations using lstms']","['follow  #TAUTHOR_TAG, learning global context representations using lstms.', 'three basic lstm structures are used :']","['follow  #TAUTHOR_TAG, learning global context representations using lstms']","['follow  #TAUTHOR_TAG, learning global context representations using lstms.', '']",4
"['models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures']","['models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in particular, we take']","[', the shortest dependency path has been used by several relation extraction models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in']","['work has shown that syntactic features are useful for relation extraction  #AUTHOR_TAG.', 'for example, the shortest dependency path has been used by several relation extraction models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in particular, we take state - of - the - art syntactic parsers that use encoder - decoder neural models  #AUTHOR_TAG, where the encoder represents the syntactic features of the input sentences.', 'for example, lstm hidden states over the input word / tag sequences has been used frequently as syntactic features  #AUTHOR_TAG.', 'such features represent input words with syntactic information.', '']",4
"['models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures']","['models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in particular, we take']","[', the shortest dependency path has been used by several relation extraction models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in']","['work has shown that syntactic features are useful for relation extraction  #AUTHOR_TAG.', 'for example, the shortest dependency path has been used by several relation extraction models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in particular, we take state - of - the - art syntactic parsers that use encoder - decoder neural models  #AUTHOR_TAG, where the encoder represents the syntactic features of the input sentences.', 'for example, lstm hidden states over the input word / tag sequences has been used frequently as syntactic features  #AUTHOR_TAG.', 'such features represent input words with syntactic information.', '']",4
"['models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures']","['models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in particular, we take']","[', the shortest dependency path has been used by several relation extraction models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in']","['work has shown that syntactic features are useful for relation extraction  #AUTHOR_TAG.', 'for example, the shortest dependency path has been used by several relation extraction models  #TAUTHOR_TAG.', 'here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.', 'in particular, we take state - of - the - art syntactic parsers that use encoder - decoder neural models  #AUTHOR_TAG, where the encoder represents the syntactic features of the input sentences.', 'for example, lstm hidden states over the input word / tag sequences has been used frequently as syntactic features  #AUTHOR_TAG.', 'such features represent input words with syntactic information.', '']",4
['methods based on syntactic outputs which  #TAUTHOR_TAG'],['methods based on syntactic outputs which  #TAUTHOR_TAG'],['feature integration method with the traditional methods based on syntactic outputs which  #TAUTHOR_TAG'],"['examine the effectiveness of the proposed implicit syntactic features.', 'table 5 shows the development results using both local and global optimization.', 'the proposed features improve the relation performances significantly under both settings ( p < 10 −4 ), demonstrating that our use of syntactic features is highly effective.', 'we also compare our feature integration method with the traditional methods based on syntactic outputs which  #TAUTHOR_TAG and all previous methods use.', 'we use the same parser of  #AUTHOR_TAG, building features on its dependency outputs.', '']",4
"['classification using a single incremental model, which is similar in spirit to  #TAUTHOR_TAG by performing the task end - to - end']","['classification using a single incremental model, which is similar in spirit to  #TAUTHOR_TAG by performing the task end - to - end.', 'formally, given']","['and relation classification using a single incremental model, which is similar in spirit to  #TAUTHOR_TAG by performing the task end - to - end.', 'formally, given']","['follow  #AUTHOR_TAG and, treating relation extraction as a tablefilling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to  #TAUTHOR_TAG by performing the task end - to - end.', 'formally, given a sentence w 1 w 2 · · · w n, we maintain a table t n×n, where t ( i, j ) denotes the relation between w i and w j.', 'when i = j, t ( i, j ) denotes an entity boundary label.', 'we map entity words into labels under the bilou ( begin, inside, last, outside, unit ) scheme, assuming that there are no overlapping entities in one sentence  #TAUTHOR_TAG.', '']",3
"['classification using a single incremental model, which is similar in spirit to  #TAUTHOR_TAG by performing the task end - to - end']","['classification using a single incremental model, which is similar in spirit to  #TAUTHOR_TAG by performing the task end - to - end.', 'formally, given']","['and relation classification using a single incremental model, which is similar in spirit to  #TAUTHOR_TAG by performing the task end - to - end.', 'formally, given']","['follow  #AUTHOR_TAG and, treating relation extraction as a tablefilling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to  #TAUTHOR_TAG by performing the task end - to - end.', 'formally, given a sentence w 1 w 2 · · · w n, we maintain a table t n×n, where t ( i, j ) denotes the relation between w i and w j.', 'when i = j, t ( i, j ) denotes an entity boundary label.', 'we map entity words into labels under the bilou ( begin, inside, last, outside, unit ) scheme, assuming that there are no overlapping entities in one sentence  #TAUTHOR_TAG.', '']",5
"['i−1.', 'following  #TAUTHOR_TAG, we use a neural']","['i−1.', 'following  #TAUTHOR_TAG, we use a neural']","['the ith step, we determine the label l i of the next table slot based on the current hypothesis t i−1.', 'following  #TAUTHOR_TAG, we use a neural network to']","['the ith step, we determine the label l i of the next table slot based on the current hypothesis t i−1.', 'following  #TAUTHOR_TAG, we use a neural network to learn the vector representation of t i−1, and then use equation 1 to rank candidate next labels.', 'there are two types of input features, including the word sequence w 1 w 2 · · · w n, and the readily filled label sequence l 1 l 2 · · · l i−1.', 'we build a neural network to represent t i−1']",5
"['follow  #TAUTHOR_TAG, learning global context representations using lstms']","['follow  #TAUTHOR_TAG, learning global context representations using lstms.', 'three basic lstm structures are used :']","['follow  #TAUTHOR_TAG, learning global context representations using lstms']","['follow  #TAUTHOR_TAG, learning global context representations using lstms.', '']",5
"['dataset, we follow  #AUTHOR_TAG and  #TAUTHOR_TAG, splitting']","['ace05 dataset, we follow  #AUTHOR_TAG and  #TAUTHOR_TAG, splitting']","['five relation categories.', 'for the ace05 dataset, we follow  #AUTHOR_TAG and  #TAUTHOR_TAG, splitting']","['evaluate the proposed model on two datasets, namely the ace05 data and the corpus of  #AUTHOR_TAG ( conll04 ), respectively.', 'the ace05 dataset defines seven coarse - grained entity types and six coarse - grained relation categories, while the conll04 dataset defines four entity types and five relation categories.', 'for the ace05 dataset, we follow  #AUTHOR_TAG and  #TAUTHOR_TAG, splitting and preprocessing the dataset into training, development and test sets.', '5 for the conll04 dataset, we follow  #AUTHOR_TAG to split the data into training and test corpora, and then divide 10 % of the training corpus for development.', 'we use the micro f1 - measure as the major metric to evaluate model performances, treating an entity as correct when its head region and type are both correct, 6 and regard a relation as correct when the argument entities and the relation category are all correct.', 'we exploit pairwise t - test for measuring significance values']",5
"['local model, we follow  #TAUTHOR_TAG, training parameters only']","['local model, we follow  #TAUTHOR_TAG, training parameters only']","['better model parameters.', 'for the local model, we follow  #TAUTHOR_TAG, training parameters only']","['update all model parameters by back propagation using adam  #AUTHOR_TAG with a learning rate 10 −3, using gradient clipping by a max norm 10 and l 2 - regularization by a parameter 10 −5.', 'the dimension sizes of various vectors in neural network structure are shown in table 2.', 'all the hyper - parameters are tuned by development experiments.', 'all experiments are conducted using gcc version 4. 9. 4 ( ubuntu 4. 9. 4 - 2ubuntu1 14. 04. 1 ), on an intel ( r ) xeon ( r ) cpu e5 - 2670 @ 2. 60ghz.', 'online training is used to learn parameters, traversing over the entire training examples by 300 iterations.', 'we select the best iteration number according to the development results.', 'in particular, we exploit pre - training techniques  #AUTHOR_TAG to learn better model parameters.', 'for the local model, we follow  #TAUTHOR_TAG, training parameters only for entity detection during the first 20 iterations.', 'for the global model, we pretrain our model using local optimization for 40 iterations, before conducting beam global optimization']",5
"['consider the baseline system with no syntactic features using local training.', 'compared with  #TAUTHOR_TAG features']","['consider the baseline system with no syntactic features using local training.', 'compared with  #TAUTHOR_TAG features']","['consider the baseline system with no syntactic features using local training.', 'compared with  #TAUTHOR_TAG features']","['consider the baseline system with no syntactic features using local training.', 'compared with  #TAUTHOR_TAG features for entity detection.', 'feature ablation experiments are conducted for the two types of features.', 'table 3 shows the experimental results, which demonstrate that the character - level features and the segment features we use are both useful for relation extraction']",5
['methods based on syntactic outputs which  #TAUTHOR_TAG'],['methods based on syntactic outputs which  #TAUTHOR_TAG'],['feature integration method with the traditional methods based on syntactic outputs which  #TAUTHOR_TAG'],"['examine the effectiveness of the proposed implicit syntactic features.', 'table 5 shows the development results using both local and global optimization.', 'the proposed features improve the relation performances significantly under both settings ( p < 10 −4 ), demonstrating that our use of syntactic features is highly effective.', 'we also compare our feature integration method with the traditional methods based on syntactic outputs which  #TAUTHOR_TAG and all previous methods use.', 'we use the same parser of  #AUTHOR_TAG, building features on its dependency outputs.', '']",5
"['to entity distances.', ' #TAUTHOR_TAG exploit the shortest dependency path, which']","['to entity distances.', ' #TAUTHOR_TAG exploit the shortest dependency path, which']","['to entity distances.', ' #TAUTHOR_TAG exploit the shortest dependency path, which']","['', 'to understand the effectiveness of the proposed syntactic features, we examine the relation fscores with respect to entity distances.', ' #TAUTHOR_TAG exploit the shortest dependency path, which can make the distance between two entities closer compared with their sequential dis - tance, thus facilitating relation extraction.', 'we verify whether the proposed syntactic features can benefit our model similarly.', 'as shown in figure 7, the f - scores of entity - pairs with large distances see apparent improvements, demonstrating that our use of syntactic features has a similar effect compared to the shortest dependency path']",5
"['tasks  #TAUTHOR_TAG ;, and we follow this']","['tasks  #TAUTHOR_TAG ;, and we follow this']","[' #TAUTHOR_TAG ;, and we follow this line of']","['recognition  #AUTHOR_TAG ( florian et al.,, 2006  #AUTHOR_TAG and relation extraction  #AUTHOR_TAG have received much attention in the nlp community.', 'the dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given  #AUTHOR_TAG.', 'several studies find that extracting entities and relations jointly can benefit both tasks.', 'early work conducts joint inference for separate models  #AUTHOR_TAG 2007 ).', 'recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks  #TAUTHOR_TAG ;, and we follow this line of work in the study.', 'lstm features have been extensively exploited for nlp tasks, including tagging  #AUTHOR_TAG, parsing  #AUTHOR_TAG, relation classification  #TAUTHOR_TAG and sentiment analysis.', '']",5
"['w.', 'the above two components have also been used by  #TAUTHOR_TAG.', 'we further enhance']","['w.', 'the above two components have also been used by  #TAUTHOR_TAG.', 'we further enhance']","['t similar to e w.', 'the above two components have also been used by  #TAUTHOR_TAG.', 'we further enhance']","['', 'the above two components have also been used by  #TAUTHOR_TAG.', 'we further enhance the word representation by using its character sequence  #AUTHOR_TAG, taking a convolution neural network ( cnn ) to derive a character - based word representation h char, which has been demonstrated effective for several nlp tasks ( dos  #AUTHOR_TAG.', 'we obtain the final h w i based on a non - linear feedforward layer on e w ⊕ e w ⊕ e t ⊕ h char, where ⊕ denotes concatenation']",6
"[' #TAUTHOR_TAG.', 'while multimodal']","[' #TAUTHOR_TAG.', 'while multimodal']","['human performance  #TAUTHOR_TAG.', 'while multimodal approaches to']","['is an important type of information conveyed in human language.', 'previous sentiment analysis studies in the field of nlp have mostly been focused on the verbal modality ( i. e., text ).', 'for example, predicting the sentiment of twitter texts  #AUTHOR_TAG or news articles  #AUTHOR_TAG.', 'however, human language is multimodal in, for instance, face - toface communication and online multimedia opinion sharing.', 'understanding natural language used in such scenarios is especially important for nlp applications in human - computer / robot interaction.', 'thus, in recent years there has been growing interest in multimodal sentiment analysis.', 'the three most widely studied modalities in current multimodal sentiment analysis research are : vocal ( e. g., speech acoustics ), visual ( e. g., facial expressions ), and verbal ( e. g., lexical content ).', 'these are sometimes referred to as "" the three vs "" of communication  #AUTHOR_TAG.', 'multimodal sentiment analysis research focuses on understanding how an individual modality conveys sentiment information ( intra - modal dynamics ), and how they interact with each other ( intermodal dynamics ).', 'it is a challenging research area and state - of - the - art performance of automatic sentiment prediction has room for improvement compared to human performance  #TAUTHOR_TAG.', 'while multimodal approaches to sentiment analysis are relatively new in nlp, multimodal emotion recognition has long been a focus of affective computing.', 'for example,  #AUTHOR_TAG combined facial expressions and speech acoustics to predict the big - 6 emotion categories  #AUTHOR_TAG.', 'emotions and sentiments are closely related concepts in psychology and cognitive science research, and are often used interchangeably.', ' #AUTHOR_TAG identified the main differences between sentiments and emotions to be that sentiments are more stable and dispositional than emotions, and sentiments are formed and directed toward a specific object.', 'however, when adopting the cognitive definition of emotions which connects emotions to stimuli in the environment  #AUTHOR_TAG, the boundary between emotions and sentiments blurs.', 'in particular, the circumplex model of emotions proposed by  #AUTHOR_TAG describes emotions with two dimensions : arousal which represents the level of excitement ( active / inactive ), and valence which represents the level of liking ( positive / negative ).', 'in many sentiment analysis studies, sentiments are defined using likert scales with varying numbers of steps.', 'for example, the stanford sentiment treebank  #AUTHOR_TAG in order to decompose sentiment scores into polarity and intensity and study how they are conveyed through different modalities, we include polarity and / or intensity classification as auxiliary tasks to sentiment score prediction with multi - task learning.', 'one problem with machine learning approaches for affective computing is model robustness.', 'in multi - task']",0
"['. g.,  #TAUTHOR_TAG the verbal modality was shown to be the most effective for unimodal']","['top in hf fusion.', 'this is because in previous studies ( e. g.,  #TAUTHOR_TAG the verbal modality was shown to be the most effective for unimodal']","['. g.,  #TAUTHOR_TAG the verbal modality was shown to be the most effective for unim']","['', 'in the hf model, unimodal information is used in a hierarchy where the top layer of the lower unimodal model is concatenated with the input layer of the higher unimodal model, as shown in figure 6.', 'we use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in hf fusion.', 'this is because in previous studies ( e. g.,  #TAUTHOR_TAG the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective.', '']",0
['multimodal data software development kit ( sdk )  #TAUTHOR_TAG to load and pre - process the cm'],"['with a learning rate of 0. 0005.', 'we use the cmu multimodal data software development kit ( sdk )  #TAUTHOR_TAG to load and pre - process the cmu - mosi database, which splits the 2199 opinion segments into training (']","['intensity classification uses categorical crossentropy as the loss function.', 'following state - ofthe - art on the cmu - mosi database, during training we used adam as the optimization function with a learning rate of 0. 0005.', 'we use the cmu multimodal data software development kit ( sdk )  #TAUTHOR_TAG to load and pre - process the cmu - mosi database, which splits the 2199 opinion segments into training (']","['', 'the main task uses mean absolute error as the loss function, while polarity classification uses binary cross - entropy as the loss function, and intensity classification uses categorical crossentropy as the loss function.', 'following state - ofthe - art on the cmu - mosi database, during training we used adam as the optimization function with a learning rate of 0. 0005.', 'we use the cmu multimodal data software development kit ( sdk )  #TAUTHOR_TAG to load and pre - process the cmu - mosi database, which splits the 2199 opinion segments into training (']",5
"[' #TAUTHOR_TAG, we tables 2 and 3, the numbers in bold are the best']","[' #TAUTHOR_TAG, we tables 2 and 3, the numbers in bold are the best']","['of sentiment score prediction, following previous work  #TAUTHOR_TAG, we tables 2 and 3, the numbers in bold are the best performance for each modality or fusion strategy.', 'to identify the significant differences in results, we perform a two - sample wilcoxon test on the sentiment score predictions']","['we report our sentiment score prediction experiments.', '2 in tables 2 and 3, "" s "" is the singletask learning model ; "" s + p "" is the bi - task learning model with polarity classification as the auxillary task ; "" s + i "" is the bi - task learning model with intensity classification as the auxillary task ; "" s + p + i "" is the tri - task learning model.', 'to evaluate the performance of sentiment score prediction, following previous work  #TAUTHOR_TAG, we tables 2 and 3, the numbers in bold are the best performance for each modality or fusion strategy.', 'to identify the significant differences in results, we perform a two - sample wilcoxon test on the sentiment score predictions given by each pair of models being compared and consider p < 0. 05 as significant.', 'we also include random prediction as a baseline and the human performance reported by']",5
"['. g.,  #TAUTHOR_TAG the verbal modality was shown to be the most effective for unimodal']","['top in hf fusion.', 'this is because in previous studies ( e. g.,  #TAUTHOR_TAG the verbal modality was shown to be the most effective for unimodal']","['. g.,  #TAUTHOR_TAG the verbal modality was shown to be the most effective for unim']","['', 'in the hf model, unimodal information is used in a hierarchy where the top layer of the lower unimodal model is concatenated with the input layer of the higher unimodal model, as shown in figure 6.', 'we use the vocal modality at the bottom of the hierarchy while using the verbal modality at the top in hf fusion.', 'this is because in previous studies ( e. g.,  #TAUTHOR_TAG the verbal modality was shown to be the most effective for unimodal sentiment analysis, while the vocal modality was shown to be the least effective.', '']",3
"['studies on multiple databases ( e. g.,  #TAUTHOR_TAG.', 'this suggests that lexical information remains the most effective for sentiment analysis.', '']","['studies on multiple databases ( e. g.,  #TAUTHOR_TAG.', 'this suggests that lexical information remains the most effective for sentiment analysis.', '']","['results of unimodal sentiment prediction experiments are shown in table 2.', '3 the verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases ( e. g.,  #TAUTHOR_TAG.', 'this suggests that lexical information remains the most effective for sentiment analysis.', '']","['results of unimodal sentiment prediction experiments are shown in table 2.', '3 the verbal models have the best performance here, which is consistent with previous sentiment analysis studies on multiple databases ( e. g.,  #TAUTHOR_TAG.', 'this suggests that lexical information remains the most effective for sentiment analysis.', 'on each modality, the best performance is achieved by a multi - task learning model.', 'this answers our first research question and suggests that sentiment analysis can benefit from multi - task learning.', 'in multi - task learning, the main task gains additional information from the auxillary tasks.', 'compared to the s model, the s + p model has increased focus on the polarity of sentiment, while the s + i model has increased focus on the intensity of sentiment.', 'on the verbal modality, the s + p model achieved the best performance, while on the visual modality the s + i model achieved the best performance.', 'this suggests that the verbal modality is weaker at communicating the polarity of sentiment.', 'thus, verbal sentiment analysis benefits more from including additional information on polarity.', 'on the contrary, the visual modality is weaker at communicating the intensity of sentiment.', 'thus, visual sentiment analysis benefits more from including additional information on intensity.', 'for the vocal modality, the s + p + i model achieved the best performance, and the s + p model yielded improved performance over that of the s model.', 'this suggests that the vocal modality is weaker at communicating the polarity of sentiment.', 'thus, addressing our second research question, the results suggest that individual modalities differ when conveying each aspect of sentiment.', 'table 2 : unimodal sentiment analysis results on the cmu - mosi test set.', 'numbers in bold are the best results on each modality']",3
['recognition  #TAUTHOR_TAG have defined'],['automatic humor recognition  #TAUTHOR_TAG have defined'],"['automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so,']","['', 'in this study, we investigated the feasibility of current nlp technologies in building a system which provides expected audience reactions to public speaking.', 'studies on automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so, their classification models categorized a given sentence as a humorous or non - humorous sentence.', 'among the studies on humor classification,  #AUTHOR_TAG and  #TAUTHOR_TAG reported high performance on the task.', 'considering the performance of their systems, it is reasonable to test the applicability of their models to a real application.', 'in this study, we specifically applied a state - of - the - art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters.', 'in our application of the state - of - art system to talks, we could not achieve a comparable performance to the reported performance of the system.', 'we investigated the potential reasons for the performance difference through further analysis.', 'some humor classification studies  #TAUTHOR_TAG have used negative instances from different domains or topics, because non - humorous sentences could not be found or are very challenging to collect in target domains or topics.', 'their studies showed that it was possible to achieve promising performance using data from heterogeneous domains.', 'however, our study showed that humorous sentences which were semantically close to non - humorous sentences were very challenging to distinguish.', 'we first describe previous studies related to our study']",0
['recognition  #TAUTHOR_TAG have defined'],['automatic humor recognition  #TAUTHOR_TAG have defined'],"['automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so,']","['', 'in this study, we investigated the feasibility of current nlp technologies in building a system which provides expected audience reactions to public speaking.', 'studies on automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so, their classification models categorized a given sentence as a humorous or non - humorous sentence.', 'among the studies on humor classification,  #AUTHOR_TAG and  #TAUTHOR_TAG reported high performance on the task.', 'considering the performance of their systems, it is reasonable to test the applicability of their models to a real application.', 'in this study, we specifically applied a state - of - the - art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters.', 'in our application of the state - of - art system to talks, we could not achieve a comparable performance to the reported performance of the system.', 'we investigated the potential reasons for the performance difference through further analysis.', 'some humor classification studies  #TAUTHOR_TAG have used negative instances from different domains or topics, because non - humorous sentences could not be found or are very challenging to collect in target domains or topics.', 'their studies showed that it was possible to achieve promising performance using data from heterogeneous domains.', 'however, our study showed that humorous sentences which were semantically close to non - humorous sentences were very challenging to distinguish.', 'we first describe previous studies related to our study']",0
['recognition  #TAUTHOR_TAG have defined'],['automatic humor recognition  #TAUTHOR_TAG have defined'],"['automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so,']","['', 'in this study, we investigated the feasibility of current nlp technologies in building a system which provides expected audience reactions to public speaking.', 'studies on automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so, their classification models categorized a given sentence as a humorous or non - humorous sentence.', 'among the studies on humor classification,  #AUTHOR_TAG and  #TAUTHOR_TAG reported high performance on the task.', 'considering the performance of their systems, it is reasonable to test the applicability of their models to a real application.', 'in this study, we specifically applied a state - of - the - art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters.', 'in our application of the state - of - art system to talks, we could not achieve a comparable performance to the reported performance of the system.', 'we investigated the potential reasons for the performance difference through further analysis.', 'some humor classification studies  #TAUTHOR_TAG have used negative instances from different domains or topics, because non - humorous sentences could not be found or are very challenging to collect in target domains or topics.', 'their studies showed that it was possible to achieve promising performance using data from heterogeneous domains.', 'however, our study showed that humorous sentences which were semantically close to non - humorous sentences were very challenging to distinguish.', 'we first describe previous studies related to our study']",0
"['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non - humorous.', 'these studies collected textual data which consisted of humorous texts and non - humorous texts and built a classification model using textual features.', 'humorous and non - humorous texts were from different domains across the studies.', 'pun websites, daily joke websites, or tweets were used as sources of humorous texts.', 'resources such as news websites, proverb websites, etc. were used as sources of non - humorous texts.', ' #TAUTHOR_TAG tried to minimize genre differences between humorous and non - humorous texts in order to avoid a chance that a trained model was optimized to distinguish genre differences.', ' #AUTHOR_TAG examined cross - domain application of humor detection systems using twitter data.', ""for example, they trained a model using tweets with'# humor'and'# education'hashtags and evaluated the performance of the model on evaluation data containing tweets with'# humor'and'# politics'hashtags."", 'they also reported promising performance in the cross - domain application.', 'these studies which used data from different domains or topics reported very high performance - around 80 % accuracy.', 'distinct from the other studies,  #AUTHOR_TAG used data from a single domain, the famous tv series, friends.', '']",0
"['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non - humorous.', 'these studies collected textual data which consisted of humorous texts and non - humorous texts and built a classification model using textual features.', 'humorous and non - humorous texts were from different domains across the studies.', 'pun websites, daily joke websites, or tweets were used as sources of humorous texts.', 'resources such as news websites, proverb websites, etc. were used as sources of non - humorous texts.', ' #TAUTHOR_TAG tried to minimize genre differences between humorous and non - humorous texts in order to avoid a chance that a trained model was optimized to distinguish genre differences.', ' #AUTHOR_TAG examined cross - domain application of humor detection systems using twitter data.', ""for example, they trained a model using tweets with'# humor'and'# education'hashtags and evaluated the performance of the model on evaluation data containing tweets with'# humor'and'# politics'hashtags."", 'they also reported promising performance in the cross - domain application.', 'these studies which used data from different domains or topics reported very high performance - around 80 % accuracy.', 'distinct from the other studies,  #AUTHOR_TAG used data from a single domain, the famous tv series, friends.', '']",0
"['collected a corpus of pun of day data 1.', 'the']","['collected a corpus of pun of day data 1.', 'the']","['collected a corpus of pun of day data 1.', 'the data consisted of 2, 423 humorous ( positive ) texts and']","['collected a corpus of pun of day data 1.', 'the data consisted of 2, 423 humorous ( positive ) texts and 2, 403 non - humorous ( negative ) texts.', 'the humorous texts were from the pun of the day website, and the negative texts from ap news2, new york times, yahoo! answers and proverb websites.', 'examples of humorous and non - humorous sentences are given below.', 'humorous the one who invented the door knocker got a no - bell prize.', 'non - humorous the one who discovered / invented it had the last name of fahrenheit.', 'in order to reduce the differences between positive and negative instances in the data,  #TAUTHOR_TAG used two constraints when collecting negative instances.', 'non - humorous texts were required to have lengths between the minimum and maximum lengths of positive instances, in order to be selected as negative instances.', 'in addition, only non - humorous texts which consisted of words found in positive instances were collected']",0
"['collected a corpus of pun of day data 1.', 'the']","['collected a corpus of pun of day data 1.', 'the']","['collected a corpus of pun of day data 1.', 'the data consisted of 2, 423 humorous ( positive ) texts and']","['collected a corpus of pun of day data 1.', 'the data consisted of 2, 423 humorous ( positive ) texts and 2, 403 non - humorous ( negative ) texts.', 'the humorous texts were from the pun of the day website, and the negative texts from ap news2, new york times, yahoo! answers and proverb websites.', 'examples of humorous and non - humorous sentences are given below.', 'humorous the one who invented the door knocker got a no - bell prize.', 'non - humorous the one who discovered / invented it had the last name of fahrenheit.', 'in order to reduce the differences between positive and negative instances in the data,  #TAUTHOR_TAG used two constraints when collecting negative instances.', 'non - humorous texts were required to have lengths between the minimum and maximum lengths of positive instances, in order to be selected as negative instances.', 'in addition, only non - humorous texts which consisted of words found in positive instances were collected']",0
"['in  #TAUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","[""this section, we present expeirments that we ran to determine 1 ) how effective a model trained using'pun of day'data ( pun ) is when applied to ted talk data ( talk ), and 2 ) whether the performance of a model trained using talk data would be similar to the performance reported in  #TAUTHOR_TAG."", 'we reimplemented features developed by  #TAUTHOR_TAG and evaluated those features on talk data.', ""considering the different characteristics of talk data versus pun data, we sought to investigate whether yang's model could achieve the reported performance ( over 85 % accuracy ) on our talk data."", 'the differences were 1 ) humorous sentences in talk data were sentences which induced audience laughters, compared to pun data which used canned textual humor, 2 ) all non - humorous sentences in talk data were also from ted talks, and 3 ) each pair of humorous and non - humorous sentences were semantically close because they were closely placed.', 'these differences made the humor classification task more challenging.', '']",0
['recognition  #TAUTHOR_TAG have defined'],['automatic humor recognition  #TAUTHOR_TAG have defined'],"['automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so,']","['', 'in this study, we investigated the feasibility of current nlp technologies in building a system which provides expected audience reactions to public speaking.', 'studies on automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so, their classification models categorized a given sentence as a humorous or non - humorous sentence.', 'among the studies on humor classification,  #AUTHOR_TAG and  #TAUTHOR_TAG reported high performance on the task.', 'considering the performance of their systems, it is reasonable to test the applicability of their models to a real application.', 'in this study, we specifically applied a state - of - the - art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters.', 'in our application of the state - of - art system to talks, we could not achieve a comparable performance to the reported performance of the system.', 'we investigated the potential reasons for the performance difference through further analysis.', 'some humor classification studies  #TAUTHOR_TAG have used negative instances from different domains or topics, because non - humorous sentences could not be found or are very challenging to collect in target domains or topics.', 'their studies showed that it was possible to achieve promising performance using data from heterogeneous domains.', 'however, our study showed that humorous sentences which were semantically close to non - humorous sentences were very challenging to distinguish.', 'we first describe previous studies related to our study']",1
['recognition  #TAUTHOR_TAG have defined'],['automatic humor recognition  #TAUTHOR_TAG have defined'],"['automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so,']","['', 'in this study, we investigated the feasibility of current nlp technologies in building a system which provides expected audience reactions to public speaking.', 'studies on automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so, their classification models categorized a given sentence as a humorous or non - humorous sentence.', 'among the studies on humor classification,  #AUTHOR_TAG and  #TAUTHOR_TAG reported high performance on the task.', 'considering the performance of their systems, it is reasonable to test the applicability of their models to a real application.', 'in this study, we specifically applied a state - of - the - art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters.', 'in our application of the state - of - art system to talks, we could not achieve a comparable performance to the reported performance of the system.', 'we investigated the potential reasons for the performance difference through further analysis.', 'some humor classification studies  #TAUTHOR_TAG have used negative instances from different domains or topics, because non - humorous sentences could not be found or are very challenging to collect in target domains or topics.', 'their studies showed that it was possible to achieve promising performance using data from heterogeneous domains.', 'however, our study showed that humorous sentences which were semantically close to non - humorous sentences were very challenging to distinguish.', 'we first describe previous studies related to our study']",1
['recognition  #TAUTHOR_TAG have defined'],['automatic humor recognition  #TAUTHOR_TAG have defined'],"['automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so,']","['', 'in this study, we investigated the feasibility of current nlp technologies in building a system which provides expected audience reactions to public speaking.', 'studies on automatic humor recognition  #TAUTHOR_TAG have defined the recognition task as a binary classification task.', 'so, their classification models categorized a given sentence as a humorous or non - humorous sentence.', 'among the studies on humor classification,  #AUTHOR_TAG and  #TAUTHOR_TAG reported high performance on the task.', 'considering the performance of their systems, it is reasonable to test the applicability of their models to a real application.', 'in this study, we specifically applied a state - of - the - art automatic humor recognition model to talks and investigated if the model could be used to provide simulated laughters.', 'in our application of the state - of - art system to talks, we could not achieve a comparable performance to the reported performance of the system.', 'we investigated the potential reasons for the performance difference through further analysis.', 'some humor classification studies  #TAUTHOR_TAG have used negative instances from different domains or topics, because non - humorous sentences could not be found or are very challenging to collect in target domains or topics.', 'their studies showed that it was possible to achieve promising performance using data from heterogeneous domains.', 'however, our study showed that humorous sentences which were semantically close to non - humorous sentences were very challenging to distinguish.', 'we first describe previous studies related to our study']",4
"['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task,']","['studies  #TAUTHOR_TAG dealt with the humor recognition task as a binary classification task, which was to categorize a given text as humorous or non - humorous.', 'these studies collected textual data which consisted of humorous texts and non - humorous texts and built a classification model using textual features.', 'humorous and non - humorous texts were from different domains across the studies.', 'pun websites, daily joke websites, or tweets were used as sources of humorous texts.', 'resources such as news websites, proverb websites, etc. were used as sources of non - humorous texts.', ' #TAUTHOR_TAG tried to minimize genre differences between humorous and non - humorous texts in order to avoid a chance that a trained model was optimized to distinguish genre differences.', ' #AUTHOR_TAG examined cross - domain application of humor detection systems using twitter data.', ""for example, they trained a model using tweets with'# humor'and'# education'hashtags and evaluated the performance of the model on evaluation data containing tweets with'# humor'and'# politics'hashtags."", 'they also reported promising performance in the cross - domain application.', 'these studies which used data from different domains or topics reported very high performance - around 80 % accuracy.', 'distinct from the other studies,  #AUTHOR_TAG used data from a single domain, the famous tv series, friends.', '']",4
"['in  #TAUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","[""this section, we present expeirments that we ran to determine 1 ) how effective a model trained using'pun of day'data ( pun ) is when applied to ted talk data ( talk ), and 2 ) whether the performance of a model trained using talk data would be similar to the performance reported in  #TAUTHOR_TAG."", 'we reimplemented features developed by  #TAUTHOR_TAG and evaluated those features on talk data.', ""considering the different characteristics of talk data versus pun data, we sought to investigate whether yang's model could achieve the reported performance ( over 85 % accuracy ) on our talk data."", 'the differences were 1 ) humorous sentences in talk data were sentences which induced audience laughters, compared to pun data which used canned textual humor, 2 ) all non - humorous sentences in talk data were also from ted talks, and 3 ) each pair of humorous and non - humorous sentences were semantically close because they were closely placed.', 'these differences made the humor classification task more challenging.', '']",4
"['in  #TAUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","[""this section, we present expeirments that we ran to determine 1 ) how effective a model trained using'pun of day'data ( pun ) is when applied to ted talk data ( talk ), and 2 ) whether the performance of a model trained using talk data would be similar to the performance reported in  #TAUTHOR_TAG."", 'we reimplemented features developed by  #TAUTHOR_TAG and evaluated those features on talk data.', ""considering the different characteristics of talk data versus pun data, we sought to investigate whether yang's model could achieve the reported performance ( over 85 % accuracy ) on our talk data."", 'the differences were 1 ) humorous sentences in talk data were sentences which induced audience laughters, compared to pun data which used canned textual humor, 2 ) all non - humorous sentences in talk data were also from ted talks, and 3 ) each pair of humorous and non - humorous sentences were semantically close because they were closely placed.', 'these differences made the humor classification task more challenging.', '']",4
['t able to get results comparable to  #TAUTHOR_TAG when talk data was used in both train'],"[""the experiments described in the preceding section, we weren't able to get results comparable to  #TAUTHOR_TAG when talk data was used in both train""]","[""the experiments described in the preceding section, we weren't able to get results comparable to  #TAUTHOR_TAG when talk data was used in both train""]","[""the experiments described in the preceding section, we weren't able to get results comparable to  #TAUTHOR_TAG when talk data was used in both train and evaluation data."", 'the results of our experiments raised questions about why two different results were observed for two different data sets.', 'a major difference in the two data sets was the source of negative instances.', ' #TAUTHOR_TAG borrowed negative instances from different genres such as news websites and proverbs. but, in talk - to - talk, both positive and negative instances were from the same genre.', 'furthermore, each humorous instance had a corresponding non - humorous instance from the same talk.', 'in this section, we investigate the impact of genre differences in the humor classification task, using pun and talk data.', 'the positive instances ( humorous sentences ) in the talk data may be substantially different from the ones found in pun 8.', ""humorous sentences in the pun data set are'self - contained '."", 'it means that the point of humor can be understood within a single sentence.', ""on the other hand, the humorous sentences in the talk data set may be'discourse - based ', which means that the source of humor in target sentences might be understood in the wider context of the speaker's performance."", '']",4
"['experimental setup as  #AUTHOR_TAG and', ' #TAUTHOR_TAG ( 50']","['experimental setup as  #AUTHOR_TAG and', ' #TAUTHOR_TAG ( 50 % positive and 50']","['experimental setup as  #AUTHOR_TAG and', ' #TAUTHOR_TAG ( 50 % positive and 50 % negative instances ), we selected 4', ', 726 sentences from among all collected nonhum']","['', '. the number of humorous sentences left after removing sentences with fewer than seven words', 'was 4, 726. utilizing the same experimental setup as  #AUTHOR_TAG and', ' #TAUTHOR_TAG ( 50 % positive and 50 % negative instances ), we selected 4', ', 726 sentences from among all collected nonhumorous sentences as negative instances. during selection, we minimized differences between positive and negative instances. a negative instance was selected from', 'among sentences located close to a positive instance in a talk. we made a candidate set of non -', 'humorous sentences using sentences within a window size of seven (', 'e. g. from sent - 7 to sent - 1 and from sent + 1 to sent + 7 in the following ) : sent - 7..', '.... sent - 1 and she reminded me that my favorite color was blue. humorous the fact', ""that my favorite color now is blue, but i'm still gay is evidence of both my mother's influence and its limits. sent + 1 when i"", 'was little, my mother used to say,...... sent + 7... among the candidates', ', sentences which consisted of less than seven words were removed and', 'a negative instance was randomly selected among the remaining ones']",5
"['from  #TAUTHOR_TAG, which we implemented, consisted of ( 1']","['from  #TAUTHOR_TAG, which we implemented, consisted of ( 1 ) two incongruity features,']","['from  #TAUTHOR_TAG, which we implemented, consisted of ( 1 ) two incongruity features, (']","['from  #TAUTHOR_TAG, which we implemented, consisted of ( 1 ) two incongruity features, ( 2 ) six ambiguity features, ( 3 ) four interpersonal effect features, ( 4 ) four phonetic features, ( 5 ) five k - nearest neighbor features, and ( 6 ) 300 word2vec features.', 'the total number of features used in this study was 321.', 'we describe our implementation of the features in this section.', 'the justifications for the features can be found in the original paper.', 'incongruity features : the existence of incongruous or incompatible words in a text can cause laughters ( e. g. a clean desk is a sign of a cluttered desk drawer.', ' #AUTHOR_TAG ).', 'we calculated meaning distances of all word pairs in a sentence using a word2vec implementation in python 5.', 'the maximum and minimum meaning distances among the calculated distances in a sentence were used as two incongruity features.', 'ambiguity features : the use of ambiguous words in a sentence can also trigger humorous effects ( i. e. a political prisoner is one who stands behind her convictions.', ' #AUTHOR_TAG ).', 'we calculated sense combinations of nouns, verbs, adjectives and adverbs.', '']",5
"['in  #TAUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","[""this section, we present expeirments that we ran to determine 1 ) how effective a model trained using'pun of day'data ( pun ) is when applied to ted talk data ( talk ), and 2 ) whether the performance of a model trained using talk data would be similar to the performance reported in  #TAUTHOR_TAG."", 'we reimplemented features developed by  #TAUTHOR_TAG and evaluated those features on talk data.', ""considering the different characteristics of talk data versus pun data, we sought to investigate whether yang's model could achieve the reported performance ( over 85 % accuracy ) on our talk data."", 'the differences were 1 ) humorous sentences in talk data were sentences which induced audience laughters, compared to pun data which used canned textual humor, 2 ) all non - humorous sentences in talk data were also from ted talks, and 3 ) each pair of humorous and non - humorous sentences were semantically close because they were closely placed.', 'these differences made the humor classification task more challenging.', '']",5
"['in  #TAUTHOR_TAG.', 'we']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","['in  #TAUTHOR_TAG.', 'we reimplemented features developed by  #TAUTHOR_TAG']","[""this section, we present expeirments that we ran to determine 1 ) how effective a model trained using'pun of day'data ( pun ) is when applied to ted talk data ( talk ), and 2 ) whether the performance of a model trained using talk data would be similar to the performance reported in  #TAUTHOR_TAG."", 'we reimplemented features developed by  #TAUTHOR_TAG and evaluated those features on talk data.', ""considering the different characteristics of talk data versus pun data, we sought to investigate whether yang's model could achieve the reported performance ( over 85 % accuracy ) on our talk data."", 'the differences were 1 ) humorous sentences in talk data were sentences which induced audience laughters, compared to pun data which used canned textual humor, 2 ) all non - humorous sentences in talk data were also from ted talks, and 3 ) each pair of humorous and non - humorous sentences were semantically close because they were closely placed.', 'these differences made the humor classification task more challenging.', '']",6
['t able to get results comparable to  #TAUTHOR_TAG when talk data was used in both train'],"[""the experiments described in the preceding section, we weren't able to get results comparable to  #TAUTHOR_TAG when talk data was used in both train""]","[""the experiments described in the preceding section, we weren't able to get results comparable to  #TAUTHOR_TAG when talk data was used in both train""]","[""the experiments described in the preceding section, we weren't able to get results comparable to  #TAUTHOR_TAG when talk data was used in both train and evaluation data."", 'the results of our experiments raised questions about why two different results were observed for two different data sets.', 'a major difference in the two data sets was the source of negative instances.', ' #TAUTHOR_TAG borrowed negative instances from different genres such as news websites and proverbs. but, in talk - to - talk, both positive and negative instances were from the same genre.', 'furthermore, each humorous instance had a corresponding non - humorous instance from the same talk.', 'in this section, we investigate the impact of genre differences in the humor classification task, using pun and talk data.', 'the positive instances ( humorous sentences ) in the talk data may be substantially different from the ones found in pun 8.', ""humorous sentences in the pun data set are'self - contained '."", 'it means that the point of humor can be understood within a single sentence.', ""on the other hand, the humorous sentences in the talk data set may be'discourse - based ', which means that the source of humor in target sentences might be understood in the wider context of the speaker's performance."", '']",7
[')  #TAUTHOR_TAG for'],['of structural correspondence learning ( scl )  #TAUTHOR_TAG for'],[')  #TAUTHOR_TAG for domain adaptation'],"['paper presents an application of structural correspondence learning ( scl )  #TAUTHOR_TAG for domain adaptation of a stochastic attribute - value grammar ( savg ).', 'so far, scl has been applied successfully in nlp for part - of - speech tagging and sentiment analysis  #TAUTHOR_TAG ; ).', 'an attempt was made in the conll 2007 shared task to apply scl to non - projective dependency parsing  #AUTHOR_TAG, however, without any clear conclusions.', 'we report on our exploration of applying scl to adapt a syntactic disambiguation model and show promising initial results on wikipedia domains']",5
"[' #TAUTHOR_TAG ;.', 'of these,']","[' #TAUTHOR_TAG ;.', 'of these, mc  #AUTHOR_TAG deal specifically with selftraining']","[' #TAUTHOR_TAG ;.', 'of these,']","['several authors have looked at the supervised adaptation case, there are less ( and especially less successful )', 'studies on semi - supervised domain adaptation  #TAUTHOR_TAG ;.', 'of these, mc  #AUTHOR_TAG deal specifically with selftraining for data - driven statistical parsing.', 'they show that together with a re - ranker, improvements are obtained.', '']",5
"[')  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing the algorithm in detail, let us illustrate the intuition behind sc']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing the algorithm in detail, let us illustrate the intuition behind scl with an example, borrowed from.', '']",5
"['were used  #TAUTHOR_TAG,']","['were used  #TAUTHOR_TAG, e. g.']","['were used  #TAUTHOR_TAG,']","['property of the pivot predictors is that they can be trained from unlabeled data, as they represent properties of the input.', 'so far, pivot features on the word level were used  #TAUTHOR_TAG, e. g. "" does the bigram not buy occur in this document? ""  #AUTHOR_TAG.', 'pivot features are the key ingredient for scl, and they should align well with the nlp task.', 'for pos tagging and sentiment analysis, features on the word level are intuitively well - related to the problem at hand.', 'for the task of parse disambiguation based on a conditional model this is not the case.', 'hence, we actually introduce an additional and new layer of abstraction, which, we hypothesize, aligns well with the task of parse disambiguation : we first parse the unlabeled data.', 'in this way we obtain full parses for given sentences as produced by the grammar, allowing access to more abstract representations of the underlying pivot predictor training data ( for reasons of efficiency, we here use only the first generated parse as training data for the pivot predictors, rather than n - best ).', 'thus, instead of using word - level features, our features correspond to properties of the generated parses : application of grammar rules ( r1, r2 features ), dependency relations ( dep ), pos tags ( f1, f2 ), syntactic features ( s1 ), precedence ( mf ), bilexical preferences ( z ), apposition ( appos ) and further features for unknown words, temporal phrases, coordination ( h, in year and p1, respectively ).', 'this allows us to get a possibly noisy, but more abstract representation of the underlying data.', 'the set of features used in alpino is further described in van  #AUTHOR_TAG']",5
"['out by  #TAUTHOR_TAG,']","['out by  #TAUTHOR_TAG,']","['often each feature appears in the parsed source and target domain data, and select those r1, p1, s1 features as pivot features, whose count is > t, where t is a specified threshold.', 'in all our experiments, we set t = 5000.', 'in this way we obtained on average 360 pivot features, on the datasets described in section 5.', 'predictive features as pointed out by  #TAUTHOR_TAG, each instance will']","['pivot features should be common across domains, here we restrict our pivots to be of the type r1, p1, s1 ( the most frequently occurring feature types ).', 'in more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long - distance dependencies occurred.', 'we count how often each feature appears in the parsed source and target domain data, and select those r1, p1, s1 features as pivot features, whose count is > t, where t is a specified threshold.', 'in all our experiments, we set t = 5000.', 'in this way we obtained on average 360 pivot features, on the datasets described in section 5.', 'predictive features as pointed out by  #TAUTHOR_TAG, each instance will actually contain features which are totally predictive of the pivot features ( i. e. the pivot itself ).', ""in our case, we additionally have to pay attention to'more specific'features, e. g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent ( i. e. which grammar rules applied in the construction of daughter nodes )."", 'it is crucial to remove these predictive features when creating the training data for the pivot predictors.', 'following  #TAUTHOR_TAG ( which follow  #AUTHOR_TAG ), we only use positive entries in the pivot predictors weight vectors to compute the svd.', 'thus, when constructing the matrix w, we disregard all negative entries in w and compute the svd ( w = u dv t ) on the resulting non - negative sparse matrix.', 'this sparse representation saves both time and space']",5
"['out by  #TAUTHOR_TAG,']","['out by  #TAUTHOR_TAG,']","['often each feature appears in the parsed source and target domain data, and select those r1, p1, s1 features as pivot features, whose count is > t, where t is a specified threshold.', 'in all our experiments, we set t = 5000.', 'in this way we obtained on average 360 pivot features, on the datasets described in section 5.', 'predictive features as pointed out by  #TAUTHOR_TAG, each instance will']","['pivot features should be common across domains, here we restrict our pivots to be of the type r1, p1, s1 ( the most frequently occurring feature types ).', 'in more detail, r1 indicates which grammar rule applied, p1 whether coordination conjuncts are parallel, and s1 whether topicalization or long - distance dependencies occurred.', 'we count how often each feature appears in the parsed source and target domain data, and select those r1, p1, s1 features as pivot features, whose count is > t, where t is a specified threshold.', 'in all our experiments, we set t = 5000.', 'in this way we obtained on average 360 pivot features, on the datasets described in section 5.', 'predictive features as pointed out by  #TAUTHOR_TAG, each instance will actually contain features which are totally predictive of the pivot features ( i. e. the pivot itself ).', ""in our case, we additionally have to pay attention to'more specific'features, e. g. r2 is a feature that extends r1, in the sense that it incorporates more information than its parent ( i. e. which grammar rules applied in the construction of daughter nodes )."", 'it is crucial to remove these predictive features when creating the training data for the pivot predictors.', 'following  #TAUTHOR_TAG ( which follow  #AUTHOR_TAG ), we only use positive entries in the pivot predictors weight vectors to compute the svd.', 'thus, when constructing the matrix w, we disregard all negative entries in w and compute the svd ( w = u dv t ) on the resulting non - negative sparse matrix.', 'this sparse representation saves both time and space']",5
"['pages', 'in our empirical setup, we followed  #TAUTHOR_TAG and tried to balance the size of source and target data.', '']","['pages', 'in our empirical setup, we followed  #TAUTHOR_TAG and tried to balance the size of source and target data.', 'thus, depending on']","['super category ) 3.', 'optionally, filter out certain pages', 'in our empirical setup, we followed  #TAUTHOR_TAG and tried to balance the size of source and target data.', '']","['', 'extract all pages that are related to p ( through sharing a direct, sub or super category ) 3.', 'optionally, filter out certain pages', 'in our empirical setup, we followed  #TAUTHOR_TAG and tried to balance the size of source and target data.', 'thus, depending on the size of the resulting target domain dataset, and the "" broadness "" of the categories involved in creating it, we might']",5
[')  #TAUTHOR_TAG for'],['of structural correspondence learning ( scl )  #TAUTHOR_TAG for'],[')  #TAUTHOR_TAG for domain adaptation'],"['paper presents an application of structural correspondence learning ( scl )  #TAUTHOR_TAG for domain adaptation of a stochastic attribute - value grammar ( savg ).', 'so far, scl has been applied successfully in nlp for part - of - speech tagging and sentiment analysis  #TAUTHOR_TAG ; ).', 'an attempt was made in the conll 2007 shared task to apply scl to non - projective dependency parsing  #AUTHOR_TAG, however, without any clear conclusions.', 'we report on our exploration of applying scl to adapt a syntactic disambiguation model and show promising initial results on wikipedia domains']",0
"['journal is applied to the more varied brown corpus  #AUTHOR_TAG.', 'the problem itself has started to get attention only recently  #AUTHOR_TAG daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'we distinguish two main']","['journal is applied to the more varied brown corpus  #AUTHOR_TAG.', 'the problem itself has started to get attention only recently  #AUTHOR_TAG daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'we distinguish two main']","['is applied to the more varied brown corpus  #AUTHOR_TAG.', 'the problem itself has started to get attention only recently  #AUTHOR_TAG daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'we distinguish two main approaches to domain']","['current, effective natural language processing systems are based on supervised machine learning techniques.', 'the parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability : a system will be successful only as long as the training material resembles the input that the model gets.', 'therefore, whenever we have access to a large amount of labeled data from some "" source "" ( out - of - domain ), but we would like a model that performs well on some new "" target "" domain  #AUTHOR_TAG daume iii, 2007 ), we face the problem of domain adaptation.', 'the need for domain adaptation arises in many nlp tasks : part - of - speech tagging, sentiment analysis, semantic role labeling or statistical parsing, to name but a few.', 'for example, the performance of a statistical parsing system drops in an appalling way when a model trained on the wall street journal is applied to the more varied brown corpus  #AUTHOR_TAG.', 'the problem itself has started to get attention only recently  #AUTHOR_TAG daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'we distinguish two main approaches to domain adaptation that have been addressed in the literature ( daume iii, 2007 ) : supervised and semi - supervised.', 'in supervised domain adaptation  #AUTHOR_TAG daume iii, 2007 ), besides the labeled source data, we have access to a comparably small, but labeled amount of target data.', 'in contrast, semi - supervised domain adaptation  #TAUTHOR_TAG is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data.', 'semi - supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult.', 'studies on the supervised task have shown that straightforward baselines ( e. g. models based on source only, target only, or the union of the data ) achieve a relatively high performance level and are "" surprisingly difficult to beat "" ( daume iii, 2007 ).', 'thus, one conclusion from that line of work is that as soon as there is a reasonable ( often even small ) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques ( daume iii, 2007 ; plank and van  #AUTHOR_TAG']",0
"['journal is applied to the more varied brown corpus  #AUTHOR_TAG.', 'the problem itself has started to get attention only recently  #AUTHOR_TAG daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'we distinguish two main']","['journal is applied to the more varied brown corpus  #AUTHOR_TAG.', 'the problem itself has started to get attention only recently  #AUTHOR_TAG daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'we distinguish two main']","['is applied to the more varied brown corpus  #AUTHOR_TAG.', 'the problem itself has started to get attention only recently  #AUTHOR_TAG daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'we distinguish two main approaches to domain']","['current, effective natural language processing systems are based on supervised machine learning techniques.', 'the parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability : a system will be successful only as long as the training material resembles the input that the model gets.', 'therefore, whenever we have access to a large amount of labeled data from some "" source "" ( out - of - domain ), but we would like a model that performs well on some new "" target "" domain  #AUTHOR_TAG daume iii, 2007 ), we face the problem of domain adaptation.', 'the need for domain adaptation arises in many nlp tasks : part - of - speech tagging, sentiment analysis, semantic role labeling or statistical parsing, to name but a few.', 'for example, the performance of a statistical parsing system drops in an appalling way when a model trained on the wall street journal is applied to the more varied brown corpus  #AUTHOR_TAG.', 'the problem itself has started to get attention only recently  #AUTHOR_TAG daume iii and  #AUTHOR_TAG daume  #TAUTHOR_TAG.', 'we distinguish two main approaches to domain adaptation that have been addressed in the literature ( daume iii, 2007 ) : supervised and semi - supervised.', 'in supervised domain adaptation  #AUTHOR_TAG daume iii, 2007 ), besides the labeled source data, we have access to a comparably small, but labeled amount of target data.', 'in contrast, semi - supervised domain adaptation  #TAUTHOR_TAG is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data.', 'semi - supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult.', 'studies on the supervised task have shown that straightforward baselines ( e. g. models based on source only, target only, or the union of the data ) achieve a relatively high performance level and are "" surprisingly difficult to beat "" ( daume iii, 2007 ).', 'thus, one conclusion from that line of work is that as soon as there is a reasonable ( often even small ) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques ( daume iii, 2007 ; plank and van  #AUTHOR_TAG']",0
"[' #TAUTHOR_TAG ;.', 'of these,']","[' #TAUTHOR_TAG ;.', 'of these, mc  #AUTHOR_TAG deal specifically with selftraining']","[' #TAUTHOR_TAG ;.', 'of these,']","['several authors have looked at the supervised adaptation case, there are less ( and especially less successful )', 'studies on semi - supervised domain adaptation  #TAUTHOR_TAG ;.', 'of these, mc  #AUTHOR_TAG deal specifically with selftraining for data - driven statistical parsing.', 'they show that together with a re - ranker, improvements are obtained.', '']",0
"[' #TAUTHOR_TAG ;.', 'of these,']","[' #TAUTHOR_TAG ;.', 'of these, mc  #AUTHOR_TAG deal specifically with selftraining']","[' #TAUTHOR_TAG ;.', 'of these,']","['several authors have looked at the supervised adaptation case, there are less ( and especially less successful )', 'studies on semi - supervised domain adaptation  #TAUTHOR_TAG ;.', 'of these, mc  #AUTHOR_TAG deal specifically with selftraining for data - driven statistical parsing.', 'they show that together with a re - ranker, improvements are obtained.', '']",0
"[')  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing the algorithm in detail, let us illustrate the intuition behind sc']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing the algorithm in detail, let us illustrate the intuition behind scl with an example, borrowed from.', '']",0
"[')  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing the algorithm in detail, let us illustrate the intuition behind sc']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing the algorithm in detail, let us illustrate the intuition behind scl with an example, borrowed from.', '']",0
"[')  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing the algorithm in detail, let us illustrate the intuition behind sc']","['structural correspondence learning )  #TAUTHOR_TAG is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.', 'before describing the algorithm in detail, let us illustrate the intuition behind scl with an example, borrowed from.', '']",0
"['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the ones discussed above.', 'feature normalization']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the ones discussed above.', 'feature normalization and feature scaling.', ' #AUTHOR_TAG found it necessary to normalize and scale the new features obtained by the projection θ, in order to "" allow them to receive more weight from a regularized discriminative learner "".', 'for each of the features, they centered them by subtracting out the mean and normalized them to unit variance ( i. e. x − mean / sd ).', 'they then rescaled the features by a factor α found on heldout data : αθx.', 'restricted regularization.', 'when training the supervised model on the augmented feature space x, θx,  #TAUTHOR_TAG only regularize the weight vector of the original features, but not the one for the new low - dimensional features.', 'this was done to encourage the model to use the new low - dimensional representation rather than the higher - dimensional original representation  #AUTHOR_TAG.', 'dimensionality reduction by feature type.', 'an extension suggested in  #AUTHOR_TAG is to compute separate svds for blocks of the matrix w corresponding to feature types ( as illustrated in figure 2 ), and then to apply separate projection for every type.', 'due to the positive results in  #AUTHOR_TAG,  #TAUTHOR_TAG include this in their standard setting of scl and report results using block svds only']",0
"['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the ones discussed above.', 'feature normalization']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the ones discussed above.', 'feature normalization and feature scaling.', ' #AUTHOR_TAG found it necessary to normalize and scale the new features obtained by the projection θ, in order to "" allow them to receive more weight from a regularized discriminative learner "".', 'for each of the features, they centered them by subtracting out the mean and normalized them to unit variance ( i. e. x − mean / sd ).', 'they then rescaled the features by a factor α found on heldout data : αθx.', 'restricted regularization.', 'when training the supervised model on the augmented feature space x, θx,  #TAUTHOR_TAG only regularize the weight vector of the original features, but not the one for the new low - dimensional features.', 'this was done to encourage the model to use the new low - dimensional representation rather than the higher - dimensional original representation  #AUTHOR_TAG.', 'dimensionality reduction by feature type.', 'an extension suggested in  #AUTHOR_TAG is to compute separate svds for blocks of the matrix w corresponding to feature types ( as illustrated in figure 2 ), and then to apply separate projection for every type.', 'due to the positive results in  #AUTHOR_TAG,  #TAUTHOR_TAG include this in their standard setting of scl and report results using block svds only']",0
"['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the ones discussed above.', 'feature normalization']","['practice, there are more free parameters and model choices  #TAUTHOR_TAG besides the ones discussed above.', 'feature normalization and feature scaling.', ' #AUTHOR_TAG found it necessary to normalize and scale the new features obtained by the projection θ, in order to "" allow them to receive more weight from a regularized discriminative learner "".', 'for each of the features, they centered them by subtracting out the mean and normalized them to unit variance ( i. e. x − mean / sd ).', 'they then rescaled the features by a factor α found on heldout data : αθx.', 'restricted regularization.', 'when training the supervised model on the augmented feature space x, θx,  #TAUTHOR_TAG only regularize the weight vector of the original features, but not the one for the new low - dimensional features.', 'this was done to encourage the model to use the new low - dimensional representation rather than the higher - dimensional original representation  #AUTHOR_TAG.', 'dimensionality reduction by feature type.', 'an extension suggested in  #AUTHOR_TAG is to compute separate svds for blocks of the matrix w corresponding to feature types ( as illustrated in figure 2 ), and then to apply separate projection for every type.', 'due to the positive results in  #AUTHOR_TAG,  #TAUTHOR_TAG include this in their standard setting of scl and report results using block svds only']",0
"[' #TAUTHOR_TAG ;,']","[' #TAUTHOR_TAG ;,']","[' #TAUTHOR_TAG ;,']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG ;.', 'of these,']","[' #TAUTHOR_TAG ;.', 'of these, mc  #AUTHOR_TAG deal specifically with selftraining']","[' #TAUTHOR_TAG ;.', 'of these,']","['several authors have looked at the supervised adaptation case, there are less ( and especially less successful )', 'studies on semi - supervised domain adaptation  #TAUTHOR_TAG ;.', 'of these, mc  #AUTHOR_TAG deal specifically with selftraining for data - driven statistical parsing.', 'they show that together with a re - ranker, improvements are obtained.', '']",1
"[' #TAUTHOR_TAG ;,']","[' #TAUTHOR_TAG ;,']","[' #TAUTHOR_TAG ;,']",[' #TAUTHOR_TAG'],1
"['with previous findings  #TAUTHOR_TAG.', 'thus we might fix the parameter and prefer smaller dimensionalities,']","['with previous findings  #TAUTHOR_TAG.', 'thus we might fix the parameter and prefer smaller dimensionalities,']","['with previous findings  #TAUTHOR_TAG.', 'thus we might fix the parameter and prefer smaller dimensionalities,']","['', 'thus, our first instantiation of scl for parse disambiguation indeed shows promising results.', 'we can confirm that changing the dimensionality parameter h has rather little effect ( table 4 ), which is in line with previous findings  #TAUTHOR_TAG.', 'thus we might fix the parameter and prefer smaller dimensionalities, which saves space and time.', 'note that these results were obtained without any of the additional normalization, rescaling, feature - specific regularization, or block svd issues, etc.', '( discussed in section 4. 2 ).', 'we used the same gaussian regularization term ( σ 2 = 1000 ) for all features ( original and new features ), and did not perform any feature normalization or rescaling.', 'this means our current instantiation of scl is an actually simplified version of the original scl algorithm, applied to parse disambiguation.', 'of course, our results are preliminary and, rather than warranting many definite conclusions, encourage further exploration of scl and related semi - supervised adaptation techniques']",3
"['. 2 ).', 'while  #TAUTHOR_TAG']","['4. 2 ).', 'while  #TAUTHOR_TAG']","['current scl instantiation.', 'feature normalization.', 'we also tested feature normalization ( as described in section 4. 2 ).', 'while  #TAUTHOR_TAG found it necessary to normalize (']","['the following, we describe additional results obtained by extensions and / or refinements of our current scl instantiation.', 'feature normalization.', 'we also tested feature normalization ( as described in section 4. 2 ).', 'while  #TAUTHOR_TAG found it necessary to normalize ( and scale ) the projection features, we did not observe any improvement by normalizing them ( actually, it slightly degraded performance in our case ).', 'thus, we found this step unnecessary, and currently did not look at this issue any further.', '']",4
"[' #TAUTHOR_TAG ;,']","[' #TAUTHOR_TAG ;,']","[' #TAUTHOR_TAG ;,']",[' #TAUTHOR_TAG'],4
['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],"['', 'several studies have examined listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon et al. 2016 ) ; however, to the best of our knowledge, no previous studies on listenability for learners of asian languages such as chinese, korean, and japanese have been conducted.', 'the method of  #AUTHOR_TAG measured listenability based on the length of sentences and the difficulty of words.', '']",0
['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],"['', 'several studies have examined listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon et al. 2016 ) ; however, to the best of our knowledge, no previous studies on listenability for learners of asian languages such as chinese, korean, and japanese have been conducted.', 'the method of  #AUTHOR_TAG measured listenability based on the length of sentences and the difficulty of words.', '']",0
['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],['listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon'],"['', 'several studies have examined listenability for english learners ( kiyokawa 1990 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ; yoon et al. 2016 ) ; however, to the best of our knowledge, no previous studies on listenability for learners of asian languages such as chinese, korean, and japanese have been conducted.', 'the method of  #AUTHOR_TAG measured listenability based on the length of sentences and the difficulty of words.', '']",1
[';  #TAUTHOR_TAG ; kotani'],"['proficiency of a learner.', 'the linguistic ( chall 1948 ; fang 1966 ; kiyokawa 1990 ; messerklinger 2006 ;  #TAUTHOR_TAG ; kotani']","['the difficulty of a sentence, and learner features explain the proficiency of a learner.', 'the linguistic ( chall 1948 ; fang 1966 ; kiyokawa 1990 ; messerklinger 2006 ;  #TAUTHOR_TAG ; kotani']","['##ability is measured based on linguistic and learner features.', 'linguistic features explain the difficulty of a sentence, and learner features explain the proficiency of a learner.', 'the linguistic ( chall 1948 ; fang 1966 ; kiyokawa 1990 ; messerklinger 2006 ;  #TAUTHOR_TAG ; kotani & yoshimi 2016 ) used in this study were originally described elsewhere.', 'linguistic features consist of sentence length, mean word length, multiple syllable words, word difficulty, speech rate, and phonological modification patterns.', 'sentence length is calculated based on the number of words in a sentence.', 'mean word length is derived from the mean number of syllables per word.', 'multiple syllable words refer to the number of multiple syllable words in a sentence.', ""word difficulty is derived from the rate of words absent from kiyokawa's basic vocabulary list for words in a sentence."", 'speech rate is calculated in terms of spoken words per minute.', 'phonological modification patterns are derived from the rate of phonologically modified words in a sentence.', 'the types of phonological modification patterns are : elision ( elimination of phonemes ), in which vowel sounds immediately follow a stressed syllable, such as the second "" o "" sound in "" chocolate "" ; reduction ( weakening a sound by changing a vowel to a schwa ), such as vowel sounds in personal / interrogative pronouns, auxiliaries, modals, prepositions, articles, and conjunctions ; contraction ( combining word pairs ), such as a modal with a subject noun ; linkage ( connecting final and initial word sounds ), such as connected a word ending with an "" n "" or "" r "" sound with a word starting with a vowel sound, for example, "" in an hour "" and "" after all "" ; and deduction ( elimination of sounds between words ), in which words share the same sound, for example, "" good day "".', 'learner features consist of listening test scores, learning experience, visiting experience, and listening frequency.', 'listening test score refers to scores on the test of english for international communication ( toeic ).', 'learning experience refers to the number of months for which learners have been studying english.', 'visiting experience refers to the number of months learners have spent in english - speaking countries.', 'listening frequency refers to scores on a five - point likert scale for the frequency of english use ( 1 : infrequently, 2 : somewhat infrequently, 3 : moderate, 4 : somewhat frequently, and 5 : frequently )']",5
"[""algorithm were constructed using the learner corpus of  #TAUTHOR_TAG, which includes learners '""]","[""algorithm were constructed using the learner corpus of  #TAUTHOR_TAG, which includes learners '""]","[""/ test data for a decision tree classification algorithm were constructed using the learner corpus of  #TAUTHOR_TAG, which includes learners'judgment of listenability""]","[""/ test data for a decision tree classification algorithm were constructed using the learner corpus of  #TAUTHOR_TAG, which includes learners'judgment of listenability."", 'listenability was judged by learners of english as a foreign language using scores on a five - point likert scale ( 1 : easy, 2 : somewhat easy, 3 : average, 4 : somewhat difficult, or 5 : difficult ).', 'scores were judged on a sentenceby - sentence basis where each learner listened to and assigned scores for 80 sentences from four news clips selected from the editorial and special sections for english learners on the voice of america ( voa ) website ( http : / / www. voanews. com ).', 'news clips in the special section were intended for learners, while news clips in the editorial section were intended for native speakers of english.', ""the news clips in the special section consisted of short, simple sentences using the voa's basic vocabulary of 1, 500 words ; idiomatic expressions were avoided."", 'by contrast, the news clips in the editorial section were made without any restrictions on vocabulary and sentence construction, as long as they were appropriate as news clips for native speakers of english.', 'the speech rate of the news clips in the special section were two - thirds slower than those in the editorial section, which were read aloud at a natural speech rate of approximately 250 syllables per minute ( robb & gillon 2007 ).', 'the learners were 90 university students ( 48 males, 42 females ; mean age ± sd, 21. 5 ± 2. 6 years ) who were compensated for their participation.', 'all learners were asked to submit valid scores from toeic tests taken in the current or previous year.', 'the mean toeic listening score was 334. 78 ± 98. 14.', 'the minimum sore was 130 ( n = 1 ), and the maximum score was 495 ( n = 8 ).', 'although the training / test data should have consisted of 7, 200 instances ( 90 learners × 80 sentences ) for valid listenability measurement, only 6, 804 instances were actually observed.', 'assuming that the missing 396 instances resulted from listening difficulties, these instances were scored as having the lowest listenability.', '']",5
"['9, 1, 13, 18,  #TAUTHOR_TAG 12, 15 ].', 'in this']","['to a particular topic [ 9, 1, 13, 18,  #TAUTHOR_TAG 12, 15 ].', 'in this work, we present debbie, a novel arguing bot,']","['9, 1, 13, 18,  #TAUTHOR_TAG 12, 15 ].', 'in this work, we present debbie, a novel arguing bot,']","['', 'we build on previous work in our lab on disagreement detection, classifying stance, identifying high quality arguments, measuring the properties and the persuasive effects of factual vs. emotional arguments, and clustering arguments into their facets or frames related to a particular topic [ 9, 1, 13, 18,  #TAUTHOR_TAG 12, 15 ].', 'in this work, we present debbie, a novel arguing bot, that uses retrieval from existing conversations in order to argue with users.', ""debbie's main aim is to keep the conversation going, by successfully producing arguments and counter - arguments that will keep the user talking about the topic."", 'our initial prototype of debbie works with three topics : death penalty, gun control, gay marriage.', 'this paper focuses on our basic investigations on the initial prototype.', 'while we are aware of other retrieval based chatbot systems [ 6, 14, 3, 2 ], debbie is novel in that it is the first to deal with argument retrieval']",5
"['##s 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regress']","['forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate']","['online debate forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate']","[""media conversations are a good source of argumentative data but many sentences either do not express an argument or cannot be understood out of context and hence cannot be used to build debbie's response pool."", ' #AUTHOR_TAG created a large corpus consisting of 109, 074 posts on the topics gay marriage ( gm, 22425 posts ), gun control ( gc, 38102 posts ), death penalty ( dp, 5283 posts ) by combining the internet argument corpus ( iac ) [ 17 ], with dialogues from online debate forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate the argument quality ( aq ) using a continuous slider ranging from hard ( 0. 0 ) to easy to interpret ( 1. 0 ).', ""the aq score is intended to reflect how easily the speaker's argument can be understood from the sentence without any context."", ""easily understandable sentences are assumed to be prime candidates for debbie's response pool."", 'note that a threshold of predicted aq > 0. 55 maintained both diversity and quality in the arguments [ 12 ].', 'for example, the sentence the death penalty is also discriminatory in its application what i mean is that through out the world the death penalty is disproportionately used against disadvantaged people was given a score of 0. 98.', 'we started with the argument quality ( aq ) regressor from  #TAUTHOR_TAG, which predicts a quality score for each sentence.', 'the stance for these argument segments is obtained from iac [ 1 ].', 'we keep only stance bearing statements from the above dataset.', 'had improved upon the aq predictor from  #TAUTHOR_TAG, giving a much larger and diverse corpus [ 12 ].', 'since generating a cohesive dialogue is a challenging task, we first evaluated our prototype with hand labeled 2000 argument quality sentence pairs for the topic of death penalty obtained from [ 12 ].', 'we tested our model for both appropriateness of responses and response times.', 'once we had a working system for death penalty, we added the best quality 250 arguments for gay marriage and gun control, each, from the corpus of [ 12 ] ( this had 174405 arguments from gay marriage and 258763 for gun control )']",5
"['##s 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regress']","['forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate']","['online debate forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate']","[""media conversations are a good source of argumentative data but many sentences either do not express an argument or cannot be understood out of context and hence cannot be used to build debbie's response pool."", ' #AUTHOR_TAG created a large corpus consisting of 109, 074 posts on the topics gay marriage ( gm, 22425 posts ), gun control ( gc, 38102 posts ), death penalty ( dp, 5283 posts ) by combining the internet argument corpus ( iac ) [ 17 ], with dialogues from online debate forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate the argument quality ( aq ) using a continuous slider ranging from hard ( 0. 0 ) to easy to interpret ( 1. 0 ).', ""the aq score is intended to reflect how easily the speaker's argument can be understood from the sentence without any context."", ""easily understandable sentences are assumed to be prime candidates for debbie's response pool."", 'note that a threshold of predicted aq > 0. 55 maintained both diversity and quality in the arguments [ 12 ].', 'for example, the sentence the death penalty is also discriminatory in its application what i mean is that through out the world the death penalty is disproportionately used against disadvantaged people was given a score of 0. 98.', 'we started with the argument quality ( aq ) regressor from  #TAUTHOR_TAG, which predicts a quality score for each sentence.', 'the stance for these argument segments is obtained from iac [ 1 ].', 'we keep only stance bearing statements from the above dataset.', 'had improved upon the aq predictor from  #TAUTHOR_TAG, giving a much larger and diverse corpus [ 12 ].', 'since generating a cohesive dialogue is a challenging task, we first evaluated our prototype with hand labeled 2000 argument quality sentence pairs for the topic of death penalty obtained from [ 12 ].', 'we tested our model for both appropriateness of responses and response times.', 'once we had a working system for death penalty, we added the best quality 250 arguments for gay marriage and gun control, each, from the corpus of [ 12 ] ( this had 174405 arguments from gay marriage and 258763 for gun control )']",0
"['##s 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regress']","['forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate']","['online debate forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate']","[""media conversations are a good source of argumentative data but many sentences either do not express an argument or cannot be understood out of context and hence cannot be used to build debbie's response pool."", ' #AUTHOR_TAG created a large corpus consisting of 109, 074 posts on the topics gay marriage ( gm, 22425 posts ), gun control ( gc, 38102 posts ), death penalty ( dp, 5283 posts ) by combining the internet argument corpus ( iac ) [ 17 ], with dialogues from online debate forums 1  #TAUTHOR_TAG.', 'it includes topic annotations, response characterizations ( 4forums ), and stance.', 'they build an argument quality regressor to rate the argument quality ( aq ) using a continuous slider ranging from hard ( 0. 0 ) to easy to interpret ( 1. 0 ).', ""the aq score is intended to reflect how easily the speaker's argument can be understood from the sentence without any context."", ""easily understandable sentences are assumed to be prime candidates for debbie's response pool."", 'note that a threshold of predicted aq > 0. 55 maintained both diversity and quality in the arguments [ 12 ].', 'for example, the sentence the death penalty is also discriminatory in its application what i mean is that through out the world the death penalty is disproportionately used against disadvantaged people was given a score of 0. 98.', 'we started with the argument quality ( aq ) regressor from  #TAUTHOR_TAG, which predicts a quality score for each sentence.', 'the stance for these argument segments is obtained from iac [ 1 ].', 'we keep only stance bearing statements from the above dataset.', 'had improved upon the aq predictor from  #TAUTHOR_TAG, giving a much larger and diverse corpus [ 12 ].', 'since generating a cohesive dialogue is a challenging task, we first evaluated our prototype with hand labeled 2000 argument quality sentence pairs for the topic of death penalty obtained from [ 12 ].', 'we tested our model for both appropriateness of responses and response times.', 'once we had a working system for death penalty, we added the best quality 250 arguments for gay marriage and gun control, each, from the corpus of [ 12 ] ( this had 174405 arguments from gay marriage and 258763 for gun control )']",6
"[' #TAUTHOR_TAG, predictive state neural']","[' #TAUTHOR_TAG, predictive state neural']","[', stochastic neural network  #TAUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient']","['tutorial introduces the advances in deep bayesian learning with abundant applications for natural language understanding ranging from speech recognition  #AUTHOR_TAG to document summarization  #AUTHOR_TAG, text classification  #AUTHOR_TAG, text segmentation  #AUTHOR_TAG, information extraction  #AUTHOR_TAG, image caption generation  #AUTHOR_TAG, sentence generation  #AUTHOR_TAG b ), dialogue control  #AUTHOR_TAG a ), sentiment classification, recommendation system, question answering  #AUTHOR_TAG and machine translation, to name a few.', 'traditionally, "" deep learning "" is taken to be a learning process where the inference or optimization is based on the real - valued deterministic model.', 'the "" semantic structure "" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.', 'the "" distribution function "" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.', 'this tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced bayesian models and deep models including hierarchical dirichlet process, chinese restaurant process  #AUTHOR_TAG, hierarchical pitman - yor process  #AUTHOR_TAG, indian buffet process  #AUTHOR_TAG, recurrent neural network  #AUTHOR_TAG, long short - term memory  #AUTHOR_TAG sequence - to - sequence model  #AUTHOR_TAG, variational auto - encoder  #AUTHOR_TAG, generative adversarial network  #AUTHOR_TAG, attention mechanism  #AUTHOR_TAG, memory - augmented neural network  #AUTHOR_TAG, stochastic neural network  #TAUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient  #AUTHOR_TAG and reinforcement learning  #AUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG, predictive state neural']","[' #TAUTHOR_TAG, predictive state neural']","[', stochastic neural network  #TAUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient']","['tutorial introduces the advances in deep bayesian learning with abundant applications for natural language understanding ranging from speech recognition  #AUTHOR_TAG to document summarization  #AUTHOR_TAG, text classification  #AUTHOR_TAG, text segmentation  #AUTHOR_TAG, information extraction  #AUTHOR_TAG, image caption generation  #AUTHOR_TAG, sentence generation  #AUTHOR_TAG b ), dialogue control  #AUTHOR_TAG a ), sentiment classification, recommendation system, question answering  #AUTHOR_TAG and machine translation, to name a few.', 'traditionally, "" deep learning "" is taken to be a learning process where the inference or optimization is based on the real - valued deterministic model.', 'the "" semantic structure "" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.', 'the "" distribution function "" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.', 'this tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced bayesian models and deep models including hierarchical dirichlet process, chinese restaurant process  #AUTHOR_TAG, hierarchical pitman - yor process  #AUTHOR_TAG, indian buffet process  #AUTHOR_TAG, recurrent neural network  #AUTHOR_TAG, long short - term memory  #AUTHOR_TAG sequence - to - sequence model  #AUTHOR_TAG, variational auto - encoder  #AUTHOR_TAG, generative adversarial network  #AUTHOR_TAG, attention mechanism  #AUTHOR_TAG, memory - augmented neural network  #AUTHOR_TAG, stochastic neural network  #TAUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient  #AUTHOR_TAG and reinforcement learning  #AUTHOR_TAG.', '']",0
"['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['detection of fake from legitimate news in different formats such as headlines, tweets and full news articles has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the most important challenge in automatic misinformation detection using modern nlp techniques, especially at the level of full news articles, is data.', 'most previous systems built to identify fake news articles rely on training data labeled with respect to the general reputation of the sources, i. e., domains / user accounts  #TAUTHOR_TAG.', ""even though some of these studies try to identify fake news based on linguistic cues, the question is whether they learn publishers'general writing style ( e. g., common writing features of a few clickbaity websites ) or deceptive style ( similarities among news articles that contain misinformation )."", 'in this study, we collect two new datasets that include the full text of news articles and individually assigned veracity labels.', 'we then address the above question, by conducting a set of crossdomain experiments : training a text classification system on data collected in a batch manner from suspicious and reputable websites and then testing the system on news articles that have been assessed in a one - by - one fashion.', 'our experiments reveal that the generalization power of a model trained on reputation - based labeled data is not impressive on individually assessed articles.', 'therefore, we propose to collect and verify larger collections of news articles with reliably assigned labels that would be useful for building more robust fake news detection systems']",0
"["". e., actual'fake news') to extract discriminative linguistic features of misinformation  #TAUTHOR_TAG."", 'the issue with these studies is the data collection methodology.', 'texts are']","['form of short statements.', ""a few recent studies have examined full articles ( i. e., actual'fake news') to extract discriminative linguistic features of misinformation  #TAUTHOR_TAG."", 'the issue with these studies is the data collection methodology.', 'texts are']","["". e., actual'fake news') to extract discriminative linguistic features of misinformation  #TAUTHOR_TAG."", 'the issue with these studies is the data collection methodology.', 'texts are harvested']","['studies on fake news detection have examined microblogs, headlines and claims in the form of short statements.', ""a few recent studies have examined full articles ( i. e., actual'fake news') to extract discriminative linguistic features of misinformation  #TAUTHOR_TAG."", 'the issue with these studies is the data collection methodology.', '']",0
"[' #TAUTHOR_TAG fore, we use this model to demonstrate how a']","[' #TAUTHOR_TAG fore, we use this model to demonstrate how a']","['fake news detection in previous work  #TAUTHOR_TAG fore, we use this model to demonstrate how a classifier trained on']","['text classification, convolutional neural networks ( cnns ) have been competing with the tf - idf model, a simple but strong baseline using scored n - grams  #AUTHOR_TAG.', ""these methods have been used for fake news detection in previous work  #TAUTHOR_TAG fore, we use this model to demonstrate how a classifier trained on data labeled according to publisher's reputation would identify misinformative news articles."", ""it is evident in the first section of figure 1, that the model performs well on similarly collected test items, i. e., hoax, satire, propaganda and trusted news articles within rashkin et al.'s test dataset."", ""however, when the model is applied to rubin et al.'s data, which was carefully assessed for satirical cues in each and every article, the performance drops considerably ( see the second section of the figure )."", ""although the classifier detects more of the satirical texts in rubin et al.'s data, the distribution of the given labels is not very different to that of legitimate texts."", ""one important feature of rubin et al.'s data is that topics of the legitimate instances were matched and balanced with topics of the satirical instances."", 'the results here suggest that similarities captured by the classifier can be very dependent on the topics of the news articles.', 'next we examine the same model on our collected datasets, buzzfeeduse and snopes312, as test material.', 'the buzzfeeduse data comes with 4 categories ( figure 1 ).', 'the classifier does seem to have some sensitivity to true vs. false information in this dataset, as more of the mostly true articles were labeled as trusted.', '']",0
"['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['detection of fake from legitimate news in different formats such as headlines, tweets and full news articles has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the most important challenge in automatic misinformation detection using modern nlp techniques, especially at the level of full news articles, is data.', 'most previous systems built to identify fake news articles rely on training data labeled with respect to the general reputation of the sources, i. e., domains / user accounts  #TAUTHOR_TAG.', ""even though some of these studies try to identify fake news based on linguistic cues, the question is whether they learn publishers'general writing style ( e. g., common writing features of a few clickbaity websites ) or deceptive style ( similarities among news articles that contain misinformation )."", 'in this study, we collect two new datasets that include the full text of news articles and individually assigned veracity labels.', 'we then address the above question, by conducting a set of crossdomain experiments : training a text classification system on data collected in a batch manner from suspicious and reputable websites and then testing the system on news articles that have been assessed in a one - by - one fashion.', 'our experiments reveal that the generalization power of a model trained on reputation - based labeled data is not impressive on individually assessed articles.', 'therefore, we propose to collect and verify larger collections of news articles with reliably assigned labels that would be useful for building more robust fake news detection systems']",1
"[' #TAUTHOR_TAG fore, we use this model to demonstrate how a']","[' #TAUTHOR_TAG fore, we use this model to demonstrate how a']","['fake news detection in previous work  #TAUTHOR_TAG fore, we use this model to demonstrate how a classifier trained on']","['text classification, convolutional neural networks ( cnns ) have been competing with the tf - idf model, a simple but strong baseline using scored n - grams  #AUTHOR_TAG.', ""these methods have been used for fake news detection in previous work  #TAUTHOR_TAG fore, we use this model to demonstrate how a classifier trained on data labeled according to publisher's reputation would identify misinformative news articles."", ""it is evident in the first section of figure 1, that the model performs well on similarly collected test items, i. e., hoax, satire, propaganda and trusted news articles within rashkin et al.'s test dataset."", ""however, when the model is applied to rubin et al.'s data, which was carefully assessed for satirical cues in each and every article, the performance drops considerably ( see the second section of the figure )."", ""although the classifier detects more of the satirical texts in rubin et al.'s data, the distribution of the given labels is not very different to that of legitimate texts."", ""one important feature of rubin et al.'s data is that topics of the legitimate instances were matched and balanced with topics of the satirical instances."", 'the results here suggest that similarities captured by the classifier can be very dependent on the topics of the news articles.', 'next we examine the same model on our collected datasets, buzzfeeduse and snopes312, as test material.', 'the buzzfeeduse data comes with 4 categories ( figure 1 ).', 'the classifier does seem to have some sensitivity to true vs. false information in this dataset, as more of the mostly true articles were labeled as trusted.', '']",5
"['sari  #TAUTHOR_TAG, and readability metrics (']","['sari  #TAUTHOR_TAG, and readability metrics ( e. g. fkgl  #AUTHOR_TAG ).', '']","['sari  #TAUTHOR_TAG, and readability metrics (']","['simplification ( ss ) consists of modifying the content and structure of a sentence to improve its readability while retaining its original meaning.', 'for automatic evaluation of a simplification output, it is common practice to use machine translation ( mt ) metrics ( e. g. bleu  #AUTHOR_TAG ), simplicity metrics ( e. g. sari  #TAUTHOR_TAG, and readability metrics ( e. g. fkgl  #AUTHOR_TAG ).', 'most of these metrics are available in individual code repositories, with particular software requirements that sometimes differ even in programming language ( e. g. corpus - level sari is implemented in java, whilst sentence - level sari is available in both java and python ).', 'other metrics ( e. g. samsa  #AUTHOR_TAG b ) ) suffer from insufficient documentation or require executing multiple scripts with hard - coded paths, which prevents researchers from using them.', 'easse ( easier automatic sentence simplification evaluation ) is a python package that provides access to popular automatic metrics in ss evaluation and ready - to - use public datasets through a simple command - line interface.', 'with this tool, we make the following contributions : ( 1 ) we provide popular automatic metrics in a single software package, ( 2 ) we supplement these metrics with word - level transformation analysis and referenceless quality estimation ( qe ) features, ( 3 ) we provide straightforward access to commonly used evaluation datasets, and ( 4 ) we generate a comprehensive html report for quantitative and qualitative evaluation of a ss system.', 'we believe this package will facilitate evaluation and improve reproducibility of results in ss.', 'easse is available in https : / / github. com / feralvam / easse']",0
[' #TAUTHOR_TAG has shown that ble'],[' #TAUTHOR_TAG has shown that bleu correlates fairly well with human'],"['s ).', 'previous work  #TAUTHOR_TAG has shown that ble']","[""human judgements on grammaticality, meaning preservation and simplicity are considered the most reliable method for evaluating a ss system's output ( stajner et al., 2016 ), it is common practice to use automatic metrics."", 'they are useful for either assessing systems at development stage, to compare different architectures, for model selection or as part of a training policy.', 'easse implements works as a wrapper for the most common evaluation metrics in ss :', ""bleu is a precision - oriented metric that relies on the proportion of n - gram matches between a system's output and reference ( s )."", 'previous work  #TAUTHOR_TAG has shown that bleu correlates fairly well with human judgements of grammaticality and meaning preservation.', 'easse uses sacrebleu  #AUTHOR_TAG 1 to calculate bleu.', ""this package was designed to standardise the process by which bleu is calculated : it only expects a detokenised system's output and the name of a test set."", 'it ensures that the same pre - processing steps are used for the system output and reference sentences.', 'sari measures how the simplicity of a sentence was improved based on the words added, deleted and kept by a system.', ""the metric compares the system's output to multiple simplification references and the original sentence."", 'sari has shown positive correlation with human judgements of simplicity gain.', ""we re - implement sari's corpuslevel version in python ( it was originally available in java )."", 'in this version, for each operation ( ope ∈ { add, del, keep } ) and n - gram order, precision p ope ( n ), recall r ope ( n ) and f1 f ope ( n ) scores are calculated.', 'these are then averaged over the n - gram order to get the overall operation f1 score', 'although  #TAUTHOR_TAG indicate that only precision should be considered for the deletion operation, we follow the java implementation that uses f1 score for all operations in corpus - level sari.', 'samsa measures structural simplicity ( i. e. sentence splitting ).', 'this is in contrast to sari, which is designed to evaluate simplifications involving paraphrasing.', 'easse re - uses the original samsa implementation 2 with some modifications : ( 1 ) an internal call to the tupa parser  #AUTHOR_TAG, which generates the semantic annotations for each original sentence ; ( 2 ) a modified version of the monolingual word aligner  #AUTHOR_TAG that is compatible with python 3, and uses stanford corenlp  #AUTHOR_TAG 3 through their official python interface, and ( 3 ) a single function call to get']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['##pus  #TAUTHOR_TAG,']","['##se provides access to three publicly available datasets for automatic ss evaluation ( table 1 ) : pwkp  #AUTHOR_TAG, turkcorpus  #TAUTHOR_TAG, and hsplit  #AUTHOR_TAG a ).', 'all of them consist of the data from the original datasets, which are sentences extracted from english wikipedia ( ew ) articles.', ""it is important to highlight that easse can also evaluate system's outputs in other datasets provided by the user."", 'pwkp  #AUTHOR_TAG automatically aligned sentences in 65, 133 ew articles to their corresponding versions in simple ew ( sew ).', 'since the latter is aimed at english learners, its articles are expected to contain fewer words and simpler grammar structures than those in their ew counterpart.', 'the test set split of pwkp contains 100 sentences, with 1 - to - 1 and 1 - to - n alignments ( resp.', '93 and 7 instances ).', 'the latter correspond to instances of sentence splitting.', 'since this dataset has only one reference for each original sentence, 5 https : / / github. com / facebookresearch / text - simplification - evaluation 6 the lexical complexity score of a simplified sentence is computed by taking the log - ranks of each word in the frequency it is not ideal for calculating automatic metrics that rely on multiple references, such as sari.', 'turkcorpus  #TAUTHOR_TAG asked crowdworkers to simplify 2, 359 original sentences extracted from pwkp to collect eight simplification references for each one.', 'this dataset was then randomly split into tuning ( 2, 000 instances ) and test ( 359 instances ) sets.', '']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['##pus  #TAUTHOR_TAG,']","['##se provides access to three publicly available datasets for automatic ss evaluation ( table 1 ) : pwkp  #AUTHOR_TAG, turkcorpus  #TAUTHOR_TAG, and hsplit  #AUTHOR_TAG a ).', 'all of them consist of the data from the original datasets, which are sentences extracted from english wikipedia ( ew ) articles.', ""it is important to highlight that easse can also evaluate system's outputs in other datasets provided by the user."", 'pwkp  #AUTHOR_TAG automatically aligned sentences in 65, 133 ew articles to their corresponding versions in simple ew ( sew ).', 'since the latter is aimed at english learners, its articles are expected to contain fewer words and simpler grammar structures than those in their ew counterpart.', 'the test set split of pwkp contains 100 sentences, with 1 - to - 1 and 1 - to - n alignments ( resp.', '93 and 7 instances ).', 'the latter correspond to instances of sentence splitting.', 'since this dataset has only one reference for each original sentence, 5 https : / / github. com / facebookresearch / text - simplification - evaluation 6 the lexical complexity score of a simplified sentence is computed by taking the log - ranks of each word in the frequency it is not ideal for calculating automatic metrics that rely on multiple references, such as sari.', 'turkcorpus  #TAUTHOR_TAG asked crowdworkers to simplify 2, 359 original sentences extracted from pwkp to collect eight simplification references for each one.', 'this dataset was then randomly split into tuning ( 2, 000 instances ) and test ( 359 instances ) sets.', '']",0
"['of these observations.', 'according to  #TAUTHOR_TAG, the annotators in turkcorpus were instructed to mainly']","['of these observations.', 'according to  #TAUTHOR_TAG, the annotators in turkcorpus were instructed to mainly']","['of these observations.', 'according to  #TAUTHOR_TAG, the annotators in turkcorpus were instructed to mainly']","['order to better understand the previous results, we use the wordlevel annotations of text transformations ( table 3 ).', 'since sari was design to evaluate mainly paraphrasing transformations, the fact that sbsmt - sari is the best at performing replacements and second place in copying explains its high sari score.', 'dmass - dcss is second best in replacements, while pbsmt - r ( which achieved the highest bleu score ) is the best at copying.', 'hybrid is the best at performing deletions, but is the worst at replacements, which sari mainly measures.', 'the origin of the turkcorpus set itself could explain some of these observations.', 'according to  #TAUTHOR_TAG, the annotators in turkcorpus were instructed to mainly produce paraphrases, i. e. mostly replacements with virtually no deletions.', 'as such, copying words is also a significant transformation, so systems that are good at performing it better mimic the characteristics of the human simplifications in this dataset.', 'table 3 : transformation - based performance of the sentence simplification systems in the turkcorpus test set.', 'table 4 : quality estimation features, which give additional information on the output of different systems']",0
[' #TAUTHOR_TAG has shown that ble'],[' #TAUTHOR_TAG has shown that bleu correlates fairly well with human'],"['s ).', 'previous work  #TAUTHOR_TAG has shown that ble']","[""human judgements on grammaticality, meaning preservation and simplicity are considered the most reliable method for evaluating a ss system's output ( stajner et al., 2016 ), it is common practice to use automatic metrics."", 'they are useful for either assessing systems at development stage, to compare different architectures, for model selection or as part of a training policy.', 'easse implements works as a wrapper for the most common evaluation metrics in ss :', ""bleu is a precision - oriented metric that relies on the proportion of n - gram matches between a system's output and reference ( s )."", 'previous work  #TAUTHOR_TAG has shown that bleu correlates fairly well with human judgements of grammaticality and meaning preservation.', 'easse uses sacrebleu  #AUTHOR_TAG 1 to calculate bleu.', ""this package was designed to standardise the process by which bleu is calculated : it only expects a detokenised system's output and the name of a test set."", 'it ensures that the same pre - processing steps are used for the system output and reference sentences.', 'sari measures how the simplicity of a sentence was improved based on the words added, deleted and kept by a system.', ""the metric compares the system's output to multiple simplification references and the original sentence."", 'sari has shown positive correlation with human judgements of simplicity gain.', ""we re - implement sari's corpuslevel version in python ( it was originally available in java )."", 'in this version, for each operation ( ope ∈ { add, del, keep } ) and n - gram order, precision p ope ( n ), recall r ope ( n ) and f1 f ope ( n ) scores are calculated.', 'these are then averaged over the n - gram order to get the overall operation f1 score', 'although  #TAUTHOR_TAG indicate that only precision should be considered for the deletion operation, we follow the java implementation that uses f1 score for all operations in corpus - level sari.', 'samsa measures structural simplicity ( i. e. sentence splitting ).', 'this is in contrast to sari, which is designed to evaluate simplifications involving paraphrasing.', 'easse re - uses the original samsa implementation 2 with some modifications : ( 1 ) an internal call to the tupa parser  #AUTHOR_TAG, which generates the semantic annotations for each original sentence ; ( 2 ) a modified version of the monolingual word aligner  #AUTHOR_TAG that is compatible with python 3, and uses stanford corenlp  #AUTHOR_TAG 3 through their official python interface, and ( 3 ) a single function call to get']",4
"['. g. hybrid  #AUTHOR_TAG ).', 'we also included sbsmt - sari  #TAUTHOR_TAG,']","['by itself ( e. g. pbsmt - r  #AUTHOR_TAG ), or coupled with semantic analysis, ( e. g. hybrid  #AUTHOR_TAG ).', 'we also included sbsmt - sari  #TAUTHOR_TAG,']","['. g. hybrid  #AUTHOR_TAG ).', 'we also included sbsmt - sari  #TAUTHOR_TAG,']","['##se provides access to various ss system outputs that follow different approaches for the task.', 'for instance, we included those that rely on phrase - based statistical mt, either by itself ( e. g. pbsmt - r  #AUTHOR_TAG ), or coupled with semantic analysis, ( e. g. hybrid  #AUTHOR_TAG ).', 'we also included sbsmt - sari  #TAUTHOR_TAG, which relies on syntaxbased statistical mt ; dress - ls  #AUTHOR_TAG, a neural model using the standard encoder - decoder architecture with attention combined with reinforcement learning ; and dmass - dcss  #AUTHOR_TAG, the current state - of - theart in the turkcorpus, which is based on the transformer architecture  #AUTHOR_TAG']",5
['. model  #TAUTHOR_TAG.'],"['1 ], stacked attention network ( san ) [ 12 ] and teney et al. model  #TAUTHOR_TAG.']",['. model  #TAUTHOR_TAG.'],"['', '##gnet [ 3 ] and lstm [ 7 ]. this model has been revised over the years, employing newer architectures and mathematical formulations. along with this, many authors have worked on producing', 'datasets for eliminating bias, strengthening the performance of the model by robust question - answer pairs which try to cover the various', 'types of questions, testing the visual and language understanding of the system. in this survey, first we cover major datasets published for validating the visual question answering task, such as vqa dataset [ 1 ], daquar', '[ 8 ], visual7w [ 9 ] and most recent datasets up to 2019 include tally - qa [ 10 ]', 'and kvqa [ 11 ]. next, we discuss the stateof - the', '- art architectures designed for the task of visual question answering such as vanilla vqa [ 1 ], stacked attention networks [ 12 ] and pythia v1. 0 [ 13 ]. next we present some of our computed results', 'over the three architectures : vanilla vqa model [ 1 ], stacked attention network ( san ) [ 12 ] and teney et al. model  #TAUTHOR_TAG. finally, we discuss the observations and future directions']",5
"['. model  #TAUTHOR_TAG.', '']","['al. model  #TAUTHOR_TAG.', '']","['. model  #TAUTHOR_TAG.', 'we considered the widely adapted']","['reported results for different methods over different datasets are summarized in table i and table ii.', 'it can be observed that vqa dataset is very commonly used by different methods to test the performance.', 'other datasets like visual7w, tally - qa and kvqa are also very challenging and recent datasets.', 'it can be also seen that the pythia v1. 0 is one of the recent methods performing very well over vqa dataset.', 'the differentail network is the very recent method proposed for vqa task and shows very promising performance over different datasets.', 'as part of this survey, we also implemented different methods over different datasets and performed the experiments.', 'we considered the following three models for our experiments, 1 ) the baseline vanilla vqa model [ 1 ] which uses the vgg16 cnn architecture [ 3 ] and lstms [ 7 ], 2 ) the stacked attention networks [ 12 ] architecture, and 3 ) the 2017 vqa challenge winner teney et al. model  #TAUTHOR_TAG.', '']",5
"['. model  #TAUTHOR_TAG.', '']","['al. model  #TAUTHOR_TAG.', '']","['. model  #TAUTHOR_TAG.', 'we considered the widely adapted']","['reported results for different methods over different datasets are summarized in table i and table ii.', 'it can be observed that vqa dataset is very commonly used by different methods to test the performance.', 'other datasets like visual7w, tally - qa and kvqa are also very challenging and recent datasets.', 'it can be also seen that the pythia v1. 0 is one of the recent methods performing very well over vqa dataset.', 'the differentail network is the very recent method proposed for vqa task and shows very promising performance over different datasets.', 'as part of this survey, we also implemented different methods over different datasets and performed the experiments.', 'we considered the following three models for our experiments, 1 ) the baseline vanilla vqa model [ 1 ] which uses the vgg16 cnn architecture [ 3 ] and lstms [ 7 ], 2 ) the stacked attention networks [ 12 ] architecture, and 3 ) the 2017 vqa challenge winner teney et al. model  #TAUTHOR_TAG.', '']",5
[' #TAUTHOR_TAG with reduced computations with'],"[' #TAUTHOR_TAG with reduced computations with elementwise multiplication, use of glove vectors [ 23 ], and ensemble of 30 models.', 'differential networks [ 20 ] : this model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features.', 'image features are']",['.  #TAUTHOR_TAG with reduced computations with'],"['', 'this model is better suited for the vqa in videos which has more use cases than images.', '[ 20 ] vqa [ 1 ], tdiuc [ 29 ], coco - qa [ 21 ] faster - rcnn [ 22 ], differential modules [ 30 ], gru [ 31 ] 68. 59 ( vqa - v2 ), 86. 73 ( tdiuc ), 69. 36 ( coco - qa ) aaai 2019 pythia v1. 0 [ 28 ] : pythia v1. 0 is the award winning architecture for vqa challenge 2018', '1. the architecture is similar to teney et al.  #TAUTHOR_TAG with reduced computations with elementwise multiplication, use of glove vectors [ 23 ], and ensemble of 30 models.', 'differential networks [ 20 ] : this model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features.', 'image features are extracted using faster - rcnn [ 22 ].', 'the differential modules [ 30 ] are used to refine the features in both text and images.', '']",3
"['from.', 'we modify an existing system, hier - sum  #TAUTHOR_TAG, to use our objective, which significantly outperforms']","['from.', 'we modify an existing system, hier - sum  #TAUTHOR_TAG, to use our objective, which significantly outperforms']","['from.', 'we modify an existing system, hier - sum  #TAUTHOR_TAG, to use our objective, which significantly outperforms the original hiersum in pairwise user evaluation.', 'additionally, our rouge scores advance']","['approaches to multi - document summarization consist of two steps : finding a content model of the documents to be summarized, and then generating a summary that best represents the most salient information of the documents.', 'in this paper, we present a sentence selection objective for extractive summarization in which sentences are penalized for containing content that is specific to the documents they were extracted from.', 'we modify an existing system, hier - sum  #TAUTHOR_TAG, to use our objective, which significantly outperforms the original hiersum in pairwise user evaluation.', 'additionally, our rouge scores advance the current state - of - the - art for both supervised and unsupervised systems with statistical significance']",6
"['##lmaz & hakkani  #AUTHOR_TAG.', ' #TAUTHOR_TAG demonstrated that these models can']","[' #AUTHOR_TAG celikyilmaz & hakkani  #AUTHOR_TAG.', ' #TAUTHOR_TAG demonstrated that these models can']","[' #AUTHOR_TAG celikyilmaz & hakkani  #AUTHOR_TAG.', ' #TAUTHOR_TAG demonstrated that these models can improve']","['', 'summaries can be evaluated manually, or with automatic metrics such as rouge  #AUTHOR_TAG.', 'the use of structured probabilistic topic models has made it possible to represent document set content with increasing complexity ( daume &  #AUTHOR_TAG celikyilmaz & hakkani  #AUTHOR_TAG.', ' #TAUTHOR_TAG demonstrated that these models can improve the quality of generic multi - document summaries over simpler surface models.', 'their most complex hierarchial model improves summary content by teasing out the words that are not general enough to represent the document set as a whole.', 'once those words are no longer included in the content word distribution, they are implicitly less likely to appear in the extracted summary as well. but this objective does not sufficiently keep document - specific content from appearing in multi - document summaries.', 'in this paper, we present a selection objective that explicitly excludes document - specific content.', 'we re - implement the hiersum system from  #TAUTHOR_TAG, and show that using our objective dramatically improves the content of extracted summaries']",6
"['##lmaz & hakkani  #AUTHOR_TAG.', ' #TAUTHOR_TAG demonstrated that these models can']","[' #AUTHOR_TAG celikyilmaz & hakkani  #AUTHOR_TAG.', ' #TAUTHOR_TAG demonstrated that these models can']","[' #AUTHOR_TAG celikyilmaz & hakkani  #AUTHOR_TAG.', ' #TAUTHOR_TAG demonstrated that these models can improve']","['', 'summaries can be evaluated manually, or with automatic metrics such as rouge  #AUTHOR_TAG.', 'the use of structured probabilistic topic models has made it possible to represent document set content with increasing complexity ( daume &  #AUTHOR_TAG celikyilmaz & hakkani  #AUTHOR_TAG.', ' #TAUTHOR_TAG demonstrated that these models can improve the quality of generic multi - document summaries over simpler surface models.', 'their most complex hierarchial model improves summary content by teasing out the words that are not general enough to represent the document set as a whole.', 'once those words are no longer included in the content word distribution, they are implicitly less likely to appear in the extracted summary as well. but this objective does not sufficiently keep document - specific content from appearing in multi - document summaries.', 'in this paper, we present a selection objective that explicitly excludes document - specific content.', 'we re - implement the hiersum system from  #TAUTHOR_TAG, and show that using our objective dramatically improves the content of extracted summaries']",0
"['summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution']","['original documents.', 'the highest frequency words ( after removing stop words ) have a high likelihood of appearing in human - authored summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution']","['##ed summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution may contain words']","['easiest way to model document content is to find a probability distribution of all unigrams that appear in the original documents.', 'the highest frequency words ( after removing stop words ) have a high likelihood of appearing in human - authored summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution may contain words that appear frequently in one document, but do not reflect the content of the document set as a whole.', 'probabilistic topic models provide a more principled approach to finding a distribution of content words.', '']",0
"['summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution']","['original documents.', 'the highest frequency words ( after removing stop words ) have a high likelihood of appearing in human - authored summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution']","['##ed summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution may contain words']","['easiest way to model document content is to find a probability distribution of all unigrams that appear in the original documents.', 'the highest frequency words ( after removing stop words ) have a high likelihood of appearing in human - authored summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution may contain words that appear frequently in one document, but do not reflect the content of the document set as a whole.', 'probabilistic topic models provide a more principled approach to finding a distribution of content words.', '']",0
"['summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution']","['original documents.', 'the highest frequency words ( after removing stop words ) have a high likelihood of appearing in human - authored summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution']","['##ed summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution may contain words']","['easiest way to model document content is to find a probability distribution of all unigrams that appear in the original documents.', 'the highest frequency words ( after removing stop words ) have a high likelihood of appearing in human - authored summaries  #AUTHOR_TAG.', 'however, the raw  #TAUTHOR_TAG', 'unigram distribution may contain words that appear frequently in one document, but do not reflect the content of the document set as a whole.', 'probabilistic topic models provide a more principled approach to finding a distribution of content words.', '']",0
"['summary sentence selection in several systems including lerman and mc  #AUTHOR_TAG and  #TAUTHOR_TAG, and was used as a feature in']","['summary sentence selection in several systems including lerman and mc  #AUTHOR_TAG and  #TAUTHOR_TAG, and was used as a feature in']","['summary sentence selection in several systems including lerman and mc  #AUTHOR_TAG and  #TAUTHOR_TAG, and was used as a feature in the discrimitive sentence ranking']","['kl - divergence between two unigram word distributions p and q is given by kl ( p | | q ) = w p ( w ) log p ( w ) q ( w ).', 'this quantity is used for summary sentence selection in several systems including lerman and mc  #AUTHOR_TAG and  #TAUTHOR_TAG, and was used as a feature in the discrimitive sentence ranking of daume and  #AUTHOR_TAG.', 'topicsum and hiersum use the following kl objective, which finds s *, the summary that minimizes the kl - divergence between the estimated content distribution φ c and the summary word distribution p s :', 'a greedy approximation is used to find s *.', 'starting with an empty summary, sentences are greedily added to the summary one at a time until the summary has reached the maximum word limit, l. the values of p s are smoothed uniformly in order to ensure finite values of kl ( φ c | | p s )']",0
['- 5  #TAUTHOR_TAG'],['ques - 5  #TAUTHOR_TAG q1'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
"['.', 'predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [ 3, 9, 13,  #TAUTHOR_TAG.', 'such approaches have the']","['.', 'predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [ 3, 9, 13,  #TAUTHOR_TAG.', 'such approaches have the']","['.', 'predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [ 3, 9, 13,  #TAUTHOR_TAG.', 'such approaches have the']","['.', 'predicting mental health from smartphone and social media data on a longitudinal basis has recently attracted great interest, with very promising results being reported across many studies [ 3, 9, 13,  #TAUTHOR_TAG.', 'such approaches have the potential to revolutionise mental health assessment, if their development and evaluation follows a real world deployment setting.', 'in this work we take a closer look at state - of - the - art approaches, using different mental health datasets and indicators, different feature sources and multiple simulations, in order to assess their ability to generalise.', 'we demonstrate that under a pragmatic evaluation framework, none of the approaches deliver or even approach the reported performances.', 'in fact, we show that current state - of - the - art approaches can barely outperform the most naive baselines in the real - world setting, posing serious questions not only about their deployment ability, but also about the contribution of the derived features for the mental health assessment task and how to make better use of such data in the future']",0
"['- being, and recent research [ 1, 2, 3, 5, 9, 10, 13, 14, 22, 23,  #TAUTHOR_TAG has started exploring the effectiveness of these modalities for automatically']","['failing to capture well - being on an individual basis.', 'the latter is only possible via self - reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long - term aggregates instead of the current state of the individual.', 'the widespread use of smart - phones and social media offers new ways of assessing mental well - being, and recent research [ 1, 2, 3, 5, 9, 10, 13, 14, 22, 23,  #TAUTHOR_TAG has started exploring the effectiveness of these modalities for automatically']","['- reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long - term aggregates instead of the current state of the individual.', 'the widespread use of smart - phones and social media offers new ways of assessing mental well - being, and recent research [ 1, 2, 3, 5, 9, 10, 13, 14, 22, 23,  #TAUTHOR_TAG has started exploring the effectiveness of these modalities for automatically']","['the right indicators of mental well - being is a grand challenge posed by the world health organisation [ 7 ].', 'poor mental health is highly correlated with low motivation, lack of satisfaction, low productivity and a negative economic impact [ 20 ].', 'the current approach is to combine census data at the population level [ 19 ], thus failing to capture well - being on an individual basis.', 'the latter is only possible via self - reporting on the basis of established psychological scales, which are hard to acquire consistently on a longitudinal basis, and they capture long - term aggregates instead of the current state of the individual.', 'the widespread use of smart - phones and social media offers new ways of assessing mental well - being, and recent research [ 1, 2, 3, 5, 9, 10, 13, 14, 22, 23,  #TAUTHOR_TAG has started exploring the effectiveness of these modalities for automatically']",0
"['[ 2, 5, 9, 10, 22,  #TAUTHOR_TAG has combined the instances { x uj i, y uj i } from different individuals u j and performed evaluation using randomised cross validation ( mixed ).', 'while such']","['[ 2, 5, 9, 10, 22,  #TAUTHOR_TAG has combined the instances { x uj i, y uj i } from different individuals u j and performed evaluation using randomised cross validation ( mixed ).', 'while such']","['[ 2, 5, 9, 10, 22,  #TAUTHOR_TAG has combined the instances { x uj i, y uj i } from different individuals u j and performed evaluation using randomised cross validation ( mixed ).', 'while such approaches']","['mental health of a subject, reporting very high accuracy. what is typically done in these studies is to use features based on the subjects\'smart phone logs and social media, to predict some self - reported mental health index ( e. g., "" wellbeing "", "" depression "" and others ), which is provided either on a likert scale or on the basis of a psychological questionnaire ( e. g., phq - 8 [ 12 ], panas [ 29 ], wemwbs [ 25 ] and others ).', 'most of these studies are longitudinal, where data about individuals is collected over a period of time and predictions of mental health are made over a sliding time window.', 'having such longitudinal studies is highly desirable, as it can allow fine - grained monitoring of mental health.', 'however, a crucial question is what constitutes an appropriate evaluation framework, in order for such approaches to be employable in a real world setting.', 'generalisation to previously unobserved users can only be assessed via leave - n - users - out cross - validation setups, where typically, n is equal to one ( louocv, see table 1 ).', 'however, due to the small number of subjects that are available, such generalisation is hard to achieve by any approach [ 13 ].', 'alternatively, personalised models [ 3, 13 ] for every individual can be evaluated via a within - subject, leave - n - instances - out cross - validation ( for n = 1, loiocv ), where an instance for a user u at time i is defined as a { x ui, y ui } tuple of { features ( u, i ), mental - health - score ( u, i ) }. in a real world setting, a loiocv model is trained on some user - specific instances, aiming to predict her mental health state at some future time points.', 'again however, the limited number of instances for every user make such models unable to generalize well.', 'in order to overcome these issues, previous work [ 2, 5, 9, 10, 22,  #TAUTHOR_TAG has combined the instances { x uj i, y uj i } from different individuals u j and performed evaluation using randomised cross validation ( mixed ).', 'while such approaches can attain optimistic performance, the corresponding models fail to generalise to the general population and also fail to ensure effective personalised assessment of the mental health state of a single individual.', 'in this paper we demonstrate the challenges that current state - of - the - art models face, when tested in a real - world setting']",0
"[""automatically predicting a user's mental state ( target ), either in a classification or a regression manner [ 1, 2, 3, 9, 10, 13,  #TAUTHOR_TAG."", 'examples of']","[""automatically predicting a user's mental state ( target ), either in a classification or a regression manner [ 1, 2, 3, 9, 10, 13,  #TAUTHOR_TAG."", 'examples of']","[""automatically predicting a user's mental state ( target ), either in a classification or a regression manner [ 1, 2, 3, 9, 10, 13,  #TAUTHOR_TAG."", 'examples of']","[""in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state ( target ), either in a classification or a regression manner [ 1, 2, 3, 9, 10, 13,  #TAUTHOR_TAG."", 'examples of state - of - the - art work in this domain are listed in table 2, along with the number of subjects that was used and the method upon which evaluation took place.', 'most approaches have used the "" mixed "" approach to evaluate models [ 1, 2, 5, 9, 10, 22,  #TAUTHOR_TAG, which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.', 'loiocv approaches that have not ensured that their train / test sets are independent are also vulnerable to bias in a realistic setting [ 3, 13 ].', 'from the works listed in table 2, only suhara et al. [ 23 ] achieves unbiased results with respect to model generalisability ; however, the features employed for their prediction task are derived from self - reported questionnaires of the subjects and not by automatic means']",0
"[""automatically predicting a user's mental state ( target ), either in a classification or a regression manner [ 1, 2, 3, 9, 10, 13,  #TAUTHOR_TAG."", 'examples of']","[""automatically predicting a user's mental state ( target ), either in a classification or a regression manner [ 1, 2, 3, 9, 10, 13,  #TAUTHOR_TAG."", 'examples of']","[""automatically predicting a user's mental state ( target ), either in a classification or a regression manner [ 1, 2, 3, 9, 10, 13,  #TAUTHOR_TAG."", 'examples of']","[""in assessing mental health on a longitudinal basis aims to make use of relevant features extracted from various modalities, in order to train models for automatically predicting a user's mental state ( target ), either in a classification or a regression manner [ 1, 2, 3, 9, 10, 13,  #TAUTHOR_TAG."", 'examples of state - of - the - art work in this domain are listed in table 2, along with the number of subjects that was used and the method upon which evaluation took place.', 'most approaches have used the "" mixed "" approach to evaluate models [ 1, 2, 5, 9, 10, 22,  #TAUTHOR_TAG, which, as we will show, is vulnerable to bias, due to the danger of recognising the user in the test set and thus simply inferring her average mood score.', 'loiocv approaches that have not ensured that their train / test sets are independent are also vulnerable to bias in a realistic setting [ 3, 13 ].', 'from the works listed in table 2, only suhara et al. [ 23 ] achieves unbiased results with respect to model generalisability ; however, the features employed for their prediction task are derived from self - reported questionnaires of the subjects and not by automatic means']",0
"['[ 2, 9, 10,  #TAUTHOR_TAG.', 'this is problematic,']","['[ 2, 9, 10,  #TAUTHOR_TAG.', 'this is problematic,']","['merge all the instances from different subjects, in an attempt to build user - agnostic models in a randomised cross - validation framework [ 2, 9, 10,  #TAUTHOR_TAG.', 'this is problematic,']","['on past values of the target variable : this issue arises when the past n mood scores of a user are required to predict his / her next mood score in an autoregressive manner.', 'since such an approach would require the previous n scores of past mood forms, it would limit its ability to generalise without the need of manual user input in a continuous basis.', 'this makes it impractical for a real - world scenario.', 'most importantly, it is difficult to measure the contribution of the features towards the prediction task, unless the model is evaluated using target feature ablation.', 'for demonstration purposes, we have followed the experimental setup by likamwa et al. [ 13 ], which is one of the leading works in this field.', 'p2 inferring test set labels : when training personalised models ( loiocv ) in a longitudinal study, it is important to make sure that there are no overlapping instances across consecutive time windows.', 'some past works have extracted features { f ( t − n ),..., f ( t ) } over n days, in order to predict the score t on day n + 1 [ 3, 13 ].', 'such approaches are biased if there are overlapping days of train / test data.', 'to illustrate this problem we have followed the approach by canzian and musolesi [ 3 ], as one of the pioneering works on predicting depression with gps traces, on a longitudinal basis.', 'p3 predicting users instead of mood scores : most approaches merge all the instances from different subjects, in an attempt to build user - agnostic models in a randomised cross - validation framework [ 2, 9, 10,  #TAUTHOR_TAG.', 'this is problematic, especially when dealing with a small number of subjects, whose behaviour ( as captured through their data ) and mental health scores differ on an individual basis.', 'such approaches are in danger of "" predicting "" the user in the test set, since her ( test set ) features might be highly correlated with her features in the training set, and thus infer her average well - being score, based on the corresponding observations of the training set.', 'such approaches cannot guarantee that they will generalise on either a population - wide ( louocv ) or a personalised ( loiocv ) level.', 'in order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by tsakalidis et al.  #TAUTHOR_TAG and jaques et al. [ 9 ]']",0
['.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],['##kalidis et al.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],['.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],"['##kalidis et al.  #TAUTHOR_TAG monitored the behaviour of 19 individuals over four months.', 'the subjects were asked to complete two psychological scales [ 25, 29 ] on a daily basis, leading to three target scores ( positive, negative, mental well - being ) ; various features from smartphones ( e. g., time spent on the preferred locations ) and textual features ( e. g., ngrams ) were extracted passively over the 24 hours preceding a mood form timestamp.', 'model training and evaluation was performed in a randomised ( mixed ) cross - validation setup, leading to high accuracy ( r 2 = 0. 76 ).', 'however, a case demonstrating the potential user bias is when the models are trained on the textual sources : initially the highest r 2 ( 0. 22 ) is achieved when a model is applied to the mental - wellbeing target ; by normalising the textual features on a per - user basis, the r 2 increases to 0. 65.', 'while this is likely to happen because the vocabulary used by different users is normalised, there is also the danger of over - fitting the trained model to the identity of the user.', 'to examine this potential, the loiocv / louocv setups need to be studied alongside the mixed validation approach, with and without the per - user feature normalisation step.', 'a similar issue is encountered in jaques et al. [ 9 ] who monitored 68 subjects over a period of a month.', 'four types of features were extracted from survey and smart devices carried by subjects.', 'self - reported scores on a daily basis served as the ground truth.', 'the authors labelled the instances with the top 30 % of all the scores as "" happy "" and the lowest 30 % as "" sad "" and randomly separated them into training, validation and test sets, leading to the same user bias issue.', 'since different users exhibit different mood scores on average  #TAUTHOR_TAG, by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.', 'a more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a loiocv or in a louocv setup.', 'while we focus on the works of tsakalidis et al.  #TAUTHOR_TAG and jaques et al. [ 9 ], similar experimental setups were also followed in [ 10 ], using the median of scores to separate the instances and performing five - fold cross - validation, and by bogomolov et al. in [ 2 ], working on']",0
['.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],['##kalidis et al.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],['.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],"['##kalidis et al.  #TAUTHOR_TAG monitored the behaviour of 19 individuals over four months.', 'the subjects were asked to complete two psychological scales [ 25, 29 ] on a daily basis, leading to three target scores ( positive, negative, mental well - being ) ; various features from smartphones ( e. g., time spent on the preferred locations ) and textual features ( e. g., ngrams ) were extracted passively over the 24 hours preceding a mood form timestamp.', 'model training and evaluation was performed in a randomised ( mixed ) cross - validation setup, leading to high accuracy ( r 2 = 0. 76 ).', 'however, a case demonstrating the potential user bias is when the models are trained on the textual sources : initially the highest r 2 ( 0. 22 ) is achieved when a model is applied to the mental - wellbeing target ; by normalising the textual features on a per - user basis, the r 2 increases to 0. 65.', 'while this is likely to happen because the vocabulary used by different users is normalised, there is also the danger of over - fitting the trained model to the identity of the user.', 'to examine this potential, the loiocv / louocv setups need to be studied alongside the mixed validation approach, with and without the per - user feature normalisation step.', 'a similar issue is encountered in jaques et al. [ 9 ] who monitored 68 subjects over a period of a month.', 'four types of features were extracted from survey and smart devices carried by subjects.', 'self - reported scores on a daily basis served as the ground truth.', 'the authors labelled the instances with the top 30 % of all the scores as "" happy "" and the lowest 30 % as "" sad "" and randomly separated them into training, validation and test sets, leading to the same user bias issue.', 'since different users exhibit different mood scores on average  #TAUTHOR_TAG, by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.', 'a more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a loiocv or in a louocv setup.', 'while we focus on the works of tsakalidis et al.  #TAUTHOR_TAG and jaques et al. [ 9 ], similar experimental setups were also followed in [ 10 ], using the median of scores to separate the instances and performing five - fold cross - validation, and by bogomolov et al. in [ 2 ], working on']",0
"['[ 2, 9, 10,  #TAUTHOR_TAG.', 'this is problematic,']","['[ 2, 9, 10,  #TAUTHOR_TAG.', 'this is problematic,']","['merge all the instances from different subjects, in an attempt to build user - agnostic models in a randomised cross - validation framework [ 2, 9, 10,  #TAUTHOR_TAG.', 'this is problematic,']","['on past values of the target variable : this issue arises when the past n mood scores of a user are required to predict his / her next mood score in an autoregressive manner.', 'since such an approach would require the previous n scores of past mood forms, it would limit its ability to generalise without the need of manual user input in a continuous basis.', 'this makes it impractical for a real - world scenario.', 'most importantly, it is difficult to measure the contribution of the features towards the prediction task, unless the model is evaluated using target feature ablation.', 'for demonstration purposes, we have followed the experimental setup by likamwa et al. [ 13 ], which is one of the leading works in this field.', 'p2 inferring test set labels : when training personalised models ( loiocv ) in a longitudinal study, it is important to make sure that there are no overlapping instances across consecutive time windows.', 'some past works have extracted features { f ( t − n ),..., f ( t ) } over n days, in order to predict the score t on day n + 1 [ 3, 13 ].', 'such approaches are biased if there are overlapping days of train / test data.', 'to illustrate this problem we have followed the approach by canzian and musolesi [ 3 ], as one of the pioneering works on predicting depression with gps traces, on a longitudinal basis.', 'p3 predicting users instead of mood scores : most approaches merge all the instances from different subjects, in an attempt to build user - agnostic models in a randomised cross - validation framework [ 2, 9, 10,  #TAUTHOR_TAG.', 'this is problematic, especially when dealing with a small number of subjects, whose behaviour ( as captured through their data ) and mental health scores differ on an individual basis.', 'such approaches are in danger of "" predicting "" the user in the test set, since her ( test set ) features might be highly correlated with her features in the training set, and thus infer her average well - being score, based on the corresponding observations of the training set.', 'such approaches cannot guarantee that they will generalise on either a population - wide ( louocv ) or a personalised ( loiocv ) level.', 'in order to examine this effect in both a regression and a classification setting, we have followed the experimental framework by tsakalidis et al.  #TAUTHOR_TAG and jaques et al. [ 9 ]']",5
"['.  #TAUTHOR_TAG, a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.', 'from a textual perspective, this dataset consists of social media posts ( 1,']","['al.  #TAUTHOR_TAG, a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.', 'from a textual perspective, this dataset consists of social media posts ( 1, 854 / 5, 167 facebook / twitter posts ) and private messages ( 64, 221 / 132 / 47, 043']","['.  #TAUTHOR_TAG, a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.', 'from a textual perspective, this dataset consists of social media posts ( 1,']","['employed the dataset obtained by tsakalidis et al.  #TAUTHOR_TAG, a pioneering dataset which contains a mix of longitudinal textual and mobile phone usage data for 30 subjects.', 'from a textual perspective, this dataset consists of social media posts ( 1, 854 / 5, 167 facebook / twitter posts ) and private messages ( 64, 221 / 132 / 47, 043 facebook / twitter / sms messages ) sent by the subjects.', 'for our ground truth, we use the { positive, negative, mental well - being } mood scores ( in the ranges of,,, respectively ) derived from self - assessed psychological scales during the study period']",5
['1 we follow the setup in  #TAUTHOR_TAG : we'],['1 we follow the setup in  #TAUTHOR_TAG : we'],"['before the completion of a mood form.', 'in experiment 1 we follow the setup in  #TAUTHOR_TAG : we perform']","['followed the evaluation settings of two past works ( see section 3. 3 ), with the only difference being the use of 5 - fold cv instead of a train / dev / test split that was used in [ 9 ].', 'the features of every instance are extracted from the past day before the completion of a mood form.', 'in experiment 1 we follow the setup in  #TAUTHOR_TAG : we perform 5 - fold cv ( mixed ) using svm ( svr rbf ) and evaluate performance based on r 2 and rm se.', 'we compare the performance when tested under the loiocv / louocv setups, with and without the per - user feature normalisation step.', 'we also compare the performance of the mixed setting, when our model is trained on the one - hot - encoded user id only.', 'in experiment 2 we follow the setup in [ 9 ] : we label the instances as "" high "" ( "" low "" ), if they belong to the top - 30 % ( bottom - 30 % ) of mood score values ( "" uniq "" - for "" unique "" - setup ).', 'we train an svm classifier in 5 - fold cv using accuracy for evaluation and compare performance in the loiocv and louocv settings.', 'in order to further examine user bias, we perform the same experiments, this time by labelling the instances on a per - user basis ( "" pers "" - for "" personalised "" - setup ), aiming to predict the per - user high / low mood days 6.', 'table 3.', 'summary of experiments.', 'the highlighted settings indicate the settings used in the original papers ; "" period "" indicates the period before each mood form completion during which the features were extracted']",5
"['.  #TAUTHOR_TAG.', 'in']","['al.  #TAUTHOR_TAG.', 'in']","['.  #TAUTHOR_TAG.', 'in the mixed cases, the pattern is consistent with  #TAUTHOR_TAG, indicating that normalising the features on a per - user basis yields better results, when dealing with sparse textual features ( positive, negative, wellbeing targets ).', 'the explanation of this effect lies']","['1 : table 7 shows the results based on the evaluation setup of tsakalidis et al.  #TAUTHOR_TAG.', 'in the mixed cases, the pattern is consistent with  #TAUTHOR_TAG, indicating that normalising the features on a per - user basis yields better results, when dealing with sparse textual features ( positive, negative, wellbeing targets ).', ""the explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores."", 'this is why the per - user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones : the vocabulary used by the subjects for the other targets is more indicative of their identity.', 'in order to further support this statement, we trained the svr model using only the one - hot encoded user id as a feature, without any textual features.', 'our results yielded r 2 = { 0. 64, 0. 50, 0. 66 } and rm se = { 5. 50, 5. 32, 6. 50 } for the { positive, negative, wellbeing } targets, clearly demonstrating the user bias in the mixed setting.', '']",5
"['.  #TAUTHOR_TAG.', 'in']","['al.  #TAUTHOR_TAG.', 'in']","['.  #TAUTHOR_TAG.', 'in the mixed cases, the pattern is consistent with  #TAUTHOR_TAG, indicating that normalising the features on a per - user basis yields better results, when dealing with sparse textual features ( positive, negative, wellbeing targets ).', 'the explanation of this effect lies']","['1 : table 7 shows the results based on the evaluation setup of tsakalidis et al.  #TAUTHOR_TAG.', 'in the mixed cases, the pattern is consistent with  #TAUTHOR_TAG, indicating that normalising the features on a per - user basis yields better results, when dealing with sparse textual features ( positive, negative, wellbeing targets ).', ""the explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores."", 'this is why the per - user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones : the vocabulary used by the subjects for the other targets is more indicative of their identity.', 'in order to further support this statement, we trained the svr model using only the one - hot encoded user id as a feature, without any textual features.', 'our results yielded r 2 = { 0. 64, 0. 50, 0. 66 } and rm se = { 5. 50, 5. 32, 6. 50 } for the { positive, negative, wellbeing } targets, clearly demonstrating the user bias in the mixed setting.', '']",5
['.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],['##kalidis et al.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],['.  #TAUTHOR_TAG monitored the behaviour of 19 individuals'],"['##kalidis et al.  #TAUTHOR_TAG monitored the behaviour of 19 individuals over four months.', 'the subjects were asked to complete two psychological scales [ 25, 29 ] on a daily basis, leading to three target scores ( positive, negative, mental well - being ) ; various features from smartphones ( e. g., time spent on the preferred locations ) and textual features ( e. g., ngrams ) were extracted passively over the 24 hours preceding a mood form timestamp.', 'model training and evaluation was performed in a randomised ( mixed ) cross - validation setup, leading to high accuracy ( r 2 = 0. 76 ).', 'however, a case demonstrating the potential user bias is when the models are trained on the textual sources : initially the highest r 2 ( 0. 22 ) is achieved when a model is applied to the mental - wellbeing target ; by normalising the textual features on a per - user basis, the r 2 increases to 0. 65.', 'while this is likely to happen because the vocabulary used by different users is normalised, there is also the danger of over - fitting the trained model to the identity of the user.', 'to examine this potential, the loiocv / louocv setups need to be studied alongside the mixed validation approach, with and without the per - user feature normalisation step.', 'a similar issue is encountered in jaques et al. [ 9 ] who monitored 68 subjects over a period of a month.', 'four types of features were extracted from survey and smart devices carried by subjects.', 'self - reported scores on a daily basis served as the ground truth.', 'the authors labelled the instances with the top 30 % of all the scores as "" happy "" and the lowest 30 % as "" sad "" and randomly separated them into training, validation and test sets, leading to the same user bias issue.', 'since different users exhibit different mood scores on average  #TAUTHOR_TAG, by selecting instances from the top and bottom scores, one might end up separating users and convert the mood prediction task into a user identification one.', 'a more suitable task could have been to try to predict the highest and lowest scores of every individual separately, either in a loiocv or in a louocv setup.', 'while we focus on the works of tsakalidis et al.  #TAUTHOR_TAG and jaques et al. [ 9 ], similar experimental setups were also followed in [ 10 ], using the median of scores to separate the instances and performing five - fold cross - validation, and by bogomolov et al. in [ 2 ], working on']",7
"['snippet written in english  #TAUTHOR_TAG, which were used in']","['snippet written in english  #TAUTHOR_TAG, which were used in']","['some commonly used feature sets for every snippet written in english  #TAUTHOR_TAG, which were used in all experiments.', 'to ensure sufficient data density, we excluded users for']","['dataset 1, we first defined a "" user snippet "" as the concatenation of all texts generated by a user within a set time interval, such that the maximum time difference between two consecutive document timestamps is less than 20 minutes.', 'we performed some standard noise reduction steps ( converted text to lowercase, replaced urls / user mentions and performed language identification 7 and tokenisation [ 6 ] ).', 'given a mood form and a set of snippets produced by a user before the completion of a mood form, we extracted some commonly used feature sets for every snippet written in english  #TAUTHOR_TAG, which were used in all experiments.', 'to ensure sufficient data density, we excluded users for whom we had overall fewer than 25 snippets on the days before the completion of the mood form or fewer than 40 mood forms overall, leading to 27 users and 2, 368 mood forms.', 'for dataset 2, we extracted the features presented in table 4.', 'we only kept the users that had at least 10 self - reported stress questionnaires, leading to 44 users and 2, 146 instances.', 'for our random experiments used in p2, in dataset 1 we replaced the text representation of every snippet with random noise ( [UNK] = 0, σ = 1 ) of the same feature dimensionality ; in dataset 2, we replaced the actual inferred value of every activity / audio sample with a random inference class ; we also replaced each of the detected conversation samples and samples detected in a dark environment / locked / charging, with a random number ( < 100, uniformly distributed ) indicating the number of pseudo - detected samples.', 'table 5 presents the results on the basis of the methodology by likamwa et al. [ 13 ], along with the average scores reported in [ 13 ] - note that the range of the mood scores varies on a per - target basis ; hence, the reported results of different models should be compared among each other when tested on the same target.', 'as in [ 13 ], always predicting the average score ( avg ) for an unseen user performs better than applying a lr model trained on other users in a louocv setting.', 'if the same lr model used in louocv is trained without using the previously self - reported ground - truth scores ( model d, - mood ), its performance drops further.', 'this showcases that personalised models are needed for more table 5.', 'p1 : results following the approach in [ 13 ]']",7
"['.  #TAUTHOR_TAG.', 'in']","['al.  #TAUTHOR_TAG.', 'in']","['.  #TAUTHOR_TAG.', 'in the mixed cases, the pattern is consistent with  #TAUTHOR_TAG, indicating that normalising the features on a per - user basis yields better results, when dealing with sparse textual features ( positive, negative, wellbeing targets ).', 'the explanation of this effect lies']","['1 : table 7 shows the results based on the evaluation setup of tsakalidis et al.  #TAUTHOR_TAG.', 'in the mixed cases, the pattern is consistent with  #TAUTHOR_TAG, indicating that normalising the features on a per - user basis yields better results, when dealing with sparse textual features ( positive, negative, wellbeing targets ).', ""the explanation of this effect lies within the danger of predicting the user's identity instead of her mood scores."", 'this is why the per - user normalisation does not have any effect for the stress target, since for that we are using dense features derived from smartphones : the vocabulary used by the subjects for the other targets is more indicative of their identity.', 'in order to further support this statement, we trained the svr model using only the one - hot encoded user id as a feature, without any textual features.', 'our results yielded r 2 = { 0. 64, 0. 50, 0. 66 } and rm se = { 5. 50, 5. 32, 6. 50 } for the { positive, negative, wellbeing } targets, clearly demonstrating the user bias in the mixed setting.', '']",3
['( which does not use source language ) is  #TAUTHOR_TAG but'],['( which does not use source language ) is  #TAUTHOR_TAG but'],"['', 'the first attempt to build an end - to - end speech - to - text translation system ( which does not use source language ) is  #TAUTHOR_TAG but']","['spoken language translation ( slt ) systems integrate ( loosely or closely ) two main modules : source language speech recognition ( asr ) and source - to - target text translation ( mt ).', 'in these approaches, a symbolic sequence of words ( or characters ) in the source language is used as an intermediary representation during the speech translation process.', 'however, recent works have attempted to build end - to - end speech - to - text translation without using source language transcription during learning or decoding.', 'one attempt to translate directly a source speech signal into target language text is that of [ 1 ].', 'however, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end - to - end translation system.', 'the first attempt to build an end - to - end speech - to - text translation system ( which does not use source language ) is  #TAUTHOR_TAG but it was applied to a synthetic ( tts ) speech corpus.', 'a similar approach was then proposed and evaluated on a real speech corpus by [ 3 ].', 'this paper is a follow - up of  #TAUTHOR_TAG.', 'we now investigate end - to - end speech - to - text translation on a corpus of audiobooks - librispeech [ 4 ] - specifically augmented to perform end - to - end speech translation [ 5 ].', 'while previous works  #TAUTHOR_TAG 3 ] investigated the extreme case where source language transcription is not available during learning nor decoding ( unwritten language scenario defined in [ 6, 7 ] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training.', 'in this intermediate scenario, a unique ( endto - end ) model is trained to decode source speech into target text through a single pass ( which can be interesting if compact speech translation models are needed ).', 'this paper is organized as follows : after presenting our corpus in section 2, we present our end - to - end models in section 3.', 'section 4 describes our evaluation on two datasets : the synthetic dataset used in  #TAUTHOR_TAG and the audiobook dataset described in section 2.', '']",0
['( which does not use source language ) is  #TAUTHOR_TAG but'],['( which does not use source language ) is  #TAUTHOR_TAG but'],"['', 'the first attempt to build an end - to - end speech - to - text translation system ( which does not use source language ) is  #TAUTHOR_TAG but']","['spoken language translation ( slt ) systems integrate ( loosely or closely ) two main modules : source language speech recognition ( asr ) and source - to - target text translation ( mt ).', 'in these approaches, a symbolic sequence of words ( or characters ) in the source language is used as an intermediary representation during the speech translation process.', 'however, recent works have attempted to build end - to - end speech - to - text translation without using source language transcription during learning or decoding.', 'one attempt to translate directly a source speech signal into target language text is that of [ 1 ].', 'however, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end - to - end translation system.', 'the first attempt to build an end - to - end speech - to - text translation system ( which does not use source language ) is  #TAUTHOR_TAG but it was applied to a synthetic ( tts ) speech corpus.', 'a similar approach was then proposed and evaluated on a real speech corpus by [ 3 ].', 'this paper is a follow - up of  #TAUTHOR_TAG.', 'we now investigate end - to - end speech - to - text translation on a corpus of audiobooks - librispeech [ 4 ] - specifically augmented to perform end - to - end speech translation [ 5 ].', 'while previous works  #TAUTHOR_TAG 3 ] investigated the extreme case where source language transcription is not available during learning nor decoding ( unwritten language scenario defined in [ 6, 7 ] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training.', 'in this intermediate scenario, a unique ( endto - end ) model is trained to decode source speech into target text through a single pass ( which can be interesting if compact speech translation models are needed ).', 'this paper is organized as follows : after presenting our corpus in section 2, we present our end - to - end models in section 3.', 'section 4 describes our evaluation on two datasets : the synthetic dataset used in  #TAUTHOR_TAG and the audiobook dataset described in section 2.', '']",6
['( which does not use source language ) is  #TAUTHOR_TAG but'],['( which does not use source language ) is  #TAUTHOR_TAG but'],"['', 'the first attempt to build an end - to - end speech - to - text translation system ( which does not use source language ) is  #TAUTHOR_TAG but']","['spoken language translation ( slt ) systems integrate ( loosely or closely ) two main modules : source language speech recognition ( asr ) and source - to - target text translation ( mt ).', 'in these approaches, a symbolic sequence of words ( or characters ) in the source language is used as an intermediary representation during the speech translation process.', 'however, recent works have attempted to build end - to - end speech - to - text translation without using source language transcription during learning or decoding.', 'one attempt to translate directly a source speech signal into target language text is that of [ 1 ].', 'however, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end - to - end translation system.', 'the first attempt to build an end - to - end speech - to - text translation system ( which does not use source language ) is  #TAUTHOR_TAG but it was applied to a synthetic ( tts ) speech corpus.', 'a similar approach was then proposed and evaluated on a real speech corpus by [ 3 ].', 'this paper is a follow - up of  #TAUTHOR_TAG.', 'we now investigate end - to - end speech - to - text translation on a corpus of audiobooks - librispeech [ 4 ] - specifically augmented to perform end - to - end speech translation [ 5 ].', 'while previous works  #TAUTHOR_TAG 3 ] investigated the extreme case where source language transcription is not available during learning nor decoding ( unwritten language scenario defined in [ 6, 7 ] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training.', 'in this intermediate scenario, a unique ( endto - end ) model is trained to decode source speech into target text through a single pass ( which can be interesting if compact speech translation models are needed ).', 'this paper is organized as follows : after presenting our corpus in section 2, we present our end - to - end models in section 3.', 'section 4 describes our evaluation on two datasets : the synthetic dataset used in  #TAUTHOR_TAG and the audiobook dataset described in section 2.', '']",6
"['from btec ( follow - up to  #TAUTHOR_TAG.', 'we show that, while cascading two neural models for asr and mt gives the best results,']","['from btec ( follow - up to  #TAUTHOR_TAG.', 'we show that, while cascading two neural models for asr and mt gives the best results, end - to - end methods that incorporate the source language transcript come close in performance']","['from btec ( follow - up to  #TAUTHOR_TAG.', 'we show that, while cascading two neural models for asr and mt gives the best results, end - to - end methods that incorporate the source language transcript come close in performance']","['present baseline results on end - to - end automatic speech translation on a new speech translation corpus of audiobooks, and on a synthetic corpus extracted from btec ( follow - up to  #TAUTHOR_TAG.', 'we show that, while cascading two neural models for asr and mt gives the best results, end - to - end methods that incorporate the source language transcript come close in performance']",6
['( which does not use source language ) is  #TAUTHOR_TAG but'],['( which does not use source language ) is  #TAUTHOR_TAG but'],"['', 'the first attempt to build an end - to - end speech - to - text translation system ( which does not use source language ) is  #TAUTHOR_TAG but']","['spoken language translation ( slt ) systems integrate ( loosely or closely ) two main modules : source language speech recognition ( asr ) and source - to - target text translation ( mt ).', 'in these approaches, a symbolic sequence of words ( or characters ) in the source language is used as an intermediary representation during the speech translation process.', 'however, recent works have attempted to build end - to - end speech - to - text translation without using source language transcription during learning or decoding.', 'one attempt to translate directly a source speech signal into target language text is that of [ 1 ].', 'however, the authors focus on the alignment between source speech utterances and their text translation without proposing a complete end - to - end translation system.', 'the first attempt to build an end - to - end speech - to - text translation system ( which does not use source language ) is  #TAUTHOR_TAG but it was applied to a synthetic ( tts ) speech corpus.', 'a similar approach was then proposed and evaluated on a real speech corpus by [ 3 ].', 'this paper is a follow - up of  #TAUTHOR_TAG.', 'we now investigate end - to - end speech - to - text translation on a corpus of audiobooks - librispeech [ 4 ] - specifically augmented to perform end - to - end speech translation [ 5 ].', 'while previous works  #TAUTHOR_TAG 3 ] investigated the extreme case where source language transcription is not available during learning nor decoding ( unwritten language scenario defined in [ 6, 7 ] ), we also investigate, in this paper, a midway case where a certain amount of source language transcription is available during training.', 'in this intermediate scenario, a unique ( endto - end ) model is trained to decode source speech into target text through a single pass ( which can be interesting if compact speech translation models are needed ).', 'this paper is organized as follows : after presenting our corpus in section 2, we present our end - to - end models in section 3.', 'section 4 describes our evaluation on two datasets : the synthetic dataset used in  #TAUTHOR_TAG and the audiobook dataset described in section 2.', '']",5
"['aligned references with the google translate references.', 'we also mirror our experiments on the btec synthetic speech corpus, as a follow - up to  #TAUTHOR_TAG']","['aligned references with the google translate references.', 'we also mirror our experiments on the btec synthetic speech corpus, as a follow - up to  #TAUTHOR_TAG']","['our experiments using train only ( without extended train ).', 'furthermore, we double the training size by concatenating the aligned references with the google translate references.', 'we also mirror our experiments on the btec synthetic speech corpus, as a follow - up to  #TAUTHOR_TAG']","['paper focuses on the speech translation ( ast ) task of audiobooks from english to french, using the augmented librispeech corpus.', 'we compare a direct ( end - to - end ) approach, with a cascaded approach that combines a neural speech transcription ( asr ) model with a neural machine translation model ( mt ).', 'the asr and mt results are also reported as baselines for future uses of this corpus.', 'augmented librispeech contains 236 hours of speech in total, which is split into 4 parts : a test set of 4 hours, a dev set of 2 hours, a clean train set of 100 hours, and an extended train set with the remaining 130 hours.', 'table 1 gives detailed information about the size of each corpus.', 'all segments in the corpus were sorted according to their alignment confidence scores, as produced by the alignment software used by the authors of the corpus [ 5 ].', 'the test, dev and train sets correspond to the highest rated alignments.', 'the remaining data ( extended train ) is more noisy, as it contains more incorrect alignments.', 'the test set was manually checked, and incorrect alignments were removed.', 'we perform all our experiments using train only ( without extended train ).', 'furthermore, we double the training size by concatenating the aligned references with the google translate references.', 'we also mirror our experiments on the btec synthetic speech corpus, as a follow - up to  #TAUTHOR_TAG']",5
"['the three tasks, we use encoder - decoder models with attention [ 9, 10, 11,  #TAUTHOR_TAG 3 ].', 'because we']","['the three tasks, we use encoder - decoder models with attention [ 9, 10, 11,  #TAUTHOR_TAG 3 ].', 'because we']","['the three tasks, we use encoder - decoder models with attention [ 9, 10, 11,  #TAUTHOR_TAG 3 ].', 'because we want to share some parts of the model between tasks ( multi - task training ), the asr']","['the three tasks, we use encoder - decoder models with attention [ 9, 10, 11,  #TAUTHOR_TAG 3 ].', 'because we want to share some parts of the model between tasks ( multi - task training ), the asr and ast models use the same encoder architecture, and the ast and mt models use the same decoder architecture']",5
"['previously proposed encoder  #TAUTHOR_TAG.', 'it takes as input a sequence of audio features :']","['previously proposed encoder  #TAUTHOR_TAG.', 'it takes as input a sequence of audio features :']","['previously proposed encoder  #TAUTHOR_TAG.', 'it takes as input a sequence of audio features : x = ( x 1,..., x tx ) ∈ r tx×n.', 'like  #TAUTHOR_TAG, these features']","['speech encoder is a mix between the convolutional encoder presented in [ 3 ] and our previously proposed encoder  #TAUTHOR_TAG.', 'it takes as input a sequence of audio features : x = ( x 1,..., x tx ) ∈ r tx×n.', 'like  #TAUTHOR_TAG, these features are given as input to two non - linear ( tanh ) layers, which output new features of size n.', 'like [ 3 ], this new set of features is then passed to a stack of two convolutional layers.', 'each layer applies 16 convolution filters of shape ( 3, 3, depth ) with a stride of ( 2, 2 ) w. r. t.', 'time and feature dimensions ; depth is 1 for the first layer, and 16 for the second layer.', 'we get features of shape ( t x / 2, n / 2, 16 ) after the 1 st layer, and ( t x / 4, n / 4, 16 ) after the 2 nd layer.', 'this latter tensor is flattened with shape ( t x = t x / 4, 4n ) before being passed to a stack of three bidirectional lstms.', 'this set of features has 1 / 4th the time length of the initial features, which speeds up training, as the complexity of the model is quadratic with respect to the source length.', 'in our models, we use n = 128, which gives features of size 512.', 'the last bidirectional lstm layer computes a sequence of annotations h = h 1, · · ·, h t x, where each annotation h i is a concatenation of the corresponding forward and backward states :', 'this model differs from [ 2 ], which did not use convolutions, but time pooling between each lstm layer, resulting in a shorter sequence ( pyramidal encoder )']",5
"['ms, following [ 14,  #TAUTHOR_TAG.', 'we tokenize']","['ms, following [ 14,  #TAUTHOR_TAG.', 'we tokenize']","['yaafe [ 13 ], to extract 40 mfcc features and frame energy for each frame with a step size of 10 ms and window size of 40 ms, following [ 14,  #TAUTHOR_TAG.', 'we tokenize']","['files were preprocessed using yaafe [ 13 ], to extract 40 mfcc features and frame energy for each frame with a step size of 10 ms and window size of 40 ms, following [ 14,  #TAUTHOR_TAG.', 'we tokenize and lowercase all the text, and normalize the punctuation, with the moses scripts 3 for librispeech are of size 46 for english ( transcriptions ) and 167 for french ( translation ).', 'the decoder outputs are always at the character - level ( for ast, mt and asr ).', 'for the mt task, the librispeech english ( source ) side is preprocessed into subword units [ 15 ].', 'we limit the number of merge operations to 30k, which gives a vocabulary of size 27k.', 'the mt encoder for btec takes entire words as input.', 'our btec models use an lstm size of m = m = 256, while the librispeech models use a cell size of 512, except for the speech encoder layers which use a cell size of m = 256 in each direction.', 'we use character embeddings of size k = 64 for btec, and k = 128 for librispeech.', 'the mt encoders are more shallow, with a single bidirectional layer.', 'the source embedding sizes for words ( btec ) and subwords ( librispeech ) are respectively 128 and 256.', 'the input layers in the speech encoders have a size of 256 for the first layer and n = 128 for the second.', 'the lib - rispeech decoders use an output layer size of l = 512.', 'for btec, we do not use any non - linear output layer, as we found that this led to overfitting']",5
"['previously proposed encoder  #TAUTHOR_TAG.', 'it takes as input a sequence of audio features :']","['previously proposed encoder  #TAUTHOR_TAG.', 'it takes as input a sequence of audio features :']","['previously proposed encoder  #TAUTHOR_TAG.', 'it takes as input a sequence of audio features : x = ( x 1,..., x tx ) ∈ r tx×n.', 'like  #TAUTHOR_TAG, these features']","['speech encoder is a mix between the convolutional encoder presented in [ 3 ] and our previously proposed encoder  #TAUTHOR_TAG.', 'it takes as input a sequence of audio features : x = ( x 1,..., x tx ) ∈ r tx×n.', 'like  #TAUTHOR_TAG, these features are given as input to two non - linear ( tanh ) layers, which output new features of size n.', 'like [ 3 ], this new set of features is then passed to a stack of two convolutional layers.', 'each layer applies 16 convolution filters of shape ( 3, 3, depth ) with a stride of ( 2, 2 ) w. r. t.', 'time and feature dimensions ; depth is 1 for the first layer, and 16 for the second layer.', 'we get features of shape ( t x / 2, n / 2, 16 ) after the 1 st layer, and ( t x / 4, n / 4, 16 ) after the 2 nd layer.', 'this latter tensor is flattened with shape ( t x = t x / 4, 4n ) before being passed to a stack of three bidirectional lstms.', 'this set of features has 1 / 4th the time length of the initial features, which speeds up training, as the complexity of the model is quadratic with respect to the source length.', 'in our models, we use n = 128, which gives features of size 512.', 'the last bidirectional lstm layer computes a sequence of annotations h = h 1, · · ·, h t x, where each annotation h i is a concatenation of the corresponding forward and backward states :', 'this model differs from [ 2 ], which did not use convolutions, but time pooling between each lstm layer, resulting in a shorter sequence ( pyramidal encoder )']",3
"['combines the pre - trained and multi - task models.', 'contrary to  #TAUTHOR_TAG, we only present mono - reference results.', 'asr mono asr multi mt mono mt multi']","['combines the pre - trained and multi - task models.', 'contrary to  #TAUTHOR_TAG, we only present mono - reference results.', 'asr mono asr multi mt mono mt multi fig. 1 : augmented']","['- to - end, but its encoder and decoder are initialized with our asr and mt table 3 : results of the ast task on btec test.', '† was obtained with an ensemble of 5 models, while we use ensembles of 2 models.', 'the non - cascaded ensemble combines the pre - trained and multi - task models.', 'contrary to  #TAUTHOR_TAG, we only present mono - reference results.', 'asr mono asr multi mt mono mt multi fig. 1 : augmented']","['train our models with adam [ 16 ], with a learning rate of 0. 001, and a mini - batch size of 64 for btec, and 32 for librispeech ( because of memory constraints ).', 'we use variational dropout [ 17 ], i. e., the same dropout mask is applied to all elements in a batch at all time steps, with a rate of 0. 2 for librispeech and 0. 4 for btec.', 'in the mt tasks, we also drop source and target symbols at random, with probability 0. 2.', 'dropout is not applied on recurrent connections [ 18 ].', 'we train all our models on librispeech train augmented with the google translate references, i. e., the source side of the corpus ( speech ) is duplicated, and the target side ( translations ) is a concatenation of the aligned references with the google translate references.', 'because of gpu memory limits, we set the maximum length to 1400 frames for librispeech input, and 300 characters for its output.', 'this covers about 90 % of the training corpus.', 'longer sequences are kept but truncated to the maximum size.', 'we evaluate our models on the dev set every 1000 mini - batch updates using bleu for ast and mt, and wer for asr, and keep the best performing checkpoint for final evaluation on the test set.', 'our models are implemented with tensorflow [ 19 ] as part of the lig - cristal nmt toolkit 4.', 'table 2 presents the results for the asr and mt tasks on btec and librispeech.', 'the mt task ( and by extension the ast task ) on librispeech ( translating novels ) looks particularly challenging, as we observe bleu scores around 20 % 5 for automatic speech translation ( ast ), we try four settings.', 'the cascaded model combines both the asr and mt models ( as a pipeline ).', 'the end - to - end model ( described in section 3 ) does not make any use of source language transcripts.', 'the pre - trained model is identical to end - to - end, but its encoder and decoder are initialized with our asr and mt table 3 : results of the ast task on btec test.', '† was obtained with an ensemble of 5 models, while we use ensembles of 2 models.', 'the non - cascaded ensemble combines the pre - trained and multi - task models.', 'contrary to  #TAUTHOR_TAG, we only present mono - reference results.', 'asr mono asr multi mt mono mt multi fig. 1 : augmented']",4
"['ast on the btec corpus, compared to  #TAUTHOR_TAG']","['ast on the btec corpus, compared to  #TAUTHOR_TAG']","['ast on the btec corpus, compared to  #TAUTHOR_TAG']","['.', 'the multi - task model is also pre - trained, but continues training for all tasks, by alternating updates like [ 20 ], with 60 % of updates for ast and 20 % for asr and mt.', 'table 3 and 4 present the results for the end - to - end ast task on btec and librispeech.', 'on both corpora, we show that : ( 1 ) it is possible to train compact end - to - end ast models with a performance close to cascaded models ; ( 2 ) pretraining and multi - task learning 6 improve ast performance ;', '( 3 ) contrary to [ 3 ], in both btec and librispeech settings, best ast performance is observed when a symbolic sequence of symbols in the source language is used as an intermediary representation during the speech translation process ( cascaded system ) ; ( 4 ) finally, the ast results presented on lib - rispeech demonstrate that our augmented corpus is useful, although challenging, to benchmark end - to - end ast systems on real speech at a large scale.', 'we hope that our baseline on augmented librispeech will be challenged in the future.', 'the large improvements on mt and ast on the btec corpus, compared to  #TAUTHOR_TAG are mostly due to our use of a better decoder, which outputs characters instead of words.', '6 bleu end - to - end pre - train multi - task fig. 2 : dev bleu scores on 3 models for end - to - end ast of audiobooks.', 'best scores on the dev set for the end - to - end ( mono - task ), pre - train and multi - task models were achieved at steps 369k, 129k and 95k.', 'figure 1 shows the evolution of bleu and wer scores for mt and asr tasks with single models, and when we continue training them as part of a multi - task model.', 'the multi - task procedure does more updates on ast, which explains the degraded results, but we observe that the speech encoder and text decoder are still able to generalize well to other tasks.', 'figure 2 shows the evolution of dev bleu scores for our three ast models on librispeech.', 'we see that pre - training helps the model converge much faster.', 'eventually, the end - to - end system reaches a similarly good solution, but after three times as many updates.', 'multi - task training does not seem to be helpful when combined with pre - training']",4
"['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstan']","['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']","['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']","['', 'then generate the audio signal. normalizing the written form text to its spoken form is difficult due to the following bottlenecks : 1. lack of supervision - there is no incentive for people to produce spoken form text. thus, it is hard', 'to obtain a supervised dataset for training machine learning models ; 2. ambiguity - for written text', ', a change in context may require a different normalization. for example, "" 2 / 3', '"" can be verbalized as a date or fraction depending on the meaning of the', 'sentence. traditionally, the task of nsw normalization has been approached by manually authoring grammars in', 'the form of finite - state transducers  #AUTHOR_TAG such as integer grammars ( e. g., "" 26 "" → "" twenty six', '"" ) or time grammars ( e. g., "" 5 : 26 "" → "" five twenty six "" ). constructing such grammars is time consuming and error - prone and requires extensive linguistic knowledge and programming proficiency. recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']",0
"['##n  #TAUTHOR_TAG.', 'to overcome']","['to tn and itn  #TAUTHOR_TAG.', 'to overcome']","['##n  #TAUTHOR_TAG.', 'to overcome']","[', methods based on neural networks have been applied to tn and itn  #TAUTHOR_TAG.', 'to overcome one of the biggest problems - a lack of supervision, wfsts have been used to transform large amounts of written - form text to its spoken form.', 'researchers hope a vast amount of such data can counteract the errors inherited in wfst - based models.', 'recent data - driven approaches examine window - based sequence - to - sequence ( seq2seq ) models and convolutional neural networks ( cnn ) to normalize a central piece of text with the help of context  #TAUTHOR_TAG.', 'window - based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special < self > token.', 'hybrid neural / wfst models have also been proposed and applied to the text normalization problem  #AUTHOR_TAG.', 'tokens in the input are first tagged with labels using machine learned models whereupon a handcrafted grammar corresponding to each label conducts conversion.', 'in both methods, a tagger is needed to first segment / label the input tokens and conversion must be applied to each segment to normalize a full sentence.', 'our seq2seq model does not require the aforementioned tagger ( although could benefit from the tagger as we will show later ) and directly translates a written - form sentence to its spoken form without grammars']",0
"['##n  #TAUTHOR_TAG.', 'to overcome']","['to tn and itn  #TAUTHOR_TAG.', 'to overcome']","['##n  #TAUTHOR_TAG.', 'to overcome']","[', methods based on neural networks have been applied to tn and itn  #TAUTHOR_TAG.', 'to overcome one of the biggest problems - a lack of supervision, wfsts have been used to transform large amounts of written - form text to its spoken form.', 'researchers hope a vast amount of such data can counteract the errors inherited in wfst - based models.', 'recent data - driven approaches examine window - based sequence - to - sequence ( seq2seq ) models and convolutional neural networks ( cnn ) to normalize a central piece of text with the help of context  #TAUTHOR_TAG.', 'window - based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special < self > token.', 'hybrid neural / wfst models have also been proposed and applied to the text normalization problem  #AUTHOR_TAG.', 'tokens in the input are first tagged with labels using machine learned models whereupon a handcrafted grammar corresponding to each label conducts conversion.', 'in both methods, a tagger is needed to first segment / label the input tokens and conversion must be applied to each segment to normalize a full sentence.', 'our seq2seq model does not require the aforementioned tagger ( although could benefit from the tagger as we will show later ) and directly translates a written - form sentence to its spoken form without grammars']",0
"['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstan']","['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']","['especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']","['', 'then generate the audio signal. normalizing the written form text to its spoken form is difficult due to the following bottlenecks : 1. lack of supervision - there is no incentive for people to produce spoken form text. thus, it is hard', 'to obtain a supervised dataset for training machine learning models ; 2. ambiguity - for written text', ', a change in context may require a different normalization. for example, "" 2 / 3', '"" can be verbalized as a date or fraction depending on the meaning of the', 'sentence. traditionally, the task of nsw normalization has been approached by manually authoring grammars in', 'the form of finite - state transducers  #AUTHOR_TAG such as integer grammars ( e. g., "" 26 "" → "" twenty six', '"" ) or time grammars ( e. g., "" 5 : 26 "" → "" five twenty six "" ). constructing such grammars is time consuming and error - prone and requires extensive linguistic knowledge and programming proficiency. recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data -', 'driven approaches to this field  #TAUTHOR_TAG. in this paper, we present our approach to nonstandar', '##d text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively']",1
"['##n  #TAUTHOR_TAG.', 'to overcome']","['to tn and itn  #TAUTHOR_TAG.', 'to overcome']","['##n  #TAUTHOR_TAG.', 'to overcome']","[', methods based on neural networks have been applied to tn and itn  #TAUTHOR_TAG.', 'to overcome one of the biggest problems - a lack of supervision, wfsts have been used to transform large amounts of written - form text to its spoken form.', 'researchers hope a vast amount of such data can counteract the errors inherited in wfst - based models.', 'recent data - driven approaches examine window - based sequence - to - sequence ( seq2seq ) models and convolutional neural networks ( cnn ) to normalize a central piece of text with the help of context  #TAUTHOR_TAG.', 'window - based methods have the advantage of limiting the output vocabulary size, as most tokens that do not need to be transformed are labeled with a special < self > token.', 'hybrid neural / wfst models have also been proposed and applied to the text normalization problem  #AUTHOR_TAG.', 'tokens in the input are first tagged with labels using machine learned models whereupon a handcrafted grammar corresponding to each label conducts conversion.', 'in both methods, a tagger is needed to first segment / label the input tokens and conversion must be applied to each segment to normalize a full sentence.', 'our seq2seq model does not require the aforementioned tagger ( although could benefit from the tagger as we will show later ) and directly translates a written - form sentence to its spoken form without grammars']",1
"[' #TAUTHOR_TAG, we implement a seq2seq']","[' #TAUTHOR_TAG, we implement a seq2seq']","[' #TAUTHOR_TAG, we implement a seq2seq model']","[' #TAUTHOR_TAG, we implement a seq2seq model trained on window - based data.', 'table 1 illustrates the window - based model\'s training examples corresponding to one sentence "" wake me up at 8 am. "" which is broken down into 6 pairs.', '< n > and < / n > indicate the center of the window.', 'a window center might contain 1 or more words ( e. g., "" 8 am "" ) and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as time, date, ordinal  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG, we implement a seq2seq']","[' #TAUTHOR_TAG, we implement a seq2seq']","[' #TAUTHOR_TAG, we implement a seq2seq model']","[' #TAUTHOR_TAG, we implement a seq2seq model trained on window - based data.', 'table 1 illustrates the window - based model\'s training examples corresponding to one sentence "" wake me up at 8 am. "" which is broken down into 6 pairs.', '< n > and < / n > indicate the center of the window.', 'a window center might contain 1 or more words ( e. g., "" 8 am "" ) and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as time, date, ordinal  #TAUTHOR_TAG.', '']",5
"['##ted text from  #TAUTHOR_TAG.', 'the set']","['of parallel written / speech formatted text from  #TAUTHOR_TAG.', 'the set']","['##ted text from  #TAUTHOR_TAG.', 'the set']","['data for the window - based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written / speech formatted text from  #TAUTHOR_TAG.', ""the set consists of wikipedia text which was processed through google tts's kestrel text normalization system relying primarily on handcrafted rules to produce speech - formatted text."", 'although a large parallel dataset is available for english, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.', 'therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.', 'as summarized in table 2, both window - based and sentencebased models are trained with 500k training instances.', 'our datasets were randomly sampled from a set of 4. 9m sentences in the training data portion of the  #TAUTHOR_TAG data release and split into training, validation, and test data.', 'however, the training data for window - based and sentencebased models are not identical due to differences in input configurations.', 'while the window - based model uses 500k randomly sampled windows, the sentence - based models use 500k sentences.', 'for testing, 62. 5k identical test sentences are used across all models.', 'in order to decode sentences with the window - based model, sentences are first segmented into windows before inference.', 'among 16 edit labels available in the dataset release, we found the normalization target for table 2 : size of training, validation, and test datasets.', 'for the window - baseline, the data are pairs of windows and the normalization of the central piece of the window.', 'for the sent - baseline and subword models, the data are pairs of sentences but in different formats - sent - baseline : ( character sequence, word sequence ) ; subword : ( subword sequence, subword sequence ).', 'all models are evaluated on the same set of 62. 5k sentences']",5
"['##ted text from  #TAUTHOR_TAG.', 'the set']","['of parallel written / speech formatted text from  #TAUTHOR_TAG.', 'the set']","['##ted text from  #TAUTHOR_TAG.', 'the set']","['data for the window - based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written / speech formatted text from  #TAUTHOR_TAG.', ""the set consists of wikipedia text which was processed through google tts's kestrel text normalization system relying primarily on handcrafted rules to produce speech - formatted text."", 'although a large parallel dataset is available for english, we consider the feasibility of developing neural models for other languages which may not have text normalization systems in place.', 'therefore, we choose to scale the training data size to a limited set of text which could be generated by annotators in a reasonable time frame.', 'as summarized in table 2, both window - based and sentencebased models are trained with 500k training instances.', 'our datasets were randomly sampled from a set of 4. 9m sentences in the training data portion of the  #TAUTHOR_TAG data release and split into training, validation, and test data.', 'however, the training data for window - based and sentencebased models are not identical due to differences in input configurations.', 'while the window - based model uses 500k randomly sampled windows, the sentence - based models use 500k sentences.', 'for testing, 62. 5k identical test sentences are used across all models.', 'in order to decode sentences with the window - based model, sentences are first segmented into windows before inference.', 'among 16 edit labels available in the dataset release, we found the normalization target for table 2 : size of training, validation, and test datasets.', 'for the window - baseline, the data are pairs of windows and the normalization of the central piece of the window.', 'for the sent - baseline and subword models, the data are pairs of sentences but in different formats - sent - baseline : ( character sequence, word sequence ) ; subword : ( subword sequence, subword sequence ).', 'all models are evaluated on the same set of 62. 5k sentences']",5
"['forbes dot com "" ).', 'therefore, we exclude electronic data in our experiments.', 'there are large numbers of < self > tokens present in the dataset.', 'we follow  #TAUTHOR_TAG "" tokens to']","['r b e s dot c o m "" ( as opposed to "" forbes dot com "" ).', 'therefore, we exclude electronic data in our experiments.', 'there are large numbers of < self > tokens present in the dataset.', 'we follow  #TAUTHOR_TAG "" tokens to 10 % of the data.', '']","['forbes dot com "" ).', 'therefore, we exclude electronic data in our experiments.', 'there are large numbers of < self > tokens present in the dataset.', 'we follow  #TAUTHOR_TAG "" tokens to']","['electronic text is not suitable for our system as it primarily reads out urls letter by letter, e. g., "" forbes. com "" → "" f o r b e s dot c o m "" ( as opposed to "" forbes dot com "" ).', 'therefore, we exclude electronic data in our experiments.', 'there are large numbers of < self > tokens present in the dataset.', 'we follow  #TAUTHOR_TAG "" tokens to 10 % of the data.', 'for training sentence - based models, the source sentence is segmented into characters while the target sentence is broken into tokens.', 'for the subword model, both the source and target sentences are segmented into subword sequences.', 'subword units are concatenated to words for evaluation']",5
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",5
"['capitalization :', 'upper, lower, mixed, nonalphanumerical, foreign characters ; 2 ) position :  #AUTHOR_TAG.', 'edit labels are the most expensive to obtain in real life.', 'our labels are generated directly from the google fst  #TAUTHOR_TAG.', '']","['capitalization :', 'upper, lower, mixed, nonalphanumerical, foreign characters ; 2 ) position :  #AUTHOR_TAG.', 'edit labels are the most expensive to obtain in real life.', 'our labels are generated directly from the google fst  #TAUTHOR_TAG.', '']","['use the following linguistic features : 1 ) capitalization :', 'upper, lower, mixed, nonalphanumerical, foreign characters ; 2 ) position :  #AUTHOR_TAG.', 'edit labels are the most expensive to obtain in real life.', 'our labels are generated directly from the google fst  #TAUTHOR_TAG.', 'each type of feature is represented by a one - hot encoding.', 'to combine linguistic features with sub']","['use the following linguistic features : 1 ) capitalization :', 'upper, lower, mixed, nonalphanumerical, foreign characters ; 2 ) position :  #AUTHOR_TAG.', 'edit labels are the most expensive to obtain in real life.', 'our labels are generated directly from the google fst  #TAUTHOR_TAG.', 'each type of feature is represented by a one - hot encoding.', ""to combine linguistic features with subword units, one can add or concatenate each subword's embedding with its corresponding linguistic feature embedding and feed a combined embedding to the bi - lstm encoder."", 'or, a multi - layer perceptron ( mlp ) can be applied to combine information in a non - linear way.', 'our experiments find that concatenation outperforms the other two methods.', 'in table 4 we can see that the subword model with linguistic features produces the lowest ser ( 0. 78 % ) and wer ( 0. 17 % ).', 'in addition, results from the ablation study show that each feature makes a positive contribution to the model.', 'however, edit labels seem to make the strongest contribution.', 'we acknowledge that edit labels may not always be readily available.', 'the model which utilizes all linguistic features except for edit labels still shows a 16 % relative ser reduction and 14 % wer reduction over the subword model without linguistic features']",5
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",6
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",4
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",4
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",4
"['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input']","['first approach replicates the window - based seq2seq model of  #TAUTHOR_TAG.', 'the model encodes the central piece of text ( 1 or more tokens ) including its context of n previous and following tokens at the character level.', 'the output is a target token or a sequence of tokens.', 'the input vocabulary consists of 250 common characters including letters, digits and symbols ( e. g., $ ).', 'the decoder vocabulary consists of 1k tokens including < self > and < sil >, the latter of which is used to normalize punctuation.', ' #AUTHOR_TAG, we use a stacked ( 2 - layer ) bi - directional long short term memory network ( bi - lstm ) as encoder and a stacked ( 2 - layer ) lstm as decoder.', 'we use 512 hidden states for the ( bi - ) lstm.', 'a softmax output distribution is computed over output vocabulary at each decoding step.', 'decoding uses the attention mechanism from  #AUTHOR_TAG and a beam size of 5.', 'word and character embeddings are trained from scratch.', 'we use the opennmt toolkit  #AUTHOR_TAG to train our models on a single p2. 8xlarge amazon ec2 instance.', 'models were trained with stochastic gradient descent ( sgd ) on 200k timesteps ( approximately 13 epochs ).', '']",3
"['proposed in  #TAUTHOR_TAG, where each task is optimized for a fixed number of parameter updates ( or mini - batches ) before switching to the', 'next task ( which is a different language pair )']","['proposed in  #TAUTHOR_TAG, where each task is optimized for a fixed number of parameter updates ( or mini - batches ) before switching to the', 'next task ( which is a different language pair )']","['requires days of gpu computing  #AUTHOR_TAG ) in tensorflow  #AUTHOR_TAG seq2seq model  #AUTHOR_TAG. to', 'avoid complexities of asynchronous parallel training with shared parameter server  #AUTHOR_TAG, the architecture in fig. 2 and fig. 3 instead can be trained using the alternating training approach proposed in  #TAUTHOR_TAG, where each task is optimized for a fixed number of parameter updates ( or mini - batches ) before switching to the', 'next task ( which is a different language pair ). although such alternating approach prolongs the training process, it is preferred for', 'simplicity and robustness reasons. once produced within']","['', 'and video captioning  #AUTHOR_TAG, unsupervised learning of document representations by autoencoders  #AUTHOR_TAG. these recent deep learning breakthroughs along with massively parallel gpu computing allow addressing the media monitoring tasks in the completely new end - toend manner rather than relying on the legacy nlp pipelines. the novelty of the summa project approach is that all languages covered by the project ( table 1 ) can be', 'embedded in the same vectorspace by means of joint multitask learning  #AUTHOR_TAG of eight lstm - rnn translational autoencoders with hidden layer parameters shared as illustrated in fig', '. 2. sharing the same vectorspace for sentences in all project languages enables accurate multilingual news story clustering without resorting to the clustering of the less accurate target ( english ) language machine translations. this shared vectorspace approach extends also to the unsupervised multi - task learning of language models from the large monolingual corpora ( fig. 3 ), which', 'is crucial for low - resourced languages : having a generic language model learned in parallel from', 'the monolingual corpora reduces  #AUTHOR_TAG the need for large supervised parallel corpora to achieve the same translational accuracy for the fig. 2 setup. the joint training of seventeen translational and samelanguage autoencoders with shared parameters ( fig. 2 and fig. 3 together ) to our knowledge has not been attempted so far. even training of a single state - of - the - art sentencelevel translational autoencoder requires days of gpu computing  #AUTHOR_TAG ) in tensorflow  #AUTHOR_TAG seq2seq model  #AUTHOR_TAG. to', 'avoid complexities of asynchronous parallel training with shared parameter server  #AUTHOR_TAG, the architecture in fig. 2 and fig. 3 instead can be trained using the alternating training approach proposed in  #TAUTHOR_TAG, where each task is optimized for a fixed number of parameter updates ( or mini - batches ) before switching to the', 'next task ( which is a different language pair ). although such alternating approach prolongs the training process, it is preferred for', 'simplicity and robustness reasons. once produced within summa project, these translational autoencoders with shared vectorspace will be a unique language resource of likely interest also to the wider nlp community for multilingual applications outside the media monitoring domain']",0
['is not compatible with the multi - task multilingual translation models  #TAUTHOR_TAG described in the previous'],['is not compatible with the multi - task multilingual translation models  #TAUTHOR_TAG described in the previous'],"['translation attention mechanism  #AUTHOR_TAG has been shown to be highly beneficial for bi - lingual neural translation of long sentences, but it is not compatible with the multi - task multilingual translation models  #TAUTHOR_TAG described in the previous section and character - level translation models  #AUTHOR_TAG described in this section.', '']","['translation attention mechanism  #AUTHOR_TAG has been shown to be highly beneficial for bi - lingual neural translation of long sentences, but it is not compatible with the multi - task multilingual translation models  #TAUTHOR_TAG described in the previous section and character - level translation models  #AUTHOR_TAG described in this section.', '']",0
"['generation  #TAUTHOR_TAG ; chan et al. 2019 ).', 'inspired by these advances, we propose a new gan architecture for keyphras']","['keyphrase generation  #TAUTHOR_TAG ; chan et al. 2019 ).', 'inspired by these advances, we propose a new gan architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human - curated and machine - generated keyphrases']","['keyphrase generation  #TAUTHOR_TAG ; chan et al. 2019 ).', 'inspired by these advances, we propose a new gan architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human - curated and machine - generated keyphrases']","['##phrases are employed to capture the most salient topics of a long document and are indexed in databases for convenient retrieval.', 'researchers annotate their scientific publications with high quality keyphrases to ensure discoverability in large scientific repositories.', 'keyphrases could either be extractive ( part of the document ) or abstractive.', 'keyphrase generation is the process of predicting both extractive and abstractive keyphrases from a given document.', 'this process is similar to abstractive summarization but instead of a summary the models generate keyphrases.', 'researchers have achieved considerable success in the field of abstractive summarization using conditional - gans ( wang and lee 2018 ).', 'there has also been growing interest in deep learning models for keyphrase generation  #TAUTHOR_TAG ; chan et al. 2019 ).', 'inspired by these advances, we propose a new gan architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human - curated and machine - generated keyphrases']",1
"['are trained in an alternating fashion  #TAUTHOR_TAG.', 'copyright']","['are trained in an alternating fashion  #TAUTHOR_TAG.', 'copyright']","['are trained in an alternating fashion  #TAUTHOR_TAG.', 'copyright']","['with most gan architectures, our model also consists of a generator ( g ) and discriminator ( d ), which are trained in an alternating fashion  #TAUTHOR_TAG.', 'copyright c 2019, association for the advancement of artificial intelligence ( www. aaai. org ).', 'all rights reserved.', '']",5
"['are trained in an alternating fashion  #TAUTHOR_TAG.', 'copyright']","['are trained in an alternating fashion  #TAUTHOR_TAG.', 'copyright']","['are trained in an alternating fashion  #TAUTHOR_TAG.', 'copyright']","['with most gan architectures, our model also consists of a generator ( g ) and discriminator ( d ), which are trained in an alternating fashion  #TAUTHOR_TAG.', 'copyright c 2019, association for the advancement of artificial intelligence ( www. aaai. org ).', 'all rights reserved.', '']",5
"['annotated with dialog acts  #TAUTHOR_TAG.', 'the corpus contains 122 email threads with 360 messages, 1734 utterances and']","['annotated with dialog acts  #TAUTHOR_TAG.', 'the corpus contains 122 email threads with 360 messages, 1734 utterances and 20, 740 word tokens.', 'we trained an']","['##ron email threads which has been previously annotated with dialog acts  #TAUTHOR_TAG.', 'the corpus contains 122 email threads with 360 messages, 1734 utterances and']","['our study, we use a small corpus of enron email threads which has been previously annotated with dialog acts  #TAUTHOR_TAG.', 'the corpus contains 122 email threads with 360 messages, 1734 utterances and 20, 740 word tokens.', 'we trained an annotator using the definition for odp given in section 3. she was given full email threads whose messages were already segmented into utterances.', 'she identified 86 utterances ( about 5 % ) to have an odp.', '1 in order to validate the annotations, we trained another annotator using the same definitions and examples and had him annotate 46 randomly selected threads from the corpus, which contained a total of 595 utterances ( 34. 3 % of whole corpus ).', 'we obtained a reasonable inter annotator agreement, κ value, of 0. 669, which validates the annotations while confirming that the task is not a trivial one']",5
"['commit and conventional ( see  #TAUTHOR_TAG for details ).', 'for example, for']","['of 5 dialog acts : requestaction, requestinformation, inform, commit and conventional ( see  #TAUTHOR_TAG for details ).', 'for example, for']","['of 5 dialog acts : requestaction, requestinformation, inform, commit and conventional ( see  #TAUTHOR_TAG for details ).', 'for example, for utterance s2, fv would be']","['present experiments using counts of three types of ngrams : lemma ngrams ( ln ), pos ngrams ( pn ) and mixed ngrams ( mn we also used a feature ( fv ) to denote the first verb lemma in the utterance.', 'since odps, like dialog acts, constrain how the addressee should react, we also include dialog acts as features ( da ).', 'we use the manual gold dialog act annotations present in our corpus, which use a very small dialog act tag set.', 'an utterance has one of 5 dialog acts : requestaction, requestinformation, inform, commit and conventional ( see  #TAUTHOR_TAG for details ).', ""for example, for utterance s2, fv would be'need'and da would be'inform '.""]",5
"['of  #TAUTHOR_TAG,']","['da tagger of  #TAUTHOR_TAG,']","['instead use the da tagger of  #TAUTHOR_TAG,']","['also evaluate the performance of our odp tagger without using gold da tags.', 'we instead use the da tagger of  #TAUTHOR_TAG, which we re - trained using the training sets for each of our cross validation folds, applying it to the test set of that fold.', 'we then did cross validation for the odp tagger using gold dialog acts for training and automatically tagged dialog acts for testing.', '']",5
"['commit and conventional ( see  #TAUTHOR_TAG for details ).', 'for example, for']","['of 5 dialog acts : requestaction, requestinformation, inform, commit and conventional ( see  #TAUTHOR_TAG for details ).', 'for example, for']","['of 5 dialog acts : requestaction, requestinformation, inform, commit and conventional ( see  #TAUTHOR_TAG for details ).', 'for example, for utterance s2, fv would be']","['present experiments using counts of three types of ngrams : lemma ngrams ( ln ), pos ngrams ( pn ) and mixed ngrams ( mn we also used a feature ( fv ) to denote the first verb lemma in the utterance.', 'since odps, like dialog acts, constrain how the addressee should react, we also include dialog acts as features ( da ).', 'we use the manual gold dialog act annotations present in our corpus, which use a very small dialog act tag set.', 'an utterance has one of 5 dialog acts : requestaction, requestinformation, inform, commit and conventional ( see  #TAUTHOR_TAG for details ).', ""for example, for utterance s2, fv would be'need'and da would be'inform '.""]",0
"['##s.', '1  #TAUTHOR_TAG challenged the view that using one']","['10, 000 prepositions.', '1  #TAUTHOR_TAG challenged the view that using one']","[', 000 prepositions.', '1  #TAUTHOR_TAG challenged the view that using one rater is adequate by showing that prep']","['', 'this means that to collect a corpus of 1, 000 preposition errors, an annotator would have to check over 10, 000 prepositions.', '1  #TAUTHOR_TAG challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter - annotator reliability.', '']",0
"[""can predict, or select, the writer's preposition given the context around the preposition."", ' #TAUTHOR_TAG showed that trained']","[""can predict, or select, the writer's preposition given the context around the preposition."", ' #TAUTHOR_TAG showed that trained']","[""on well - formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition."", ' #TAUTHOR_TAG showed that']","['provides a service called the mechanical turk which allows requesters ( companies, researchers, etc. ) to post simple tasks ( known as human intelligence tasks, or hits ) to the amt website for untrained raters to perform for payments as low as $ 0. 01 in many cases  #AUTHOR_TAG.', 'recently, amt has been shown to be an effective tool for annotation and evalatuation in nlp tasks ranging from word similarity detection and emotion detection  #AUTHOR_TAG to machine translation quality evaluation ( callison -  #AUTHOR_TAG.', 'in these cases, a handful of untrained amt workers ( or turkers ) were found to be as effective as trained raters, but with the advantage of being considerably faster and less expensive.', 'given the success of using amt in other areas of nlp, we test whether we can leverage it for our work in grammatical error detection, which is the focus of the pilot studies in the next two sections.', 'the presence of a gold standard in the above papers is crucial.', ""in fact, the usability of amt for text annotation has been demostrated in those studies by showing that non - experts'annotation converges to the gold standard developed by expert annotators."", 'however, in our work we concentrate on tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules.', ""typically, an early step in developing a preposition or article error detection system is to test the system on well - formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition."", ' #TAUTHOR_TAG showed that trained human raters can achieve very high agreement ( 78 % ) on this task.', 'in their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used.', '']",0
"['to attain high reliability.', 'for example,  #TAUTHOR_TAG found kappa between']","['to attain high reliability.', 'for example,  #TAUTHOR_TAG found kappa between']","['to attain high reliability.', 'for example,  #TAUTHOR_TAG found kappa between']","['the previous results look quite encouraging, the task they are based on, preposition selection in well - formed text, is quite different from, and less challenging than, the task that a system must perform in detecting errors in learner writing.', 'to examine the reliability of turker preposition error judgments, we ran another experiment in which turkers were presented with a preposition highlighted in a sentence taken from an esl corpus, and were in - structed to judge its usage as either correct, incorrect, or the context is too ungrammatical to make a judgment.', 'the set consisted of 152 prepositions in total, and we requested 20 judgments per preposition.', 'previous work has shown this task to be a difficult one for trainer raters to attain high reliability.', 'for example,  #TAUTHOR_TAG found kappa between two raters averaged 0. 630.', 'because there is no gold standard for the error detection task, kappa was used to compare turker responses to those of three trained annotators.', 'among the trained annotators, inter - kappa agreement ranged from 0. 574 to 0. 650, for a mean kappa of 0. 606.', 'in figure 2, kappa is shown for the comparisons of turker responses to each annotator for samples of various sizes ranging from n = 1 to n = 18.', 'at sample size n = 13, the average kappa is 0. 608, virtually identical to the mean found among the trained annotators']",0
"[""can predict, or select, the writer's preposition given the context around the preposition."", ' #TAUTHOR_TAG showed that trained']","[""can predict, or select, the writer's preposition given the context around the preposition."", ' #TAUTHOR_TAG showed that trained']","[""on well - formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition."", ' #TAUTHOR_TAG showed that']","['provides a service called the mechanical turk which allows requesters ( companies, researchers, etc. ) to post simple tasks ( known as human intelligence tasks, or hits ) to the amt website for untrained raters to perform for payments as low as $ 0. 01 in many cases  #AUTHOR_TAG.', 'recently, amt has been shown to be an effective tool for annotation and evalatuation in nlp tasks ranging from word similarity detection and emotion detection  #AUTHOR_TAG to machine translation quality evaluation ( callison -  #AUTHOR_TAG.', 'in these cases, a handful of untrained amt workers ( or turkers ) were found to be as effective as trained raters, but with the advantage of being considerably faster and less expensive.', 'given the success of using amt in other areas of nlp, we test whether we can leverage it for our work in grammatical error detection, which is the focus of the pilot studies in the next two sections.', 'the presence of a gold standard in the above papers is crucial.', ""in fact, the usability of amt for text annotation has been demostrated in those studies by showing that non - experts'annotation converges to the gold standard developed by expert annotators."", 'however, in our work we concentrate on tasks where there is no single gold standard, either because there are multiple prepositions that are acceptable in a given context or because the conventions of preposition usage simply do not conform to strict rules.', ""typically, an early step in developing a preposition or article error detection system is to test the system on well - formed text written by native speakers to see how well the system can predict, or select, the writer's preposition given the context around the preposition."", ' #TAUTHOR_TAG showed that trained human raters can achieve very high agreement ( 78 % ) on this task.', 'in their work, a rater was shown a sentence with a target preposition replaced with a blank, and the rater was asked to select the preposition that the writer may have used.', '']",6
"['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine']","['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine']","['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine translation  #AUTHOR_TAG b ;  #AUTHOR_TAG,']","['text normalization systems aim to convert historical wordforms to their modern equivalents, in order to make historical documents more searchable or to improve the performance of downstream nlp tools.', 'in historical texts, a single word type may be realized with several different orthographic forms, which may not correspond to the modern form.', 'for example, the modern english word said might be realized as sayed, seyd, said, sayd, etc.', 'spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently.', 'over the years, researchers have proposed normalization methods based on rules and / or edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine translation  #AUTHOR_TAG b ;  #AUTHOR_TAG, and most recently neural network models ( bollmann and søgaard, 2016 ;  #AUTHOR_TAG.', 'however, most of these systems have been developed and tested on a single language ( or even a single corpus ), and many have not been compared to the naive but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.', '']",0
"['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine']","['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine']","['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine translation  #AUTHOR_TAG b ;  #AUTHOR_TAG,']","['text normalization systems aim to convert historical wordforms to their modern equivalents, in order to make historical documents more searchable or to improve the performance of downstream nlp tools.', 'in historical texts, a single word type may be realized with several different orthographic forms, which may not correspond to the modern form.', 'for example, the modern english word said might be realized as sayed, seyd, said, sayd, etc.', 'spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently.', 'over the years, researchers have proposed normalization methods based on rules and / or edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine translation  #AUTHOR_TAG b ;  #AUTHOR_TAG, and most recently neural network models ( bollmann and søgaard, 2016 ;  #AUTHOR_TAG.', 'however, most of these systems have been developed and tested on a single language ( or even a single corpus ), and many have not been compared to the naive but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.', '']",0
"['much annotation might be required for a new corpus  #TAUTHOR_TAG ; bollmann and søgaard, 2016']","['much annotation might be required for a new corpus  #TAUTHOR_TAG ; bollmann and søgaard, 2016']","['much annotation might be required for a new corpus  #TAUTHOR_TAG ; bollmann and søgaard, 2016']","['', 'stemming from the reasoning above, we argue that a full evaluation of any spelling normalization system requires more complete dataset statistics and experimental results.', 'in describing the training and test sets, researchers should not only report the number of types and tokens, but also the per - centage of unseen tokens in the test ( or dev ) set and the percentage of training items ( h, m ) where h = m. this last statistic measures the degree of spelling variation, which varies considerably between corpora.', 'as for reporting results, we have argued that accuracy should be reported separately for seen vs unseen tokens, and overall results compared to the naive memorization baseline.', 'since historical spelling normalization is typically a low - resource task, systems should also ideally be tested with varying amounts of training data to assess how much annotation might be required for a new corpus  #TAUTHOR_TAG ; bollmann and søgaard, 2016']",0
"['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine']","['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine']","['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine translation  #AUTHOR_TAG b ;  #AUTHOR_TAG,']","['text normalization systems aim to convert historical wordforms to their modern equivalents, in order to make historical documents more searchable or to improve the performance of downstream nlp tools.', 'in historical texts, a single word type may be realized with several different orthographic forms, which may not correspond to the modern form.', 'for example, the modern english word said might be realized as sayed, seyd, said, sayd, etc.', 'spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently.', 'over the years, researchers have proposed normalization methods based on rules and / or edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine translation  #AUTHOR_TAG b ;  #AUTHOR_TAG, and most recently neural network models ( bollmann and søgaard, 2016 ;  #AUTHOR_TAG.', 'however, most of these systems have been developed and tested on a single language ( or even a single corpus ), and many have not been compared to the naive but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.', '']",1
"['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine']","['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine']","['edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine translation  #AUTHOR_TAG b ;  #AUTHOR_TAG,']","['text normalization systems aim to convert historical wordforms to their modern equivalents, in order to make historical documents more searchable or to improve the performance of downstream nlp tools.', 'in historical texts, a single word type may be realized with several different orthographic forms, which may not correspond to the modern form.', 'for example, the modern english word said might be realized as sayed, seyd, said, sayd, etc.', 'spellings change over time, but also vary within a single time period and even within a single author, since orthography only became standardized in many languages fairly recently.', 'over the years, researchers have proposed normalization methods based on rules and / or edit distances  #AUTHOR_TAG a ;  #TAUTHOR_TAG, statistical machine translation  #AUTHOR_TAG b ;  #AUTHOR_TAG, and most recently neural network models ( bollmann and søgaard, 2016 ;  #AUTHOR_TAG.', 'however, most of these systems have been developed and tested on a single language ( or even a single corpus ), and many have not been compared to the naive but strong baseline that only changes words seen in the training data, normalizing each to its most frequent modern form observed during training.', '']",1
['to previous results from  #TAUTHOR_TAG'],['to previous results from  #TAUTHOR_TAG'],"['to previous results from  #TAUTHOR_TAG.', 'the first model  #AUTHOR_TAG 4 uses a fairly standard architecture with a bi - directional lstm encoder and an lstm decoder with soft attention  #AUTHOR_TAG,']","['focus on two neural encoder - decoder models for spelling normalization, comparing them against the memorization baseline and to previous results from  #TAUTHOR_TAG.', 'the first model  #AUTHOR_TAG 4 uses a fairly standard architecture with a bi - directional lstm encoder and an lstm decoder with soft attention  #AUTHOR_TAG, and is trained using cross - entropy loss.', 'the second model is a new approach to spelling normalization, which adapts the morphological reinflection system of  #AUTHOR_TAG.', '5 the reinflection model generates the characters in an inflected wordform ( y 1 : n ), given the characters of its lemma ( x 1 : m ) and a set of corresponding morphological features ( f ).', 'rather than using a soft attention mechanism that computes a weight vector over the entire sequence, this model exploits the generally monotonic character alignment between x 1 : m and y 1 : n and attends to only a single encoded input character at a time during decoding.', 'architecturally, the model uses a standard bidirectional encoder.', 'the decoder steps through the characters of the input and considers jointly the output of the previous step, the morphological features, and the currently attended encoded input.', '']",7
"['datasets as  #TAUTHOR_TAG, with data from five languages over a range of historical periods.', '6 we use']","['datasets as  #TAUTHOR_TAG, with data from five languages over a range of historical periods.', '6 we use']","['as  #TAUTHOR_TAG, with data from five languages over a range of historical periods.', '6 we use']","['use the same datasets as  #TAUTHOR_TAG, with data from five languages over a range of historical periods.', '6 we use the same train / dev / test splits as pettersson ; dataset statistics are shown in table 1. because we do no hyperparameter tuning, we do not use the development sets, and all results are reported on the test sets.', 'each system was tested as recommended above, with accuracy reported separately on seen and unseen items, and for different training data sizes.', 'to evaluate the downstream effects of normalization, we applied the models to a collection of unseen documents and then tagged them with the stan - ford pos tagger, which comes pre - trained on modern english.', 'the documents are from the parsed corpus of early english correspondence ( pceec )  #AUTHOR_TAG, comprised of 84 letter collections from the 15th - 17th centuries.', '( our english normalization training data is from the 14th - 17th centuries. ) pceec contains roughly 2. 2m manually pos - tagged tokens but no spelling annotation.', ""because it uses a large and somewhat idiosyncratic set of pos tags, we converted these to better match the stanford tags before evaluating ( though the match still isn't perfect ; accuracy would be higher in all cases if the tag sets were identical )."", 'baselines are provided by tagging the unnormalized text and the output of the naive normalization baseline.', 'table 2 gives test set results for all models, broken down into seen and unseen items where possible.', '7 the split into seen / unseen highlights the fact that neither of the neural models does as well on seen items as the baseline ; indeed the soft attention model is considerably worse in english and hungarian, the two largest datasets.', '8 the result is that this model actually underperforms the baseline when applied to all tokens, although a hybrid model ( baseline for seen, soft attention for unseen ) would outperform the baseline.', 'nevertheless, the hard attention model performs best on unseen tokens in all cases, often by a wide margin, and also yields competitive overall performance']",5
['by  #TAUTHOR_TAG for'],['by  #TAUTHOR_TAG for'],['by  #TAUTHOR_TAG for a hybrid model ( apply memorization baseline to seen tokens'],"['', 'table 2 : tokens normalized correctly ( % ) for each dataset.', 'upper half : results on ( a ) ll tokens reported by  #TAUTHOR_TAG for a hybrid model ( apply memorization baseline to seen tokens and an edit - distance - based model to unseen tokens ) and two smt models ( which align character unigrams and bigrams, respectively ).', 'lower half : results from our experiments, including accuracy reported separately on ( s ) een and ( u ) nseen tokens. and presumably more difficult as training data size increases, so the baseline gets worse.', 'in contrast, the neural models are able to maintain or increase performance on this set.', 'we expected that the bias toward monotonic alignments would help the hard attention model at smaller data sizes, but it is the soft attention model that seems to do better there, while the hard attention model does better in most cases at the larger data sizes.', 'note that  #AUTHOR_TAG trained their model on individual manuscripts, with no training set containing more than 13. 2k tokens.', 'the fact that this model struggles with larger data sizes, especially for seen tokens, suggests that the default hyperparameters may be tuned to work well with small training sets at the cost of underfitting the larger datasets.', '']",5
"['##s has become very popular [ 5, 6,  #TAUTHOR_TAG 8 ].', 'low power consumption requirement']","['( nn ) based kws has become very popular [ 5, 6,  #TAUTHOR_TAG 8 ].', 'low power consumption requirement']","['##s has become very popular [ 5, 6,  #TAUTHOR_TAG 8 ].', 'low power consumption requirement']","['', 'on the other hand, the kws system should detect the keywords with high accuracy and low latency, for best user experience.', 'these conflicting system requirements make kws an active area of research ever since its inception over 50 years ago [ 4 ].', 'recently, with the renaissance of artificial neural networks in the form of deep learning algorithms, neural network ( nn ) based kws has become very popular [ 5, 6,  #TAUTHOR_TAG 8 ].', 'low power consumption requirement for keyword spotting systems make microcontrollers an obvious choice for deploying kws in an always - on system.', 'microcontrollers are low - cost energy - efficient processors that are ubiquitous in our everyday life with their presence in a variety of devices ranging from home appliances, automobiles and consumer electronics to wearables.', 'however, deployment of neural network based kws on microcontrollers comes with following challenges :', 'limited memory footprint : typical microcontroller systems have only tens to few hundred kb of memory available.', 'the entire neural network model, including input / output, weights and activations, has to fit within this small memory budget']",0
['##s is investigated in  #TAUTHOR_TAG and demonstrate the'],['##s is investigated in  #TAUTHOR_TAG and demonstrate the'],"['', 'based kws is investigated in  #TAUTHOR_TAG and demonstrate the robustness of the model to noise. while all the prior kws neural networks are']","['', 'functions are introduced in [ 5 ], which outperforms the hmm models with a very small detection latency. furthermore, low - rank approximation techniques are used to compress the dnn model', 'weights achieving similar accuracy with less hardware resources [ 15, 16 ]. the main drawback of dnns is', 'that they ignore the local temporal and spectral correlation in the input speech features. in order to exploit these correlations, different variants of convolutional neural network ( cnn ) based kws are explored in [ 6 ], which demonstrate higher', 'accuracy than dnns. the drawback of cnns in modeling time varying signals ( e. g. speech ) is that they ignore', 'long term temporal dependencies. combining the strengths of cnns and rnns, convolutional recurrent neural network', 'based kws is investigated in  #TAUTHOR_TAG and demonstrate the robustness of the model to noise. while all the prior kws neural networks are trained with cross entropy loss', 'function, a max - pooling based loss function for training kws model with long short - term memory (', 'lstm ) is proposed in [ 8 ], which achieves better accuracy than the dnns and lstms trained with cross entropy loss. although many neural network models for kws are presented in literature, it is difficult to make a', 'fair comparison between them as they are all trained and evaluated on different proprietary datasets ( e. g. "" talktype "" dataset in  #TAUTHOR_TAG, ""', 'alexa "" dataset in [ 8 ], etc. ) with different input speech features and audio duration. also,', 'the primary focus of prior research has been to maximize the accuracy with a small memory footprint model, without explicit constraints of underlying hardware, such as limits on number of operations per', 'inference. in contrast, this work is more hardware - centric and targeted towards neural network architectures that maximize accuracy on microcontroller devices.', 'the constraints on memory and compute significantly limit the neural network parameters and the number of operations']",0
['##s is investigated in  #TAUTHOR_TAG and demonstrate the'],['##s is investigated in  #TAUTHOR_TAG and demonstrate the'],"['', 'based kws is investigated in  #TAUTHOR_TAG and demonstrate the robustness of the model to noise. while all the prior kws neural networks are']","['', 'functions are introduced in [ 5 ], which outperforms the hmm models with a very small detection latency. furthermore, low - rank approximation techniques are used to compress the dnn model', 'weights achieving similar accuracy with less hardware resources [ 15, 16 ]. the main drawback of dnns is', 'that they ignore the local temporal and spectral correlation in the input speech features. in order to exploit these correlations, different variants of convolutional neural network ( cnn ) based kws are explored in [ 6 ], which demonstrate higher', 'accuracy than dnns. the drawback of cnns in modeling time varying signals ( e. g. speech ) is that they ignore', 'long term temporal dependencies. combining the strengths of cnns and rnns, convolutional recurrent neural network', 'based kws is investigated in  #TAUTHOR_TAG and demonstrate the robustness of the model to noise. while all the prior kws neural networks are trained with cross entropy loss', 'function, a max - pooling based loss function for training kws model with long short - term memory (', 'lstm ) is proposed in [ 8 ], which achieves better accuracy than the dnns and lstms trained with cross entropy loss. although many neural network models for kws are presented in literature, it is difficult to make a', 'fair comparison between them as they are all trained and evaluated on different proprietary datasets ( e. g. "" talktype "" dataset in  #TAUTHOR_TAG, ""', 'alexa "" dataset in [ 8 ], etc. ) with different input speech features and audio duration. also,', 'the primary focus of prior research has been to maximize the accuracy with a small memory footprint model, without explicit constraints of underlying hardware, such as limits on number of operations per', 'inference. in contrast, this work is more hardware - centric and targeted towards neural network architectures that maximize accuracy on microcontroller devices.', 'the constraints on memory and compute significantly limit the neural network parameters and the number of operations']",0
['recurrent neural network  #TAUTHOR_TAG is a hybrid of cnn and rn'],"['recurrent neural network  #TAUTHOR_TAG is a hybrid of cnn and rnn, which takes advantages of both.', '']",['recurrent neural network  #TAUTHOR_TAG is a hybrid of cnn and rn'],"['recurrent neural network  #TAUTHOR_TAG is a hybrid of cnn and rnn, which takes advantages of both.', 'it exploits the local temporal / spatial correlation using convolution layers and global temporal dependencies in the speech features using recurrent layers.', 'as shown in fig. 3, a crnn model starts with a convolution layer, followed by an rnn to encode the signal and a dense fully - connected layer to map the information.', 'here, the recurrent layer is bi - directional [ 28 ] and has multiple stages, increasing the network learning capability.', 'gated recurrent units ( gru ) [ 25 ] is used as the base cell for recurrent layers, as it uses fewer parameters than lstms and gave better convergence in our experiments']",0
"['5, 6,  #TAUTHOR_TAG 8 ] trained on google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from']","['[ 5, 6,  #TAUTHOR_TAG 8 ] trained on google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from']","['5, 6,  #TAUTHOR_TAG 8 ] trained on google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from section 4']","['discussed in section 2. 2, memory footprint and execution time are the two important considerations in being able to run keyword spotting on microcontrollers.', 'these should be considered when designing and optimizing neural networks for running keyword spotting.', 'based on typical microcontroller system configurations ( as described in table 1 ), we derive three sets of constraints for the neural networks in table 3, targeting small, medium and large microcontroller systems.', 'both memory and compute limit are derived with assumptions that some amount of resources will be allocated for running other tasks such as os, i / o, network communication, etc.', 'the operations per inference limit assumes that the system is running 10 inferences per second.', 'table 3 : neural network ( nn ) classes for kws models considered in this work, assuming 10 inferences per second and 8 - bit weights / activations.', 'figure 5 shows the number of operations per inference, memory requirement and test accuracy of neural network models from prior work [ 5, 6,  #TAUTHOR_TAG 8 ] trained on google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from section 4']",0
"['.', '[ 5, 6,  #TAUTHOR_TAG 8 ] trained on the speech commands dataset']","['still achieve high accuracy.', '[ 5, 6,  #TAUTHOR_TAG 8 ] trained on the speech commands dataset [ 9 ]']","['still achieve high accuracy.', '[ 5, 6,  #TAUTHOR_TAG 8 ] trained on the speech commands dataset [ 9 ].', 'as shown in']","['', '[ 5, 6,  #TAUTHOR_TAG 8 ] trained on the speech commands dataset [ 9 ].', 'as shown in fig. 1, from each input speech signal, t × f features are extracted and the number of these features impact the model size, number of operations and accuracy.', '']",0
"['5, 6,  #TAUTHOR_TAG 8 ] on google speech commands dataset']","['as follows :', '• we first train the popular kws neural net models from the literature [ 5, 6,  #TAUTHOR_TAG 8 ] on google speech commands dataset [ 9 ] and compare']","['5, 6,  #TAUTHOR_TAG 8 ] on google speech commands dataset']","['kws is always - on, the real - time requirement limits the total number of operations per neural network inference.', 'these microcontroller resource constraints in conjunction with the high accuracy and low latency requirements of kws call for a resource - constrained neural network architecture exploration to find lean neural network structures suitable for kws, which is the primary focus of our work.', 'the main contributions of this work are as follows :', '• we first train the popular kws neural net models from the literature [ 5, 6,  #TAUTHOR_TAG 8 ] on google speech commands dataset [ 9 ] and compare them in terms of accuracy, memory footprint and number of operations per inference.', '• in addition, we implement a new kws model using depth - wise separable convolutions and point - wise convolutions, inspired by the success of resource - efficient mobilenet [ 10 ] in computer vision.', 'this model outperforms the other prior models in all aspects of accuracy, model size and number of operations.', '• finally, we perform resource - constrained neural network architecture exploration and present comprehensive comparison of different network architectures within a set of compute and memory constraints of typical microcontrollers.', 'the code, model definitions and pretrained models are available at https : / / github. com / arm - software / ml - kws - for - mcu']",3
"['5, 6,  #TAUTHOR_TAG 8 ]']","['test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6,  #TAUTHOR_TAG 8 ]']","['5, 6,  #TAUTHOR_TAG 8 ]']","['', 'the training data is augmented with background noise and random time shift of up to 100ms.', 'the trained models are evaluated based on the classification accuracy on the test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6,  #TAUTHOR_TAG 8 ] trained on google speech commands dataset [ 9 ].', 'for all the models, we use 40 mfcc features extracted from a speech frame of length 40ms with a stride of 20ms, which gives 1960 ( 49×40 ) features for 1 second of audio.', 'the accuracy shown in the table is the accuracy on test set.', 'the memory shown in the table assumes 8 - bit weights and activations, which is sufficient to achieve same accuracy as that from a full - precision network']",3
"['5, 6,  #TAUTHOR_TAG 8 ] on google speech commands dataset']","['as follows :', '• we first train the popular kws neural net models from the literature [ 5, 6,  #TAUTHOR_TAG 8 ] on google speech commands dataset [ 9 ] and compare']","['5, 6,  #TAUTHOR_TAG 8 ] on google speech commands dataset']","['kws is always - on, the real - time requirement limits the total number of operations per neural network inference.', 'these microcontroller resource constraints in conjunction with the high accuracy and low latency requirements of kws call for a resource - constrained neural network architecture exploration to find lean neural network structures suitable for kws, which is the primary focus of our work.', 'the main contributions of this work are as follows :', '• we first train the popular kws neural net models from the literature [ 5, 6,  #TAUTHOR_TAG 8 ] on google speech commands dataset [ 9 ] and compare them in terms of accuracy, memory footprint and number of operations per inference.', '• in addition, we implement a new kws model using depth - wise separable convolutions and point - wise convolutions, inspired by the success of resource - efficient mobilenet [ 10 ] in computer vision.', 'this model outperforms the other prior models in all aspects of accuracy, model size and number of operations.', '• finally, we perform resource - constrained neural network architecture exploration and present comprehensive comparison of different network architectures within a set of compute and memory constraints of typical microcontrollers.', 'the code, model definitions and pretrained models are available at https : / / github. com / arm - software / ml - kws - for - mcu']",5
"['5, 6,  #TAUTHOR_TAG 8 ]']","['test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6,  #TAUTHOR_TAG 8 ]']","['5, 6,  #TAUTHOR_TAG 8 ]']","['', 'the training data is augmented with background noise and random time shift of up to 100ms.', 'the trained models are evaluated based on the classification accuracy on the test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6,  #TAUTHOR_TAG 8 ] trained on google speech commands dataset [ 9 ].', 'for all the models, we use 40 mfcc features extracted from a speech frame of length 40ms with a stride of 20ms, which gives 1960 ( 49×40 ) features for 1 second of audio.', 'the accuracy shown in the table is the accuracy on test set.', 'the memory shown in the table assumes 8 - bit weights and activations, which is sufficient to achieve same accuracy as that from a full - precision network']",5
"[', the mapping is always exact and one - to - one.  #TAUTHOR_TAG constructed bilingual word embeddings based on monolingual data', 'and panlex. in this']","['embedding to each other.', 'in this way, the mapping is always exact and one - to - one.  #TAUTHOR_TAG constructed bilingual word embeddings based on monolingual data', 'and panlex. in this']","[', the mapping is always exact and one - to - one.  #TAUTHOR_TAG constructed bilingual word embeddings based on monolingual data', 'and panlex. in this']","['', ', performing canonical correlation analysis ( cca ) for multiple languages using english as the pivot. a short', '##coming of multicca is that it ignores polysemous translations by retaining only one - to - one dictionary pairs  #AUTHOR_TAG', ', disregarding much information. as a simple solution, we propose a simple post', 'hoc method by mapping the english parts of each bilingual word embedding to each other.', 'in this way, the mapping is always exact and one - to - one.  #TAUTHOR_TAG constructed bilingual word embeddings based on monolingual data', 'and panlex. in this way, their approach can be applied to more languages as panlex covers more than a thousand languages. they solve the polysemy problem', 'by integrating an em algorithm for selecting a lexicon. relative to many previous crosslingual word embeddings, their joint training algorithm achieved state', '- of - the - art performance for the bilingual lexicon induction task, performing significantly', 'better on monolingual similarity and achieving a competitive result on cross lingual', 'document classification. here we also adopt  #TAUTHOR_TAG approach, and extend it to multilingual embeddings']",0
"['neural ranking model  #TAUTHOR_TAG.', '']","['recent state - of - the - art neural ranking model  #TAUTHOR_TAG.', '']","['nrm, a recent state - of - the - art neural ranking model  #TAUTHOR_TAG.', '']","['ir models have received much attention due to their continuous text representations, soft - matching of terms, and sophisticated non - linear models.', 'however, the non - convexity and stochastic training of neural ir models raises questions about their consistency compared to heuristic and learning - to - rank models that use discrete representations and simpler methods of combining evidence.', 'consistent behavior under slightly different conditions is essential to reproducible research and deployment in industry.', 'this paper studies the stability of k - nrm, a recent state - of - the - art neural ranking model  #TAUTHOR_TAG.', 'k - nrm learns the word embeddings and ranking model from relevance signals.', '']",0
"['neural ranking model  #TAUTHOR_TAG.', '']","['recent state - of - the - art neural ranking model  #TAUTHOR_TAG.', '']","['nrm, a recent state - of - the - art neural ranking model  #TAUTHOR_TAG.', '']","['ir models have received much attention due to their continuous text representations, soft - matching of terms, and sophisticated non - linear models.', 'however, the non - convexity and stochastic training of neural ir models raises questions about their consistency compared to heuristic and learning - to - rank models that use discrete representations and simpler methods of combining evidence.', 'consistent behavior under slightly different conditions is essential to reproducible research and deployment in industry.', 'this paper studies the stability of k - nrm, a recent state - of - the - art neural ranking model  #TAUTHOR_TAG.', 'k - nrm learns the word embeddings and ranking model from relevance signals.', '']",0
"['and document words and neural networks that learn matching patterns [ 2,  #TAUTHOR_TAG.', '']","['and document words and neural networks that learn matching patterns [ 2,  #TAUTHOR_TAG.', 'k - nrm  #TAUTHOR_TAG is an interaction - based model']","['based models use local interactions between the query and document words and neural networks that learn matching patterns [ 2,  #TAUTHOR_TAG.', '']","['neural ir methods can be categorized as representationbased and interaction - based [ 2 ].', 'representation - based models use distributed representations of the query and document that are matched in the representation space [ 4, 8 ].', 'interaction - based models use local interactions between the query and document words and neural networks that learn matching patterns [ 2,  #TAUTHOR_TAG.', 'k - nrm  #TAUTHOR_TAG is an interaction - based model that uses kernel pooling to summarize word - word interactions.', ""it builds a word - word similarity matrix from word embeddings, and uses kernel pooling to'count'the soft matches at multiple similarity levels using gaussian kernels."", 'a linear learning - to - rank layer combines the kernel features.', 'the whole model is end - to - end trainable.', 'when trained from a search log, k - nrm outperforms neural ir methods and feature - based learning - to - rank methods.', 'most neural ir research focuses on ranking accuracy.', 'however, the high variance of deep learning models causes concern about their consistency.', 'haber et al. [ 3 ] identify the causes for lack of stability and high variance as the dimensionality and non - convexity of the optimization problem.', 'a common method to reduce variance and improve generalization is to create an ensemble of models [ 6 ].', 'krogh and vedelsby [ 6 ] argue that a good ensemble is one where the components are all accurate but disagree on individual examples.', 'ensembles of neural network have been applied successfully to tasks such as image classification [ 5 ] and machine translation [ 9 ]']",0
"['and document words and neural networks that learn matching patterns [ 2,  #TAUTHOR_TAG.', '']","['and document words and neural networks that learn matching patterns [ 2,  #TAUTHOR_TAG.', 'k - nrm  #TAUTHOR_TAG is an interaction - based model']","['based models use local interactions between the query and document words and neural networks that learn matching patterns [ 2,  #TAUTHOR_TAG.', '']","['neural ir methods can be categorized as representationbased and interaction - based [ 2 ].', 'representation - based models use distributed representations of the query and document that are matched in the representation space [ 4, 8 ].', 'interaction - based models use local interactions between the query and document words and neural networks that learn matching patterns [ 2,  #TAUTHOR_TAG.', 'k - nrm  #TAUTHOR_TAG is an interaction - based model that uses kernel pooling to summarize word - word interactions.', ""it builds a word - word similarity matrix from word embeddings, and uses kernel pooling to'count'the soft matches at multiple similarity levels using gaussian kernels."", 'a linear learning - to - rank layer combines the kernel features.', 'the whole model is end - to - end trainable.', 'when trained from a search log, k - nrm outperforms neural ir methods and feature - based learning - to - rank methods.', 'most neural ir research focuses on ranking accuracy.', 'however, the high variance of deep learning models causes concern about their consistency.', 'haber et al. [ 3 ] identify the causes for lack of stability and high variance as the dimensionality and non - convexity of the optimization problem.', 'a common method to reduce variance and improve generalization is to create an ensemble of models [ 6 ].', 'krogh and vedelsby [ 6 ] argue that a good ensemble is one where the components are all accurate but disagree on individual examples.', 'ensembles of neural network have been applied successfully to tasks such as image classification [ 5 ] and machine translation [ 9 ]']",0
"['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the same click log data from sogou. com, a chinese web search engine.', 'the training set contained 95m queries, each with 12 candidate documents on average.', 'the testing set contained 1, 000 queries, each with 30 candidate documents on average.', 'documents were represented by titles.', 'xiong, et al.  #TAUTHOR_TAG built the vocabulary from queries and titles, but we built it from the queries, titles and urls for better term coverage.', 'training labels : the relevance labels for training were generated by the dctr [ 1 ] click model from user clicks in the training sessions.', '']",3
"['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the same click log data from sogou. com, a chinese web search engine.', 'the training set contained 95m queries, each with 12 candidate documents on average.', 'the testing set contained 1, 000 queries, each with 30 candidate documents on average.', 'documents were represented by titles.', 'xiong, et al.  #TAUTHOR_TAG built the vocabulary from queries and titles, but we built it from the queries, titles and urls for better term coverage.', 'training labels : the relevance labels for training were generated by the dctr [ 1 ] click model from user clicks in the training sessions.', '']",3
"['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the same click log data from sogou. com, a chinese web search engine.', 'the training set contained 95m queries, each with 12 candidate documents on average.', 'the testing set contained 1, 000 queries, each with 30 candidate documents on average.', 'documents were represented by titles.', 'xiong, et al.  #TAUTHOR_TAG built the vocabulary from queries and titles, but we built it from the queries, titles and urls for better term coverage.', 'training labels : the relevance labels for training were generated by the dctr [ 1 ] click model from user clicks in the training sessions.', '']",3
"['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the same click log data from sogou. com, a chinese web search engine.', 'the training set contained 95m queries, each with 12 candidate documents on average.', 'the testing set contained 1, 000 queries, each with 30 candidate documents on average.', 'documents were represented by titles.', 'xiong, et al.  #TAUTHOR_TAG built the vocabulary from queries and titles, but we built it from the queries, titles and urls for better term coverage.', 'training labels : the relevance labels for training were generated by the dctr [ 1 ] click model from user clicks in the training sessions.', '']",5
"['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the same click log data from sogou. com, a chinese web search engine.', 'the training set contained 95m queries, each with 12 candidate documents on average.', 'the testing set contained 1, 000 queries, each with 30 candidate documents on average.', 'documents were represented by titles.', 'xiong, et al.  #TAUTHOR_TAG built the vocabulary from queries and titles, but we built it from the queries, titles and urls for better term coverage.', 'training labels : the relevance labels for training were generated by the dctr [ 1 ] click model from user clicks in the training sessions.', '']",5
"['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the same click log data from sogou. com, a chinese web search engine.', 'the training set contained 95m queries, each with 12 candidate documents on average.', 'the testing set contained 1, 000 queries, each with 30 candidate documents on average.', 'documents were represented by titles.', 'xiong, et al.  #TAUTHOR_TAG built the vocabulary from queries and titles, but we built it from the queries, titles and urls for better term coverage.', 'training labels : the relevance labels for training were generated by the dctr [ 1 ] click model from user clicks in the training sessions.', '']",5
"['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the same click log data from sogou. com, a chinese web search engine.', 'the training set contained 95m queries, each with 12 candidate documents on average.', 'the testing set contained 1, 000 queries, each with 30 candidate documents on average.', 'documents were represented by titles.', 'xiong, et al.  #TAUTHOR_TAG built the vocabulary from queries and titles, but we built it from the queries, titles and urls for better term coverage.', 'training labels : the relevance labels for training were generated by the dctr [ 1 ] click model from user clicks in the training sessions.', '']",4
"[' #TAUTHOR_TAG.', 'their model']","[' #TAUTHOR_TAG.', 'their model']","['##ong et al  #TAUTHOR_TAG.', 'their model performance falls in']","['', 'table 1 also shows results reported by xiong et al  #TAUTHOR_TAG.', 'their model performance falls in the lower end of our trials, probably due to different vocabularies and stopping conditions.', '']",4
"['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used']","['experiments followed the original k - nrm work  #TAUTHOR_TAG and used its open - source implementation 1.', 'we used the same click log data from sogou. com, a chinese web search engine.', 'the training set contained 95m queries, each with 12 candidate documents on average.', 'the testing set contained 1, 000 queries, each with 30 candidate documents on average.', 'documents were represented by titles.', 'xiong, et al.  #TAUTHOR_TAG built the vocabulary from queries and titles, but we built it from the queries, titles and urls for better term coverage.', 'training labels : the relevance labels for training were generated by the dctr [ 1 ] click model from user clicks in the training sessions.', '']",6
"[' #TAUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG,']","[' #TAUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG,']","[' #AUTHOR_TAG, english  #TAUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG,']","['##y categorial grammar ( ccg ;  #AUTHOR_TAG is a grammar formalism distinguished by its transparent syntax - semantics interface and its elegant handling of coordination.', 'it is a popular tool in semantic parsing, and treebank creation efforts have been made for turkish ( c akıcı, 2005 ), german  #AUTHOR_TAG, english  #TAUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG, and hindi  #AUTHOR_TAG.', 'however, all of these treebanks were not directly annotated according to the ccg formalism, but automatically converted from phrase structure or dependency treebanks, which is an error - prone process.', 'direct annotation in ccg has so far mostly been limited to small datasets for seeding or testing semantic parsers ( e. g.,  #AUTHOR_TAG, and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale.', 'the only exceptions we are aware of are the groningen meaning bank and the parallel meaning bank  #AUTHOR_TAG, two annotation efforts which use a graphical user interface for annotating sentences with ccg derivations and other annotation layers, and which have produced ccg treebanks for english, german, italian, and dutch.', 'however, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation.', 'their annotation tool is limited in that annotators only have control over lexical categories, not larger constituents.', 'even though ccg is a lexicalized formalism, where most decisions can be made on the lexical level, there is no full control over attachment phenomena in the lexicon.', 'moreover, these annotation tools are not open - source and cannot easily be deployed to support other annotation efforts.', 'in this paper, we present an open - source, lightweight, easy - to - use graphical annotation tool that employs a statistical parser to create initial ccg derivations for sentences, and allows annotators to correct these annotations via lexical category constraints and span constraints.', 'together, these constraints make it possible to effect ( almost ) all annotation decisions consistent with the principles of ccg.', 'we also present a pilot study for multilingual ccg annotation, in which a parallel corpus of 4x100 sentences ( in english, german, italian, and dutch ) was annotated by two annotators per sentence, a detailed annotation manual was created, and adjudication was performed to create a final version.', 'we publicly release the manual, the annotation tool, and the adjudicated data.', 'our release also includes an additional > 10 k derivations, each manually corrected by a single annotator, and an additional > 82 k sentences, each partially corrected by a']",0
"[' #TAUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG,']","[' #TAUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG,']","[' #AUTHOR_TAG, english  #TAUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG,']","['##y categorial grammar ( ccg ;  #AUTHOR_TAG is a grammar formalism distinguished by its transparent syntax - semantics interface and its elegant handling of coordination.', 'it is a popular tool in semantic parsing, and treebank creation efforts have been made for turkish ( c akıcı, 2005 ), german  #AUTHOR_TAG, english  #TAUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG, and hindi  #AUTHOR_TAG.', 'however, all of these treebanks were not directly annotated according to the ccg formalism, but automatically converted from phrase structure or dependency treebanks, which is an error - prone process.', 'direct annotation in ccg has so far mostly been limited to small datasets for seeding or testing semantic parsers ( e. g.,  #AUTHOR_TAG, and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale.', 'the only exceptions we are aware of are the groningen meaning bank and the parallel meaning bank  #AUTHOR_TAG, two annotation efforts which use a graphical user interface for annotating sentences with ccg derivations and other annotation layers, and which have produced ccg treebanks for english, german, italian, and dutch.', 'however, these efforts are focused on semantics and have not released explicit guidelines for syntactic annotation.', 'their annotation tool is limited in that annotators only have control over lexical categories, not larger constituents.', 'even though ccg is a lexicalized formalism, where most decisions can be made on the lexical level, there is no full control over attachment phenomena in the lexicon.', 'moreover, these annotation tools are not open - source and cannot easily be deployed to support other annotation efforts.', 'in this paper, we present an open - source, lightweight, easy - to - use graphical annotation tool that employs a statistical parser to create initial ccg derivations for sentences, and allows annotators to correct these annotations via lexical category constraints and span constraints.', 'together, these constraints make it possible to effect ( almost ) all annotation decisions consistent with the principles of ccg.', 'we also present a pilot study for multilingual ccg annotation, in which a parallel corpus of 4x100 sentences ( in english, german, italian, and dutch ) was annotated by two annotators per sentence, a detailed annotation manual was created, and adjudication was performed to create a final version.', 'we publicly release the manual, the annotation tool, and the adjudicated data.', 'our release also includes an additional > 10 k derivations, each manually corrected by a single annotator, and an additional > 82 k sentences, each partially corrected by a']",1
"['##ank  #TAUTHOR_TAG,']","['ccgbank  #TAUTHOR_TAG,']","['##ank  #TAUTHOR_TAG,']","['test the viability of creating multilingual ccg treebanks by direct annotation, we conducted an annotation experiment on 110 short sentences from the tatoeba corpus  #AUTHOR_TAG, each in four translations ( english, german, italian, and dutch ).', 'the main annotation guideline was to copy the annotation style of ccgrebank  #AUTHOR_TAG, a ccg treebank adapted from ccgbank  #TAUTHOR_TAG, which is in turn based on the penn treebank  #AUTHOR_TAG.', 'since ccgrebank only covers english and lacks some constructions observed in our corpus, an annotation manual with more specific instructions was needed.', 'we initially annotated ten sentences in four languages and discussed disagreements.', 'the results were recorded in an initial annotation manual, and the initial annotations were discarded.', 'each of the remaining 4x100 sentences was then annotated independently by at least two of the authors.', 'table 1 ( upper part ) shows the number of nonoverlapping category and span constraints that each annotator created on average per sentence before marking the sentence as correct.', '']",5
[' #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG that outperformed'],['.  #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG that outperformed'],['.  #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG that outperformed'],['.  #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG'],0
['.  #TAUTHOR_TAG use double embeddings that leverage both general embeddings and domain embeddings as a feature for a cnn model and let the cnn model'],"['terms using tensor operators [ 2 ].', 'xu et al.  #TAUTHOR_TAG use double embeddings that leverage both general embeddings and domain embeddings as a feature for a cnn model and let the cnn model']",['.  #TAUTHOR_TAG use double embeddings that leverage both general embeddings and domain embeddings as a feature for a cnn model and let the cnn model decide'],"['and opinion terms extraction can be viewed as an information extraction task.', 'one of the approaches used in aspect and opinion terms extraction is supervised learning.', 'in supervised learning, aspect and opinion terms extraction is treated as a sequence labelling problem [ 6 ].', 'jin and ho [ 7 ] use hidden markov model ( hmm ) with part of speech ( pos ) and lexical features to extract aspect and opinion terms.', ""jakob and gurevych [ 8 ] used conditional random field ( crf ) with token, pos, short dependency path, word distance, and opinion sentence as it's features."", 'for indonesian reviews, aspect and / or opinion terms extraction have been conducted by [ 9 ], [ 10 ], and [ 11 ] for restaurant domain as one of task in aspect - based sentiment analysis.', 'gojali and khodra [ 9 ] performed aspect and opinion terms extraction by using crf classifier with token and pos tag as features.', 'ekawati and khodra [ 10 ] and cahyadi and khodra [ 11 ] only performed aspect term extraction.', 'ekawati and khodra [ 10 ] used crf classifier with distributional semantic model, lexical, and syntactic features.', 'cahyadi and khodra [ 11 ] also used crf classifier to do aspect term extraction.', 'the features used in [ 11 ] are lexical features and output probabilities from bidirectional long short - term memory ( b - lstm ).', 'recently, deep learning approaches have been proposed to extract aspect and / or opinion terms.', 'wang et al. [ 2 ] used attention mechanism [ 12 ] to identify the possibility of each token being an aspect or opinion term.', 'the coupled multilayer attentions that was proposed by [ 2 ] models the relations among tokens automatically without any syntactic / dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction.', 'the coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [ 2 ].', 'xu et al.  #TAUTHOR_TAG use double embeddings that leverage both general embeddings and domain embeddings as a feature for a cnn model and let the cnn model decide which embeddings have more useful information.', 'the experiment conducted in  #TAUTHOR_TAG demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone']",0
['.  #TAUTHOR_TAG use double embeddings that leverage both general embeddings and domain embeddings as a feature for a cnn model and let the cnn model'],"['terms using tensor operators [ 2 ].', 'xu et al.  #TAUTHOR_TAG use double embeddings that leverage both general embeddings and domain embeddings as a feature for a cnn model and let the cnn model']",['.  #TAUTHOR_TAG use double embeddings that leverage both general embeddings and domain embeddings as a feature for a cnn model and let the cnn model decide'],"['and opinion terms extraction can be viewed as an information extraction task.', 'one of the approaches used in aspect and opinion terms extraction is supervised learning.', 'in supervised learning, aspect and opinion terms extraction is treated as a sequence labelling problem [ 6 ].', 'jin and ho [ 7 ] use hidden markov model ( hmm ) with part of speech ( pos ) and lexical features to extract aspect and opinion terms.', ""jakob and gurevych [ 8 ] used conditional random field ( crf ) with token, pos, short dependency path, word distance, and opinion sentence as it's features."", 'for indonesian reviews, aspect and / or opinion terms extraction have been conducted by [ 9 ], [ 10 ], and [ 11 ] for restaurant domain as one of task in aspect - based sentiment analysis.', 'gojali and khodra [ 9 ] performed aspect and opinion terms extraction by using crf classifier with token and pos tag as features.', 'ekawati and khodra [ 10 ] and cahyadi and khodra [ 11 ] only performed aspect term extraction.', 'ekawati and khodra [ 10 ] used crf classifier with distributional semantic model, lexical, and syntactic features.', 'cahyadi and khodra [ 11 ] also used crf classifier to do aspect term extraction.', 'the features used in [ 11 ] are lexical features and output probabilities from bidirectional long short - term memory ( b - lstm ).', 'recently, deep learning approaches have been proposed to extract aspect and / or opinion terms.', 'wang et al. [ 2 ] used attention mechanism [ 12 ] to identify the possibility of each token being an aspect or opinion term.', 'the coupled multilayer attentions that was proposed by [ 2 ] models the relations among tokens automatically without any syntactic / dependency parsing or linguistic resources as additional information for the input and achieves good performance for aspect and opinion terms extraction.', 'the coupled attentions are used to exploit the correlations between aspect and opinion terms using tensor operators [ 2 ].', 'xu et al.  #TAUTHOR_TAG use double embeddings that leverage both general embeddings and domain embeddings as a feature for a cnn model and let the cnn model decide which embeddings have more useful information.', 'the experiment conducted in  #TAUTHOR_TAG demonstrated that double embedding mechanism achieved better performance for aspect terms extraction compared to the use of general embeddings or domain embeddings alone']",0
[' #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG that outperformed'],['.  #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG'],3
['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions'],"['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings']","['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in']","['', 'model ( except tokenization ). we use 142810 reviews to train the domain', 'embeddings. the hybrid embeddings use the combined corpus between indonesian wikipedia articles and indonesian hotel reviews. all of the', 'word embeddings are trained using fasttext [ 16 ]. for the general embeddings and domain', 'embeddings, we use the same dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in table iii', '. for the rest of the hyperparameters, we use the defaults in fasttext.', 'we use fasttext for the word embedding because it can use subword n - gram embedding to calculate out - ofvocabulary word embeddings. iv. experiment and evaluation we conduct experiments using 5000 indonesian hotel reviews with a', 'total of 78. 604 tokens obtained from airyrooms. we split the data into 3000 reviews for train data, 1000 reviews for validation data, and 1000 reviews for test data. the label distribution for each data can be seen in table iv. a. experiment scenario there are four experiment scenarios that we conducted in this work. the aim of each scenarios can be seen in table v. the experiments are carried out in sequence starting with experiment p1, with each experiment uses the result from the previous experiment. we', 'train the model using nadam optimizer with batch size of 32 categorical cross entropy as its loss function', "". we use early stopping with the patience set to 5 and the number of epochs set to 200. for experiment p1 and p2, we use the best hyperparameter values from [ 2 ]'s experiment. we use double embeddings as feature for experiment p1"", '. table vi shows the result for experiment p1. there are four variations of rnn that we', 'try : gru, lstm, b - gru, and b - lstm. the', 'experiment result shows that the variation of rnn that gives the best performance for both token level and entity level is b - lstm. the result for experiment p2 can be seen in table vii. we try four types of word embeddings : double embeddings, general embeddings, domain embeddings, and hybrid embeddings. the word embeddings that produce the best performance for token and entity level based on the experiment is double embeddings and the worst feature is hybrid embeddings that is trained from the combined corpus between general corpus and domain corpus']",3
['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions'],"['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings']","['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in']","['', 'model ( except tokenization ). we use 142810 reviews to train the domain', 'embeddings. the hybrid embeddings use the combined corpus between indonesian wikipedia articles and indonesian hotel reviews. all of the', 'word embeddings are trained using fasttext [ 16 ]. for the general embeddings and domain', 'embeddings, we use the same dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in table iii', '. for the rest of the hyperparameters, we use the defaults in fasttext.', 'we use fasttext for the word embedding because it can use subword n - gram embedding to calculate out - ofvocabulary word embeddings. iv. experiment and evaluation we conduct experiments using 5000 indonesian hotel reviews with a', 'total of 78. 604 tokens obtained from airyrooms. we split the data into 3000 reviews for train data, 1000 reviews for validation data, and 1000 reviews for test data. the label distribution for each data can be seen in table iv. a. experiment scenario there are four experiment scenarios that we conducted in this work. the aim of each scenarios can be seen in table v. the experiments are carried out in sequence starting with experiment p1, with each experiment uses the result from the previous experiment. we', 'train the model using nadam optimizer with batch size of 32 categorical cross entropy as its loss function', "". we use early stopping with the patience set to 5 and the number of epochs set to 200. for experiment p1 and p2, we use the best hyperparameter values from [ 2 ]'s experiment. we use double embeddings as feature for experiment p1"", '. table vi shows the result for experiment p1. there are four variations of rnn that we', 'try : gru, lstm, b - gru, and b - lstm. the', 'experiment result shows that the variation of rnn that gives the best performance for both token level and entity level is b - lstm. the result for experiment p2 can be seen in table vii. we try four types of word embeddings : double embeddings, general embeddings, domain embeddings, and hybrid embeddings. the word embeddings that produce the best performance for token and entity level based on the experiment is double embeddings and the worst feature is hybrid embeddings that is trained from the combined corpus between general corpus and domain corpus']",3
[' #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG that outperformed'],['.  #TAUTHOR_TAG that outperformed'],[' #TAUTHOR_TAG'],5
['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions'],"['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings']","['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in']","['', 'model ( except tokenization ). we use 142810 reviews to train the domain', 'embeddings. the hybrid embeddings use the combined corpus between indonesian wikipedia articles and indonesian hotel reviews. all of the', 'word embeddings are trained using fasttext [ 16 ]. for the general embeddings and domain', 'embeddings, we use the same dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in table iii', '. for the rest of the hyperparameters, we use the defaults in fasttext.', 'we use fasttext for the word embedding because it can use subword n - gram embedding to calculate out - ofvocabulary word embeddings. iv. experiment and evaluation we conduct experiments using 5000 indonesian hotel reviews with a', 'total of 78. 604 tokens obtained from airyrooms. we split the data into 3000 reviews for train data, 1000 reviews for validation data, and 1000 reviews for test data. the label distribution for each data can be seen in table iv. a. experiment scenario there are four experiment scenarios that we conducted in this work. the aim of each scenarios can be seen in table v. the experiments are carried out in sequence starting with experiment p1, with each experiment uses the result from the previous experiment. we', 'train the model using nadam optimizer with batch size of 32 categorical cross entropy as its loss function', "". we use early stopping with the patience set to 5 and the number of epochs set to 200. for experiment p1 and p2, we use the best hyperparameter values from [ 2 ]'s experiment. we use double embeddings as feature for experiment p1"", '. table vi shows the result for experiment p1. there are four variations of rnn that we', 'try : gru, lstm, b - gru, and b - lstm. the', 'experiment result shows that the variation of rnn that gives the best performance for both token level and entity level is b - lstm. the result for experiment p2 can be seen in table vii. we try four types of word embeddings : double embeddings, general embeddings, domain embeddings, and hybrid embeddings. the word embeddings that produce the best performance for token and entity level based on the experiment is double embeddings and the worst feature is hybrid embeddings that is trained from the combined corpus between general corpus and domain corpus']",5
['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions'],"['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings']","['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in']","['', 'model ( except tokenization ). we use 142810 reviews to train the domain', 'embeddings. the hybrid embeddings use the combined corpus between indonesian wikipedia articles and indonesian hotel reviews. all of the', 'word embeddings are trained using fasttext [ 16 ]. for the general embeddings and domain', 'embeddings, we use the same dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in table iii', '. for the rest of the hyperparameters, we use the defaults in fasttext.', 'we use fasttext for the word embedding because it can use subword n - gram embedding to calculate out - ofvocabulary word embeddings. iv. experiment and evaluation we conduct experiments using 5000 indonesian hotel reviews with a', 'total of 78. 604 tokens obtained from airyrooms. we split the data into 3000 reviews for train data, 1000 reviews for validation data, and 1000 reviews for test data. the label distribution for each data can be seen in table iv. a. experiment scenario there are four experiment scenarios that we conducted in this work. the aim of each scenarios can be seen in table v. the experiments are carried out in sequence starting with experiment p1, with each experiment uses the result from the previous experiment. we', 'train the model using nadam optimizer with batch size of 32 categorical cross entropy as its loss function', "". we use early stopping with the patience set to 5 and the number of epochs set to 200. for experiment p1 and p2, we use the best hyperparameter values from [ 2 ]'s experiment. we use double embeddings as feature for experiment p1"", '. table vi shows the result for experiment p1. there are four variations of rnn that we', 'try : gru, lstm, b - gru, and b - lstm. the', 'experiment result shows that the variation of rnn that gives the best performance for both token level and entity level is b - lstm. the result for experiment p2 can be seen in table vii. we try four types of word embeddings : double embeddings, general embeddings, domain embeddings, and hybrid embeddings. the word embeddings that produce the best performance for token and entity level based on the experiment is double embeddings and the worst feature is hybrid embeddings that is trained from the combined corpus between general corpus and domain corpus']",5
['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions'],"['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings']","['dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in']","['', 'model ( except tokenization ). we use 142810 reviews to train the domain', 'embeddings. the hybrid embeddings use the combined corpus between indonesian wikipedia articles and indonesian hotel reviews. all of the', 'word embeddings are trained using fasttext [ 16 ]. for the general embeddings and domain', 'embeddings, we use the same dimension and number of iterations as in  #TAUTHOR_TAG. the embedding dimensions and', 'number of iterations used to train the word embeddings can be seen in table iii', '. for the rest of the hyperparameters, we use the defaults in fasttext.', 'we use fasttext for the word embedding because it can use subword n - gram embedding to calculate out - ofvocabulary word embeddings. iv. experiment and evaluation we conduct experiments using 5000 indonesian hotel reviews with a', 'total of 78. 604 tokens obtained from airyrooms. we split the data into 3000 reviews for train data, 1000 reviews for validation data, and 1000 reviews for test data. the label distribution for each data can be seen in table iv. a. experiment scenario there are four experiment scenarios that we conducted in this work. the aim of each scenarios can be seen in table v. the experiments are carried out in sequence starting with experiment p1, with each experiment uses the result from the previous experiment. we', 'train the model using nadam optimizer with batch size of 32 categorical cross entropy as its loss function', "". we use early stopping with the patience set to 5 and the number of epochs set to 200. for experiment p1 and p2, we use the best hyperparameter values from [ 2 ]'s experiment. we use double embeddings as feature for experiment p1"", '. table vi shows the result for experiment p1. there are four variations of rnn that we', 'try : gru, lstm, b - gru, and b - lstm. the', 'experiment result shows that the variation of rnn that gives the best performance for both token level and entity level is b - lstm. the result for experiment p2 can be seen in table vii. we try four types of word embeddings : double embeddings, general embeddings, domain embeddings, and hybrid embeddings. the word embeddings that produce the best performance for token and entity level based on the experiment is double embeddings and the worst feature is hybrid embeddings that is trained from the combined corpus between general corpus and domain corpus']",5
"['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['finally, the analysis of a compound relies on its pragmatic or contextual features  #AUTHOR_TAG. recently, there', 'has been a concerted effort in studying the nature of compositionality in compounds by leveraging on distributional word - representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a framework for semantic type classification of compounds in sanskrit. they proposed a multi', '- class classifier using random forests  #AUTHOR_TAG, where they classified a given compound into one of the four coarse level compound classes, namely, avyayibhava, tatpur', '##us. a, bahuvrihi and dvandva. they have used an elaborate feature set, which summarily consists of rules from the grammar treatise as. t. a dhyayi pertaining', 'to compounding, semantic relations between the compound components from a lexical database amarakos. a and distributional subword patterns from the', 'data using adaptor grammar  #AUTHOR_TAG. inspired from the recent advances in using neural models for compound', 'analysis in nlp, we revisit the task of compound class identification and validate the efficacy of such', 'models under the low - resource setting like that of sanskrit. in this work, we experiment with multiple deep learning models for compound type classification. our extensive experiments', '']",0
"['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['finally, the analysis of a compound relies on its pragmatic or contextual features  #AUTHOR_TAG. recently, there', 'has been a concerted effort in studying the nature of compositionality in compounds by leveraging on distributional word - representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a framework for semantic type classification of compounds in sanskrit. they proposed a multi', '- class classifier using random forests  #AUTHOR_TAG, where they classified a given compound into one of the four coarse level compound classes, namely, avyayibhava, tatpur', '##us. a, bahuvrihi and dvandva. they have used an elaborate feature set, which summarily consists of rules from the grammar treatise as. t. a dhyayi pertaining', 'to compounding, semantic relations between the compound components from a lexical database amarakos. a and distributional subword patterns from the', 'data using adaptor grammar  #AUTHOR_TAG. inspired from the recent advances in using neural models for compound', 'analysis in nlp, we revisit the task of compound class identification and validate the efficacy of such', 'models under the low - resource setting like that of sanskrit. in this work, we experiment with multiple deep learning models for compound type classification. our extensive experiments', '']",0
"['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['- representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a']","['finally, the analysis of a compound relies on its pragmatic or contextual features  #AUTHOR_TAG. recently, there', 'has been a concerted effort in studying the nature of compositionality in compounds by leveraging on distributional word - representations or word embeddings and then learning function approximators to predict the nature of compositional', '##ity of such words  #AUTHOR_TAG. in sanskrit,  #TAUTHOR_TAG have proposed a framework for semantic type classification of compounds in sanskrit. they proposed a multi', '- class classifier using random forests  #AUTHOR_TAG, where they classified a given compound into one of the four coarse level compound classes, namely, avyayibhava, tatpur', '##us. a, bahuvrihi and dvandva. they have used an elaborate feature set, which summarily consists of rules from the grammar treatise as. t. a dhyayi pertaining', 'to compounding, semantic relations between the compound components from a lexical database amarakos. a and distributional subword patterns from the', 'data using adaptor grammar  #AUTHOR_TAG. inspired from the recent advances in using neural models for compound', 'analysis in nlp, we revisit the task of compound class identification and validate the efficacy of such', 'models under the low - resource setting like that of sanskrit. in this work, we experiment with multiple deep learning models for compound type classification. our extensive experiments', '']",4
"['to  #TAUTHOR_TAG,']","['to  #TAUTHOR_TAG,']","['to  #TAUTHOR_TAG, we expect the users to provide a compound in']","['this work, we address the challenge of semantic type identification of compounds in sanskrit.', 'this is generally treated as a word - level semantic task in nlp  #AUTHOR_TAG.', 'we treat the task as a supervised multiclass classification problem.', 'here, similar to  #TAUTHOR_TAG, we expect the users to provide a compound in its component - wise segmented form as input to the model.', 'but our model relies on distributed representations or embeddings of the input as features, instead of the linguistically involved feature set proposed in  #TAUTHOR_TAG.', 'approaches for compound analysis have been of great interest in nlp for multiple languages including english, italian, dutch and german ( seaghdha and  #AUTHOR_TAG a ).', 'these methods primarily rely on lexical networks, distributional information ( seaghdha and  #AUTHOR_TAG or a combination of both lexical and distributional information  #AUTHOR_TAG.', 'in sanskrit,  #TAUTHOR_TAG proposed a similar statistical approach which combined lexical and distributional information by using information from the lexical network amarakos.', 'a ( nair and and variable length n - grams learned from data using adaptor grammar  #AUTHOR_TAG.', 'here, the authors also adopted rules from as.', 't. a dhyayi as potentially discriminative features for compound type identification  #AUTHOR_TAG.', 'while this model has shown to be effective for the task, it nevertheless is a linguistically involved model.', ' #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG have shown that use of word embedding as the sole features can produce models with competitive results as compared to other feature - rich models.', 'inspired from these observations, we attempt to build similar models which use only embeddings as features for the compound type identification task.', 'compounds in sanskrit can be categorized into 30 possible classes based on how granular categorizations one would like to have  #AUTHOR_TAG.', 'there are slightly altered set of categorizations considered by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG.', 'semantically as.', 't. a dhyayi categorizes the sanskrit compounds into four major semantic classes, namely, avyayibhava, tatpurus.', 'a, bahuvrihi and dvandva  #AUTHOR_TAG.', 'similar to prior computational approaches in sanskrit compounding  #TAUTHOR_TAG, we follow this four class coarse level categorization of the semantic classes in compounds.', 'compounding in sanskrit is extremely productive, or rather recursive, resulting in compound words with multiple components  #AUTHOR_TAG.', 'further, it is observed that compounding of a pair of components may result in compounds of different semantic classes.', 'avyayibhava and tatpurus.', 'a may likely be confusing due to particular sub - category of tatpurus.', 'a if the first component is an avya']",4
"['to  #TAUTHOR_TAG,']","['to  #TAUTHOR_TAG,']","['to  #TAUTHOR_TAG, we expect the users to provide a compound in']","['this work, we address the challenge of semantic type identification of compounds in sanskrit.', 'this is generally treated as a word - level semantic task in nlp  #AUTHOR_TAG.', 'we treat the task as a supervised multiclass classification problem.', 'here, similar to  #TAUTHOR_TAG, we expect the users to provide a compound in its component - wise segmented form as input to the model.', 'but our model relies on distributed representations or embeddings of the input as features, instead of the linguistically involved feature set proposed in  #TAUTHOR_TAG.', 'approaches for compound analysis have been of great interest in nlp for multiple languages including english, italian, dutch and german ( seaghdha and  #AUTHOR_TAG a ).', 'these methods primarily rely on lexical networks, distributional information ( seaghdha and  #AUTHOR_TAG or a combination of both lexical and distributional information  #AUTHOR_TAG.', 'in sanskrit,  #TAUTHOR_TAG proposed a similar statistical approach which combined lexical and distributional information by using information from the lexical network amarakos.', 'a ( nair and and variable length n - grams learned from data using adaptor grammar  #AUTHOR_TAG.', 'here, the authors also adopted rules from as.', 't. a dhyayi as potentially discriminative features for compound type identification  #AUTHOR_TAG.', 'while this model has shown to be effective for the task, it nevertheless is a linguistically involved model.', ' #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG have shown that use of word embedding as the sole features can produce models with competitive results as compared to other feature - rich models.', 'inspired from these observations, we attempt to build similar models which use only embeddings as features for the compound type identification task.', 'compounds in sanskrit can be categorized into 30 possible classes based on how granular categorizations one would like to have  #AUTHOR_TAG.', 'there are slightly altered set of categorizations considered by  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG and  #AUTHOR_TAG.', 'semantically as.', 't. a dhyayi categorizes the sanskrit compounds into four major semantic classes, namely, avyayibhava, tatpurus.', 'a, bahuvrihi and dvandva  #AUTHOR_TAG.', 'similar to prior computational approaches in sanskrit compounding  #TAUTHOR_TAG, we follow this four class coarse level categorization of the semantic classes in compounds.', 'compounding in sanskrit is extremely productive, or rather recursive, resulting in compound words with multiple components  #AUTHOR_TAG.', 'further, it is observed that compounding of a pair of components may result in compounds of different semantic classes.', 'avyayibhava and tatpurus.', 'a may likely be confusing due to particular sub - category of tatpurus.', 'a if the first component is an avya']",6
"['- cumulative n - grams  #TAUTHOR_TAG.', 'however pcfg surpr']","['non - cumulative n - grams  #TAUTHOR_TAG.', 'however pcfg surprisal,']","['to be more predictive of reading times than the usual non - cumulative n - grams  #TAUTHOR_TAG.', 'however pcfg surprisal,']","['', 'therefore, the generative model assumed by those studies does not account for the information contributed by the skipped words even though those words must be processed by readers.', '1 this deficiency can be addressed by summing surprisal measures over the saccade region ( see figure 1 ), and the resulting cumulative n - grams have been shown to be more predictive of reading times than the usual non - cumulative n - grams  #TAUTHOR_TAG.', 'however pcfg surprisal, which has a similar deficiency when non - cumulatively modeling reading times, has not previously been found to be predictive when accumulated over saccade regions.', 'this paper uses a reading time corpus to investigate two accumulation techniques ( pre - and post - saccade ) and finds that both forms of accumulation improve the fit of n - gram surprisal to reading times.', 'however, even though accumulated n - grams demonstrate that the lexical sequence of the saccade region is processed, pcfg surprisal does not seem to be improved by either accumulation technique.', 'the results of this work call into question the usual formulation of pcfg surprisal as a reading time predictor and suggest future directions for investigation of the influence of upcoming material on reading times.', 'figure 1 : eye movements jump between non - adjacent fixation regions ( 1, 2 ), while traditional n - gram measures are conditioned on the preceding adjacent context, which is never generated by the typical surprisal models used in eye - tracking studies.', 'cumulative n - grams sum the n - gram measures over the entire skipped region in order to better capture the information that readers need to process']",0
"['1 ).', 'in line with previous findings on the dundee corpus  #TAUTHOR_TAG 0. 05 ).', 'the benefit of cumulative n - grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words']","['1 ).', 'in line with previous findings on the dundee corpus  #TAUTHOR_TAG 0. 05 ).', 'the benefit of cumulative n - grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words']","['the 5 - gram measures ( see table 1 ).', 'in line with previous findings on the dundee corpus  #TAUTHOR_TAG 0. 05 ).', 'the benefit of cumulative n - grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words']","['. 1 cumulative n - gram surprisal n - gram surprisal is conditioned on the preceding context ( see equation 1 ).', 'as stated in the introduction, however, direct use of this factor in a reading time model ignores the fact that some or all of the preceding context may not be generated if the associated lexical targets were not previously fixated by readers ( see figure 1 ).', 'the lack of a generated condition results in a probability model that does not reflect the influence of words skipped during saccades.', 'this deficiency can be corrected by accumulating n - gram surprisal over the entire saccade region ( see', 'where w is a vector of input tokens, f t−1 is the index of the previous fixation, f t is the index of the current fixation.', 'the linear mixed model 3 that was used in this experiment included item, subject, and sentence id - crossed - with - subject random intercepts 4 as well as by - subject random slopes and fixed effects for the following predictors : sentence position ( sentpos ), word length ( wlen ), region length ( rlen ), 5 whether the previous word was fixated ( prevfix ), 5 - grams and cumulative 5 - grams.', 'likelihood ratio tests were used to compare the mixed model with and without fixed effects for the 5 - gram measures ( see table 1 ).', 'in line with previous findings on the dundee corpus  #TAUTHOR_TAG 0. 05 ).', 'the benefit of cumulative n - grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words']",3
"['##e corpus  #TAUTHOR_TAG.', 'in fact, not even basic pc']","['corpus  #TAUTHOR_TAG.', 'in fact, not even basic pcfg surprisal was predictive ( p > 0. 05 ) over']","['the dundee corpus  #TAUTHOR_TAG.', 'in fact, not even basic pcfg surprisal was predictive ( p > 0. 05 ) over this baseline model in']","['context - free grammar ( pcfg ) surprisal is similar to n - gram surprisal in that it is also conditioned on preceding context, but pcfg surprisal is conditioned on hierarchic structure rather than on linear lexical sequences ( see equation 3 ).', 'pcfg surprisal, therefore, suffers from the same deficiency as non - cumulative n - gram surprisal when modeling reading times : the condition context is never generated by the model.', 'where w is a vector of input tokens, f t−1 is the index of the previous fixation, f t is the index of the current fixation, t is a random variable over syntactic trees and t i is a terminal symbol in a tree.', 'this experiment tested both pcfg surprisal predictors as fixed effects over the baseline from the previous section ( now including cumulative n - gram surprisal as a fixed and by - subject random effect ).', 'accumulated pcfg surprisal ( see equation 4 ) did not improve reading time fit ( p > 0. 05 ), unlike n - gram surprisal, which replicates a previous result using the dundee corpus  #TAUTHOR_TAG.', 'in fact, not even basic pcfg surprisal was predictive ( p > 0. 05 ) over this baseline model in the ucl corpus, whereas it was predictive over this baseline in the dundee corpus.', 'posthoc testing on the exploratory data partition revealed that pcfg surprisal becomes predictive on the ucl corpus when the n - gram predictors are removed from the baseline ( p < 0. 001 ), which could indicate that pcfg surprisal may simply help predict reading times when the n - gram model is too weak.', 'alternatively, since ucl sentences were chosen for their brevity during corpus construction, there just may not be enough syntactic complexity in the corpus to provide an advantage to pcfg surprisal over the n - gram measures, which would explain why pcfg surprisal is still predictive for dundee reading times where there is greater syntactic complexity.', 'however, since cumulative n - gram surprisal is a better predictor of reading times than basic n - gram surprisal, it is conceivable that some other cumulative pcfg surprisal feature could still show up as predictive of ucl reading times even when basic pcfg surprisal fails to be predictive on this corpus.', 'the next experiment formulates a new calculation of cumulative surprisal to explore this possibility']",3
"['- cumulative reading times  #TAUTHOR_TAG.', 'in addition, this']","['non - cumulative reading times  #TAUTHOR_TAG.', 'in addition, this']","['work has confirmed previous findings that cumulative n - grams provide a better model of reading times than the typical non - cumulative reading times  #TAUTHOR_TAG.', 'in addition, this work has confirmed previous findings that upcoming lexical items']","['work has confirmed previous findings that cumulative n - grams provide a better model of reading times than the typical non - cumulative reading times  #TAUTHOR_TAG.', 'in addition, this work has confirmed previous findings that upcoming lexical items can affect reading times in an n - gram successor effect  #AUTHOR_TAG, presumably ruling out incompatible expectations before directly fixating on that material or so that such material can be skipped via saccade.', 'the fact that cumulative n - gram models strongly predict reading times suggests pcfg surprisal should be similarly affected, but this work has failed to find such an effect either before or at each given target word.', 'the improved reading time fit for accumulated n - gram surprisal suggests that the material skipped during a saccade is processed with a reading time cost.', 'therefore, although pcfg surprisal has previously been found to predict reading times over an n - gram baseline  #AUTHOR_TAG, the lack of accumulation raises questions about pcfg surprisal as a predictor of the reading time influence of syntactic processing.', 'finally, the existence of n - gram successor effects raises questions about other informationtheoretic measures such as entropy reduction  #AUTHOR_TAG.', 'entropy reduction measures the change in uncertainty at each new word.', 'in practice, the entropy of an observation is often approximated by estimating uncertainty about the next word in a sequence given the preceding observations, but this measurement does not make much sense if the following two words are already being integrated along with the target observation ( i. e. there is very little to no uncertainty about the next word in the sequence ).', 'thus, the frontier of processing must be determined for a well - motivated measure of entropy reduction.', 'in conclusion, the results of this study provide greater insight into how lexical sequence information is processed during reading, providing stronger baseline measures against which to test higher level theories of sentence processing in the future']",3
"['1 ).', 'in line with previous findings on the dundee corpus  #TAUTHOR_TAG 0. 05 ).', 'the benefit of cumulative n - grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words']","['1 ).', 'in line with previous findings on the dundee corpus  #TAUTHOR_TAG 0. 05 ).', 'the benefit of cumulative n - grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words']","['the 5 - gram measures ( see table 1 ).', 'in line with previous findings on the dundee corpus  #TAUTHOR_TAG 0. 05 ).', 'the benefit of cumulative n - grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words']","['. 1 cumulative n - gram surprisal n - gram surprisal is conditioned on the preceding context ( see equation 1 ).', 'as stated in the introduction, however, direct use of this factor in a reading time model ignores the fact that some or all of the preceding context may not be generated if the associated lexical targets were not previously fixated by readers ( see figure 1 ).', 'the lack of a generated condition results in a probability model that does not reflect the influence of words skipped during saccades.', 'this deficiency can be corrected by accumulating n - gram surprisal over the entire saccade region ( see', 'where w is a vector of input tokens, f t−1 is the index of the previous fixation, f t is the index of the current fixation.', 'the linear mixed model 3 that was used in this experiment included item, subject, and sentence id - crossed - with - subject random intercepts 4 as well as by - subject random slopes and fixed effects for the following predictors : sentence position ( sentpos ), word length ( wlen ), region length ( rlen ), 5 whether the previous word was fixated ( prevfix ), 5 - grams and cumulative 5 - grams.', 'likelihood ratio tests were used to compare the mixed model with and without fixed effects for the 5 - gram measures ( see table 1 ).', 'in line with previous findings on the dundee corpus  #TAUTHOR_TAG 0. 05 ).', 'the benefit of cumulative n - grams suggests that the lexical processing of words skipped during a saccade has a time cost similar to directly fixated words']",4
"[')  #TAUTHOR_TAG.', 'the grammar formalism']","['( smt )  #TAUTHOR_TAG.', 'the grammar formalism']","[')  #TAUTHOR_TAG.', 'the grammar formalism']","['use of various synchronous grammar based formalisms has been a trend for statistical machine translation ( smt )  #TAUTHOR_TAG.', 'the grammar formalism determines the intrinsic capacities and computational efficiency of the smt systems.', 'to evaluate the capacity of a grammar formalism, two factors, i. e. generative power and expressive power are usually considered  #AUTHOR_TAG.', 'the generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities.', '']",0
"[')  #TAUTHOR_TAG.', 'the grammar formalism']","['( smt )  #TAUTHOR_TAG.', 'the grammar formalism']","[')  #TAUTHOR_TAG.', 'the grammar formalism']","['use of various synchronous grammar based formalisms has been a trend for statistical machine translation ( smt )  #TAUTHOR_TAG.', 'the grammar formalism determines the intrinsic capacities and computational efficiency of the smt systems.', 'to evaluate the capacity of a grammar formalism, two factors, i. e. generative power and expressive power are usually considered  #AUTHOR_TAG.', 'the generative power refers to the ability to generate the strings of the language, and the expressive power to the ability to describe the same language with fewer or no extra ambiguities.', '']",4
"['in current implementation can be considered as a combination of the ones in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'given the sentence pair in']","['in current implementation can be considered as a combination of the ones in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'given the sentence pair in']","['in current implementation can be considered as a combination of the ones in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'given the sentence pair in figure 1, some ssg rules']","[', the proposed synthetic synchronous grammar ( ssg ) is a tuple', 'where σ s ( σ t ) is the alphabet set of source ( target ) terminals, namely the vocabulary ; n s ( n t ) is the alphabet set of source ( target ) non - terminals, such as the pos tags and the syntax labels ; x represents the special nonterminal label in fscfg ; and p is the grammar rule set which is the core part of a grammar.', 'every rule r in p is as :', 'where α ∈ [ { x }, n s, σ s ] + is a sequence of one or more source words in σ s and nonterminals symbols in [ { x }, n s ] ; γ ∈ [ { x }, n t, σ t ] + is a sequence of one or more target words in σ t and nonterminals symbols in [ { x }, n t ] ; a t is a many - tomany corresponding set which includes the alignments between the terminal leaf nodes from source and target side, and a n t is a one - to - one corresponding set which includes the synchronizing relations between the non - terminal leaf nodes from source and target side ; ω contains feature values associated with each rule.', 'through this formalization, we can see that fscfg rules and lstssg rules are both included.', 'however, we should point out that the rules with mixture of x non - terminals and syntactic non - terminals are not included in our current implementation despite that they are legal under the proposed formalism.', 'the rule extraction in current implementation can be considered as a combination of the ones in  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'given the sentence pair in figure 1, some ssg rules can be extracted as illustrated in figure 2']",5
"['adopt the conventional features as in  #TAUTHOR_TAG in our current implementation.', '']","['adopt the conventional features as in  #TAUTHOR_TAG in our current implementation.', '']","['adopt the conventional features as in  #TAUTHOR_TAG in our current implementation.', 'figure 2 : some synthetic synchronous grammar rules']","['', 'for example, some features related with structure richness and grammar consistency 1 of a derivation should be designed to distinguish the derivations involved various heterogeneous rule applications.', 'for the page limit and the fair comparison, we only adopt the conventional features as in  #TAUTHOR_TAG in our current implementation.', 'figure 2 : some synthetic synchronous grammar rules can be extracted from the sentence pair in figure 1.', '']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'fscfg an in - house implementation of purely formally scfg based model similar']","['system, named hitree, is implemented in standard c + + and stl.', 'in this section we report on experiments with chinese - to - english translation base on it.', 'we used fbis chinese - to - english parallel corpora ( 7. 2m + 9. 2m words ) as the training data.', 'we also used sri language modeling toolkit to train a 4 - gram language model on the xinhua portion of the english gigaword corpus ( 181m words ).', 'nist mt2002 test set is used as the development set.', 'the nist mt2005 test set is used as the test set.', 'the evaluation metric is case - sensitive bleu4.', ""for significant test, we used zhang's implementation  #AUTHOR_TAG ( confidence level of 95 % )."", 'for comparisons, we used the following three baseline systems : lstssg an in - house implementation of linguistically motivated stssg based model similar to  #TAUTHOR_TAG.', 'fscfg an in - house implementation of purely formally scfg based model similar to  #AUTHOR_TAG.', 'mbr we use an in - house combination system which is an implementation of a classic sentence level combination method based on the minimum bayes risk ( mbr ) decoding  #AUTHOR_TAG']",5
[' #TAUTHOR_TAG reported a precision close to'],[' #TAUTHOR_TAG reported a precision close to'],[' #TAUTHOR_TAG reported a precision close to'],"[' #TAUTHOR_TAG reported a precision close to 0. 6 over a random sample of 49 words, we take another random sample of 100 words separately and repeat manual evaluation.', 'when we extract the novel senses by comparing the dts from 1909 - 1953 and 2002 - 2005, the precision obtained for these 100 words is as low as 0. 32.', 'similarly if we extract the novel senses comparing the dts of 1909 - 1953 with 2006 - 2008, the precision stands at 0. 23.', 'we then explore another unsupervised approach presented in lau et al. [ 16 ] over the same google books corpus 1, apply topic modeling for sense induction and directly adapt their similarity measure to get the new senses.', 'using a set intersecting with the 100 random samples for  #TAUTHOR_TAG, we obtain the precision values of 0. 21 and 0. 28, respectively.', 'clearly, none of the precision values are good enough for reliable novel sense detection.', 'this motivates us to devise a new approach to improve the precision of the existing approaches.', 'further, being inspired by the recent works of applying complex network theory in nlp applications like co - hyponymy detection [ 13 ], evaluating machine generated summaries [ 20 ], detection of ambiguity in a text [ 4 ], etc. we opt for a solution using complex network measures']",6
"['positives and thereby, increase the overall precision of the method proposed by  #TAUTHOR_TAG.', 'in particular, if']","['positives and thereby, increase the overall precision of the method proposed by  #TAUTHOR_TAG.', 'in particular, if']","['false positives and thereby, increase the overall precision of the method proposed by  #TAUTHOR_TAG.', ""in particular, if a target word qualifies as a'birth'as per their method, we construct two induced subgraphs of those words that""]","['propose a method based on the network features to reduce the number of false positives and thereby, increase the overall precision of the method proposed by  #TAUTHOR_TAG.', ""in particular, if a target word qualifies as a'birth'as per their method, we construct two induced subgraphs of those words that form the cluster corresponding to this'birth'sense from the corresponding distributional thesauri ( dt ) networks of the two time points."", 'next we compare the following three network properties : ( i ) the edge density, ( ii ) the structural similarity and ( iii ) the average path length [ 27, 29 ] of the two induced subgraphs from the two time points.', ""a remarkable observation is that although this is a small set of only three features, for the actual'birth'cases, each of them has a significantly different value for the later time point and are therefore very discriminative indicators."", 'in fact, the features are so powerful that even a small set of training instances is sufficient for making highly accurate predictions.', 'results : manual evaluation of the results by 3 evaluators shows that this classification achieves an overall precision of 0. 86 and 0. 74 for the two time point pairs over the same set of samples, in contrast with the precision values of 0. 32 and 0. 23 by the original method.', 'note that we would like to stress here that an improvement of more than double in the precision of novel sense detection that we achieve has the potential to be the new stepping stone in many nlp and ir applications that are sensitive to novel senses of a word']",6
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",6
[' #TAUTHOR_TAG reported a precision close to'],[' #TAUTHOR_TAG reported a precision close to'],[' #TAUTHOR_TAG reported a precision close to'],"[' #TAUTHOR_TAG reported a precision close to 0. 6 over a random sample of 49 words, we take another random sample of 100 words separately and repeat manual evaluation.', 'when we extract the novel senses by comparing the dts from 1909 - 1953 and 2002 - 2005, the precision obtained for these 100 words is as low as 0. 32.', 'similarly if we extract the novel senses comparing the dts of 1909 - 1953 with 2006 - 2008, the precision stands at 0. 23.', 'we then explore another unsupervised approach presented in lau et al. [ 16 ] over the same google books corpus 1, apply topic modeling for sense induction and directly adapt their similarity measure to get the new senses.', 'using a set intersecting with the 100 random samples for  #TAUTHOR_TAG, we obtain the precision values of 0. 21 and 0. 28, respectively.', 'clearly, none of the precision values are good enough for reliable novel sense detection.', 'this motivates us to devise a new approach to improve the precision of the existing approaches.', 'further, being inspired by the recent works of applying complex network theory in nlp applications like co - hyponymy detection [ 13 ], evaluating machine generated summaries [ 20 ], detection of ambiguity in a text [ 4 ], etc. we opt for a solution using complex network measures']",5
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",5
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",5
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",5
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",5
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",5
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",5
['.  #TAUTHOR_TAG'],['.  #TAUTHOR_TAG'],"['.  #TAUTHOR_TAG. after getting the novel sense clusters, we pick up 50 random samples, of which']","['genre balanced and is a well constructed prototype of american english over 200 years, from the time period 1810 to 2000', '. we extract the raw text data of two time slices : 1880 - 1900 and 1990 - 2000 for', 'our experiment. experiment details and results : we first construct distributional thesauri ( dt )', 'networks [ 22 ] for the coha corpus at two different time points, 1880 - 1900 and 1990 - 2000. we apply chinese whispers algorithm [ 2 ] to produce a set of', 'clusters for each target word in the dt network. the chinese whispers clusters for the target word', ""' web'are shown in figure 3. note that we have reported only some of the representative words for each cluster. each of the clusters represents a particular sense of the target. we now compare"", 'the sense clusters extracted across two different time points to obtain the suitable signals of sense change following the approach proposed', ""in mitra et al.  #TAUTHOR_TAG. after getting the novel sense clusters, we pick up 50 random samples, of which 25 cases are flagged as'true"", '']",5
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",4
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",4
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",4
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",4
['.  #TAUTHOR_TAG'],['.  #TAUTHOR_TAG'],"['.  #TAUTHOR_TAG. after getting the novel sense clusters, we pick up 50 random samples, of which']","['genre balanced and is a well constructed prototype of american english over 200 years, from the time period 1810 to 2000', '. we extract the raw text data of two time slices : 1880 - 1900 and 1990 - 2000 for', 'our experiment. experiment details and results : we first construct distributional thesauri ( dt )', 'networks [ 22 ] for the coha corpus at two different time points, 1880 - 1900 and 1990 - 2000. we apply chinese whispers algorithm [ 2 ] to produce a set of', 'clusters for each target word in the dt network. the chinese whispers clusters for the target word', ""' web'are shown in figure 3. note that we have reported only some of the representative words for each cluster. each of the clusters represents a particular sense of the target. we now compare"", 'the sense clusters extracted across two different time points to obtain the suitable signals of sense change following the approach proposed', ""in mitra et al.  #TAUTHOR_TAG. after getting the novel sense clusters, we pick up 50 random samples, of which 25 cases are flagged as'true"", '']",4
"['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG']","['detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the']","['', ') respectively, quite consistent with those reported in table 6. we did another experiment in order to estimate the performance of our model for detecting novel sense, independent of', 'the method of  #TAUTHOR_TAG. we take 100 random words from the two time point pairs ( t 1 and t 2 ), along with all the induced clusters from the newer time period and', '']",3
"['systems when evaluated on standard benchmarks  #TAUTHOR_TAG.', 'in addition, ukb has been extended']","['systems when evaluated on standard benchmarks  #TAUTHOR_TAG.', 'in addition, ukb has been extended']","['based systems when evaluated on standard benchmarks  #TAUTHOR_TAG.', 'in addition, ukb has been extended']","['sense disambiguation ( wsd ) is the problem of assigning the correct sense of a word in a context  #AUTHOR_TAG.', 'traditionally, supervised approaches have attained the best results in the area, but they are expensive to build because of the need of large amounts of manually annotated examples.', 'alternatively, knowledge based approaches rely on lexical resources such as wordnet, which are nowadays widely available in many languages  #AUTHOR_TAG 1.', 'in particular, graph - based approaches represent the knowledge base as a graph, and apply several well - known graph analysis algorithms to perform wsd.', 'ukb is a collection of programs which was first released for performing graph - based word sense disambiguation using a preexisting knowledge base such as wordnet, and attained state - of - the - art results among knowledge - based systems when evaluated on standard benchmarks  #TAUTHOR_TAG.', 'in addition, ukb has been extended to perform disambiguation of medical entities  #AUTHOR_TAG, named - entities  #AUTHOR_TAG word similarity and to create knowledge - based word embeddings  #AUTHOR_TAG.', 'all programs are open source 2, 3 and are accompanied by the resources and instructions necessary to reproduce the results.', 'the software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011.', 'the software is coded in c + + and released under the gpl v3. 0 license.', 'when ukb was released, the papers specified the optimal parameters for wsd  #TAUTHOR_TAG, as well as other key issues like the underlying knowledge - base version, specific set of relations to be used, and method to pre - process the input text.', 'at the time, we assumed that future researchers would use the optimal parameters and settings specified in the papers, and that they would contact the authors if in doubt.', 'the default parameters of the software were not optimal, and the other issues were left under the users responsibility.', 'the assumption failed, and several papers reported low results in some new datasets ( including updated versions of older datasets ), as we will see in the following sections']",0
"['systems when evaluated on standard benchmarks  #TAUTHOR_TAG.', 'in addition, ukb has been extended']","['systems when evaluated on standard benchmarks  #TAUTHOR_TAG.', 'in addition, ukb has been extended']","['based systems when evaluated on standard benchmarks  #TAUTHOR_TAG.', 'in addition, ukb has been extended']","['sense disambiguation ( wsd ) is the problem of assigning the correct sense of a word in a context  #AUTHOR_TAG.', 'traditionally, supervised approaches have attained the best results in the area, but they are expensive to build because of the need of large amounts of manually annotated examples.', 'alternatively, knowledge based approaches rely on lexical resources such as wordnet, which are nowadays widely available in many languages  #AUTHOR_TAG 1.', 'in particular, graph - based approaches represent the knowledge base as a graph, and apply several well - known graph analysis algorithms to perform wsd.', 'ukb is a collection of programs which was first released for performing graph - based word sense disambiguation using a preexisting knowledge base such as wordnet, and attained state - of - the - art results among knowledge - based systems when evaluated on standard benchmarks  #TAUTHOR_TAG.', 'in addition, ukb has been extended to perform disambiguation of medical entities  #AUTHOR_TAG, named - entities  #AUTHOR_TAG word similarity and to create knowledge - based word embeddings  #AUTHOR_TAG.', 'all programs are open source 2, 3 and are accompanied by the resources and instructions necessary to reproduce the results.', 'the software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011.', 'the software is coded in c + + and released under the gpl v3. 0 license.', 'when ukb was released, the papers specified the optimal parameters for wsd  #TAUTHOR_TAG, as well as other key issues like the underlying knowledge - base version, specific set of relations to be used, and method to pre - process the input text.', 'at the time, we assumed that future researchers would use the optimal parameters and settings specified in the papers, and that they would contact the authors if in doubt.', 'the default parameters of the software were not optimal, and the other issues were left under the users responsibility.', 'the assumption failed, and several papers reported low results in some new datasets ( including updated versions of older datasets ), as we will see in the following sections']",0
"['a linear combination', 'of sense frequencies and graph', '- based sense probabilities. the use of sense frequencies with', 'ukb was introduced in  #TAUTHOR_TAG']","['a linear combination', 'of sense frequencies and graph', '- based sense probabilities. the use of sense frequencies with', 'ukb was introduced in  #TAUTHOR_TAG']","['initializing context words, and is also used to produce the final sense weights as a linear combination', 'of sense frequencies and graph', '- based sense probabilities. the use of sense frequencies with', 'ukb was introduced in  #TAUTHOR_TAG']","['', 'perform a random walk in the graph personalized on the word context. it yields the best results overall, at the cost of', 'being more time consuming that', 'the rest. ppr : same as above, but apply personalized pagerank to each sentence', 'only once, disambiguating all content words in the sentence in one go. it is thus faster that the previous approach,', 'but obtains worse results. dfs : unlike the two previous algorithms, which consider the wordnet graph as a whole, this algorithm first creates', 'a subgraph for each', 'context, following the method first presented in  #AUTHOR_TAG, and then runs the page', '##rank algorithm over the subgraph. this option represents a compromise between ppr w2w and ppr, as it faster than than the former while better than the latter. • the page', '##rank algorithm has two parameters which were set as follows : number of iterations of power method ( pra', '##nk iter ) 30, and damping factor ( prank damping ) 0. 85.  #AUTHOR_TAG a ). best results in bold. note that  #AUTHOR_TAG b', ') used s07 for development. • use of sense frequencies ( dict', 'weight ). sense frequencies are a valuable piece of information that describe the frequencies of the associations between a word and its possible senses. the frequencies are often derived from manually sense annotated corpora, such as semcor  #AUTHOR_TAG. we use the sense', 'frequency accompanying wordnet, which, according to the documentation, "" represents the decimal number of times the sense is tagged in various semantic concordance', 'texts "". the frequencies are smoothed adding one to all counts ( dict weight smooth ). the sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination', 'of sense frequencies and graph', '- based sense probabilities. the use of sense frequencies with', 'ukb was introduced in  #TAUTHOR_TAG']",0
"['a linear combination', 'of sense frequencies and graph', '- based sense probabilities. the use of sense frequencies with', 'ukb was introduced in  #TAUTHOR_TAG']","['a linear combination', 'of sense frequencies and graph', '- based sense probabilities. the use of sense frequencies with', 'ukb was introduced in  #TAUTHOR_TAG']","['initializing context words, and is also used to produce the final sense weights as a linear combination', 'of sense frequencies and graph', '- based sense probabilities. the use of sense frequencies with', 'ukb was introduced in  #TAUTHOR_TAG']","['', 'perform a random walk in the graph personalized on the word context. it yields the best results overall, at the cost of', 'being more time consuming that', 'the rest. ppr : same as above, but apply personalized pagerank to each sentence', 'only once, disambiguating all content words in the sentence in one go. it is thus faster that the previous approach,', 'but obtains worse results. dfs : unlike the two previous algorithms, which consider the wordnet graph as a whole, this algorithm first creates', 'a subgraph for each', 'context, following the method first presented in  #AUTHOR_TAG, and then runs the page', '##rank algorithm over the subgraph. this option represents a compromise between ppr w2w and ppr, as it faster than than the former while better than the latter. • the page', '##rank algorithm has two parameters which were set as follows : number of iterations of power method ( pra', '##nk iter ) 30, and damping factor ( prank damping ) 0. 85.  #AUTHOR_TAG a ). best results in bold. note that  #AUTHOR_TAG b', ') used s07 for development. • use of sense frequencies ( dict', 'weight ). sense frequencies are a valuable piece of information that describe the frequencies of the associations between a word and its possible senses. the frequencies are often derived from manually sense annotated corpora, such as semcor  #AUTHOR_TAG. we use the sense', 'frequency accompanying wordnet, which, according to the documentation, "" represents the decimal number of times the sense is tagged in various semantic concordance', 'texts "". the frequencies are smoothed adding one to all counts ( dict weight smooth ). the sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination', 'of sense frequencies and graph', '- based sense probabilities. the use of sense frequencies with', 'ukb was introduced in  #TAUTHOR_TAG']",5
"['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['in  #TAUTHOR_TAG,']","['', 'as the results show, that paper reports a suboptimal use of ukb.', 'in more recent work,  #AUTHOR_TAG take up that result and report it in their paper as well.', 'the difference is of nearly 10 absolute f1 points overall.', '5 this decrease could be caused by the fact that  #AUTHOR_TAG a ) did not use sense frequencies.', 'in addition to ukb, the table also reports the 5 note that the ukb results for s2, s3 and s07 ( 62. 6, 63. 0 and 48. 6 respectively ) are different from those in  #TAUTHOR_TAG, which is to be expected, as the new datasets have been converted to wordnet 3. 0 ( we confirmed experimentally that this is the sole difference between the two experiments ).', 'best performing knowledge - based systems on this dataset.', ' #AUTHOR_TAG a ) run several wellknown algorithms when presenting their datasets.', 'we also report  #AUTHOR_TAG, the latest work on this area, as well as the most frequent sense as given by wordnet counts ( see section 3 ).', 'the table shows that ukb yields the best overall result.', 'note that  #AUTHOR_TAG do not use sense frequency information.', 'for completeness, table 2 reports the results of supervised systems on the same dataset, taken from the two works that use the dataset  #AUTHOR_TAG b ).', 'as expected, supervised systems outperform knowledge - based systems, by a small margin in some of the cases']",4
['addition to the results of ukb using the setting in  #TAUTHOR_TAG as specified in'],['addition to the results of ukb using the setting in  #TAUTHOR_TAG as specified in'],"['addition to the results of ukb using the setting in  #TAUTHOR_TAG as specified in section 3, we checked']","['addition to the results of ukb using the setting in  #TAUTHOR_TAG as specified in section 3, we checked whether some reasonable settings would obtain better results.', 'table 3 shows the results when applying the three algorithms described in section 3, both with and without sense frequencies, as well as using a single sentence for context or extended context.', 'the table shows that the key factor is the use of sense frequencies, and systems that do not use them ( those with a nf subscript ) suffer a loss between 7 and 8 percentage points in f1.', 'this would explain part of the decrease in performance reported in  #AUTHOR_TAG a ), as they explicitly mention that they did not activate the use of sense frequencies in ukb.', 'the table also shows that extending the context is mildly effective.', 'regarding the algorithm, the table confirms that the best method is ppr w2w, followed by the subgraph approach ( dfs ) and ppr']",6
['text correspondence model  #TAUTHOR_TAG and demonstrate that exploiting the noncontradic'],"['discovers correspondence between lexical entries and latent semantic concepts.', 'we consider the generative semantics - text correspondence model  #TAUTHOR_TAG and demonstrate that exploiting the noncontradiction relation']","['##s correspondence between lexical entries and latent semantic concepts.', 'we consider the generative semantics - text correspondence model  #TAUTHOR_TAG and demonstrate that exploiting the noncontradiction relation']","['argue that groups of unannotated texts with overlapping and non - contradictory semantics represent a valuable source of information for learning semantic representations.', 'a simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts.', 'we consider the generative semantics - text correspondence model  #TAUTHOR_TAG and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human - written weather forecasts']",7
[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where the original textual'],"['', 'or "" sunny "". this model is hard to evaluate directly as text does not provide information about all the fields and does not necessarily provide it at', 'the sufficient granularity level. therefore, it is natural to evaluate their model on the database - text alignment problem  #AUTHOR_TAG, i. e. measuring how well the model predicts the alignment between the text and the observable records describing the entire world state. we follow their set - up, but assume that instead of', 'having access to the full semantic state for every training example, we have', 'a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics. we', 'study our set - up on the weather forecast data  #TAUTHOR_TAG where the original textual weather forecasts were complemented by additional forecasts describing the same', 'weather states ( see figure 1 for an example ). the average overlap between the verbalized fields in each group', 'of noncontradictory forecasts was below 35 %, and more than 60 % of fields are mentioned only in a single forecast from a group. our model, learned from 100 labeled forecasts and 259 groups of unannotated non - contradictory', 'forecasts ( 750 texts in total ), achieved 73. 9 % f 1. this compares favorably with 69. 1 % shown by a semi - supervised learning approach, though, as expected, does not reach the score of', 'the model which, in training, observed semantics states for all the 750 documents ( 77. 7 % f 1 ). the rest of the paper is structured as follows. in section 2 we describe our inference algorithm for groups of non - contradictory documents. section 3 redescribes the semantics - text', '']",7
"['reader to the original publication  #TAUTHOR_TAG. in our experiments, when choosing a world state s, we generate the field values', 'independently. this is clearly a suboptimal regime as often there are', 'very strong dependencies between field values :']","['reader to the original publication  #TAUTHOR_TAG. in our experiments, when choosing a world state s, we generate the field values', 'independently. this is clearly a suboptimal regime as often there are', 'very strong dependencies between field values : e. g., in']","['reader to the original publication  #TAUTHOR_TAG. in our experiments, when choosing a world state s, we generate the field values', 'independently. this is clearly a suboptimal regime as often there are', 'very strong dependencies between field values :']","['', ') or distorting ( up or down, modeled by a geometric distribution ). the parameters corresponding to each form of generation are estimated during learning.', 'for details on these emission models, as well as for details on modeling record and field transitions, we refer the reader to the original publication  #TAUTHOR_TAG. in our experiments, when choosing a world state s, we generate the field values', 'independently. this is clearly a suboptimal regime as often there are', 'very strong dependencies between field values : e. g., in the weather domain many record types', 'contain groups of related fields defining minimal, maximal and average values of some parameter. extending the method', 'to model, e. g., pairwise dependencies', 'between field values is relatively straightforward. as explained above, semantics of a text m is defined by the assignment of state variables s', '. analogously, an alignment a between', 'semantics m and a text w is represented by all the remaining latent variables : by the sequence of record types t = ( t 1,..., t | t | ), choice of records r', 'i for each t i, the field sequence f i and the segment length c ij', 'for every field f ij']",7
[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where the original textual'],"['', 'or "" sunny "". this model is hard to evaluate directly as text does not provide information about all the fields and does not necessarily provide it at', 'the sufficient granularity level. therefore, it is natural to evaluate their model on the database - text alignment problem  #AUTHOR_TAG, i. e. measuring how well the model predicts the alignment between the text and the observable records describing the entire world state. we follow their set - up, but assume that instead of', 'having access to the full semantic state for every training example, we have', 'a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics. we', 'study our set - up on the weather forecast data  #TAUTHOR_TAG where the original textual weather forecasts were complemented by additional forecasts describing the same', 'weather states ( see figure 1 for an example ). the average overlap between the verbalized fields in each group', 'of noncontradictory forecasts was below 35 %, and more than 60 % of fields are mentioned only in a single forecast from a group. our model, learned from 100 labeled forecasts and 259 groups of unannotated non - contradictory', 'forecasts ( 750 texts in total ), achieved 73. 9 % f 1. this compares favorably with 69. 1 % shown by a semi - supervised learning approach, though, as expected, does not reach the score of', 'the model which, in training, observed semantics states for all the 750 documents ( 77. 7 % f 1 ). the rest of the paper is structured as follows. in section 2 we describe our inference algorithm for groups of non - contradictory documents. section 3 redescribes the semantics - text', '']",0
[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where the original textual'],"['', 'or "" sunny "". this model is hard to evaluate directly as text does not provide information about all the fields and does not necessarily provide it at', 'the sufficient granularity level. therefore, it is natural to evaluate their model on the database - text alignment problem  #AUTHOR_TAG, i. e. measuring how well the model predicts the alignment between the text and the observable records describing the entire world state. we follow their set - up, but assume that instead of', 'having access to the full semantic state for every training example, we have', 'a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics. we', 'study our set - up on the weather forecast data  #TAUTHOR_TAG where the original textual weather forecasts were complemented by additional forecasts describing the same', 'weather states ( see figure 1 for an example ). the average overlap between the verbalized fields in each group', 'of noncontradictory forecasts was below 35 %, and more than 60 % of fields are mentioned only in a single forecast from a group. our model, learned from 100 labeled forecasts and 259 groups of unannotated non - contradictory', 'forecasts ( 750 texts in total ), achieved 73. 9 % f 1. this compares favorably with 69. 1 % shown by a semi - supervised learning approach, though, as expected, does not reach the score of', 'the model which, in training, observed semantics states for all the 750 documents ( 77. 7 % f 1 ). the rest of the paper is structured as follows. in section 2 we describe our inference algorithm for groups of non - contradictory documents. section 3 redescribes the semantics - text', '']",0
"['- up of learning with unaligned semantic states considered in  #TAUTHOR_TAG. the induced alignments a 1,', '..., a k of semantics m to texts w 1,..., w k at the same time']","['- up of learning with unaligned semantic states considered in  #TAUTHOR_TAG. the induced alignments a 1,', '..., a k of semantics m to texts w 1,..., w k at the same time']","['of meaning representations made on the earlier stages. as soon as semantics m k are', 'inferred for every k, we find ourselves', 'in the set - up of learning with unaligned semantic states considered in  #TAUTHOR_TAG. the induced alignments a 1,', '..., a k of semantics m to texts w 1,..., w k at the same time']","['', 'w n k. it starts with an empty ordering n = ( ) and an empty list of meanings m = ( ) ( line 1 ). then it iteratively predicts meaning representationsm j conditioned on the list of semantics m = ( m 1,..., m i−1 ) fixed on the', 'previous stages and does it for all the remaining texts w j ( lines 3 - 5 ).', 'the algorithm selects a single meaningm j which maximizes the probability of all the remaining texts and excludes the text j', 'from future consideration ( lines 6 - 7 ). though the semantics m k ( k / ∈ n∪ { j } ) used in the', 'estimates ( line 6 ) can be inconsistent with each other, the final list of meanings m is guaranteed to be consistent. it holds because on each iteration we add a single meaningm n i to m ( line 7 ), and m n i is guaranteed to', 'be consistent with m, as the semanticsm n i was conditioned on the meaning m during inference ( line 4', ""). an important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future '"", ') texts do affect the choice of meaning representations made on the earlier stages. as soon as semantics m k are', 'inferred for every k, we find ourselves', 'in the set - up of learning with unaligned semantic states considered in  #TAUTHOR_TAG. the induced alignments a 1,', '..., a k of semantics m to texts w 1,..., w k at the same time induce alignments between the', 'texts', '. the problem of producing multiple sequence alignment, especially in the context of sentence alignments', ', has been extensively studied in nlp  #AUTHOR_TAG. in this paper, we use', 'semantic structures as a pivot for finding the best alignment in the hope that presence of meaningful text alignments will improve the quality of the resulting semantic', 'structures by enforcing a form of agreement between them']",0
"[',', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","['inferred,', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","[',', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","['order to use the algorithm, we need to understand how the conditional probabilities of the form p ( m | m ) are computed, as they play the key role in the inference procedure ( see equation ( 2 ) ). if there is a contradiction ( m', '⊥m ) then p ( m | m ) = 0, conversely', ', if m is subsumed by m ( m → m ) then this probability is 1. otherwise, p ( m | m ) equals the probability of new assignments ∧ | m \\ m | q =', '1 ( s ( defined by m \\ m ) conditioned on the previously fixed values of s ( given by', 'm ). summarizing, when predicting the most likely semanticsm j ( line 4 )', ', for each span the decoder weighs alternatives of either ( 1 ) aligning this span to the previously induced meaning m, or', '( 2 ) aligning it to a new field and paying the cost of generation of its value. the exact computation of the most probable semantics ( line 4 of the algorithm ) is intractable, and we have to resort to an', 'approximation. instead of predicting the most probable semanticsm j we search for the most probable pair ( a j, m j ), thus assuming that the probability mass is mostly concentrated on a single alignment. the alignment a j is then discarded and not used in any other computations. though the most likely alignmenta j for a fixed semantic', 'representationm j can be found efficiently using a viterbi algorithm, computing the most probable pair ( a', 'j, m j ) is still intractable. we use a modification of the beam search algorithm, where we keep a set of candidate meanings ( partial semantic representations ) and compute an alignment for each of them using a form of the viterbi algorithm. as soon as the meaning representations m are inferred,', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s may still not be specified by m, we prohibit utterances from aligning to these non - specified fields. on the m - step', 'of em the parameters are estimated as proportional to the expected marginal counts computed on', 'the e - step. we smooth the distributions of values for numerical fields with convolution smoothing equivalent to the assumption that the fields are affected by distortion in the form of a two - sided geometric distribution with the', 'success rate parameter equal to 0. 67. we use add - 0. 1 smoothing for all the remaining multinomial distributions']",0
[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where'],[' #TAUTHOR_TAG where the original textual'],"['', 'or "" sunny "". this model is hard to evaluate directly as text does not provide information about all the fields and does not necessarily provide it at', 'the sufficient granularity level. therefore, it is natural to evaluate their model on the database - text alignment problem  #AUTHOR_TAG, i. e. measuring how well the model predicts the alignment between the text and the observable records describing the entire world state. we follow their set - up, but assume that instead of', 'having access to the full semantic state for every training example, we have', 'a very small amount of data annotated with semantic states and a larger number of unannotated texts with noncontradictory semantics. we', 'study our set - up on the weather forecast data  #TAUTHOR_TAG where the original textual weather forecasts were complemented by additional forecasts describing the same', 'weather states ( see figure 1 for an example ). the average overlap between the verbalized fields in each group', 'of noncontradictory forecasts was below 35 %, and more than 60 % of fields are mentioned only in a single forecast from a group. our model, learned from 100 labeled forecasts and 259 groups of unannotated non - contradictory', 'forecasts ( 750 texts in total ), achieved 73. 9 % f 1. this compares favorably with 69. 1 % shown by a semi - supervised learning approach, though, as expected, does not reach the score of', 'the model which, in training, observed semantics states for all the 750 documents ( 77. 7 % f 1 ). the rest of the paper is structured as follows. in section 2 we describe our inference algorithm for groups of non - contradictory documents. section 3 redescribes the semantics - text', '']",5
"['- up of learning with unaligned semantic states considered in  #TAUTHOR_TAG. the induced alignments a 1,', '..., a k of semantics m to texts w 1,..., w k at the same time']","['- up of learning with unaligned semantic states considered in  #TAUTHOR_TAG. the induced alignments a 1,', '..., a k of semantics m to texts w 1,..., w k at the same time']","['of meaning representations made on the earlier stages. as soon as semantics m k are', 'inferred for every k, we find ourselves', 'in the set - up of learning with unaligned semantic states considered in  #TAUTHOR_TAG. the induced alignments a 1,', '..., a k of semantics m to texts w 1,..., w k at the same time']","['', 'w n k. it starts with an empty ordering n = ( ) and an empty list of meanings m = ( ) ( line 1 ). then it iteratively predicts meaning representationsm j conditioned on the list of semantics m = ( m 1,..., m i−1 ) fixed on the', 'previous stages and does it for all the remaining texts w j ( lines 3 - 5 ).', 'the algorithm selects a single meaningm j which maximizes the probability of all the remaining texts and excludes the text j', 'from future consideration ( lines 6 - 7 ). though the semantics m k ( k / ∈ n∪ { j } ) used in the', 'estimates ( line 6 ) can be inconsistent with each other, the final list of meanings m is guaranteed to be consistent. it holds because on each iteration we add a single meaningm n i to m ( line 7 ), and m n i is guaranteed to', 'be consistent with m, as the semanticsm n i was conditioned on the meaning m during inference ( line 4', ""). an important aspect of this algorithm is that unlike usual greedy inference, the remaining ('future '"", ') texts do affect the choice of meaning representations made on the earlier stages. as soon as semantics m k are', 'inferred for every k, we find ourselves', 'in the set - up of learning with unaligned semantic states considered in  #TAUTHOR_TAG. the induced alignments a 1,', '..., a k of semantics m to texts w 1,..., w k at the same time induce alignments between the', 'texts', '. the problem of producing multiple sequence alignment, especially in the context of sentence alignments', ', has been extensively studied in nlp  #AUTHOR_TAG. in this paper, we use', 'semantic structures as a pivot for finding the best alignment in the hope that presence of meaningful text alignments will improve the quality of the resulting semantic', 'structures by enforcing a form of agreement between them']",5
"[',', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","['inferred,', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","[',', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","['order to use the algorithm, we need to understand how the conditional probabilities of the form p ( m | m ) are computed, as they play the key role in the inference procedure ( see equation ( 2 ) ). if there is a contradiction ( m', '⊥m ) then p ( m | m ) = 0, conversely', ', if m is subsumed by m ( m → m ) then this probability is 1. otherwise, p ( m | m ) equals the probability of new assignments ∧ | m \\ m | q =', '1 ( s ( defined by m \\ m ) conditioned on the previously fixed values of s ( given by', 'm ). summarizing, when predicting the most likely semanticsm j ( line 4 )', ', for each span the decoder weighs alternatives of either ( 1 ) aligning this span to the previously induced meaning m, or', '( 2 ) aligning it to a new field and paying the cost of generation of its value. the exact computation of the most probable semantics ( line 4 of the algorithm ) is intractable, and we have to resort to an', 'approximation. instead of predicting the most probable semanticsm j we search for the most probable pair ( a j, m j ), thus assuming that the probability mass is mostly concentrated on a single alignment. the alignment a j is then discarded and not used in any other computations. though the most likely alignmenta j for a fixed semantic', 'representationm j can be found efficiently using a viterbi algorithm, computing the most probable pair ( a', 'j, m j ) is still intractable. we use a modification of the beam search algorithm, where we keep a set of candidate meanings ( partial semantic representations ) and compute an alignment for each of them using a form of the viterbi algorithm. as soon as the meaning representations m are inferred,', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s may still not be specified by m, we prohibit utterances from aligning to these non - specified fields. on the m - step', 'of em the parameters are estimated as proportional to the expected marginal counts computed on', 'the e - step. we smooth the distributions of values for numerical fields with convolution smoothing equivalent to the assumption that the fields are affected by distortion in the form of a two - sided geometric distribution with the', 'success rate parameter equal to 0. 67. we use add - 0. 1 smoothing for all the remaining multinomial distributions']",5
"['introduced in  #TAUTHOR_TAG.', 'the original']","['introduced in  #TAUTHOR_TAG.', 'the original']","['introduced in  #TAUTHOR_TAG.', 'the original dataset contains']",[' #TAUTHOR_TAG'],5
"['introduced in  #TAUTHOR_TAG.', 'the original']","['introduced in  #TAUTHOR_TAG.', 'the original']","['introduced in  #TAUTHOR_TAG.', 'the original dataset contains']",[' #TAUTHOR_TAG'],5
"['introduced in  #TAUTHOR_TAG.', 'the original']","['introduced in  #TAUTHOR_TAG.', 'the original']","['introduced in  #TAUTHOR_TAG.', 'the original dataset contains']",[' #TAUTHOR_TAG'],5
['text correspondence model  #TAUTHOR_TAG and evaluated'],['be instantiated for the semantics - text correspondence model  #TAUTHOR_TAG and evaluated'],['for the semantics - text correspondence model  #TAUTHOR_TAG and evaluated'],"['this work we studied the use of weak supervision in the form of non - contradictory relations between documents in learning semantic representations.', 'we argued that this type of supervision encodes information which is hard to discover in an unsupervised way.', 'however, exact inference for groups of documents with overlapping semantic representation is generally prohibitively expensive, as the shared latent semantics introduces nonlocal dependences between semantic representations of individual documents.', 'to combat it, we proposed a simple iterative inference algorithm.', 'we showed how it can be instantiated for the semantics - text correspondence model  #TAUTHOR_TAG and evaluated it on a dataset of weather forecasts.', 'our approach resulted in an improvement over the scores of both the supervised baseline and of the traditional semi - supervised learning.', 'there are many directions we plan on investigating in the future for the problem of learning semantics with non - contradictory relations.', 'a promising and challenging possibility is to consider models which induce full semantic representations of meaning.', 'another direction would be to investigate purely unsupervised set - up, though it would make evaluation of the resulting method much more complex.', 'one potential alternative would be to replace the initial supervision with a set of posterior constraints  #AUTHOR_TAG or generalized expectation criteria ( mc  #AUTHOR_TAG']",5
['this section we redescribe the semantics - text correspondence model  #TAUTHOR_TAG with an extension needed'],['this section we redescribe the semantics - text correspondence model  #TAUTHOR_TAG with an extension needed'],['this section we redescribe the semantics - text correspondence model  #TAUTHOR_TAG with an extension needed'],"['this section we redescribe the semantics - text correspondence model  #TAUTHOR_TAG with an extension needed to model examples with latent states, and also explain how the inference algorithm defined in section 2 can be applied to this model']",6
"[',', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","['inferred,', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","[',', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s']","['order to use the algorithm, we need to understand how the conditional probabilities of the form p ( m | m ) are computed, as they play the key role in the inference procedure ( see equation ( 2 ) ). if there is a contradiction ( m', '⊥m ) then p ( m | m ) = 0, conversely', ', if m is subsumed by m ( m → m ) then this probability is 1. otherwise, p ( m | m ) equals the probability of new assignments ∧ | m \\ m | q =', '1 ( s ( defined by m \\ m ) conditioned on the previously fixed values of s ( given by', 'm ). summarizing, when predicting the most likely semanticsm j ( line 4 )', ', for each span the decoder weighs alternatives of either ( 1 ) aligning this span to the previously induced meaning m, or', '( 2 ) aligning it to a new field and paying the cost of generation of its value. the exact computation of the most probable semantics ( line 4 of the algorithm ) is intractable, and we have to resort to an', 'approximation. instead of predicting the most probable semanticsm j we search for the most probable pair ( a j, m j ), thus assuming that the probability mass is mostly concentrated on a single alignment. the alignment a j is then discarded and not used in any other computations. though the most likely alignmenta j for a fixed semantic', 'representationm j can be found efficiently using a viterbi algorithm, computing the most probable pair ( a', 'j, m j ) is still intractable. we use a modification of the beam search algorithm, where we keep a set of candidate meanings ( partial semantic representations ) and compute an alignment for each of them using a form of the viterbi algorithm. as soon as the meaning representations m are inferred,', 'we find ourselves in the set - up studied in  #TAUTHOR_TAG : the state s is no longer latent and we can run efficient inference on the', 'e - step. though some fields of the state s may still not be specified by m, we prohibit utterances from aligning to these non - specified fields. on the m - step', 'of em the parameters are estimated as proportional to the expected marginal counts computed on', 'the e - step. we smooth the distributions of values for numerical fields with convolution smoothing equivalent to the assumption that the fields are affected by distortion in the form of a two - sided geometric distribution with the', 'success rate parameter equal to 0. 67. we use add - 0. 1 smoothing for all the remaining multinomial distributions']",3
"['', 'however, correlation between rain and overcast, as also noted in  #TAUTHOR_TAG, results in']","['to correct categories.', 'however, correlation between rain and overcast, as also noted in  #TAUTHOR_TAG, results in']","['', 'however, correlation between rain and overcast, as also noted in  #TAUTHOR_TAG, results in']","['', 'additionally, note that the success of our weaklysupervised scenario indirectly suggests that the model is sufficiently accurate in predicting semantics of an unlabeled text, as otherwise there would be no useful information passed in between semantically overlapping documents during learning and, consequently, no improvement from sharing the state.', '8 to confirm that the model trained by our approach indeed assigns new words to correct fields and records, we visualize top words for the field characterizing sky cover ( table 2 ).', 'note that the words "" sun "", "" cloudiness "" or "" gaps "" were not appearing in the labeled part of the data, but seem to be assigned to correct categories.', 'however, correlation between rain and overcast, as also noted in  #TAUTHOR_TAG, results in the wrong assignment of the rain - related words to the field value corresponding to very cloudy weather']",3
"['introduced in  #TAUTHOR_TAG.', 'the original']","['introduced in  #TAUTHOR_TAG.', 'the original']","['introduced in  #TAUTHOR_TAG.', 'the original dataset contains']",[' #TAUTHOR_TAG'],4
"['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['or languages that have been stored in a machine - readable format  #AUTHOR_TAG. a parallel corpus can be aligned either at sentence level or word level. sentence and word alignment', 'of parallel corpus is the identification of the corresponding sentences and words ( respectively ) in both halves of the parallel text. sentence alignment could be of various combinations including', 'one to one where one sentence maps to one sentence in the other corpus, one to many where one sentence maps to more than one sentences in the other corpus, many to', 'many where many sentences map to many sentences in the other corpus or even', 'one to zero where there is no mapping for a particular sentence in the other corpus. for statistical machine translation, the more the number of parallel', 'sentence pairs, the higher the quality of translation  #AUTHOR_TAG. however, manual alignment of a large number of sentences is time consuming, and requires personnel fluent in both languages. automatic sentence alignment of a parallel corpus is the widely accepted solution for this problem. already many sentence alignment techniques have been implemented for some languages pairs such as english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth et al., 2008 ). however, none of these techniques have been', 'evaluated for sinhala and tamil, the two official languages in sri lanka. this paper presents the first ever', 'study on automatically creating a sentence aligned parallel corpus for sinhala and tamil.', 'sinhala and tamil are both under - resourced languages, and research implementing basic nlp tool such as pos taggers and morphological analysers is at its inception stage  #AUTHOR_TAG', '. therefore, not all the aforementioned sentence alignment techniques are applicable in the context of sinhala and tamil. with this limitation', 'in mind, an extensive literature study was carried out to identify the applicable sentence alignment techniques for sinhala and tam', '##il. we implemented six such methods, and evaluated their performance using a corpus of 1300 sentences based on the precision, recall, and f - measure using annual reports of sri lankan government', 'departments as the source text. the highest f - measure value of 0. 791 was obtained for var', ""##ga et al.'s ( 2005 ) hunalign method, the hybrid method that combined the use of a bilingual dictionary with the statistical"", 'method by  #TAUTHOR_TAG. the rest of the paper is organized as follows. section 2 identifies related work in this area. section 3 describes how different techniques were employed in the alignment process, and section 4 presents the results', '']",0
"["".'s ( 1991 ) method aligns sentences based on sentence length measured using word count."", 'here anchor points are used for alignment.', 'gale and church use the number of characters as the length measure.', 'while the parameters such as mean and variance for  #TAUTHOR_TAG method are considered language independent for european languages, tuning these for']","['align.', ""brown et al.'s ( 1991 ) method aligns sentences based on sentence length measured using word count."", 'here anchor points are used for alignment.', 'gale and church use the number of characters as the length measure.', ""while the parameters such as mean and variance for  #TAUTHOR_TAG method are considered language independent for european languages, tuning these for non -'european language pairs has improved results  #AUTHOR_TAG."", 'both these methods have given good']","["".'s ( 1991 ) method aligns sentences based on sentence length measured using word count."", 'here anchor points are used for alignment.', 'gale and church use the number of characters as the length measure.', 'while the parameters such as mean and variance for  #TAUTHOR_TAG method are considered language independent for european languages, tuning these for']","['methods have also been used with non - european languages such as english - chinese ( mc  #AUTHOR_TAG, italian - japanese  #AUTHOR_TAG, english - arabic  #AUTHOR_TAG, and english - malay  #AUTHOR_TAG.', 'the general idea of these methods is that the closer in length two sentences are, the more likely they align.', ""brown et al.'s ( 1991 ) method aligns sentences based on sentence length measured using word count."", 'here anchor points are used for alignment.', 'gale and church use the number of characters as the length measure.', ""while the parameters such as mean and variance for  #TAUTHOR_TAG method are considered language independent for european languages, tuning these for non -'european language pairs has improved results  #AUTHOR_TAG."", 'both these methods have given good accuracy in alignment ; however they require some form of initial alignment or anchor points.', 'method by  #AUTHOR_TAG exploits the statistically ordered matching of punctuation marks in the two languages english and chinese to achieve high accuracy in sentence alignment compared with using the length - based methods alone']",0
"['pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error']","['expressive discriminative learner provides a boost in precision. all features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be']","['features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be built without lexical constraints. according to the authors, lexical constraints might slow down the program']","['language to another. instead of computing an alignment between the source and target text directly, this technique bases its alignment search on a machine translation', '( mt ) of the source text. the yasa method by  #AUTHOR_TAG also operates a two - step process through the parallel data. cognates', 'are first recognized in order to accomplish a first token - level alignment', 'that ( efficiently ) delimits a fruitful search space. then, sentence alignment is performed on this reduced search space. the speed of the yasa aligner and memory use is comparatively better', 'than  #AUTHOR_TAG aligner  #AUTHOR_TAG. though the method by  #AUTHOR_TAG is four times slower than  #AUTHOR_TAG method, it supports one to many and', 'many to one alignments as well. it uses an improved pruning method and in', 'the second pass, the sentences are optimally aligned and merged. this method', 'uses a two - step clustering approach in the second pass of the alignment. the', 'method by toth et al. ( 2008 ) exploits the fact that named entities cannot be ignored from any translation process, so a sentence and its translation equivalent contain the same named entities. the method', 'by mujdricza -  #AUTHOR_TAG uses a two - step process to align sentences. machine alignments known as "" wood standard "" annotations, produced using state -', 'of - the - art sentence aligners in a first step, are used in a second step, to train a discriminative learner. this combination of arbitrary amounts of machine aligned data and an expressive discriminative learner provides a boost in precision. all features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be built without lexical constraints. according to the authors, lexical constraints might slow down the program and make it less useful in the first pass. linguistic methods can produce better results if the performance', 'of the system is not a concern. hybrid methods such as that of  #AUTHOR_TAG', 'that do not require particular knowledge about the corpus or the languages involved are faster as they tend to build the bilingual', 'dictionary for aligning using the input to the aligner based on previous word - correspondence - based models. furthermore, results of some', 'of the above methods such as hunalign  #AUTHOR_TAG, bleualign  #AUTHOR_TAG and gargantua  #AUTHOR_TAG could be improved by applying linguistic factors such', 'as word forms, chunks and collocations ( navlea and todirascu, 2010 ). some have used morphologically processed ( lemmatized and morphologically tagged )', 'data and have used taggers ( pos tagger ) because it significantly increases the value of the data  #AUTHOR_TAG']",0
"['pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error']","['expressive discriminative learner provides a boost in precision. all features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be']","['features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be built without lexical constraints. according to the authors, lexical constraints might slow down the program']","['language to another. instead of computing an alignment between the source and target text directly, this technique bases its alignment search on a machine translation', '( mt ) of the source text. the yasa method by  #AUTHOR_TAG also operates a two - step process through the parallel data. cognates', 'are first recognized in order to accomplish a first token - level alignment', 'that ( efficiently ) delimits a fruitful search space. then, sentence alignment is performed on this reduced search space. the speed of the yasa aligner and memory use is comparatively better', 'than  #AUTHOR_TAG aligner  #AUTHOR_TAG. though the method by  #AUTHOR_TAG is four times slower than  #AUTHOR_TAG method, it supports one to many and', 'many to one alignments as well. it uses an improved pruning method and in', 'the second pass, the sentences are optimally aligned and merged. this method', 'uses a two - step clustering approach in the second pass of the alignment. the', 'method by toth et al. ( 2008 ) exploits the fact that named entities cannot be ignored from any translation process, so a sentence and its translation equivalent contain the same named entities. the method', 'by mujdricza -  #AUTHOR_TAG uses a two - step process to align sentences. machine alignments known as "" wood standard "" annotations, produced using state -', 'of - the - art sentence aligners in a first step, are used in a second step, to train a discriminative learner. this combination of arbitrary amounts of machine aligned data and an expressive discriminative learner provides a boost in precision. all features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be built without lexical constraints. according to the authors, lexical constraints might slow down the program and make it less useful in the first pass. linguistic methods can produce better results if the performance', 'of the system is not a concern. hybrid methods such as that of  #AUTHOR_TAG', 'that do not require particular knowledge about the corpus or the languages involved are faster as they tend to build the bilingual', 'dictionary for aligning using the input to the aligner based on previous word - correspondence - based models. furthermore, results of some', 'of the above methods such as hunalign  #AUTHOR_TAG, bleualign  #AUTHOR_TAG and gargantua  #AUTHOR_TAG could be improved by applying linguistic factors such', 'as word forms, chunks and collocations ( navlea and todirascu, 2010 ). some have used morphologically processed ( lemmatized and morphologically tagged )', 'data and have used taggers ( pos tagger ) because it significantly increases the value of the data  #AUTHOR_TAG']",0
"['pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error']","['expressive discriminative learner provides a boost in precision. all features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be']","['features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be built without lexical constraints. according to the authors, lexical constraints might slow down the program']","['language to another. instead of computing an alignment between the source and target text directly, this technique bases its alignment search on a machine translation', '( mt ) of the source text. the yasa method by  #AUTHOR_TAG also operates a two - step process through the parallel data. cognates', 'are first recognized in order to accomplish a first token - level alignment', 'that ( efficiently ) delimits a fruitful search space. then, sentence alignment is performed on this reduced search space. the speed of the yasa aligner and memory use is comparatively better', 'than  #AUTHOR_TAG aligner  #AUTHOR_TAG. though the method by  #AUTHOR_TAG is four times slower than  #AUTHOR_TAG method, it supports one to many and', 'many to one alignments as well. it uses an improved pruning method and in', 'the second pass, the sentences are optimally aligned and merged. this method', 'uses a two - step clustering approach in the second pass of the alignment. the', 'method by toth et al. ( 2008 ) exploits the fact that named entities cannot be ignored from any translation process, so a sentence and its translation equivalent contain the same named entities. the method', 'by mujdricza -  #AUTHOR_TAG uses a two - step process to align sentences. machine alignments known as "" wood standard "" annotations, produced using state -', 'of - the - art sentence aligners in a first step, are used in a second step, to train a discriminative learner. this combination of arbitrary amounts of machine aligned data and an expressive discriminative learner provides a boost in precision. all features used in the', 'second step, with the exception of the pos agreement feature, are language - independent. according to  #TAUTHOR_TAG a considerably large parallel corpus having a small error percentage', 'can be built without lexical constraints. according to the authors, lexical constraints might slow down the program and make it less useful in the first pass. linguistic methods can produce better results if the performance', 'of the system is not a concern. hybrid methods such as that of  #AUTHOR_TAG', 'that do not require particular knowledge about the corpus or the languages involved are faster as they tend to build the bilingual', 'dictionary for aligning using the input to the aligner based on previous word - correspondence - based models. furthermore, results of some', 'of the above methods such as hunalign  #AUTHOR_TAG, bleualign  #AUTHOR_TAG and gargantua  #AUTHOR_TAG could be improved by applying linguistic factors such', 'as word forms, chunks and collocations ( navlea and todirascu, 2010 ). some have used morphologically processed ( lemmatized and morphologically tagged )', 'data and have used taggers ( pos tagger ) because it significantly increases the value of the data  #AUTHOR_TAG']",0
"['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used for english and french sentence alignment.', 'both these languages have many similarities, which include the sentence structure and the sentence length.', 'the sentence structure of these languages is of the form subject - verb - object and the sentence length is quite close.', 'the same similarities can also be found in sinhala and tamil languages.', 'sinhala and tamil languages have the same sentence structure, subject - object - verb.', 'also the average sentence lengths of the two languages are quite close.', 'considering 700 sentences, average length of sinhala is 113. 76 and for tamil it is 130. 53.', 'therefore statistical methods have given good results in our case.', 'the lexical components used in the hybrid methods suggested above are also language independent.', 'thus the hybrid methods are also applicable for sinhala and tamil.', 'we used  #TAUTHOR_TAG method even though we could not align the paragraphs before aligning the sentences, due the dissimilarities among the text converted from pdfs.', 'the length of tamil sentences was comparatively higher than sinhala sentences and the correlation between sinhala and tamil was comparatively low, hence we cannot consider mean and variance as language independent as suggested by  #TAUTHOR_TAG.', 'therefore we calculated the mean and variance for sinhala and tamil using 700 sentences.', ' #AUTHOR_TAG introduced 1 as mean and 6. 8 as variance for english and french languages.', '']",0
"['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['or languages that have been stored in a machine - readable format  #AUTHOR_TAG. a parallel corpus can be aligned either at sentence level or word level. sentence and word alignment', 'of parallel corpus is the identification of the corresponding sentences and words ( respectively ) in both halves of the parallel text. sentence alignment could be of various combinations including', 'one to one where one sentence maps to one sentence in the other corpus, one to many where one sentence maps to more than one sentences in the other corpus, many to', 'many where many sentences map to many sentences in the other corpus or even', 'one to zero where there is no mapping for a particular sentence in the other corpus. for statistical machine translation, the more the number of parallel', 'sentence pairs, the higher the quality of translation  #AUTHOR_TAG. however, manual alignment of a large number of sentences is time consuming, and requires personnel fluent in both languages. automatic sentence alignment of a parallel corpus is the widely accepted solution for this problem. already many sentence alignment techniques have been implemented for some languages pairs such as english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth et al., 2008 ). however, none of these techniques have been', 'evaluated for sinhala and tamil, the two official languages in sri lanka. this paper presents the first ever', 'study on automatically creating a sentence aligned parallel corpus for sinhala and tamil.', 'sinhala and tamil are both under - resourced languages, and research implementing basic nlp tool such as pos taggers and morphological analysers is at its inception stage  #AUTHOR_TAG', '. therefore, not all the aforementioned sentence alignment techniques are applicable in the context of sinhala and tamil. with this limitation', 'in mind, an extensive literature study was carried out to identify the applicable sentence alignment techniques for sinhala and tam', '##il. we implemented six such methods, and evaluated their performance using a corpus of 1300 sentences based on the precision, recall, and f - measure using annual reports of sri lankan government', 'departments as the source text. the highest f - measure value of 0. 791 was obtained for var', ""##ga et al.'s ( 2005 ) hunalign method, the hybrid method that combined the use of a bilingual dictionary with the statistical"", 'method by  #TAUTHOR_TAG. the rest of the paper is organized as follows. section 2 identifies related work in this area. section 3 describes how different techniques were employed in the alignment process, and section 4 presents the results', '']",4
"['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used for english and french sentence alignment.', 'both these languages have many similarities, which include the sentence structure and the sentence length.', 'the sentence structure of these languages is of the form subject - verb - object and the sentence length is quite close.', 'the same similarities can also be found in sinhala and tamil languages.', 'sinhala and tamil languages have the same sentence structure, subject - object - verb.', 'also the average sentence lengths of the two languages are quite close.', 'considering 700 sentences, average length of sinhala is 113. 76 and for tamil it is 130. 53.', 'therefore statistical methods have given good results in our case.', 'the lexical components used in the hybrid methods suggested above are also language independent.', 'thus the hybrid methods are also applicable for sinhala and tamil.', 'we used  #TAUTHOR_TAG method even though we could not align the paragraphs before aligning the sentences, due the dissimilarities among the text converted from pdfs.', 'the length of tamil sentences was comparatively higher than sinhala sentences and the correlation between sinhala and tamil was comparatively low, hence we cannot consider mean and variance as language independent as suggested by  #TAUTHOR_TAG.', 'therefore we calculated the mean and variance for sinhala and tamil using 700 sentences.', ' #AUTHOR_TAG introduced 1 as mean and 6. 8 as variance for english and french languages.', '']",4
"['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth']","['or languages that have been stored in a machine - readable format  #AUTHOR_TAG. a parallel corpus can be aligned either at sentence level or word level. sentence and word alignment', 'of parallel corpus is the identification of the corresponding sentences and words ( respectively ) in both halves of the parallel text. sentence alignment could be of various combinations including', 'one to one where one sentence maps to one sentence in the other corpus, one to many where one sentence maps to more than one sentences in the other corpus, many to', 'many where many sentences map to many sentences in the other corpus or even', 'one to zero where there is no mapping for a particular sentence in the other corpus. for statistical machine translation, the more the number of parallel', 'sentence pairs, the higher the quality of translation  #AUTHOR_TAG. however, manual alignment of a large number of sentences is time consuming, and requires personnel fluent in both languages. automatic sentence alignment of a parallel corpus is the widely accepted solution for this problem. already many sentence alignment techniques have been implemented for some languages pairs such as english - french  #TAUTHOR_TAG english - chinese  #AUTHOR_TAG and', 'hungarian - english  #AUTHOR_TAG toth et al., 2008 ). however, none of these techniques have been', 'evaluated for sinhala and tamil, the two official languages in sri lanka. this paper presents the first ever', 'study on automatically creating a sentence aligned parallel corpus for sinhala and tamil.', 'sinhala and tamil are both under - resourced languages, and research implementing basic nlp tool such as pos taggers and morphological analysers is at its inception stage  #AUTHOR_TAG', '. therefore, not all the aforementioned sentence alignment techniques are applicable in the context of sinhala and tamil. with this limitation', 'in mind, an extensive literature study was carried out to identify the applicable sentence alignment techniques for sinhala and tam', '##il. we implemented six such methods, and evaluated their performance using a corpus of 1300 sentences based on the precision, recall, and f - measure using annual reports of sri lankan government', 'departments as the source text. the highest f - measure value of 0. 791 was obtained for var', ""##ga et al.'s ( 2005 ) hunalign method, the hybrid method that combined the use of a bilingual dictionary with the statistical"", 'method by  #TAUTHOR_TAG. the rest of the paper is organized as follows. section 2 identifies related work in this area. section 3 describes how different techniques were employed in the alignment process, and section 4 presents the results', '']",5
"['by  #TAUTHOR_TAG citing the close linguistic similarities between languages of these pairs, causing parallel sentences to be of similar lengths']","['by  #TAUTHOR_TAG citing the close linguistic similarities between languages of these pairs, causing parallel sentences to be of similar lengths']","['by  #TAUTHOR_TAG citing the close linguistic similarities between languages of these pairs, causing parallel sentences to be of similar lengths']","['alignment of sentences has been attempted for few indic language pairs from the south asian subcontinent including hindi - urdu  #AUTHOR_TAG and hindi - punjabi  #AUTHOR_TAG.', 'this research used the method proposed by  #TAUTHOR_TAG citing the close linguistic similarities between languages of these pairs, causing parallel sentences to be of similar lengths']",5
"['use of punctuations.', 'constrained by the available resources, we compared methods by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'these methods have shown promising results for languages']","['use of punctuations.', 'constrained by the available resources, we compared methods by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'these methods have shown promising results for languages']","['also the translators of the original text have not been consistent with the use of punctuations.', 'constrained by the available resources, we compared methods by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'these methods have shown promising results for languages']","['on the similarities and dissimilarities between the languages and the quality of the data source, different techniques discussed in section 2 have given different results for the alignment for different language pairs.', 'for example, a method like that of  #AUTHOR_TAG would work well for parallel text where punctuations are consistent, while that of  #AUTHOR_TAG would work better for languages that lack etymological relations.', 'thus the objective of this research is to experiment with these techniques for sinhala - tamil, and identify the best technique.', 'however, not all methods described in section 2 can be used in the context of sinhala and tamil.', 'for example, methods by toth et al. ( 2008 ) and mujdricza -  #AUTHOR_TAG cannot be used because ner systems and comprehensive pos taggers are not fully developed for sinhala  #AUTHOR_TAG and tamil  #AUTHOR_TAG.', 'also methods that align using the punctuations in the two languages similar to that of  #AUTHOR_TAG cannot be used in this case because when extracting text from pdf, some punctuations are lost, and also the translators of the original text have not been consistent with the use of punctuations.', 'constrained by the available resources, we compared methods by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'these methods have shown promising results for languages that show close linguistic relationships, which is also the case with sinhala and tamil.', '']",5
"['use of punctuations.', 'constrained by the available resources, we compared methods by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'these methods have shown promising results for languages']","['use of punctuations.', 'constrained by the available resources, we compared methods by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'these methods have shown promising results for languages']","['also the translators of the original text have not been consistent with the use of punctuations.', 'constrained by the available resources, we compared methods by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'these methods have shown promising results for languages']","['on the similarities and dissimilarities between the languages and the quality of the data source, different techniques discussed in section 2 have given different results for the alignment for different language pairs.', 'for example, a method like that of  #AUTHOR_TAG would work well for parallel text where punctuations are consistent, while that of  #AUTHOR_TAG would work better for languages that lack etymological relations.', 'thus the objective of this research is to experiment with these techniques for sinhala - tamil, and identify the best technique.', 'however, not all methods described in section 2 can be used in the context of sinhala and tamil.', 'for example, methods by toth et al. ( 2008 ) and mujdricza -  #AUTHOR_TAG cannot be used because ner systems and comprehensive pos taggers are not fully developed for sinhala  #AUTHOR_TAG and tamil  #AUTHOR_TAG.', 'also methods that align using the punctuations in the two languages similar to that of  #AUTHOR_TAG cannot be used in this case because when extracting text from pdf, some punctuations are lost, and also the translators of the original text have not been consistent with the use of punctuations.', 'constrained by the available resources, we compared methods by  #TAUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'these methods have shown promising results for languages that show close linguistic relationships, which is also the case with sinhala and tamil.', '']",5
"['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used for english and french sentence alignment.', 'both these languages have many similarities, which include the sentence structure and the sentence length.', 'the sentence structure of these languages is of the form subject - verb - object and the sentence length is quite close.', 'the same similarities can also be found in sinhala and tamil languages.', 'sinhala and tamil languages have the same sentence structure, subject - object - verb.', 'also the average sentence lengths of the two languages are quite close.', 'considering 700 sentences, average length of sinhala is 113. 76 and for tamil it is 130. 53.', 'therefore statistical methods have given good results in our case.', 'the lexical components used in the hybrid methods suggested above are also language independent.', 'thus the hybrid methods are also applicable for sinhala and tamil.', 'we used  #TAUTHOR_TAG method even though we could not align the paragraphs before aligning the sentences, due the dissimilarities among the text converted from pdfs.', 'the length of tamil sentences was comparatively higher than sinhala sentences and the correlation between sinhala and tamil was comparatively low, hence we cannot consider mean and variance as language independent as suggested by  #TAUTHOR_TAG.', 'therefore we calculated the mean and variance for sinhala and tamil using 700 sentences.', ' #AUTHOR_TAG introduced 1 as mean and 6. 8 as variance for english and french languages.', '']",5
"['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used for english and french sentence alignment.', 'both these languages have many similarities, which include the sentence structure and the sentence length.', 'the sentence structure of these languages is of the form subject - verb - object and the sentence length is quite close.', 'the same similarities can also be found in sinhala and tamil languages.', 'sinhala and tamil languages have the same sentence structure, subject - object - verb.', 'also the average sentence lengths of the two languages are quite close.', 'considering 700 sentences, average length of sinhala is 113. 76 and for tamil it is 130. 53.', 'therefore statistical methods have given good results in our case.', 'the lexical components used in the hybrid methods suggested above are also language independent.', 'thus the hybrid methods are also applicable for sinhala and tamil.', 'we used  #TAUTHOR_TAG method even though we could not align the paragraphs before aligning the sentences, due the dissimilarities among the text converted from pdfs.', 'the length of tamil sentences was comparatively higher than sinhala sentences and the correlation between sinhala and tamil was comparatively low, hence we cannot consider mean and variance as language independent as suggested by  #TAUTHOR_TAG.', 'therefore we calculated the mean and variance for sinhala and tamil using 700 sentences.', ' #AUTHOR_TAG introduced 1 as mean and 6. 8 as variance for english and french languages.', '']",7
"['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used for english and french sentence alignment.', 'both these languages have many similarities, which include the sentence structure and the sentence length.', 'the sentence structure of these languages is of the form subject - verb - object and the sentence length is quite close.', 'the same similarities can also be found in sinhala and tamil languages.', 'sinhala and tamil languages have the same sentence structure, subject - object - verb.', 'also the average sentence lengths of the two languages are quite close.', 'considering 700 sentences, average length of sinhala is 113. 76 and for tamil it is 130. 53.', 'therefore statistical methods have given good results in our case.', 'the lexical components used in the hybrid methods suggested above are also language independent.', 'thus the hybrid methods are also applicable for sinhala and tamil.', 'we used  #TAUTHOR_TAG method even though we could not align the paragraphs before aligning the sentences, due the dissimilarities among the text converted from pdfs.', 'the length of tamil sentences was comparatively higher than sinhala sentences and the correlation between sinhala and tamil was comparatively low, hence we cannot consider mean and variance as language independent as suggested by  #TAUTHOR_TAG.', 'therefore we calculated the mean and variance for sinhala and tamil using 700 sentences.', ' #AUTHOR_TAG introduced 1 as mean and 6. 8 as variance for english and french languages.', '']",7
"['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used']","['of the above methods  #TAUTHOR_TAG chen and s. f, 1993 ;  #AUTHOR_TAG have been first used for english and french sentence alignment.', 'both these languages have many similarities, which include the sentence structure and the sentence length.', 'the sentence structure of these languages is of the form subject - verb - object and the sentence length is quite close.', 'the same similarities can also be found in sinhala and tamil languages.', 'sinhala and tamil languages have the same sentence structure, subject - object - verb.', 'also the average sentence lengths of the two languages are quite close.', 'considering 700 sentences, average length of sinhala is 113. 76 and for tamil it is 130. 53.', 'therefore statistical methods have given good results in our case.', 'the lexical components used in the hybrid methods suggested above are also language independent.', 'thus the hybrid methods are also applicable for sinhala and tamil.', 'we used  #TAUTHOR_TAG method even though we could not align the paragraphs before aligning the sentences, due the dissimilarities among the text converted from pdfs.', 'the length of tamil sentences was comparatively higher than sinhala sentences and the correlation between sinhala and tamil was comparatively low, hence we cannot consider mean and variance as language independent as suggested by  #TAUTHOR_TAG.', 'therefore we calculated the mean and variance for sinhala and tamil using 700 sentences.', ' #AUTHOR_TAG introduced 1 as mean and 6. 8 as variance for english and french languages.', '']",6
"['', 'it was observed by  #TAUTHOR_TAG that']","['entity - pair.', 'it was observed by  #TAUTHOR_TAG that']","['', 'it was observed by  #TAUTHOR_TAG that']","['', 'it was observed by  #TAUTHOR_TAG that 50 % of the sentences in the riedel2010 distant supervision dataset [  #AUTHOR_TAG ], a popular ds benchmark dataset, had 40 or more words in them.', 'we note that not all the words in these long sentences contribute towards expressing the given relation.', 'in this work, we formulate various word attention mechanisms to help the relation extraction model focus on the right context in a given sentence.', 'the miml assumption states that in an instance set corresponding to an entity pair, at least one sentence in that set should express the true relation assigned to the set.', 'however, we observe that this is not always true in currently available benchmark datasets for re in the distantly supervised setting.', 'in particular, current datasets have noise in the test set, for example, a fact may be labelled false if it is missing in the knowledge base, leading to a false negative label in train and test set.', 'noise in test set impedes the right comparison of models and may favor overfitted models.', 'to']",0
[':  #TAUTHOR_TAG proposed the piecewise convolution neural network ( pc'],"['2 ) 0 otherwise pcnn :  #TAUTHOR_TAG proposed the piecewise convolution neural network ( pcnn ), a successful model for distantly supervised relation extraction.', 'the']",['2 ) 0 otherwise pcnn :  #TAUTHOR_TAG proposed the piecewise convolution neural network ( pc'],"['extraction : a relation is defined as a semantic property between a set of entities { e k }.', 'in our task, we consider binary relations where k ∈ [ 1, 2 ], such as born in ( barack obama, hawaii ).', 'given a set of sentences s = { s i } ; i ∈ [ 1... n ], where each sentence s i contains both the entities, the task of relation extraction with distantly supervised dataset is to learn a function f r : f r ( s, ( e 1, e 2 ) ) = 1 if relation r is true for pair ( e 1, e 2 ) 0 otherwise pcnn :  #TAUTHOR_TAG proposed the piecewise convolution neural network ( pcnn ), a successful model for distantly supervised relation extraction.', 'the success of the relation extraction task depends on extracting the right structural features from the sentence containing the entity - pair.', 'neural networks, such as convolutional neural networks ( cnns ), have been proposed to alleviate the need to manually design features for a given task [  #AUTHOR_TAG ].', 'as the output of cnns is dependent on the number of tokens in the sentence, max pooling operation is often applied to remove this dependence.', 'however, the use of a single max - pool misses out on some of these structural features useful for relation extraction task.', ""pcnn model divides a sentence s i's convolution filter output c i containing two entities into three parts c i1, c i2, c i3 - sentence context to the left of first entity, between the two entities, and to right of the second entity respectively - and performs max - pooling on each of the three parts, shown in figure 2."", 'thereby, leveraging the entity location information to retain the structural features of a sentence after the max - pooling operation.', 'the output of this operation is the concatenation of { pc i1, pc i2, pc i3 } yielding a fixed size output.', 'the fixed size output is processed through a tanh non - linearity followed by a linear layer to produce relation probabilities']",0
"['', '[  #TAUTHOR_TAG aimed to leverage inter -']","['label prediction.', '[  #TAUTHOR_TAG aimed to leverage inter - sentence information']","['relation label prediction.', '[  #TAUTHOR_TAG aimed to leverage inter - sentence information']","['extraction in distantly supervised datasets is posed in a multi - instance multi - label ( miml ) setting [  #AUTHOR_TAG ].', 'a large proportion of the subsequent work in this field has aimed to relax the strong assumptions that the original ds model made.', '[  #AUTHOR_TAG ] introduced the expressed - at - least - once assumption in a factor graph model as an aggregating mechanism over mention level predictions.', 'work by [  #AUTHOR_TAG ] are crucial increments to [  #AUTHOR_TAG ].', 'in past few years, deep learning models [  #AUTHOR_TAG ] have reduced the dependence of algorithms on manually de - signed features.', '[  #AUTHOR_TAG ] introduced the use of a cnn based model for relation extraction.', '[  #AUTHOR_TAG ] proposed a piecewise convolutional neural network ( pcnn ) model to preserve the structural features of a sentence using piecewise max - pooling approach, improving the precisionrecall curve significantly.', 'however, pcnn method used only one sentence in the instance - set to predict the relation label and for backpropagation.', '[  #AUTHOR_TAG ] improves upon pcnn results by introducing an attention mechanism to select a set of sentences from instance set for relation label prediction.', '[  #TAUTHOR_TAG aimed to leverage inter - sentence information for relation extraction in a ranking model.', 'the hypothesis explored is that for a particular entity - pair, each mention alone may not be expressive enough of the relation in question, but information from several mentions may be required to decisively make a prediction.', 'recently, work by [  #AUTHOR_TAG ] exploit the connections between relation ( class ties ) to improve relation extraction performance.', 'a few papers propose the addition of background knowledge to reduce noise in training data.', '[  #AUTHOR_TAG ] proposes a joint - embedding model for text and kb entities where the known part of the kb is utilized as part of the supervision signal.', '[  #AUTHOR_TAG ] use indirect supervision like consistency between relation labels, consistency between relations and arguments, and consistency between neighbour instances using markov logic networks.', '[  #AUTHOR_TAG ] uses inter - instance - set couplings for relation extraction in multi - task setup to improve performance.', 'attention models learn the importance of a feature in the supervised task through back - propogation.', 'attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation [  #AUTHOR_TAG ], image captioning [  #AUTHOR_TAG ], supervised relation extraction [  #AUTHOR_TAG ], distantly - supervised relation extraction  #TAUTHOR_TAG etc.', 'in our work, we focus on selecting the right words in a sentence using the word and entity - based attention mechanism']",0
"['', '[  #TAUTHOR_TAG aimed to leverage inter -']","['label prediction.', '[  #TAUTHOR_TAG aimed to leverage inter - sentence information']","['relation label prediction.', '[  #TAUTHOR_TAG aimed to leverage inter - sentence information']","['extraction in distantly supervised datasets is posed in a multi - instance multi - label ( miml ) setting [  #AUTHOR_TAG ].', 'a large proportion of the subsequent work in this field has aimed to relax the strong assumptions that the original ds model made.', '[  #AUTHOR_TAG ] introduced the expressed - at - least - once assumption in a factor graph model as an aggregating mechanism over mention level predictions.', 'work by [  #AUTHOR_TAG ] are crucial increments to [  #AUTHOR_TAG ].', 'in past few years, deep learning models [  #AUTHOR_TAG ] have reduced the dependence of algorithms on manually de - signed features.', '[  #AUTHOR_TAG ] introduced the use of a cnn based model for relation extraction.', '[  #AUTHOR_TAG ] proposed a piecewise convolutional neural network ( pcnn ) model to preserve the structural features of a sentence using piecewise max - pooling approach, improving the precisionrecall curve significantly.', 'however, pcnn method used only one sentence in the instance - set to predict the relation label and for backpropagation.', '[  #AUTHOR_TAG ] improves upon pcnn results by introducing an attention mechanism to select a set of sentences from instance set for relation label prediction.', '[  #TAUTHOR_TAG aimed to leverage inter - sentence information for relation extraction in a ranking model.', 'the hypothesis explored is that for a particular entity - pair, each mention alone may not be expressive enough of the relation in question, but information from several mentions may be required to decisively make a prediction.', 'recently, work by [  #AUTHOR_TAG ] exploit the connections between relation ( class ties ) to improve relation extraction performance.', 'a few papers propose the addition of background knowledge to reduce noise in training data.', '[  #AUTHOR_TAG ] proposes a joint - embedding model for text and kb entities where the known part of the kb is utilized as part of the supervision signal.', '[  #AUTHOR_TAG ] use indirect supervision like consistency between relation labels, consistency between relations and arguments, and consistency between neighbour instances using markov logic networks.', '[  #AUTHOR_TAG ] uses inter - instance - set couplings for relation extraction in multi - task setup to improve performance.', 'attention models learn the importance of a feature in the supervised task through back - propogation.', 'attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation [  #AUTHOR_TAG ], image captioning [  #AUTHOR_TAG ], supervised relation extraction [  #AUTHOR_TAG ], distantly - supervised relation extraction  #TAUTHOR_TAG etc.', 'in our work, we focus on selecting the right words in a sentence using the word and entity - based attention mechanism']",0
"['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['us once again consider the example sentence from section 2. 2 involving entity pair ( obama, honolulu ).', 'in the sentence, for entity obama, the word president helps in identifying that the entity is a person.', 'this extra information helps in narrowing down the relation possibilities by looking only at the relations that occur between a person and a city.', '[  #AUTHOR_TAG ] proposed an entity attention model for supervised relation extraction with a single sentence as input to the model.', 'we modify and adapt their model for the distant supervision setting and propose entity attention ( ea ) which works with a bag of sentences.', 'for a given bag of sentences, learning is done using the setting proposed by  #TAUTHOR_TAG, wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration.', 'the ea model has two components : 1 ) pcnn layer, and 2 ) entity attention layer, as shown in figure 4. consider an instance set s q with set of sentences,', '1×d is a word embedding and { e emb q1, e emb q2 } are the embeddings for the two entities.', 'the pcnn layer is applied on the words in the sentence  #TAUTHOR_TAG.', '']",5
"['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['us once again consider the example sentence from section 2. 2 involving entity pair ( obama, honolulu ).', 'in the sentence, for entity obama, the word president helps in identifying that the entity is a person.', 'this extra information helps in narrowing down the relation possibilities by looking only at the relations that occur between a person and a city.', '[  #AUTHOR_TAG ] proposed an entity attention model for supervised relation extraction with a single sentence as input to the model.', 'we modify and adapt their model for the distant supervision setting and propose entity attention ( ea ) which works with a bag of sentences.', 'for a given bag of sentences, learning is done using the setting proposed by  #TAUTHOR_TAG, wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration.', 'the ea model has two components : 1 ) pcnn layer, and 2 ) entity attention layer, as shown in figure 4. consider an instance set s q with set of sentences,', '1×d is a word embedding and { e emb q1, e emb q2 } are the embeddings for the two entities.', 'the pcnn layer is applied on the words in the sentence  #TAUTHOR_TAG.', '']",5
"['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['by  #TAUTHOR_TAG, wherein the sentence with the highest probability']","['us once again consider the example sentence from section 2. 2 involving entity pair ( obama, honolulu ).', 'in the sentence, for entity obama, the word president helps in identifying that the entity is a person.', 'this extra information helps in narrowing down the relation possibilities by looking only at the relations that occur between a person and a city.', '[  #AUTHOR_TAG ] proposed an entity attention model for supervised relation extraction with a single sentence as input to the model.', 'we modify and adapt their model for the distant supervision setting and propose entity attention ( ea ) which works with a bag of sentences.', 'for a given bag of sentences, learning is done using the setting proposed by  #TAUTHOR_TAG, wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration.', 'the ea model has two components : 1 ) pcnn layer, and 2 ) entity attention layer, as shown in figure 4. consider an instance set s q with set of sentences,', '1×d is a word embedding and { e emb q1, e emb q2 } are the embeddings for the two entities.', 'the pcnn layer is applied on the words in the sentence  #TAUTHOR_TAG.', '']",5
[')  #TAUTHOR_TAG and ( b ) neural relation'],['( pcnn )  #TAUTHOR_TAG and ( b ) neural relation'],[')  #TAUTHOR_TAG and ( b ) neural relation extraction with selective attention over'],"['', 'baselines : we compare proposed models with ( a ) piecewise convolution neural network ( pcnn )  #TAUTHOR_TAG and ( b ) neural relation extraction with selective attention over instances ( nre ) [  #AUTHOR_TAG ].', 'both nre and pcnn baseline outperform traditional baselines like miml - re and hence we use them as a representative state - of - the - art baseline to compare with proposed models.', 'model parameters : the parameters used for the various models are summarized in table 4.', 'word embeddings are initialized using the word2vec vectors from nyt dataset, similar to [  #AUTHOR_TAG ].', '']",5
[')  #TAUTHOR_TAG and ( b ) neural relation'],['( pcnn )  #TAUTHOR_TAG and ( b ) neural relation'],[')  #TAUTHOR_TAG and ( b ) neural relation extraction with selective attention over'],"['', 'baselines : we compare proposed models with ( a ) piecewise convolution neural network ( pcnn )  #TAUTHOR_TAG and ( b ) neural relation extraction with selective attention over instances ( nre ) [  #AUTHOR_TAG ].', 'both nre and pcnn baseline outperform traditional baselines like miml - re and hence we use them as a representative state - of - the - art baseline to compare with proposed models.', 'model parameters : the parameters used for the various models are summarized in table 4.', 'word embeddings are initialized using the word2vec vectors from nyt dataset, similar to [  #AUTHOR_TAG ].', '']",5
"['resolving residual ambiguity after the basic tagging is done.', 'we explain this module in detail next.', 'we train our classifiers on the exact training set defined by  #TAUTHOR_TAG, a sub']","['resolving residual ambiguity after the basic tagging is done.', 'we explain this module in detail next.', 'we train our classifiers on the exact training set defined by  #TAUTHOR_TAG, a subpart of']","['resolving residual ambiguity after the basic tagging is done.', 'we explain this module in detail next.', 'we train our classifiers on the exact training set defined by  #TAUTHOR_TAG, a subpart of']","['', 'the algorithm we proposed in  #AUTHOR_TAG for choosing the best bama analysis simply counts the number of predicted values for the set of linguistic features in each candidate analysis.', 'hajic et al. ( 2005 ), however, weigh the predicted values by their probability or confidence measure.', 'to our knowledge, no results on diacritization have been previously reported using this particular approach to tagging.', '4 in this paper, we extend our basic mada system in the following ways : first, we follow hajic et al. ( 2005 ) in including case, mood, and nunation as features, because of its importance to diacritization.', 'second, we replace the yamcha  #AUTHOR_TAG implementation of support vector machines ( svms ) with svmtool ( gimenez and marquez, 2004 ) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accuracy.', 'like hajic et al. ( 2005 ), we do not use viterbi decoding.', 'finally, we introduce a specialized module for resolving residual ambiguity after the basic tagging is done.', 'we explain this module in detail next.', 'we train our classifiers on the exact training set defined by  #TAUTHOR_TAG, a subpart of the third segment of the penn arabic treebank  #AUTHOR_TAG ( "" atb3 - train "", 288, 000 words ).', 'we also ( reluctantly ) follow them in having a single set for development and testing ( "" atb3 - devtest "", 52, 000 words ), rather than separate development and']",5
['in  #TAUTHOR_TAG for a general'],['in  #TAUTHOR_TAG for a general'],"['review three approaches that are directly relevant to us ; we refer to the excellent literature review in  #TAUTHOR_TAG for a general review.', ' #AUTHOR_TAG follow an approach similar']","['review three approaches that are directly relevant to us ; we refer to the excellent literature review in  #TAUTHOR_TAG for a general review.', ' #AUTHOR_TAG follow an approach similar to ours in that they choose from the diacritizations proposed by bama.', 'however, they train a single tagger using unannotated data and em, which necessarily leads to a lower performance.', 'the most salient difference, however, is that they are motivated by the goal of improving automatic speech recognition, and have an acoustic signal parallel to the undiacritized text.', 'all their experiments use acoustic models.', 'they show that wer for diacritization decreases by nearly 50 % ( from 50 % ) when bama is added to the acoustic information, but the tagger does not help.', 'it would be interesting to investigate ways of incorporating acoustic model information in our approach.', ' #AUTHOR_TAG also work on diacritization with the goal of improving asr.', 'they use a word - based language model ( using both diacritized and undiacritized words in the context ) but back off to a character - based model for unseen words.', 'they consult bama to narrow possible diacritizations for unseen words, but bama does not provide much improvement used in this manner.', ' #TAUTHOR_TAG use a maximum entropy classifier to assign a set of diacritics to the letters of each word.', ' #TAUTHOR_TAG use the output of a tokenizer ( segmenter ) and a part - of - speech tagger ( which presumably tags the output of the tokenizer ).', ' #TAUTHOR_TAG then use segment n - grams, segment position of the character being diacritized, the pos of the current segment, along with lexical features, including letter and word n - grams.', 'thus, while many of the same elements are used in  #TAUTHOR_TAG kenization, morphological tag, and diacritization ), they have a pipeline of processors.', 'furthermore,  #TAUTHOR_TAG do not use a morphological lexicon.', 'to our knowledge,  #TAUTHOR_TAG results']",5
['in  #TAUTHOR_TAG for a general'],['in  #TAUTHOR_TAG for a general'],"['review three approaches that are directly relevant to us ; we refer to the excellent literature review in  #TAUTHOR_TAG for a general review.', ' #AUTHOR_TAG follow an approach similar']","['review three approaches that are directly relevant to us ; we refer to the excellent literature review in  #TAUTHOR_TAG for a general review.', ' #AUTHOR_TAG follow an approach similar to ours in that they choose from the diacritizations proposed by bama.', 'however, they train a single tagger using unannotated data and em, which necessarily leads to a lower performance.', 'the most salient difference, however, is that they are motivated by the goal of improving automatic speech recognition, and have an acoustic signal parallel to the undiacritized text.', 'all their experiments use acoustic models.', 'they show that wer for diacritization decreases by nearly 50 % ( from 50 % ) when bama is added to the acoustic information, but the tagger does not help.', 'it would be interesting to investigate ways of incorporating acoustic model information in our approach.', ' #AUTHOR_TAG also work on diacritization with the goal of improving asr.', 'they use a word - based language model ( using both diacritized and undiacritized words in the context ) but back off to a character - based model for unseen words.', 'they consult bama to narrow possible diacritizations for unseen words, but bama does not provide much improvement used in this manner.', ' #TAUTHOR_TAG use a maximum entropy classifier to assign a set of diacritics to the letters of each word.', ' #TAUTHOR_TAG use the output of a tokenizer ( segmenter ) and a part - of - speech tagger ( which presumably tags the output of the tokenizer ).', ' #TAUTHOR_TAG then use segment n - grams, segment position of the character being diacritized, the pos of the current segment, along with lexical features, including letter and word n - grams.', 'thus, while many of the same elements are used in  #TAUTHOR_TAG kenization, morphological tag, and diacritization ), they have a pipeline of processors.', 'furthermore,  #TAUTHOR_TAG do not use a morphological lexicon.', 'to our knowledge,  #TAUTHOR_TAG results']",5
['of  #TAUTHOR_TAG on the last'],['of  #TAUTHOR_TAG on the'],"['of  #TAUTHOR_TAG on the last line, which we understand to be the best published results currently. we see that we improve on their results in all categories. we can see', 'the effect of our different approaches to diacritization in the numbers : while for wer we']","['', 'its own. we conclude from this that the quite simple lexeme', 'model is in fact contributing to the correct choice of the lexemic di', '##acritics. finally, we give the results of  #TAUTHOR_TAG on the last line, which we understand to be the best published results currently. we see that we improve on their results in all categories. we can see', 'the effect of our different approaches to diacritization in the numbers : while for wer we reduce the  #TAUTHOR_TAG error by 17. 2 %, the der error reduction', 'is only 10. 9 %. this is because we are choosing among complete diacritization options for white space - tokenized words, while  #TAUTHOR_TAG make choices for each diacritic. this means that when we make a mistake, it may well', 'affect several diacritics at once, so that the diacritic errors are concentrated in fewer words. this effect is even stronger when we disregard the final letter (', '30. 4 % reduction in wer versus 12. 0 % reduction in der ), suggesting that singleton errors in words tend to be in the final position (', 'case, mood ), as it is often hard for the tagger to determine these features']",5
['of  #TAUTHOR_TAG on the last'],['of  #TAUTHOR_TAG on the'],"['of  #TAUTHOR_TAG on the last line, which we understand to be the best published results currently. we see that we improve on their results in all categories. we can see', 'the effect of our different approaches to diacritization in the numbers : while for wer we']","['', 'its own. we conclude from this that the quite simple lexeme', 'model is in fact contributing to the correct choice of the lexemic di', '##acritics. finally, we give the results of  #TAUTHOR_TAG on the last line, which we understand to be the best published results currently. we see that we improve on their results in all categories. we can see', 'the effect of our different approaches to diacritization in the numbers : while for wer we reduce the  #TAUTHOR_TAG error by 17. 2 %, the der error reduction', 'is only 10. 9 %. this is because we are choosing among complete diacritization options for white space - tokenized words, while  #TAUTHOR_TAG make choices for each diacritic. this means that when we make a mistake, it may well', 'affect several diacritics at once, so that the diacritic errors are concentrated in fewer words. this effect is even stronger when we disregard the final letter (', '30. 4 % reduction in wer versus 12. 0 % reduction in der ), suggesting that singleton errors in words tend to be in the final position (', 'case, mood ), as it is often hard for the tagger to determine these features']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'it is possible to construct a combined system which uses a lexicon,']","['have shown that a diacritizer that uses a lexical resource can outperform a highly optimized ad - hoc diacritization system that draws on a large number of features.', 'we speculate that further work on wsd could further improve our results.', 'we also note the issue of unknown words, which will affect our system much more than that of  #TAUTHOR_TAG.', 'it is possible to construct a combined system which uses a lexicon, but backs off to a  #TAUTHOR_TAG - style system for unknown words.', 'however, a large portion of the unknown words are in fact foreign words and names, and it is not clear whether the models learned handle such words well']",7
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",5
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",5
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG 27. 7 % 5. 0 % 8. 5 %'],['algorithm of  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",0
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",0
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",0
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG 27. 7 % 5. 0 % 8. 5 %'],['algorithm of  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG, who achieved 77']","[' #TAUTHOR_TAG, who achieved 77 % precision']","[' #TAUTHOR_TAG, who achieved 77 % precision']","['with the results of  #TAUTHOR_TAG, who achieved 77 % precision at 55. 8 % recall for their data, our alignment scores were considerably lower ( 27. 7 % precision, 5 % recall ).', 'we found two reasons for this : language challenges and domain challenges.', 'in what follows, we discuss each reason in more detail.', 'while  #TAUTHOR_TAG aligned english / simple english texts, we dealt with german / simple german data.', 'as mentioned in section 3. 2, in german nouns ( regular nouns as well as proper names ) are capitalized.', 'this makes named entity recognition, a preprocessing step to clustering, more difficult.', 'moreover, german is an example of a morphologically rich language : its noun phrases are marked with case, leading to different inflectional forms for articles, pronouns, adjectives, and nouns.', 'english morphology is poorer ; hence, there is a greater likelihood of lexical overlap.', 'similarly, compounds are productive in german ; an example from our corpus is seniorenwohnanlagen ( "" housing complexes for the elderly "" ).', 'in contrast, english compounds are multiword units, where each word can be accessed separately by a clustering algorithm.', 'therefore, cosine similarity is more effective for english than it is for german.', 'one way to alleviate this problem would be to use extensive morphological decomposition and lemmatization.', 'in terms of domain,  #TAUTHOR_TAG used city descriptions from an encyclopedia for their experiments.', 'for these descriptions clustering worked well because all articles had the same structure ( paragraphs about culture, sports, etc. ).', 'the domain of our corpus was broader : it included information about housing, work, and events for people with disabilities as well as information about the organizations behind the respective websites.', 'apart from language and domain challenges we observed heavy transformations from as to ls in our data ( figure 1 shows a sample article in as and ls ).', 'as a result, ls paragraphs were typically very short and the clustering process returned many singleton clusters.', 'example 4 shows an as / ls sentence pair that could not be aligned because of this.', '( "" he provides them with advice and information. "" ) figure 2 shows the dendrogram of the clustering of the as texts.', 'a dendrogram shows the results of a hierarchical agglomerative clustering.', '']",0
"['of  #TAUTHOR_TAG.', 'we have shown the limits']","['of  #TAUTHOR_TAG.', 'we have shown the limits']","['of our data was from the same language, we applied the monolingual sentence alignment approach of  #TAUTHOR_TAG.', 'we have shown the limits']","['', 'the process of creating a parallel corpus for use in machine translation involves sentence alignment.', 'sentence alignment algorithms for bilingual corpora differ from those for monolingual corpora.', 'since all of our data was from the same language, we applied the monolingual sentence alignment approach of  #TAUTHOR_TAG.', 'we have shown the limits of the algorithm with respect to the language and domain of our data.', 'for example, named entity recognition, a preprocessing step to clustering, is harder for german than for english, the language  #TAUTHOR_TAG worked with.', 'moreover, german features richer morphology than english, which leads to less lexical overlap when working on the word form level.', 'the domain of our corpus was also broader than that of  #TAUTHOR_TAG, who used city descriptions from an encyclopedia for their experiments.', 'this made it harder to identify common article structures that could be exploited in clustering.', 'as a next step, we will experiment with other monolingual sentence alignment algorithms.', 'in addition, we will build a second parallel corpus for german / simple german : a person familiar with the task of text simplification will produce simple versions of german texts.', 'we will use the resulting parallel corpus as data for our experiments in automatically translating from german to simple german.', 'the parallel corpus we compiled as part of the work described in this paper can be made available to interested parties upon request']",0
"['of  #TAUTHOR_TAG.', 'we have shown the limits']","['of  #TAUTHOR_TAG.', 'we have shown the limits']","['of our data was from the same language, we applied the monolingual sentence alignment approach of  #TAUTHOR_TAG.', 'we have shown the limits']","['', 'the process of creating a parallel corpus for use in machine translation involves sentence alignment.', 'sentence alignment algorithms for bilingual corpora differ from those for monolingual corpora.', 'since all of our data was from the same language, we applied the monolingual sentence alignment approach of  #TAUTHOR_TAG.', 'we have shown the limits of the algorithm with respect to the language and domain of our data.', 'for example, named entity recognition, a preprocessing step to clustering, is harder for german than for english, the language  #TAUTHOR_TAG worked with.', 'moreover, german features richer morphology than english, which leads to less lexical overlap when working on the word form level.', 'the domain of our corpus was also broader than that of  #TAUTHOR_TAG, who used city descriptions from an encyclopedia for their experiments.', 'this made it harder to identify common article structures that could be exploited in clustering.', 'as a next step, we will experiment with other monolingual sentence alignment algorithms.', 'in addition, we will build a second parallel corpus for german / simple german : a person familiar with the task of text simplification will produce simple versions of german texts.', 'we will use the resulting parallel corpus as data for our experiments in automatically translating from german to simple german.', 'the parallel corpus we compiled as part of the work described in this paper can be made available to interested parties upon request']",0
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",6
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",6
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG 27. 7 % 5. 0 % 8. 5 %'],['algorithm of  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],6
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",3
"['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##ext']","['as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '.']","['to one sentence in the as paragraph. like  #TAUTHOR_TAG, we performed 200 iterations in boost', '##exter. after learning the mapping rules, the training phase was complete', '. the testing phase consisted of two additional', 'steps. firstly, each paragraph of each text in the test set was assigned to the cluster it was closest to.', 'this was done by calculating the cosine similarity of the word frequencies', 'in the clusters. then, every as paragraph was combined', 'with all ls paragraphs of the parallel text, and boostexter was used in classification mode to predict whether the two paragraphs', 'were to be mapped. secondly, within each pair of paragraphs mapped by boostexter', ', sentences with very high lexical similarity were aligned. in our case', ', the threshold for an alignment was a similarity of 0. 5.', 'for the remaining sentences, proximity to other aligned or similar sentences was used as an indicator. this was implemented by local sequence alignment. we set the mismatch penalty to 0. 02, as a higher mismatch penalty would have reduced recall. we set the skip penalty to 0. 001 conforming to the value of  #TAUTHOR_TAG. the resulting', 'alignments were written to files. example 3 shows a successful sentence alignment. ( 3 )', '']",3
"['of  #TAUTHOR_TAG.', 'we have shown the limits']","['of  #TAUTHOR_TAG.', 'we have shown the limits']","['of our data was from the same language, we applied the monolingual sentence alignment approach of  #TAUTHOR_TAG.', 'we have shown the limits']","['', 'the process of creating a parallel corpus for use in machine translation involves sentence alignment.', 'sentence alignment algorithms for bilingual corpora differ from those for monolingual corpora.', 'since all of our data was from the same language, we applied the monolingual sentence alignment approach of  #TAUTHOR_TAG.', 'we have shown the limits of the algorithm with respect to the language and domain of our data.', 'for example, named entity recognition, a preprocessing step to clustering, is harder for german than for english, the language  #TAUTHOR_TAG worked with.', 'moreover, german features richer morphology than english, which leads to less lexical overlap when working on the word form level.', 'the domain of our corpus was also broader than that of  #TAUTHOR_TAG, who used city descriptions from an encyclopedia for their experiments.', 'this made it harder to identify common article structures that could be exploited in clustering.', 'as a next step, we will experiment with other monolingual sentence alignment algorithms.', 'in addition, we will build a second parallel corpus for german / simple german : a person familiar with the task of text simplification will produce simple versions of german texts.', 'we will use the resulting parallel corpus as data for our experiments in automatically translating from german to simple german.', 'the parallel corpus we compiled as part of the work described in this paper can be made available to interested parties upon request']",3
['of  #TAUTHOR_TAG'],['of  #TAUTHOR_TAG 27. 7 % 5. 0 % 8. 5 %'],['algorithm of  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],4
"[' #TAUTHOR_TAG, who achieved 77']","[' #TAUTHOR_TAG, who achieved 77 % precision']","[' #TAUTHOR_TAG, who achieved 77 % precision']","['with the results of  #TAUTHOR_TAG, who achieved 77 % precision at 55. 8 % recall for their data, our alignment scores were considerably lower ( 27. 7 % precision, 5 % recall ).', 'we found two reasons for this : language challenges and domain challenges.', 'in what follows, we discuss each reason in more detail.', 'while  #TAUTHOR_TAG aligned english / simple english texts, we dealt with german / simple german data.', 'as mentioned in section 3. 2, in german nouns ( regular nouns as well as proper names ) are capitalized.', 'this makes named entity recognition, a preprocessing step to clustering, more difficult.', 'moreover, german is an example of a morphologically rich language : its noun phrases are marked with case, leading to different inflectional forms for articles, pronouns, adjectives, and nouns.', 'english morphology is poorer ; hence, there is a greater likelihood of lexical overlap.', 'similarly, compounds are productive in german ; an example from our corpus is seniorenwohnanlagen ( "" housing complexes for the elderly "" ).', 'in contrast, english compounds are multiword units, where each word can be accessed separately by a clustering algorithm.', 'therefore, cosine similarity is more effective for english than it is for german.', 'one way to alleviate this problem would be to use extensive morphological decomposition and lemmatization.', 'in terms of domain,  #TAUTHOR_TAG used city descriptions from an encyclopedia for their experiments.', 'for these descriptions clustering worked well because all articles had the same structure ( paragraphs about culture, sports, etc. ).', 'the domain of our corpus was broader : it included information about housing, work, and events for people with disabilities as well as information about the organizations behind the respective websites.', 'apart from language and domain challenges we observed heavy transformations from as to ls in our data ( figure 1 shows a sample article in as and ls ).', 'as a result, ls paragraphs were typically very short and the clustering process returned many singleton clusters.', 'example 4 shows an as / ls sentence pair that could not be aligned because of this.', '( "" he provides them with advice and information. "" ) figure 2 shows the dendrogram of the clustering of the as texts.', 'a dendrogram shows the results of a hierarchical agglomerative clustering.', '']",4
"[' #TAUTHOR_TAG, who achieved 77']","[' #TAUTHOR_TAG, who achieved 77 % precision']","[' #TAUTHOR_TAG, who achieved 77 % precision']","['with the results of  #TAUTHOR_TAG, who achieved 77 % precision at 55. 8 % recall for their data, our alignment scores were considerably lower ( 27. 7 % precision, 5 % recall ).', 'we found two reasons for this : language challenges and domain challenges.', 'in what follows, we discuss each reason in more detail.', 'while  #TAUTHOR_TAG aligned english / simple english texts, we dealt with german / simple german data.', 'as mentioned in section 3. 2, in german nouns ( regular nouns as well as proper names ) are capitalized.', 'this makes named entity recognition, a preprocessing step to clustering, more difficult.', 'moreover, german is an example of a morphologically rich language : its noun phrases are marked with case, leading to different inflectional forms for articles, pronouns, adjectives, and nouns.', 'english morphology is poorer ; hence, there is a greater likelihood of lexical overlap.', 'similarly, compounds are productive in german ; an example from our corpus is seniorenwohnanlagen ( "" housing complexes for the elderly "" ).', 'in contrast, english compounds are multiword units, where each word can be accessed separately by a clustering algorithm.', 'therefore, cosine similarity is more effective for english than it is for german.', 'one way to alleviate this problem would be to use extensive morphological decomposition and lemmatization.', 'in terms of domain,  #TAUTHOR_TAG used city descriptions from an encyclopedia for their experiments.', 'for these descriptions clustering worked well because all articles had the same structure ( paragraphs about culture, sports, etc. ).', 'the domain of our corpus was broader : it included information about housing, work, and events for people with disabilities as well as information about the organizations behind the respective websites.', 'apart from language and domain challenges we observed heavy transformations from as to ls in our data ( figure 1 shows a sample article in as and ls ).', 'as a result, ls paragraphs were typically very short and the clustering process returned many singleton clusters.', 'example 4 shows an as / ls sentence pair that could not be aligned because of this.', '( "" he provides them with advice and information. "" ) figure 2 shows the dendrogram of the clustering of the as texts.', 'a dendrogram shows the results of a hierarchical agglomerative clustering.', '']",4
"[' #TAUTHOR_TAG, who achieved 77']","[' #TAUTHOR_TAG, who achieved 77 % precision']","[' #TAUTHOR_TAG, who achieved 77 % precision']","['with the results of  #TAUTHOR_TAG, who achieved 77 % precision at 55. 8 % recall for their data, our alignment scores were considerably lower ( 27. 7 % precision, 5 % recall ).', 'we found two reasons for this : language challenges and domain challenges.', 'in what follows, we discuss each reason in more detail.', 'while  #TAUTHOR_TAG aligned english / simple english texts, we dealt with german / simple german data.', 'as mentioned in section 3. 2, in german nouns ( regular nouns as well as proper names ) are capitalized.', 'this makes named entity recognition, a preprocessing step to clustering, more difficult.', 'moreover, german is an example of a morphologically rich language : its noun phrases are marked with case, leading to different inflectional forms for articles, pronouns, adjectives, and nouns.', 'english morphology is poorer ; hence, there is a greater likelihood of lexical overlap.', 'similarly, compounds are productive in german ; an example from our corpus is seniorenwohnanlagen ( "" housing complexes for the elderly "" ).', 'in contrast, english compounds are multiword units, where each word can be accessed separately by a clustering algorithm.', 'therefore, cosine similarity is more effective for english than it is for german.', 'one way to alleviate this problem would be to use extensive morphological decomposition and lemmatization.', 'in terms of domain,  #TAUTHOR_TAG used city descriptions from an encyclopedia for their experiments.', 'for these descriptions clustering worked well because all articles had the same structure ( paragraphs about culture, sports, etc. ).', 'the domain of our corpus was broader : it included information about housing, work, and events for people with disabilities as well as information about the organizations behind the respective websites.', 'apart from language and domain challenges we observed heavy transformations from as to ls in our data ( figure 1 shows a sample article in as and ls ).', 'as a result, ls paragraphs were typically very short and the clustering process returned many singleton clusters.', 'example 4 shows an as / ls sentence pair that could not be aligned because of this.', '( "" he provides them with advice and information. "" ) figure 2 shows the dendrogram of the clustering of the as texts.', 'a dendrogram shows the results of a hierarchical agglomerative clustering.', '']",4
"['of  #TAUTHOR_TAG.', 'we have shown the limits']","['of  #TAUTHOR_TAG.', 'we have shown the limits']","['of our data was from the same language, we applied the monolingual sentence alignment approach of  #TAUTHOR_TAG.', 'we have shown the limits']","['', 'the process of creating a parallel corpus for use in machine translation involves sentence alignment.', 'sentence alignment algorithms for bilingual corpora differ from those for monolingual corpora.', 'since all of our data was from the same language, we applied the monolingual sentence alignment approach of  #TAUTHOR_TAG.', 'we have shown the limits of the algorithm with respect to the language and domain of our data.', 'for example, named entity recognition, a preprocessing step to clustering, is harder for german than for english, the language  #TAUTHOR_TAG worked with.', 'moreover, german features richer morphology than english, which leads to less lexical overlap when working on the word form level.', 'the domain of our corpus was also broader than that of  #TAUTHOR_TAG, who used city descriptions from an encyclopedia for their experiments.', 'this made it harder to identify common article structures that could be exploited in clustering.', 'as a next step, we will experiment with other monolingual sentence alignment algorithms.', 'in addition, we will build a second parallel corpus for german / simple german : a person familiar with the task of text simplification will produce simple versions of german texts.', 'we will use the resulting parallel corpus as data for our experiments in automatically translating from german to simple german.', 'the parallel corpus we compiled as part of the work described in this paper can be made available to interested parties upon request']",4
"[' #TAUTHOR_TAG.', 'we first present some background information on arabic morphology and then discuss our methodology and main results.', ""we present our best performing set of features, which we also use in our spmrl'2013 submission""]","[' #TAUTHOR_TAG.', 'we first present some background information on arabic morphology and then discuss our methodology and main results.', ""we present our best performing set of features, which we also use in our spmrl'2013 submission""]","['this section, we summarize  #TAUTHOR_TAG.', 'we first present some background information on arabic morphology and then discuss our methodology and main results.', ""we present our best performing set of features, which we also use in our spmrl'2013 submission""]","['this section, we summarize  #TAUTHOR_TAG.', 'we first present some background information on arabic morphology and then discuss our methodology and main results.', ""we present our best performing set of features, which we also use in our spmrl'2013 submission""]",0
"['in several languages  #AUTHOR_TAG. in', 'contrast to these negative results,  #TAUTHOR_TAG showed positive results for using agreement morphology for arabic']","['in several languages  #AUTHOR_TAG. in', 'contrast to these negative results,  #TAUTHOR_TAG showed positive results for using agreement morphology for arabic']","['in several languages  #AUTHOR_TAG. in', 'contrast to these negative results,  #TAUTHOR_TAG showed positive results for using agreement morphology for arabic']","['', 'values ( ratio of correct predictions out of all predictions ) of course affects the value of a feature on unseen text. even if relevant and non', '- redundant, a feature may be hard to predict with sufficient accuracy by current technology, in which case it will be of little or no help for parsing, even if helpful when its gold values are provided. the case feature is very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is', 'not useful. different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. it has been shown previously that if the relevant', 'morphological features in assignment configurations can be recognized well enough, then they contribute to parsing accuracy. for example, modeling case in czech improves czech parsing  #AUTHOR_TAG : case is relevant, not redundant, and can be predicted with sufficient accuracy. however', ', it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages  #AUTHOR_TAG. in', 'contrast to these negative results,  #TAUTHOR_TAG showed positive results for using agreement morphology for arabic']",0
"[' #TAUTHOR_TAG, we investigated morphological features']","[' #TAUTHOR_TAG, we investigated morphological features']","[' #TAUTHOR_TAG, we investigated morphological features']","[' #TAUTHOR_TAG, we investigated morphological features for dependency parsing of modern standard arabic ( msa ).', 'the goal was to find a set of relevant, accurate and non - redundant features.', ' #TAUTHOR_TAG used both the maltparser  #AUTHOR_TAG and the easy - first parser  #AUTHOR_TAG.', 'since the easy - first parser performed better, we use it in all experiments reported in this paper.', 'for msa, the space of possible morphological features is quite large.', 'we determined which morphological features help by performing a search through the feature space.', 'in order to do this, we separated part - of - speech ( pos ) from the morphological features.', 'we defined a core set of 12 pos features, and then explored combinations of morphological features in addition to this pos tagset.', 'this core set of pos tags is similar to those proposed in cross - lingual work  #AUTHOR_TAG.', 'we performed this search independently for gold input features and predicted input features.', 'we used our mada + tokan system  #AUTHOR_TAG for the prediction.', 'as the easyfirst parser predicts links separately before labels, we first optimized for unlabeled attachment score, and then optimized the easy - first parser labeler for label score.', 'as had been found in previous results, assignment features, specifically case and state, are very helpful in msa.', 'however, in msa this is true only under gold conditions : since case is rarely explicit in the typically undiacritized written msa, it has a dismal accuracy rate, which makes it useless when used in machine - predicted ( real, non - gold ) condition.', 'in contrast with previous results, we showed that agreement features are quite helpful in both gold and predicted conditions.', 'this is likely a result of msa having a rich agreement system, covering both verb - subject and noun - adjective relations.', 'additionally, almost all work to date in msa morphological analysis and part - of - speech ( pos ) tagging has concentrated on the morphemic form of the words.', 'however, often the functional morphology ( which is relevant to agreement, and relates to the meaning of the word ) is at odds with the "" surface "" ( form - based ) morphology ; a well - known example of this are the "" broken "" ( irregular ) plurals of nominals, which often have singular - form morphemes but are in fact plurals and show plural agreement if the referent is rational.', 'in  #TAUTHOR_TAG ; "" ≤ 70 "" refers to the test sentences with 70 or fewer words.', ""training set test set labeled tedeval score unlabeled tedeval score 5k ( spmrl'2013 ) test ≤ 70 86. 4 89""]",0
"[' #TAUTHOR_TAG, we investigated morphological features']","[' #TAUTHOR_TAG, we investigated morphological features']","[' #TAUTHOR_TAG, we investigated morphological features']","[' #TAUTHOR_TAG, we investigated morphological features for dependency parsing of modern standard arabic ( msa ).', 'the goal was to find a set of relevant, accurate and non - redundant features.', ' #TAUTHOR_TAG used both the maltparser  #AUTHOR_TAG and the easy - first parser  #AUTHOR_TAG.', 'since the easy - first parser performed better, we use it in all experiments reported in this paper.', 'for msa, the space of possible morphological features is quite large.', 'we determined which morphological features help by performing a search through the feature space.', 'in order to do this, we separated part - of - speech ( pos ) from the morphological features.', 'we defined a core set of 12 pos features, and then explored combinations of morphological features in addition to this pos tagset.', 'this core set of pos tags is similar to those proposed in cross - lingual work  #AUTHOR_TAG.', 'we performed this search independently for gold input features and predicted input features.', 'we used our mada + tokan system  #AUTHOR_TAG for the prediction.', 'as the easyfirst parser predicts links separately before labels, we first optimized for unlabeled attachment score, and then optimized the easy - first parser labeler for label score.', 'as had been found in previous results, assignment features, specifically case and state, are very helpful in msa.', 'however, in msa this is true only under gold conditions : since case is rarely explicit in the typically undiacritized written msa, it has a dismal accuracy rate, which makes it useless when used in machine - predicted ( real, non - gold ) condition.', 'in contrast with previous results, we showed that agreement features are quite helpful in both gold and predicted conditions.', 'this is likely a result of msa having a rich agreement system, covering both verb - subject and noun - adjective relations.', 'additionally, almost all work to date in msa morphological analysis and part - of - speech ( pos ) tagging has concentrated on the morphemic form of the words.', 'however, often the functional morphology ( which is relevant to agreement, and relates to the meaning of the word ) is at odds with the "" surface "" ( form - based ) morphology ; a well - known example of this are the "" broken "" ( irregular ) plurals of nominals, which often have singular - form morphemes but are in fact plurals and show plural agreement if the referent is rational.', 'in  #TAUTHOR_TAG ; "" ≤ 70 "" refers to the test sentences with 70 or fewer words.', ""training set test set labeled tedeval score unlabeled tedeval score 5k ( spmrl'2013 ) test ≤ 70 86. 4 89""]",0
"['in several languages  #AUTHOR_TAG. in', 'contrast to these negative results,  #TAUTHOR_TAG showed positive results for using agreement morphology for arabic']","['in several languages  #AUTHOR_TAG. in', 'contrast to these negative results,  #TAUTHOR_TAG showed positive results for using agreement morphology for arabic']","['in several languages  #AUTHOR_TAG. in', 'contrast to these negative results,  #TAUTHOR_TAG showed positive results for using agreement morphology for arabic']","['', 'values ( ratio of correct predictions out of all predictions ) of course affects the value of a feature on unseen text. even if relevant and non', '- redundant, a feature may be hard to predict with sufficient accuracy by current technology, in which case it will be of little or no help for parsing, even if helpful when its gold values are provided. the case feature is very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is', 'not useful. different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. it has been shown previously that if the relevant', 'morphological features in assignment configurations can be recognized well enough, then they contribute to parsing accuracy. for example, modeling case in czech improves czech parsing  #AUTHOR_TAG : case is relevant, not redundant, and can be predicted with sufficient accuracy. however', ', it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages  #AUTHOR_TAG. in', 'contrast to these negative results,  #TAUTHOR_TAG showed positive results for using agreement morphology for arabic']",1
"['used in  #TAUTHOR_TAG, so']","['used in  #TAUTHOR_TAG, so']","['data split used in the shared task is different from the data split we used in  #TAUTHOR_TAG, so we retrained our models on the new splits  #AUTHOR_TAG.', 'the data released for the shared task showed inconsistent availability of lemmas across gold and predicted input, so we used the almor analyzer  #AUTHOR_TAG with the sama databases  #AUTHOR_TAG']","['data split used in the shared task is different from the data split we used in  #TAUTHOR_TAG, so we retrained our models on the new splits  #AUTHOR_TAG.', 'the data released for the shared task showed inconsistent availability of lemmas across gold and predicted input, so we used the almor analyzer  #AUTHOR_TAG with the sama databases  #AUTHOR_TAG to determine a lemma given the word form and the provided ( gold or predicted ) pos tags.', 'in addition to the lemmas, the almor analyzer also provides morphological features in the feature - value representation our approach requires.', 'finally, we ran our existing converter  #AUTHOR_TAG over this representation to obtain functional number and gender, as well as the rationality feature.', '3 for simplicity reasons, we used the mle : w2 + catib model  #AUTHOR_TAG, which was the best performing model on seen words, as opposed to the combination system that used a syntactic component with better results on unseen words.', 'we did not perform alif or ya normalization on the data.', 'we trained two models : one on 5, 000 sentences of training data and one on the entire training data']",4
"[' #TAUTHOR_TAG.', 'the increase over  #TAUTHOR_TAG may simply be']","[' #TAUTHOR_TAG.', 'the increase over  #TAUTHOR_TAG may simply be']","['2 ), we also give the performance reported in our previous work  #TAUTHOR_TAG.', 'the increase over  #TAUTHOR_TAG may simply be']","['performance in the shared task for arabic dependency, gold tokenization, predicted tags, is shown in table 2.', 'our performance in the shared task for arabic dependency, predicted tokenization, predicted tags, is shown in table 3.', 'for predicted tokenization, only the ims / szeged system which uses system combination ( run 2 ) outperformed our parser on all measures ; our parser performed better than all other single - parser systems.', 'for gold tokenization, our system is the second best single - parser system after the ims / szeged single system ( run 1 ).', 'for gold tokenization and predicted morphology ( table 2 ), we also give the performance reported in our previous work  #TAUTHOR_TAG.', 'the increase over  #TAUTHOR_TAG may simply be due to the different split for training and test, but it may also be due to improvements to the functional feature prediction  #AUTHOR_TAG, and the predicted features provided by the shared task organizers']",4
"['', ' #TAUTHOR_TAG present a reformulation of']","['', ' #TAUTHOR_TAG present a reformulation of re, where the']","[').', ' #TAUTHOR_TAG present a reformulation of']","['', ' #TAUTHOR_TAG present a reformulation of re, where the task is framed as reading comprehension.', 'in this formulation, each relation type ( e. g. author, occupation ) is mapped to at least one natural language question template ( e. g. "" who is the author of x? "" ), where x is filled with an entity ( e. g. "" inferno "" ).', 'the model is then tasked with finding an answer ( "" dante alighieri "" ) to this question with respect to a given context.', 'they show that this formulation of the problem both outperforms off - the - shelf re systems in the typical re setting and, in addition, enables generalization to unspecified and unseen types of relations.', 'x - wikire enables exploration of this reformulation of re in a multilingual setting.', 'contributions we introduce a new, largescale multilingual dataset ( x - wikire ) of reading comprehension - based re for english, german, french, spanish, and italian, facilitating research on']",0
[' #TAUTHOR_TAG propose a novel approach towards achieving this'],[' #TAUTHOR_TAG propose a novel approach towards achieving this'],"['- shot relation extraction  #TAUTHOR_TAG propose a novel approach towards achieving this generalization by transforming relations into natural language question templates.', 'for instance, the relation born in ( x, y )']","['extraction we begin with a brief description of our terminology.', 'given raw text, relation extraction is the task of identifying instances of relations relation ( entity 1, entity 2 ).', 'we refer to these instances of relation and entity pairs as triples.', 'furthermore, throughout this work, we use the term property interchangeably with relation.', 'a large part of previous work on relation extraction has been concerned with extracting relations between unseen entities for a pre - defined set of relations seen during training  #AUTHOR_TAG.', 'for example, the instances ( barack obama, hawaii ), ( niels bohr, copenhagen ), and ( jacques brel, schaerbeek ) of the relation born in ( x, y ) would be seen during the training phase, and then the model would be expected to correctly identify other instances of the relation such as ( jean - paul sartre, paris ) in running text.', 'this is useful in closeddomain settings where it is possible to pre - select a set of relations of interest.', 'in an open - domain setting, however, we are interested in the far more difficult problem of extracting unseen relation types.', 'open re methods  #AUTHOR_TAG do not require relationspecific data, but treat different phrasings of the same relation as different relations and rely on a combination of syntactic features ( e. g. dependency parses ) and normalisation rules, and so have limited generalization capacity.', 'zero - shot relation extraction  #TAUTHOR_TAG propose a novel approach towards achieving this generalization by transforming relations into natural language question templates.', 'for instance, the relation born in ( x, y ) can be expressed as "" where was x born? "" or "" in which place was x born? "".', 'then, a reading comprehension model  #AUTHOR_TAG can be trained on question, answer, and context examples where the x slot is filled with an entity and the y slot is either an answer if the answer is present in the context, or nil.', 'the model is then able to extract relation instances ( given expressions of the relations as questions ) from raw text.', 'to test this "" harsh zero - shot "" setting of relation extraction, they build a dataset for re as machine comprehension from wikireading  #AUTHOR_TAG ing comprehension model is able to use linguistic cues to identify relation paraphrases and lexicosyntactic patterns of textual deviation from questions to answers, enabling it to identify instances of new relations.', 'similar work  #AUTHOR_TAG recently also showed that re can be framed as natural language inference']",0
"['corpora, 7 except for the comparison experiments described in sub - section 5. 1 where glove  #AUTHOR_TAG was used for comparability with  #TAUTHOR_TAG']","['corpora, 7 except for the comparison experiments described in sub - section 5. 1 where glove  #AUTHOR_TAG was used for comparability with  #TAUTHOR_TAG']","['##1 - score.', ""all monolingual models'word embeddings were initialised using fasttext embeddings trained on each language's wikipedia and common crawl corpora, 7 except for the comparison experiments described in sub - section 5. 1 where glove  #AUTHOR_TAG was used for comparability with  #TAUTHOR_TAG""]","['all experiments, models were trained for five epochs with a learning rate of 1. 0 using adam  #AUTHOR_TAG.', 'for finetuning in the cross - lingual transfer experiments, the learning rate was lowered to 0. 001 to prevent forgetting and a maximum of 30 finetuning iterations over the small target language training set were performed with model selection using the target language development set f1 - score.', ""all monolingual models'word embeddings were initialised using fasttext embeddings trained on each language's wikipedia and common crawl corpora, 7 except for the comparison experiments described in sub - section 5. 1 where glove  #AUTHOR_TAG was used for comparability with  #TAUTHOR_TAG""]",0
"['', 'this is pri - lang.', 'unent unrel  #TAUTHOR_TAG  #AUTHOR_TAG employed a']","['', 'this is pri - lang.', 'unent unrel  #TAUTHOR_TAG  #AUTHOR_TAG employed a']","['', 'this is pri - lang.', 'unent unrel  #TAUTHOR_TAG  #AUTHOR_TAG employed a pipeline of']","['##ingual nlu advances in natural language understanding tasks have been as impressive as they have been fast - paced.', 'until recently, however, the multilingual aspect of such tasks has not received as much attention.', 'this is pri - lang.', 'unent unrel  #TAUTHOR_TAG  #AUTHOR_TAG employed a pipeline of machine translation systems to translate to english, then open re systems to perform re on the translated text, followed by crosslingual projection back to source language.', ' #AUTHOR_TAG apply the universal schema framework  #AUTHOR_TAG on top of multilingual embeddings to extract relations from spanish text without using spanish training data.', 'this approach, however, only enables generalization to unseen entities and does not have the flexibility to predict unseen relations.', 'furthermore, both of these works faced a fundamental difficulty with evaluation.', 'the former resort to manual annotation of a small number of examples ( 1000 ) in each language and the latter use the 2012 tac spanish slot - filling evaluation dataset in which "" the coverage of facts in the available annotation is very small "".', 'with the introduction of x - wikire, this work provides the first large - scale dataset and benchmark for the evaluation of multilingual re spanning five languages.', 'while this paves the way for a wide range of research on multilingual relation extraction and knowledge base population, we hope to extend this to a larger variety of languages in future work, particularly as we have been able to show that the amount of training data required for cross - lingual model transfer is minimal, meaning that a small dataset ( when only that is available ) can go a long way']",0
"['', ' #TAUTHOR_TAG present a reformulation of']","['', ' #TAUTHOR_TAG present a reformulation of re, where the']","[').', ' #TAUTHOR_TAG present a reformulation of']","['', ' #TAUTHOR_TAG present a reformulation of re, where the task is framed as reading comprehension.', 'in this formulation, each relation type ( e. g. author, occupation ) is mapped to at least one natural language question template ( e. g. "" who is the author of x? "" ), where x is filled with an entity ( e. g. "" inferno "" ).', 'the model is then tasked with finding an answer ( "" dante alighieri "" ) to this question with respect to a given context.', 'they show that this formulation of the problem both outperforms off - the - shelf re systems in the typical re setting and, in addition, enables generalization to unspecified and unseen types of relations.', 'x - wikire enables exploration of this reformulation of re in a multilingual setting.', 'contributions we introduce a new, largescale multilingual dataset ( x - wikire ) of reading comprehension - based re for english, german, french, spanish, and italian, facilitating research on']",1
[' #TAUTHOR_TAG propose a novel approach towards achieving this'],[' #TAUTHOR_TAG propose a novel approach towards achieving this'],"['- shot relation extraction  #TAUTHOR_TAG propose a novel approach towards achieving this generalization by transforming relations into natural language question templates.', 'for instance, the relation born in ( x, y )']","['extraction we begin with a brief description of our terminology.', 'given raw text, relation extraction is the task of identifying instances of relations relation ( entity 1, entity 2 ).', 'we refer to these instances of relation and entity pairs as triples.', 'furthermore, throughout this work, we use the term property interchangeably with relation.', 'a large part of previous work on relation extraction has been concerned with extracting relations between unseen entities for a pre - defined set of relations seen during training  #AUTHOR_TAG.', 'for example, the instances ( barack obama, hawaii ), ( niels bohr, copenhagen ), and ( jacques brel, schaerbeek ) of the relation born in ( x, y ) would be seen during the training phase, and then the model would be expected to correctly identify other instances of the relation such as ( jean - paul sartre, paris ) in running text.', 'this is useful in closeddomain settings where it is possible to pre - select a set of relations of interest.', 'in an open - domain setting, however, we are interested in the far more difficult problem of extracting unseen relation types.', 'open re methods  #AUTHOR_TAG do not require relationspecific data, but treat different phrasings of the same relation as different relations and rely on a combination of syntactic features ( e. g. dependency parses ) and normalisation rules, and so have limited generalization capacity.', 'zero - shot relation extraction  #TAUTHOR_TAG propose a novel approach towards achieving this generalization by transforming relations into natural language question templates.', 'for instance, the relation born in ( x, y ) can be expressed as "" where was x born? "" or "" in which place was x born? "".', 'then, a reading comprehension model  #AUTHOR_TAG can be trained on question, answer, and context examples where the x slot is filled with an entity and the y slot is either an answer if the answer is present in the context, or nil.', 'the model is then able to extract relation instances ( given expressions of the relations as questions ) from raw text.', 'to test this "" harsh zero - shot "" setting of relation extraction, they build a dataset for re as machine comprehension from wikireading  #AUTHOR_TAG ing comprehension model is able to use linguistic cues to identify relation paraphrases and lexicosyntactic patterns of textual deviation from questions to answers, enabling it to identify instances of new relations.', 'similar work  #AUTHOR_TAG recently also showed that re can be framed as natural language inference']",1
"[' #TAUTHOR_TAG, we distinguish between']","[' #TAUTHOR_TAG, we distinguish between']","[' #TAUTHOR_TAG, we distinguish between the traditional re setting where the aim is to']","[' #TAUTHOR_TAG, we distinguish between the traditional re setting where the aim is to generalize to unseen entities ( unent ) and the zero - shot setting ( unrel ) where the aim is to do so for unseen relation types ( see section 2 ).', '']",1
"['we use the distant supervision method described by  #TAUTHOR_TAG.', 'for']","['we use the distant supervision method described by  #TAUTHOR_TAG.', 'for']","['we use the distant supervision method described by  #TAUTHOR_TAG.', 'for']","['', 'this consists of replacing the property and value ids of each statement in the document with the text label for values which are entities, and with the human readable form for numeric values ( e. g. timestamps are converted to natural forms like "" 25 may 1994 "" ) obtaining a tuple ( property, entity ).', '2 slot - filling data to extract the contexts for each triple in our dataset we use the distant supervision method described by  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG, we distinguish between']","[' #TAUTHOR_TAG, we distinguish between']","[' #TAUTHOR_TAG, we distinguish between the traditional re setting where the aim is to']","[' #TAUTHOR_TAG, we distinguish between the traditional re setting where the aim is to generalize to unseen entities ( unent ) and the zero - shot setting ( unrel ) where the aim is to do so for unseen relation types ( see section 2 ).', '']",5
"[' #TAUTHOR_TAG, we distinguish between']","[' #TAUTHOR_TAG, we distinguish between']","[' #TAUTHOR_TAG, we distinguish between the traditional re setting where the aim is to']","[' #TAUTHOR_TAG, we distinguish between the traditional re setting where the aim is to generalize to unseen entities ( unent ) and the zero - shot setting ( unrel ) where the aim is to do so for unseen relation types ( see section 2 ).', '']",5
"['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements']","['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements']","['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements obtained are in line with']","['baseline model is trained on the full monolingual training set ( 1 million instances ) for each of the languages in both the unent and unrel settings, which serve as a point of comparison for the cross - lingual transfer and multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements obtained are in line with those reported by  #AUTHOR_TAG of namanda over bidaf on reading comprehension tasks.', 'results table 3 shows the results of the monolingual baselines.', 'for the cross - lingual transfer experiments, these results can be viewed as a performance ceiling.', 'observe that the results on our dataset are in general lower than those reported in  #TAUTHOR_TAG.', 'this can be attributed to three factors : a ) on average, the context length in our dataset is longer compared to theirs ( see appendix c ) ; b ) the fasttext word embeddings we employ to facilitate multilingual sharing have a lower coverage of the vocabularies of each language than the glove word embeddings employed in that work ; c ) in the unrel setting, we employ a more challenging setup of 5 - fold cross - validation ( as opposed to 10 - fold in their experiments ), meaning that a lower number of relations is seen at training time and the test set contains a higher number of unseen relations']",5
"['we use the distant supervision method described by  #TAUTHOR_TAG.', 'for']","['we use the distant supervision method described by  #TAUTHOR_TAG.', 'for']","['we use the distant supervision method described by  #TAUTHOR_TAG.', 'for']","['', 'this consists of replacing the property and value ids of each statement in the document with the text label for values which are entities, and with the human readable form for numeric values ( e. g. timestamps are converted to natural forms like "" 25 may 1994 "" ) obtaining a tuple ( property, entity ).', '2 slot - filling data to extract the contexts for each triple in our dataset we use the distant supervision method described by  #TAUTHOR_TAG.', '']",6
[''],[''],[''],"['our framework, a machine comprehension model sees a question - context pair and is tasked with selecting an answer span within the context, or indicating that the context does not contain an answer ( returning nil ).', ""this'nil - awareness'goes beyond the traditional reading comprehension setup where it is not required."", 'it has, however, recently been incorporated into newer datasets  #AUTHOR_TAG.', 'we employ the architecture described in  #AUTHOR_TAG as our standard reading comprehension model for all the experiments.', 'this nil - aware answer extraction framework ( namanda ) is briefly described below.', 'in a set of initial trials ( see table 3 ), we found that this model far outperformed the bias - augmented bidaf model  #AUTHOR_TAG used by']",4
"['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements']","['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements']","['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements obtained are in line with']","['baseline model is trained on the full monolingual training set ( 1 million instances ) for each of the languages in both the unent and unrel settings, which serve as a point of comparison for the cross - lingual transfer and multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements obtained are in line with those reported by  #AUTHOR_TAG of namanda over bidaf on reading comprehension tasks.', 'results table 3 shows the results of the monolingual baselines.', 'for the cross - lingual transfer experiments, these results can be viewed as a performance ceiling.', 'observe that the results on our dataset are in general lower than those reported in  #TAUTHOR_TAG.', 'this can be attributed to three factors : a ) on average, the context length in our dataset is longer compared to theirs ( see appendix c ) ; b ) the fasttext word embeddings we employ to facilitate multilingual sharing have a lower coverage of the vocabularies of each language than the glove word embeddings employed in that work ; c ) in the unrel setting, we employ a more challenging setup of 5 - fold cross - validation ( as opposed to 10 - fold in their experiments ), meaning that a lower number of relations is seen at training time and the test set contains a higher number of unseen relations']",4
"[' #TAUTHOR_TAG, we distinguish between']","[' #TAUTHOR_TAG, we distinguish between']","[' #TAUTHOR_TAG, we distinguish between the traditional re setting where the aim is to']","[' #TAUTHOR_TAG, we distinguish between the traditional re setting where the aim is to generalize to unseen entities ( unent ) and the zero - shot setting ( unrel ) where the aim is to do so for unseen relation types ( see section 2 ).', '']",3
"['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements']","['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements']","['multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements obtained are in line with']","['baseline model is trained on the full monolingual training set ( 1 million instances ) for each of the languages in both the unent and unrel settings, which serve as a point of comparison for the cross - lingual transfer and multilingual models.', 'comparison with  #TAUTHOR_TAG using the bias - augmented bidaf model on their dataset ( and splits ) can be seen.', 'the clear improvements obtained are in line with those reported by  #AUTHOR_TAG of namanda over bidaf on reading comprehension tasks.', 'results table 3 shows the results of the monolingual baselines.', 'for the cross - lingual transfer experiments, these results can be viewed as a performance ceiling.', 'observe that the results on our dataset are in general lower than those reported in  #TAUTHOR_TAG.', 'this can be attributed to three factors : a ) on average, the context length in our dataset is longer compared to theirs ( see appendix c ) ; b ) the fasttext word embeddings we employ to facilitate multilingual sharing have a lower coverage of the vocabularies of each language than the glove word embeddings employed in that work ; c ) in the unrel setting, we employ a more challenging setup of 5 - fold cross - validation ( as opposed to 10 - fold in their experiments ), meaning that a lower number of relations is seen at training time and the test set contains a higher number of unseen relations']",7
"['and entity types through global cues 6', 'which  #TAUTHOR_TAG suggested are important']","['and entity types through global cues 6', 'which  #TAUTHOR_TAG suggested are important']","['that it is more difficult to transfer the ability to identify relation paraphrases and entity types through global cues 6', 'which  #TAUTHOR_TAG suggested are important']","['', 'figure 5 shows the results of the crossling', '##ual transfer experiments for unent, where transfer is accomplished through multilingually aligned fasttext embeddings. in a parallel set', 'of experiments, transfer was performed through the multilingual bert encoder. the results of this ( see appendix d ) showed a clear advantage for the former over the latter. 5 this is primarily due', 'to the low vocabulary coverage of multilingual bert which has a total vocabulary size of 100k', 'tokens for 104 languages ( see appendix c for coverage statistics ). while it is clear that the models suffer from rather low recall when no finetuning', 'is performed, the results show considerable improvements when finetuning with only 1000 target language', 'examples. with 10k target language examples, it is possible to nearly match the performance of a model', ""trained on the full target language monolingual training set. similarly, in the unrel experiments, our results ( figure 6 ) show that it's possible to recover a large part of"", ""the fully - supervised monolingual models'performance. it can be seen, however, that with 10k target language examples, a lower proportion of the performance is recovered when compared to"", 'the unent setting. this indicates that it is more difficult to transfer the ability to identify relation paraphrases and entity types through global cues 6', 'which  #TAUTHOR_TAG suggested are important for generalizing to new rela - 5 we therefore continue the rest of our experiments in the paper using the multilingual fasttext embeddings. 6 when', 'context phrasing deviates from the question in a way that is common between relations. l a n g / me a s u r e tions in this', 'framework']",7
['computed the average length of the context in out dataset and  #TAUTHOR_TAG'],['computed the average length of the context in out dataset and  #TAUTHOR_TAG'],"['computed the average length of the context in out dataset and  #TAUTHOR_TAG table 4 : avarage number of tokens in the context.', 'table 6 shows the results for our model in the unent scenario using both multilingual bert and fasttext.', 'bert performs poorly compared to fasttext in every language']","['computed the average length of the context in out dataset and  #TAUTHOR_TAG table 4 : avarage number of tokens in the context.', 'table 6 shows the results for our model in the unent scenario using both multilingual bert and fasttext.', 'bert performs poorly compared to fasttext in every language and almost for each of the finetuning settings.', ""this is likely due to the lower coverage of our dataset's vocabulary as can be seen in table 5."", 'table 6 : precision, recall and f1 - scores for unent comparing scores using bert and fasttext multilingual embeddings']",7
['millions of utterances  #TAUTHOR_TAG and'],['millions of utterances  #TAUTHOR_TAG and'],['millions of utterances  #TAUTHOR_TAG and'],"['from a candidate list, which we call next - utterance - classification ( nuc, detailed in section 2 ), and', 'are evaluated using the metric of recall. nuc is useful for several reasons : 1 ) the performance ( i. e. loss or error ) is easy to com - pute automatically, 2 ) it is simple to adjust the difficulty of', 'the task, 3 ) the task is interpretable and amenable to comparison with human performance, 4 ) it is an easier task compared to generative dialogue modeling, which is difficult for end', '##to - end systems, and 5 ) models trained with nuc can be converted to dialogue systems by retrieving from the full corpus  #AUTHOR_TAG.', 'in this case, nuc additionally allows for making hard constraints on the allowable outputs of the system ( to prevent offensive responses ), and guarantees that the responses', 'are fluent ( because they were generated by humans ). thus, nuc can be thought of both as an intermediate task that can be used to evaluate the ability of systems to understand natural language conversations, similar to the babi tasks for', 'language understanding, and as a useful framework for building chatbots. with the huge size of current dialogue datasets that contain millions of utterances  #TAUTHOR_TAG and the increasing amount of natural language data, it is conceivable that retrieval - based systems will be able', 'to have engaging conversations with humans. however, despite the current work with nuc, there has been no verification of', '']",0
['millions of utterances  #TAUTHOR_TAG and'],['millions of utterances  #TAUTHOR_TAG and'],['millions of utterances  #TAUTHOR_TAG and'],"['from a candidate list, which we call next - utterance - classification ( nuc, detailed in section 2 ), and', 'are evaluated using the metric of recall. nuc is useful for several reasons : 1 ) the performance ( i. e. loss or error ) is easy to com - pute automatically, 2 ) it is simple to adjust the difficulty of', 'the task, 3 ) the task is interpretable and amenable to comparison with human performance, 4 ) it is an easier task compared to generative dialogue modeling, which is difficult for end', '##to - end systems, and 5 ) models trained with nuc can be converted to dialogue systems by retrieving from the full corpus  #AUTHOR_TAG.', 'in this case, nuc additionally allows for making hard constraints on the allowable outputs of the system ( to prevent offensive responses ), and guarantees that the responses', 'are fluent ( because they were generated by humans ). thus, nuc can be thought of both as an intermediate task that can be used to evaluate the ability of systems to understand natural language conversations, similar to the babi tasks for', 'language understanding, and as a useful framework for building chatbots. with the huge size of current dialogue datasets that contain millions of utterances  #TAUTHOR_TAG and the increasing amount of natural language data, it is conceivable that retrieval - based systems will be able', 'to have engaging conversations with humans. however, despite the current work with nuc, there has been no verification of', '']",0
"['evaluating dialogue systems  #TAUTHOR_TAG.', 'there are several attractive properties of this']","['evaluating dialogue systems  #TAUTHOR_TAG.', 'there are several attractive properties of this approach, as detailed in the introduction : the performance is easy to compute automatically,']","['evaluating dialogue systems  #TAUTHOR_TAG.', 'there are several attractive properties of this approach, as detailed in the introduction : the performance is easy to compute automatically, the task is interpretable']","['', 'this task has gained some popularity recently for evaluating dialogue systems  #TAUTHOR_TAG.', 'there are several attractive properties of this approach, as detailed in the introduction : the performance is easy to compute automatically, the task is interpretable and amenable to comparison with human performance, and it is easier than generative dialogue modeling.', 'a particularly nice property is that one can adjust the difficulty of nuc by simply changing the number of false responses ( from one response to the full corpus ), or by altering the selection criteria of false responses ( from randomly sampled to intentionally confusing ).', 'indeed, as the number of false responses grows to encompass all natural language responses, the task becomes identical to response generation.', 'one potential limitation of the nuc approach is that, since the other candidate answers are sampled from elsewhere in the corpus, these may also represent reasonable responses given the context.', 'part of the contribution of this work is determining the significance of this limitation']",0
['millions of utterances  #TAUTHOR_TAG and'],['millions of utterances  #TAUTHOR_TAG and'],['millions of utterances  #TAUTHOR_TAG and'],"['from a candidate list, which we call next - utterance - classification ( nuc, detailed in section 2 ), and', 'are evaluated using the metric of recall. nuc is useful for several reasons : 1 ) the performance ( i. e. loss or error ) is easy to com - pute automatically, 2 ) it is simple to adjust the difficulty of', 'the task, 3 ) the task is interpretable and amenable to comparison with human performance, 4 ) it is an easier task compared to generative dialogue modeling, which is difficult for end', '##to - end systems, and 5 ) models trained with nuc can be converted to dialogue systems by retrieving from the full corpus  #AUTHOR_TAG.', 'in this case, nuc additionally allows for making hard constraints on the allowable outputs of the system ( to prevent offensive responses ), and guarantees that the responses', 'are fluent ( because they were generated by humans ). thus, nuc can be thought of both as an intermediate task that can be used to evaluate the ability of systems to understand natural language conversations, similar to the babi tasks for', 'language understanding, and as a useful framework for building chatbots. with the huge size of current dialogue datasets that contain millions of utterances  #TAUTHOR_TAG and the increasing amount of natural language data, it is conceivable that retrieval - based systems will be able', 'to have engaging conversations with humans. however, despite the current work with nuc, there has been no verification of', '']",5
"['response pairs as described in sec. 2.', 'the number of utterances in the context were sampled according to the procedure in  #TAUTHOR_TAG, with a maximum context length of 6 turns - this was done for both the human trials and ann model.', 'all conversations were preproc']","['nuc conversation - response pairs as described in sec. 2.', 'the number of utterances in the context were sampled according to the procedure in  #TAUTHOR_TAG, with a maximum context length of 6 turns - this was done for both the human trials and ann model.', 'all conversations were preprocessed in order to anonymize the utterances.', 'table 2 : average results on']","['response pairs as described in sec. 2.', 'the number of utterances in the context were sampled according to the procedure in  #TAUTHOR_TAG, with a maximum context length of 6 turns - this was done for both the human trials and ann model.', 'all conversations were preprocessed in order to anonym']","['participant was asked to answer either 30 or 40 questions ( mean = 31. 9 ).', 'to ensure a sufficient diversity of questions from each dataset, four versions of the survey with different questions were given to participants.', 'for amt respondents, the questions were approximately evenly distributed across the three datasets, while for the lab experts, half of the questions were related to ubuntu and the remainder evenly split across twitter and movies.', 'each question had 1 correct response, and 4 false responses drawn uniformly at random from elsewhere in the ( same ) corpus.', 'participants had a time limit of 40 minutes.', 'conversations were extracted to form nuc conversation - response pairs as described in sec. 2.', 'the number of utterances in the context were sampled according to the procedure in  #TAUTHOR_TAG, with a maximum context length of 6 turns - this was done for both the human trials and ann model.', 'all conversations were preprocessed in order to anonymize the utterances.', 'table 2 : average results on each corpus.', ""' number of users'indicates the number of respondents for each category."", ""' amt experts'and'amt non - experts'are combined for the movie and twitter corpora."", '95 % confidence intervals are calculated using the normal approximation, which assumes subjects answer each question independently of other examples and subjects.', 'starred ( * ) results indicate a poor approximation due to high scores with small sample size, according to the rule of thumb by  #AUTHOR_TAG.', ""for the twitter conversations, this was extended to replacing all user mentions ( words beginning with @ ) throughout the utterance with a placeholder'@ user'symbol, as these are often repeated in a conversation."", 'conversations were edited or pruned to remove offensive language according to ethical guidelines']",5
"['ann ) dialogue model ( see  #TAUTHOR_TAG for implementation details ).', 'we first observe that subjects']","['( ann ) dialogue model ( see  #TAUTHOR_TAG for implementation details ).', 'we first observe that subjects']","['ann ) dialogue model ( see  #TAUTHOR_TAG for implementation details ).', 'we first observe that subjects perform above']","['we can see from table 1, the amt participants are mostly young adults, fluent in english with some undergraduate education.', 'the split across genders is approximately equal, and the majority of respondents had never used ubuntu before.', 'table 2 shows the nuc results on each corpus.', ""the human results are separated into amt nonexperts, consisting of paid respondents who have'beginner'or no knowledge of ubuntu terminology ; amt experts, who claimed to have'intermediate'or'advanced'knowledge of ubuntu ; and lab experts."", 'we also presents results on the same task for a state - of - the - art artificial neural network ( ann ) dialogue model ( see  #TAUTHOR_TAG for implementation details ).', 'we first observe that subjects perform above chance level ( 20 % for r @ 1 ) on all domains, thus the task is doable for humans.', 'second we observe difference in performances between the three domains.', 'the twitter dataset appears to have the best predictability, with a recall @ 1 approximately 8 % points higher than for the movie dialogues for amt workers, and 18 % higher for lab experts.', ""rather than attributing this to greater familiarity with twitter than movies, it seems more likely that it is because movie utterances are often short, generic ( e. g. contain few topic - related words ), and lack proper context ( e. g., video cues and the movie's story )."", 'conversely, tweets are typically more specific, and successive tweets may have common hashtags.', 'as expected, untrained respondents scored lowest on the ubuntu dataset, as it contains the most difficult language with often unfamiliar terminology.', 'further, since the domain is narrow, randomly drawn false responses could be more likely to resemble the actual next response, especially to someone unfamiliar with ubuntu terminology.', 'we also observe that the ann model achieves similar performance to the paid human respondents from amt.', 'however, the model is still significantly behind the lab experts for recall @ 1.', 'an interesting note is that there is very little difference between the paid amt non - experts and amt experts on ubuntu.', 'this suggests that the participants do not provide accurate self - rating of expertise, either intentionally or not.', 'we also found that lab experts took on average approximately 50 % more time to complete the survey than paid testers ; this is reflected in the results, where the lab experts score 30 % higher on the ubuntu corpus, and even 5 - 10 % higher on the non - technical movie and twitter']",5
"['sentences  #AUTHOR_TAG, more recent methods  #TAUTHOR_TAG addressed the']","['sentences  #AUTHOR_TAG, more recent methods  #TAUTHOR_TAG addressed the']","['sentences  #AUTHOR_TAG, more recent methods  #TAUTHOR_TAG addressed the issue of sparsity of term - based representations by replacing term - vectors with vectors of latent topics.', 'a topical representation of text is, however, merely a vague approximation of']","['the fact that in mainstream natural language processing ( nlp ) and information retrieval ( ir ) texts are modeled as bags of unordered words, texts are sequences of semantically coherent segments, designed ( often very thoughtfully ) to ease readability and understanding of the ideas conveyed by the authors.', 'although authors may explicitly define coherent segments ( e. g., as paragraphs ), many texts, especially on the web, lack any explicit segmentation.', 'linear text segmentation aims to represent texts as sequences of semantically coherent segments.', 'besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for nlp and ir tasks such as text summarization  #AUTHOR_TAG and passage retrieval  #AUTHOR_TAG.', 'whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences  #AUTHOR_TAG, more recent methods  #TAUTHOR_TAG addressed the issue of sparsity of term - based representations by replacing term - vectors with vectors of latent topics.', 'a topical representation of text is, however, merely a vague approximation of its meaning.', 'considering that the goal of ts is to identify semantically coherent segments, we propose a ts algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity.', 'we employ word embeddings  #AUTHOR_TAG and a measure of semantic relatedness of short texts ( saric et al., 2012 ) to construct a relatedness graph of the text in which nodes denote sentences and edges are added between semantically related sentences.', 'we then derive segments using the maximal cliques of such similarity graphs.', 'the proposed algorithm displays competitive performance on the artifically - generated benchmark ts dataset  #AUTHOR_TAG and, more importantly, outperforms the best - performing topic modeling - based ts method on a real - world dataset of political manifestos']",0
"['of ts methods  #TAUTHOR_TAG, in this']","['of ts methods  #TAUTHOR_TAG, in this']","['of ts methods  #TAUTHOR_TAG, in this work we focus on linear segmentation of text, but there is also a solid body of']","['text segmentation received a lot of attention in nlp and ir communities due to its usefulness for text summarization and text indexing.', 'text segmentation can be performed in two different ways, namely ( 1 ) with the goal of obtaining linear segmentations ( i. e. detecting the sequence of different segments in a text ), or ( 2 ) in order to obtain hierarchical segmentations ( i. e. defining a structure of subtopics between the detected segments ).', 'like the majority of ts methods  #TAUTHOR_TAG, in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical ts, where each toplevel segment is further broken down  #AUTHOR_TAG.', ' #AUTHOR_TAG introduced texttiling, one of the first unsupervised algorithms for linear text segmentation.', 'she exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term - vectors.', ' #AUTHOR_TAG introduced the probabilistic algorithm using matrix - based ranking and clustering to determine similarities between segments.', ' #AUTHOR_TAG combined contentbased information with acoustic cues in order to detect discourse shifts whereas  #AUTHOR_TAG and  #AUTHOR_TAG minimized different segmentation cost functions with dynamic programming.', ""the first segmentation approach based on topic modeling  #AUTHOR_TAG employed the probabilistic latent semantic analysis ( plsa ) to derive latent representations of segments and determined the segmentation based on similarities of segments'latent vectors."", 'more recent models  #TAUTHOR_TAG employed the latent dirichlet allocation ( lda )  #AUTHOR_TAG to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets  #AUTHOR_TAG.', "" #AUTHOR_TAG used dynamic programming to find globally optimal segmentation over the set of lda - based segment representations, whereas  #TAUTHOR_TAG introduced topictiling, an lda - driven extension of hearst's texttiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain ( instead of as sparse term vectors )."", ' #TAUTHOR_TAG show that topictiling outperforms at - that - time state - of - the - art methods for unsupervised linear segmentation  #AUTHOR_TAG and that it is also faster than other lda - based methods  #AUTHOR_TAG.', 'in the most closely related work to ours,  #AUTHOR_TAG proposed a graphbased ts approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof - words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.', ' #AUTHOR_TAG builds the similarity graph, only between words instead of between sentences, using sparse co - occurrence vectors as semantic representations for words.', 'he then identifies topics by clustering the word similarity graph via the shared nearest neighbor algorithm ( ertoz et al., 2004 ).', 'unlike']",0
"['of ts methods  #TAUTHOR_TAG, in this']","['of ts methods  #TAUTHOR_TAG, in this']","['of ts methods  #TAUTHOR_TAG, in this work we focus on linear segmentation of text, but there is also a solid body of']","['text segmentation received a lot of attention in nlp and ir communities due to its usefulness for text summarization and text indexing.', 'text segmentation can be performed in two different ways, namely ( 1 ) with the goal of obtaining linear segmentations ( i. e. detecting the sequence of different segments in a text ), or ( 2 ) in order to obtain hierarchical segmentations ( i. e. defining a structure of subtopics between the detected segments ).', 'like the majority of ts methods  #TAUTHOR_TAG, in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical ts, where each toplevel segment is further broken down  #AUTHOR_TAG.', ' #AUTHOR_TAG introduced texttiling, one of the first unsupervised algorithms for linear text segmentation.', 'she exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term - vectors.', ' #AUTHOR_TAG introduced the probabilistic algorithm using matrix - based ranking and clustering to determine similarities between segments.', ' #AUTHOR_TAG combined contentbased information with acoustic cues in order to detect discourse shifts whereas  #AUTHOR_TAG and  #AUTHOR_TAG minimized different segmentation cost functions with dynamic programming.', ""the first segmentation approach based on topic modeling  #AUTHOR_TAG employed the probabilistic latent semantic analysis ( plsa ) to derive latent representations of segments and determined the segmentation based on similarities of segments'latent vectors."", 'more recent models  #TAUTHOR_TAG employed the latent dirichlet allocation ( lda )  #AUTHOR_TAG to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets  #AUTHOR_TAG.', "" #AUTHOR_TAG used dynamic programming to find globally optimal segmentation over the set of lda - based segment representations, whereas  #TAUTHOR_TAG introduced topictiling, an lda - driven extension of hearst's texttiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain ( instead of as sparse term vectors )."", ' #TAUTHOR_TAG show that topictiling outperforms at - that - time state - of - the - art methods for unsupervised linear segmentation  #AUTHOR_TAG and that it is also faster than other lda - based methods  #AUTHOR_TAG.', 'in the most closely related work to ours,  #AUTHOR_TAG proposed a graphbased ts approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof - words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.', ' #AUTHOR_TAG builds the similarity graph, only between words instead of between sentences, using sparse co - occurrence vectors as semantic representations for words.', 'he then identifies topics by clustering the word similarity graph via the shared nearest neighbor algorithm ( ertoz et al., 2004 ).', 'unlike']",0
"['of ts methods  #TAUTHOR_TAG, in this']","['of ts methods  #TAUTHOR_TAG, in this']","['of ts methods  #TAUTHOR_TAG, in this work we focus on linear segmentation of text, but there is also a solid body of']","['text segmentation received a lot of attention in nlp and ir communities due to its usefulness for text summarization and text indexing.', 'text segmentation can be performed in two different ways, namely ( 1 ) with the goal of obtaining linear segmentations ( i. e. detecting the sequence of different segments in a text ), or ( 2 ) in order to obtain hierarchical segmentations ( i. e. defining a structure of subtopics between the detected segments ).', 'like the majority of ts methods  #TAUTHOR_TAG, in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical ts, where each toplevel segment is further broken down  #AUTHOR_TAG.', ' #AUTHOR_TAG introduced texttiling, one of the first unsupervised algorithms for linear text segmentation.', 'she exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term - vectors.', ' #AUTHOR_TAG introduced the probabilistic algorithm using matrix - based ranking and clustering to determine similarities between segments.', ' #AUTHOR_TAG combined contentbased information with acoustic cues in order to detect discourse shifts whereas  #AUTHOR_TAG and  #AUTHOR_TAG minimized different segmentation cost functions with dynamic programming.', ""the first segmentation approach based on topic modeling  #AUTHOR_TAG employed the probabilistic latent semantic analysis ( plsa ) to derive latent representations of segments and determined the segmentation based on similarities of segments'latent vectors."", 'more recent models  #TAUTHOR_TAG employed the latent dirichlet allocation ( lda )  #AUTHOR_TAG to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets  #AUTHOR_TAG.', "" #AUTHOR_TAG used dynamic programming to find globally optimal segmentation over the set of lda - based segment representations, whereas  #TAUTHOR_TAG introduced topictiling, an lda - driven extension of hearst's texttiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain ( instead of as sparse term vectors )."", ' #TAUTHOR_TAG show that topictiling outperforms at - that - time state - of - the - art methods for unsupervised linear segmentation  #AUTHOR_TAG and that it is also faster than other lda - based methods  #AUTHOR_TAG.', 'in the most closely related work to ours,  #AUTHOR_TAG proposed a graphbased ts approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof - words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.', ' #AUTHOR_TAG builds the similarity graph, only between words instead of between sentences, using sparse co - occurrence vectors as semantic representations for words.', 'he then identifies topics by clustering the word similarity graph via the shared nearest neighbor algorithm ( ertoz et al., 2004 ).', 'unlike']",0
['models  #TAUTHOR_TAG and graphseg'],['models  #TAUTHOR_TAG and graphseg'],"['a google news dataset.', '4 both lda - based models  #TAUTHOR_TAG and graphseg']","['allow for comparison with previous work, we evaluate graphseg on four subsets of the choi dataset, differing in number of sentences the seg - 2008, and 2012 u. s. elections ments contain.', 'for the evaluation on the choi dataset, the graphseg algorithm made use of the publicly available word embeddings built from a google news dataset.', '4 both lda - based models  #TAUTHOR_TAG and graphseg rely on corpus - derived word representations.', 'thus, we evaluated on the manifesto dataset both the domainadapted and domain - unadapted variants of these methods.', 'the domain - adapted variants of the models used the unlabeled domain corpus - a test set of 466 unlabeled political manifestos - to train the domain - specific word representations.', 'this means that we obtain ( 1 ) in - domain topics for the ldabased topictiling model of  #TAUTHOR_TAG and ( 2 ) domain - specific embeddings for the graphseg algorithm.', 'on the manifesto dataset we also evaluate a baseline that randomly ( 50 % chance ) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments.', 'we evaluate the performance using two standard ts evaluation metrics - p k  #AUTHOR_TAG and windowdiff ( wd )  #AUTHOR_TAG.', 'p k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly - either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment.', '']",0
"['methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information -']","['methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt']","['all methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt']","['table 2 we report the performance of graph - seg and prominent ts methods on the synthetic choi dataset.', 'graphseg performs competitively, outperforming all methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt their topic models on parts of the choi dataset itself.', 'despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents - some of which belong to the the training set and others to the test set, as admitted by  #TAUTHOR_TAG and this is why their reported performance on this dataset is overestimated.', 'in table 3 we report the results on the manifesto dataset.', 'results of both topictiling and graphseg indicate that the realistic manifesto dataset is much more difficult to segment than the artificial choi dataset.', ""the graphseg algorithm significantly outperforms the topictiling method ( p < 0. 05, student's t - test )."", 'in - domain training of word representations, topics for topictiling and word embeddings for graphseg, does not significantly improve the performance for neither of the two models.', ""this result contrasts previous findings  #TAUTHOR_TAG in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the lda - based methods'with in - domain trained topics originates from information leakage between different portions of the synthetic choi dataset""]",0
"['of ts methods  #TAUTHOR_TAG, in this']","['of ts methods  #TAUTHOR_TAG, in this']","['of ts methods  #TAUTHOR_TAG, in this work we focus on linear segmentation of text, but there is also a solid body of']","['text segmentation received a lot of attention in nlp and ir communities due to its usefulness for text summarization and text indexing.', 'text segmentation can be performed in two different ways, namely ( 1 ) with the goal of obtaining linear segmentations ( i. e. detecting the sequence of different segments in a text ), or ( 2 ) in order to obtain hierarchical segmentations ( i. e. defining a structure of subtopics between the detected segments ).', 'like the majority of ts methods  #TAUTHOR_TAG, in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical ts, where each toplevel segment is further broken down  #AUTHOR_TAG.', ' #AUTHOR_TAG introduced texttiling, one of the first unsupervised algorithms for linear text segmentation.', 'she exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term - vectors.', ' #AUTHOR_TAG introduced the probabilistic algorithm using matrix - based ranking and clustering to determine similarities between segments.', ' #AUTHOR_TAG combined contentbased information with acoustic cues in order to detect discourse shifts whereas  #AUTHOR_TAG and  #AUTHOR_TAG minimized different segmentation cost functions with dynamic programming.', ""the first segmentation approach based on topic modeling  #AUTHOR_TAG employed the probabilistic latent semantic analysis ( plsa ) to derive latent representations of segments and determined the segmentation based on similarities of segments'latent vectors."", 'more recent models  #TAUTHOR_TAG employed the latent dirichlet allocation ( lda )  #AUTHOR_TAG to compute the latent topics and displayed superior performance to previous models on standard synthetic datasets  #AUTHOR_TAG.', "" #AUTHOR_TAG used dynamic programming to find globally optimal segmentation over the set of lda - based segment representations, whereas  #TAUTHOR_TAG introduced topictiling, an lda - driven extension of hearst's texttiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain ( instead of as sparse term vectors )."", ' #TAUTHOR_TAG show that topictiling outperforms at - that - time state - of - the - art methods for unsupervised linear segmentation  #AUTHOR_TAG and that it is also faster than other lda - based methods  #AUTHOR_TAG.', 'in the most closely related work to ours,  #AUTHOR_TAG proposed a graphbased ts approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof - words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments.', ' #AUTHOR_TAG builds the similarity graph, only between words instead of between sentences, using sparse co - occurrence vectors as semantic representations for words.', 'he then identifies topics by clustering the word similarity graph via the shared nearest neighbor algorithm ( ertoz et al., 2004 ).', 'unlike']",3
['models  #TAUTHOR_TAG and graphseg'],['models  #TAUTHOR_TAG and graphseg'],"['a google news dataset.', '4 both lda - based models  #TAUTHOR_TAG and graphseg']","['allow for comparison with previous work, we evaluate graphseg on four subsets of the choi dataset, differing in number of sentences the seg - 2008, and 2012 u. s. elections ments contain.', 'for the evaluation on the choi dataset, the graphseg algorithm made use of the publicly available word embeddings built from a google news dataset.', '4 both lda - based models  #TAUTHOR_TAG and graphseg rely on corpus - derived word representations.', 'thus, we evaluated on the manifesto dataset both the domainadapted and domain - unadapted variants of these methods.', 'the domain - adapted variants of the models used the unlabeled domain corpus - a test set of 466 unlabeled political manifestos - to train the domain - specific word representations.', 'this means that we obtain ( 1 ) in - domain topics for the ldabased topictiling model of  #TAUTHOR_TAG and ( 2 ) domain - specific embeddings for the graphseg algorithm.', 'on the manifesto dataset we also evaluate a baseline that randomly ( 50 % chance ) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments.', 'we evaluate the performance using two standard ts evaluation metrics - p k  #AUTHOR_TAG and windowdiff ( wd )  #AUTHOR_TAG.', 'p k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly - either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment.', '']",3
['models  #TAUTHOR_TAG and graphseg'],['models  #TAUTHOR_TAG and graphseg'],"['a google news dataset.', '4 both lda - based models  #TAUTHOR_TAG and graphseg']","['allow for comparison with previous work, we evaluate graphseg on four subsets of the choi dataset, differing in number of sentences the seg - 2008, and 2012 u. s. elections ments contain.', 'for the evaluation on the choi dataset, the graphseg algorithm made use of the publicly available word embeddings built from a google news dataset.', '4 both lda - based models  #TAUTHOR_TAG and graphseg rely on corpus - derived word representations.', 'thus, we evaluated on the manifesto dataset both the domainadapted and domain - unadapted variants of these methods.', 'the domain - adapted variants of the models used the unlabeled domain corpus - a test set of 466 unlabeled political manifestos - to train the domain - specific word representations.', 'this means that we obtain ( 1 ) in - domain topics for the ldabased topictiling model of  #TAUTHOR_TAG and ( 2 ) domain - specific embeddings for the graphseg algorithm.', 'on the manifesto dataset we also evaluate a baseline that randomly ( 50 % chance ) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments.', 'we evaluate the performance using two standard ts evaluation metrics - p k  #AUTHOR_TAG and windowdiff ( wd )  #AUTHOR_TAG.', 'p k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly - either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment.', '']",5
['models  #TAUTHOR_TAG and graphseg'],['models  #TAUTHOR_TAG and graphseg'],"['a google news dataset.', '4 both lda - based models  #TAUTHOR_TAG and graphseg']","['allow for comparison with previous work, we evaluate graphseg on four subsets of the choi dataset, differing in number of sentences the seg - 2008, and 2012 u. s. elections ments contain.', 'for the evaluation on the choi dataset, the graphseg algorithm made use of the publicly available word embeddings built from a google news dataset.', '4 both lda - based models  #TAUTHOR_TAG and graphseg rely on corpus - derived word representations.', 'thus, we evaluated on the manifesto dataset both the domainadapted and domain - unadapted variants of these methods.', 'the domain - adapted variants of the models used the unlabeled domain corpus - a test set of 466 unlabeled political manifestos - to train the domain - specific word representations.', 'this means that we obtain ( 1 ) in - domain topics for the ldabased topictiling model of  #TAUTHOR_TAG and ( 2 ) domain - specific embeddings for the graphseg algorithm.', 'on the manifesto dataset we also evaluate a baseline that randomly ( 50 % chance ) starts a new segment at points m sentences apart, with m being set to half of the average length of gold segments.', 'we evaluate the performance using two standard ts evaluation metrics - p k  #AUTHOR_TAG and windowdiff ( wd )  #AUTHOR_TAG.', 'p k is the probability that two randomly drawn sentences mutually k sentences apart are classified incorrectly - either as belonging to the same segment when they are in different gold segments or as being in different segments when they are in the same gold segment.', '']",5
"['methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information -']","['methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt']","['all methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt']","['table 2 we report the performance of graph - seg and prominent ts methods on the synthetic choi dataset.', 'graphseg performs competitively, outperforming all methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt their topic models on parts of the choi dataset itself.', 'despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents - some of which belong to the the training set and others to the test set, as admitted by  #TAUTHOR_TAG and this is why their reported performance on this dataset is overestimated.', 'in table 3 we report the results on the manifesto dataset.', 'results of both topictiling and graphseg indicate that the realistic manifesto dataset is much more difficult to segment than the artificial choi dataset.', ""the graphseg algorithm significantly outperforms the topictiling method ( p < 0. 05, student's t - test )."", 'in - domain training of word representations, topics for topictiling and word embeddings for graphseg, does not significantly improve the performance for neither of the two models.', ""this result contrasts previous findings  #TAUTHOR_TAG in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the lda - based methods'with in - domain trained topics originates from information leakage between different portions of the synthetic choi dataset""]",4
"['methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information -']","['methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt']","['all methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt']","['table 2 we report the performance of graph - seg and prominent ts methods on the synthetic choi dataset.', 'graphseg performs competitively, outperforming all methods but  #AUTHOR_TAG and domain - adapted versions of lda - based models  #TAUTHOR_TAG.', 'however, the approach by  #AUTHOR_TAG uses the gold standard information - the average gold segment size - as input.', 'on the other hand, the lda - based models adapt their topic models on parts of the choi dataset itself.', 'despite the fact that they use different documents for training the topic models from those used for evaluating segmentation quality, the evaluation is still tainted because snippets from the original documents appear in multiple artificial documents - some of which belong to the the training set and others to the test set, as admitted by  #TAUTHOR_TAG and this is why their reported performance on this dataset is overestimated.', 'in table 3 we report the results on the manifesto dataset.', 'results of both topictiling and graphseg indicate that the realistic manifesto dataset is much more difficult to segment than the artificial choi dataset.', ""the graphseg algorithm significantly outperforms the topictiling method ( p < 0. 05, student's t - test )."", 'in - domain training of word representations, topics for topictiling and word embeddings for graphseg, does not significantly improve the performance for neither of the two models.', ""this result contrasts previous findings  #TAUTHOR_TAG in which the performance boost was credited to the indomain trained topics and supports our hypothesis that the performance boost of the lda - based methods'with in - domain trained topics originates from information leakage between different portions of the synthetic choi dataset""]",4
"['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['', 'have to be selected and defined manually, usually by linguistic intuition.', 'another problem is that they may fail to effectively capture complex structured parse tree information. as for tree kernel - based methods,  #TAUTHOR_TAG captured syntactic', 'structured information for pronoun resolution by using the convolution tree kernel ( collins and duffy 2001 ) to measure the common sub - trees enumerated from the parse trees and achieved quite success on the ace', '2003 corpus. they also explored different tree span schemes and found that the', 'simple - expansion scheme performed best. one problem with their method', ""is that the sub - trees enumerated in collins and duffy's kernel computation are context - free, that is, they do not consider the information"", '']",0
"['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['', 'have to be selected and defined manually, usually by linguistic intuition.', 'another problem is that they may fail to effectively capture complex structured parse tree information. as for tree kernel - based methods,  #TAUTHOR_TAG captured syntactic', 'structured information for pronoun resolution by using the convolution tree kernel ( collins and duffy 2001 ) to measure the common sub - trees enumerated from the parse trees and achieved quite success on the ace', '2003 corpus. they also explored different tree span schemes and found that the', 'simple - expansion scheme performed best. one problem with their method', ""is that the sub - trees enumerated in collins and duffy's kernel computation are context - free, that is, they do not consider the information"", '']",0
"['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['methods,  #TAUTHOR_TAG captured syntactic', 'structured']","['', 'have to be selected and defined manually, usually by linguistic intuition.', 'another problem is that they may fail to effectively capture complex structured parse tree information. as for tree kernel - based methods,  #TAUTHOR_TAG captured syntactic', 'structured information for pronoun resolution by using the convolution tree kernel ( collins and duffy 2001 ) to measure the common sub - trees enumerated from the parse trees and achieved quite success on the ace', '2003 corpus. they also explored different tree span schemes and found that the', 'simple - expansion scheme performed best. one problem with their method', ""is that the sub - trees enumerated in collins and duffy's kernel computation are context - free, that is, they do not consider the information"", '']",0
"[', similar to  #TAUTHOR_TAG']","['node, similar to  #TAUTHOR_TAG']","[', similar to  #TAUTHOR_TAG.', 'given']",[' #TAUTHOR_TAG'],0
"[', similar to  #TAUTHOR_TAG']","['node, similar to  #TAUTHOR_TAG']","[', similar to  #TAUTHOR_TAG.', 'given']",[' #TAUTHOR_TAG'],0
"['by  #TAUTHOR_TAG in pronoun resolution.', 'however, there is one']","['by  #TAUTHOR_TAG in pronoun resolution.', 'however, there is one']","['by  #TAUTHOR_TAG in pronoun resolution.', 'however, there is one problem with this tree']","['any tree span scheme, e. g. the dynamicexpansion scheme in the last subsection, we now study how to measure the similarity between two tree spans using a convolution tree kernel.', 'a convolution kernel ( haussler d., 1999 ) aims to capture structured information in terms of substructures.', 'as a specialized convolution kernel, the convolution tree kernel, proposed in  #AUTHOR_TAG, counts the number of common subtrees ( sub - structures ) as the syntactic structure similarity between two parse trees.', 'this convolution tree kernel has been successfully applied by  #TAUTHOR_TAG in pronoun resolution.', 'however, there is one problem with this tree kernel : the subtrees involved in the tree kernel computation are context - free ( that is, they do not consider the information outside the sub - trees. ).', 'this is contrast to the tree kernel proposed in  #AUTHOR_TAG which is context - sensitive, that is, it considers the path from the tree root node to the sub - tree root node.', ""in order to integrate the advantages of both tree kernels and resolve the problem in collins and duffy's kernel, this paper applies the same context - sensitive convolution tree kernel, proposed by  #AUTHOR_TAG on relation extraction."", 'it works by taking ancestral information ( i. e. the root node path ) of sub - trees into consideration : n.', 'in the tree kernel, a sub - tree becomes context - sensitive via the "" root node path "" moving along the sub - tree root.', 'for more details, please refer to  #AUTHOR_TAG']",0
"[', similar to  #TAUTHOR_TAG']","['node, similar to  #TAUTHOR_TAG']","[', similar to  #TAUTHOR_TAG.', 'given']",[' #TAUTHOR_TAG'],3
['expansions as described in  #TAUTHOR_TAG'],"['existing three tree span schemes, min -, simple - and full - expansions as described in  #TAUTHOR_TAG']","['- sensitive convolution tree kernel and compares our dynamic - expansion tree span scheme with the existing three tree span schemes, min -, simple - and full - expansions as described in  #TAUTHOR_TAG.', 'it also shows that that our tree kernel achieves best']","['', 'the performance is evaluated using f - measure instead of accuracy since evaluation is done on all the pronouns occurring in the data.', 'in this paper, the m parameter in our contextsensitive convolution tree kernel as shown in equation ( 1 ) indicates the maximal length of root node paths and is optimized to 3 using 5 - fold cross validation on the training data.', 'table 1 systematically evaluates the impact of different m in our context - sensitive convolution tree kernel and compares our dynamic - expansion tree span scheme with the existing three tree span schemes, min -, simple - and full - expansions as described in  #TAUTHOR_TAG.', 'it also shows that that our tree kernel achieves best performance with m = 3 on the test data, which outperforms the one with m = 1 by ~ 2. 2 in f - measure.', '']",3
['expansions as described in  #TAUTHOR_TAG'],"['existing three tree span schemes, min -, simple - and full - expansions as described in  #TAUTHOR_TAG']","['- sensitive convolution tree kernel and compares our dynamic - expansion tree span scheme with the existing three tree span schemes, min -, simple - and full - expansions as described in  #TAUTHOR_TAG.', 'it also shows that that our tree kernel achieves best']","['', 'the performance is evaluated using f - measure instead of accuracy since evaluation is done on all the pronouns occurring in the data.', 'in this paper, the m parameter in our contextsensitive convolution tree kernel as shown in equation ( 1 ) indicates the maximal length of root node paths and is optimized to 3 using 5 - fold cross validation on the training data.', 'table 1 systematically evaluates the impact of different m in our context - sensitive convolution tree kernel and compares our dynamic - expansion tree span scheme with the existing three tree span schemes, min -, simple - and full - expansions as described in  #TAUTHOR_TAG.', 'it also shows that that our tree kernel achieves best performance with m = 3 on the test data, which outperforms the one with m = 1 by ~ 2. 2 in f - measure.', '']",5
"['fragments unreliable  #TAUTHOR_TAG.', '']","['fragments unreliable  #TAUTHOR_TAG.', '']","['unreliable  #TAUTHOR_TAG.', '']","['', 'some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus.', 'they locate the source and target fragments independently, making the extracted fragments unreliable  #TAUTHOR_TAG.', '']",0
"['fragments unreliable  #TAUTHOR_TAG.', '']","['fragments unreliable  #TAUTHOR_TAG.', '']","['unreliable  #TAUTHOR_TAG.', '']","['', 'some studies extract parallel fragments relying on a probabilistic translation lexicon estimated on an external parallel corpus.', 'they locate the source and target fragments independently, making the extracted fragments unreliable  #TAUTHOR_TAG.', '']",0
['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],"['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', 'the']","['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', '']",0
['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],"['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', 'the']","['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', '']",0
"['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', '']","['first applied sentence extraction on the quasicomparable corpora using our system, and 30k comparable sentences of chemistry domain were extracted.', 'we then applied fragment extraction on the extracted comparable sentences.', 'we compared our proposed method with  #TAUTHOR_TAG.', 'we applied word alignment using giza + +.', '']",0
"['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', '']","['first applied sentence extraction on the quasicomparable corpora using our system, and 30k comparable sentences of chemistry domain were extracted.', 'we then applied fragment extraction on the extracted comparable sentences.', 'we compared our proposed method with  #TAUTHOR_TAG.', 'we applied word alignment using giza + +.', '']",0
['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],"['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', 'the']","['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', '']",3
['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],"['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', 'the']","['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', '']",3
"['proposed fragment extraction method with  #TAUTHOR_TAG.', 'we manually evaluated the']","['proposed fragment extraction method with  #TAUTHOR_TAG.', 'we manually evaluated the']","['proposed fragment extraction method with  #TAUTHOR_TAG.', 'we manually evaluated the accuracy of the extracted fragments.', 'moreover, we used the extracted fragments as additional mt training data, and evaluated the effectiveness of the fragments for mt.', 'we conducted experiments on chinese - japanese data.', 'in']","['our experiments, we compared our proposed fragment extraction method with  #TAUTHOR_TAG.', 'we manually evaluated the accuracy of the extracted fragments.', 'moreover, we used the extracted fragments as additional mt training data, and evaluated the effectiveness of the fragments for mt.', 'we conducted experiments on chinese - japanese data.', 'in all our experiments, we preprocessed the data by segmenting chinese and japanese sentences using a segmenter proposed by  #AUTHOR_TAG and juman  #AUTHOR_TAG respectively']",3
"['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', '']","['proposed method with  #TAUTHOR_TAG.', '']","['first applied sentence extraction on the quasicomparable corpora using our system, and 30k comparable sentences of chemistry domain were extracted.', 'we then applied fragment extraction on the extracted comparable sentences.', 'we compared our proposed method with  #TAUTHOR_TAG.', 'we applied word alignment using giza + +.', '']",3
['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],"['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', 'the']","['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', '']",5
['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],"['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', 'the']","['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', '']",5
['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],"['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', 'the']","['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', '']",4
['by  #TAUTHOR_TAG has'],['by  #TAUTHOR_TAG has'],['by  #TAUTHOR_TAG has'],"['', 'translation results evaluated on bleu - 4, are shown in table 2.', 'we can see that appending the extracted comparable sentences have a positive effect on translation quality.', 'adding the fragments extracted by  #TAUTHOR_TAG has a negative impact, compared to appending the sentences.', '']",4
['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing'],"['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', 'the']","['that it can produce both positive and negative associations.  #TAUTHOR_TAG develop a smoothing filter applying this advantage. we extract', '']",6
"['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","[', multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors']","['', '. 4 ). while pns were originally proposed to allow a variable length decoding sequence,', 'our model differs in that it decodes for the same number of timesteps as there are inputs.', 'this is a key insight that allows for a sequence - to - sequence model to be used for structural prediction.', 'aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the ac types or connectivity', ', unlike the work of  #AUTHOR_TAG. lastly, in respect to the broad task of parsing, our model is', 'flexible because it can easily handle nonprojective, multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors. our results show that', '( 1 ) joint modeling is imperative for competitive performance on the link extraction task, ( 2 ) the presence of the', '']",0
"['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","[', multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors']","['', '. 4 ). while pns were originally proposed to allow a variable length decoding sequence,', 'our model differs in that it decodes for the same number of timesteps as there are inputs.', 'this is a key insight that allows for a sequence - to - sequence model to be used for structural prediction.', 'aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the ac types or connectivity', ', unlike the work of  #AUTHOR_TAG. lastly, in respect to the broad task of parsing, our model is', 'flexible because it can easily handle nonprojective, multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors. our results show that', '( 1 ) joint modeling is imperative for competitive performance on the link extraction task, ( 2 ) the presence of the', '']",0
"['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","[', multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors']","['', '. 4 ). while pns were originally proposed to allow a variable length decoding sequence,', 'our model differs in that it decodes for the same number of timesteps as there are inputs.', 'this is a key insight that allows for a sequence - to - sequence model to be used for structural prediction.', 'aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the ac types or connectivity', ', unlike the work of  #AUTHOR_TAG. lastly, in respect to the broad task of parsing, our model is', 'flexible because it can easily handle nonprojective, multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors. our results show that', '( 1 ) joint modeling is imperative for competitive performance on the link extraction task, ( 2 ) the presence of the', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['an integer linear programming ( ilp ) framework  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG is'],[' #TAUTHOR_TAG is'],"['following previously proposed models : base classifier  #TAUTHOR_TAG is a feature - rich, taskspecific ( ac type or link extraction ) svm classifier.', '']","['results of our experiments are presented in tables 1 and 2.', 'for each corpus, we present f1 scores for the ac type classification experiment, with a macro - averaged score of the individual class f1 scores.', 'we also present the f1 scores for predicting the presence / absence of links between acs, as well as the associated macro - average between these two values.', 'we implement and compare four types of neural models : 1 ) the previously described joint model from in both corpora we compare against the following previously proposed models : base classifier  #TAUTHOR_TAG is a feature - rich, taskspecific ( ac type or link extraction ) svm classifier.', 'neither of these classifiers enforce structural or global constraints.', 'conversely, the ilp joint model  #TAUTHOR_TAG provides constraints by sharing prediction information between the base classifiers.', 'for example, the model attempts to enforce a tree structure among acs within a given paragraph, as well as using incoming link predictions to better predict the type class claim.', 'for the mtc only, we also have the following comparative models : simple  #AUTHOR_TAG is a feature - rich logistic regression classifier.', 'best eg  #AUTHOR_TAG creates an evidence graph ( eg ) from the predictions of a set of base classifiers.', 'the eg models the potential argument structure, and offers a global optimization objective that the base classifiers attempt to optimize by adjusting their individual weights.', 'lastly, mp + p  #AUTHOR_TAG combines predictions from base classifiers with a minimum spanning tree parser ( mstparser )']",0
"['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","[', multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors']","['', '. 4 ). while pns were originally proposed to allow a variable length decoding sequence,', 'our model differs in that it decodes for the same number of timesteps as there are inputs.', 'this is a key insight that allows for a sequence - to - sequence model to be used for structural prediction.', 'aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the ac types or connectivity', ', unlike the work of  #AUTHOR_TAG. lastly, in respect to the broad task of parsing, our model is', 'flexible because it can easily handle nonprojective, multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors. our results show that', '( 1 ) joint modeling is imperative for competitive performance on the link extraction task, ( 2 ) the presence of the', '']",5
"['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","[', multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors']","['', '. 4 ). while pns were originally proposed to allow a variable length decoding sequence,', 'our model differs in that it decodes for the same number of timesteps as there are inputs.', 'this is a key insight that allows for a sequence - to - sequence model to be used for structural prediction.', 'aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the ac types or connectivity', ', unlike the work of  #AUTHOR_TAG. lastly, in respect to the broad task of parsing, our model is', 'flexible because it can easily handle nonprojective, multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors. our results show that', '( 1 ) joint modeling is imperative for competitive performance on the link extraction task, ( 2 ) the presence of the', '']",5
"['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","[', multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors']","['', '. 4 ). while pns were originally proposed to allow a variable length decoding sequence,', 'our model differs in that it decodes for the same number of timesteps as there are inputs.', 'this is a key insight that allows for a sequence - to - sequence model to be used for structural prediction.', 'aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the ac types or connectivity', ', unlike the work of  #AUTHOR_TAG. lastly, in respect to the broad task of parsing, our model is', 'flexible because it can easily handle nonprojective, multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors. our results show that', '( 1 ) joint modeling is imperative for competitive performance on the link extraction task, ( 2 ) the presence of the', '']",5
"['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","[', multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors']","['', '. 4 ). while pns were originally proposed to allow a variable length decoding sequence,', 'our model differs in that it decodes for the same number of timesteps as there are inputs.', 'this is a key insight that allows for a sequence - to - sequence model to be used for structural prediction.', 'aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the ac types or connectivity', ', unlike the work of  #AUTHOR_TAG. lastly, in respect to the broad task of parsing, our model is', 'flexible because it can easily handle nonprojective, multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors. our results show that', '( 1 ) joint modeling is imperative for competitive performance on the link extraction task, ( 2 ) the presence of the', '']",5
['of  #TAUTHOR_TAG and focus on three different types of features to represent our acs : ( 1 ) bag - of - words of the ac'],['of  #TAUTHOR_TAG and focus on three different types of features to represent our acs : ( 1 ) bag - of - words of the ac ; ( 2 ) embedding representation'],"[' #AUTHOR_TAG.', 'we follow the work of  #TAUTHOR_TAG and focus on three different types of features to represent our acs : ( 1 ) bag - of - words of the ac ;']","['each timestep of the encoder, the network takes in a representation of an ac.', 'each ac is itself figure 3 : architecture of the joint model applied to the example in figure 1. note that d1 points to itself to denote that it has not outgoing link and is therefore the head of a tree.', 'a sequence of tokens, similar to the questionanswering dataset from  #AUTHOR_TAG.', 'we follow the work of  #TAUTHOR_TAG and focus on three different types of features to represent our acs : ( 1 ) bag - of - words of the ac ; ( 2 ) embedding representation based on glove embeddings  #AUTHOR_TAG, which uses average, max, and min pooling across the token embeddings ; ( 3 ) structural features : whether or not the ac is the first ac in a paragraph, and whether the ac is in an opening, body, or closing paragraph.', 'see section 6 for an ablation study of the proposed features']",5
"['pec )  #TAUTHOR_TAG, as well']","['of persuasive essays ( pec )  #TAUTHOR_TAG, as well']","['pec )  #TAUTHOR_TAG, as well']","['we have mentioned, our work assumes that acs have already been identified.', 'the order of acs corresponds directly to the order in which the acs appear in the text.', 'we test the effectiveness of our proposed model on a dataset of persuasive essays ( pec )  #TAUTHOR_TAG, as well as a dataset of microtexts ( mtc )  #AUTHOR_TAG.', 'the feature space for the pec has roughly 3, 000 dimensions, and the mtc feature space has between 2, 500 and 3, 000 dimensions, depending on the data split.', 'the pec contains a total of 402 essays, with a frozen set of 80 essays held out for testing.', 'there are three ac types in this corpus : major claim, claim, and premise.', 'in this corpus, individual structures can be either trees or forests.', 'also, in this corpus, each essay has multiple paragraphs, and argument structure is only uncovered within a given paragraph.', 'the mtc contains 112 short texts.', '']",5
[' #TAUTHOR_TAG is'],[' #TAUTHOR_TAG is'],"['following previously proposed models : base classifier  #TAUTHOR_TAG is a feature - rich, taskspecific ( ac type or link extraction ) svm classifier.', '']","['results of our experiments are presented in tables 1 and 2.', 'for each corpus, we present f1 scores for the ac type classification experiment, with a macro - averaged score of the individual class f1 scores.', 'we also present the f1 scores for predicting the presence / absence of links between acs, as well as the associated macro - average between these two values.', 'we implement and compare four types of neural models : 1 ) the previously described joint model from in both corpora we compare against the following previously proposed models : base classifier  #TAUTHOR_TAG is a feature - rich, taskspecific ( ac type or link extraction ) svm classifier.', 'neither of these classifiers enforce structural or global constraints.', 'conversely, the ilp joint model  #TAUTHOR_TAG provides constraints by sharing prediction information between the base classifiers.', 'for example, the model attempts to enforce a tree structure among acs within a given paragraph, as well as using incoming link predictions to better predict the type class claim.', 'for the mtc only, we also have the following comparative models : simple  #AUTHOR_TAG is a feature - rich logistic regression classifier.', 'best eg  #AUTHOR_TAG creates an evidence graph ( eg ) from the predictions of a set of base classifiers.', 'the eg models the potential argument structure, and offers a global optimization objective that the base classifiers attempt to optimize by adjusting their individual weights.', 'lastly, mp + p  #AUTHOR_TAG combines predictions from base classifiers with a minimum spanning tree parser ( mstparser )']",5
"['essays  #TAUTHOR_TAG, and']","['persuasive essays  #TAUTHOR_TAG, and']","['persuasive essays  #TAUTHOR_TAG, and']","['this paper we have proposed how to use a joint sequence - to - sequence model with attention  #AUTHOR_TAG b ) to both extract links between acs and classify ac type.', 'we evaluate our models on two corpora : a corpus of persuasive essays  #TAUTHOR_TAG, and a corpus of microtexts  #AUTHOR_TAG.', 'the joint model records state - of - the - art results on the persuasive essay corpus, as well as achieving state - of - the - art results for link prediction on the microtext corpus.', 'the results show that jointly modeling the two prediction tasks is critical for high performance.', 'future work can attempt to learn the ac representations themselves, such as in  #AUTHOR_TAG.', 'lastly, future work can integrate subtasks 1 and 4 into the model.', 'the representations produced by equation 3 could potentially be used to predict link type, i. e. supporting or attacking ( the fourth subtask in the pipeline ).', 'in addition, a segmenting technique, such as the one proposed by  #AUTHOR_TAG, can accomplish subtask 1']",5
"['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","['we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the']","[', multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors']","['', '. 4 ). while pns were originally proposed to allow a variable length decoding sequence,', 'our model differs in that it decodes for the same number of timesteps as there are inputs.', 'this is a key insight that allows for a sequence - to - sequence model to be used for structural prediction.', 'aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the ac types or connectivity', ', unlike the work of  #AUTHOR_TAG. lastly, in respect to the broad task of parsing, our model is', 'flexible because it can easily handle nonprojective, multi - root dependencies. we evaluate our models on the corpora of  #TAUTHOR_TAG and  #AUTHOR_TAG, and compare our', 'results with the results of the aformentioned authors. our results show that', '( 1 ) joint modeling is imperative for competitive performance on the link extraction task, ( 2 ) the presence of the', '']",4
[' #TAUTHOR_TAG is'],[' #TAUTHOR_TAG is'],"['following previously proposed models : base classifier  #TAUTHOR_TAG is a feature - rich, taskspecific ( ac type or link extraction ) svm classifier.', '']","['results of our experiments are presented in tables 1 and 2.', 'for each corpus, we present f1 scores for the ac type classification experiment, with a macro - averaged score of the individual class f1 scores.', 'we also present the f1 scores for predicting the presence / absence of links between acs, as well as the associated macro - average between these two values.', 'we implement and compare four types of neural models : 1 ) the previously described joint model from in both corpora we compare against the following previously proposed models : base classifier  #TAUTHOR_TAG is a feature - rich, taskspecific ( ac type or link extraction ) svm classifier.', 'neither of these classifiers enforce structural or global constraints.', 'conversely, the ilp joint model  #TAUTHOR_TAG provides constraints by sharing prediction information between the base classifiers.', 'for example, the model attempts to enforce a tree structure among acs within a given paragraph, as well as using incoming link predictions to better predict the type class claim.', 'for the mtc only, we also have the following comparative models : simple  #AUTHOR_TAG is a feature - rich logistic regression classifier.', 'best eg  #AUTHOR_TAG creates an evidence graph ( eg ) from the predictions of a set of base classifiers.', 'the eg models the potential argument structure, and offers a global optimization objective that the base classifiers attempt to optimize by adjusting their individual weights.', 'lastly, mp + p  #AUTHOR_TAG combines predictions from base classifiers with a minimum spanning tree parser ( mstparser )']",1
"['averaging embeddings ( which is', 'used by  #TAUTHOR_TAG in their system ) is in']","['popular method of averaging embeddings ( which is', 'used by  #TAUTHOR_TAG in their system ) is in']","['multi - word embedding. the popular method of averaging embeddings ( which is', 'used by  #TAUTHOR_TAG in their system ) is in fact']","['important, as their absence results in the highest drop in performance. conversely, the presence of structural features provides the smallest boost in performance, as the model is still able to record state - table 4 : results of binning test data by length of', 'ac sequence. * indicates that this bin does not contain any major claim labels,', 'and this average only applies to claim and premise classes. however, we do not disable the model from', 'predicting this class : the model was able to avoid predicting this class on its own. of - the - art results compared to the ilp joint model. this shows that the joint model is able to capture structural cues through sequence', 'modeling and semantics. when considering type prediction, both bow and structural features are important, and it is the embedding features that provide the least', 'benefit. the ablation results also provide an interesting insight into the effectiveness of different pooling strategies for using individual token embeddings to create a multi - word embedding. the popular method of averaging embeddings ( which is', 'used by  #TAUTHOR_TAG in their system ) is in fact the worst method, although its performance is still competitive with the previous state - of - the - art. conversely, max pooling results are on', 'par with the joint model results in table 1. table 4 shows results on the pec test set with the test examples binned by sequence length. first, it is not surprising to', 'see that the model performs best when the sequences are the shortest ( for link prediction ; type prediction actually sees the worst performance in the middle', 'bin ). as the sequence length increases, the accuracy on link prediction drops. this is possibly due to the fact that as the length', 'increases, a given ac has more possibilities as to which other ac it can link to, making the task more difficult. conversely, there is actually a rise in no link prediction accuracy from the second to third row. this is likely due to the fact that since', 'the model predicts at most one outgoing link, it indirectly predicts no link for the remaining acs', 'in the sequence. since the chance probability is low for having a link', 'between a given ac in a long sequence, the no link performance is actually better in longer sequences. the results of', 'the length - based binning could also potentially give insight into the poor performance on the type prediction task in the mtc. since the arguments in the mtc average 5 acs, they would be in the second bin ( row 2 )', ""of table 4. the claim and premise f1 scores for this bin are similar to those from the same system's performance on the mt"", '##c']",1
"['', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus.']","['sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus. we used the']","['or occurrence of each word in the corpus. we used half a million sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus.']","['or occurrence of each word in the corpus. we used half a million sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus. we used the last layer, which was more distant from the surface input, as the embeddings. the size of the database is roughly 200mb per', 'thousand sentences. our system visualizes these searched contextualized word embedding vectors. we visualize the contextualized word embedding vectors for the provided word by projecting these vectors into a', 'twodimensional space. to visualize, we used principal component analysis ( pca ) because its fast', 'calculation is beneficial for short system response time and better interactivity. the number of points in the visualization is also', 'set to a maximum of 100 so that users can easily understand it. fig. 2 shows a use case of searching book. 2. users can directly type the word in the textbox shown at the top', '']",0
"['', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus.']","['sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus. we used the']","['or occurrence of each word in the corpus. we used half a million sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus.']","['or occurrence of each word in the corpus. we used half a million sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus. we used the last layer, which was more distant from the surface input, as the embeddings. the size of the database is roughly 200mb per', 'thousand sentences. our system visualizes these searched contextualized word embedding vectors. we visualize the contextualized word embedding vectors for the provided word by projecting these vectors into a', 'twodimensional space. to visualize, we used principal component analysis ( pca ) because its fast', 'calculation is beneficial for short system response time and better interactivity. the number of points in the visualization is also', 'set to a maximum of 100 so that users can easily understand it. fig. 2 shows a use case of searching book. 2. users can directly type the word in the textbox shown at the top', '']",5
"['', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus.']","['sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus. we used the']","['or occurrence of each word in the corpus. we used half a million sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus.']","['or occurrence of each word in the corpus. we used half a million sentences from the british national corpus ( bnc', ' #AUTHOR_TAG as the raw corpus. we built the database by applying the bert - base - uncased model of the pytorch pretrained the bert project', '1  #TAUTHOR_TAG to the corpus. we used the last layer, which was more distant from the surface input, as the embeddings. the size of the database is roughly 200mb per', 'thousand sentences. our system visualizes these searched contextualized word embedding vectors. we visualize the contextualized word embedding vectors for the provided word by projecting these vectors into a', 'twodimensional space. to visualize, we used principal component analysis ( pca ) because its fast', 'calculation is beneficial for short system response time and better interactivity. the number of points in the visualization is also', 'set to a maximum of 100 so that users can easily understand it. fig. 2 shows a use case of searching book. 2. users can directly type the word in the textbox shown at the top', '']",5
['are aware of  #TAUTHOR_TAG followed a'],['previously proposed architectures we are aware of  #TAUTHOR_TAG followed a'],['are aware of  #TAUTHOR_TAG followed a pattern similar'],"['the generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram', 'all the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '##odules to encode the definiendum and to', 'generate the definientia. in the case of  #TAUTHOR_TAG, the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters', 'derived from a characterlevel cnn, and its "" hypernym embedding ""', '.  #AUTHOR_TAG used a sigmoid - based gating module to tweak the defin', '##iendum embedding. the architecture proposed by is comprised of four modules, only one of which is used as a decoder :', '']",0
['are aware of  #TAUTHOR_TAG followed a'],['previously proposed architectures we are aware of  #TAUTHOR_TAG followed a'],['are aware of  #TAUTHOR_TAG followed a pattern similar'],"['the generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram', 'all the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '##odules to encode the definiendum and to', 'generate the definientia. in the case of  #TAUTHOR_TAG, the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters', 'derived from a characterlevel cnn, and its "" hypernym embedding ""', '.  #AUTHOR_TAG used a sigmoid - based gating module to tweak the defin', '##iendum embedding. the architecture proposed by is comprised of four modules, only one of which is used as a decoder :', '']",0
['are aware of  #TAUTHOR_TAG followed a'],['previously proposed architectures we are aware of  #TAUTHOR_TAG followed a'],['are aware of  #TAUTHOR_TAG followed a pattern similar'],"['the generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram', 'all the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '##odules to encode the definiendum and to', 'generate the definientia. in the case of  #TAUTHOR_TAG, the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters', 'derived from a characterlevel cnn, and its "" hypernym embedding ""', '.  #AUTHOR_TAG used a sigmoid - based gating module to tweak the defin', '##iendum embedding. the architecture proposed by is comprised of four modules, only one of which is used as a decoder :', '']",0
['are aware of  #TAUTHOR_TAG followed a'],['previously proposed architectures we are aware of  #TAUTHOR_TAG followed a'],['are aware of  #TAUTHOR_TAG followed a pattern similar'],"['the generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram', 'all the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '##odules to encode the definiendum and to', 'generate the definientia. in the case of  #TAUTHOR_TAG, the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters', 'derived from a characterlevel cnn, and its "" hypernym embedding ""', '.  #AUTHOR_TAG used a sigmoid - based gating module to tweak the defin', '##iendum embedding. the architecture proposed by is comprised of four modules, only one of which is used as a decoder :', '']",0
['are aware of  #TAUTHOR_TAG followed a'],['previously proposed architectures we are aware of  #TAUTHOR_TAG followed a'],['are aware of  #TAUTHOR_TAG followed a pattern similar'],"['the generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram', 'all the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '##odules to encode the definiendum and to', 'generate the definientia. in the case of  #TAUTHOR_TAG, the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters', 'derived from a characterlevel cnn, and its "" hypernym embedding ""', '.  #AUTHOR_TAG used a sigmoid - based gating module to tweak the defin', '##iendum embedding. the architecture proposed by is comprised of four modules, only one of which is used as a decoder :', '']",0
['note that the work of  #TAUTHOR_TAG had a much'],"['pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much']","['pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much']","['the full context may confuse it. on the other hand, the lower number of self -', 'referring definitions may also be linked to this richer, more varied input : this would allow the model not to fall 7 self -', 'referring definitions are those where a definiendum is used as', 'a definiens for itself. dictionaries are expected to be exempt of such definitions : as readers are assumed not to know the meaning of the definiendum when looking it up. back on simply', 'reusing the definiendum as its own definiens. self - referring definitions highlight that our', 'models equate the meaning of the definiendum to the composed meaning of its definientia. simply masking the corresponding output embedding might suffice to prevent this specific problem ; preliminary experiments in that direction suggest that this may also help decrease perplexity further. as for pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much lower rate of', '']",0
['are aware of  #TAUTHOR_TAG followed a'],['previously proposed architectures we are aware of  #TAUTHOR_TAG followed a'],['are aware of  #TAUTHOR_TAG followed a pattern similar'],"['the generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram', 'all the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '##odules to encode the definiendum and to', 'generate the definientia. in the case of  #TAUTHOR_TAG, the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters', 'derived from a characterlevel cnn, and its "" hypernym embedding ""', '.  #AUTHOR_TAG used a sigmoid - based gating module to tweak the defin', '##iendum embedding. the architecture proposed by is comprised of four modules, only one of which is used as a decoder :', '']",4
['of  #TAUTHOR_TAG ('],['of  #TAUTHOR_TAG'],"[', our experiments focus on the english language.', 'the dataset of  #TAUTHOR_TAG (']","['train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling.', 'as a consequence, our experiments focus on the english language.', 'the dataset of  #TAUTHOR_TAG ( henceforth d nor ) maps definienda to their respective definientia, as well as additional information not used here.', 'in the dataset of  #AUTHOR_TAG ( henceforth d gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence.', 'd nor contains on average shorter definitions than d gad.', '']",4
['note that the work of  #TAUTHOR_TAG had a much'],"['pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much']","['pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much']","['the full context may confuse it. on the other hand, the lower number of self -', 'referring definitions may also be linked to this richer, more varied input : this would allow the model not to fall 7 self -', 'referring definitions are those where a definiendum is used as', 'a definiens for itself. dictionaries are expected to be exempt of such definitions : as readers are assumed not to know the meaning of the definiendum when looking it up. back on simply', 'reusing the definiendum as its own definiens. self - referring definitions highlight that our', 'models equate the meaning of the definiendum to the composed meaning of its definientia. simply masking the corresponding output embedding might suffice to prevent this specific problem ; preliminary experiments in that direction suggest that this may also help decrease perplexity further. as for pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much lower rate of', '']",4
['note that the work of  #TAUTHOR_TAG had a much'],"['pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much']","['pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much']","['the full context may confuse it. on the other hand, the lower number of self -', 'referring definitions may also be linked to this richer, more varied input : this would allow the model not to fall 7 self -', 'referring definitions are those where a definiendum is used as', 'a definiens for itself. dictionaries are expected to be exempt of such definitions : as readers are assumed not to know the meaning of the definiendum when looking it up. back on simply', 'reusing the definiendum as its own definiens. self - referring definitions highlight that our', 'models equate the meaning of the definiendum to the composed meaning of its definientia. simply masking the corresponding output embedding might suffice to prevent this specific problem ; preliminary experiments in that direction suggest that this may also help decrease perplexity further. as for pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much lower rate of', '']",4
['are aware of  #TAUTHOR_TAG followed a'],['previously proposed architectures we are aware of  #TAUTHOR_TAG followed a'],['are aware of  #TAUTHOR_TAG followed a pattern similar'],"['the generation algorithm rather than a single vector. to take a more specific case of verb definitions, we observe that context explicitly represents argument structure, which is obviously useful when defining the verb. there is no guarantee that a single embedding, even if it be contextualized, would preserve this wealth of information - that is to say, that you can cram', 'all the information pertaining to the syntactic context into a single vector. despite some key differences, all of the previously proposed architectures we are aware of  #TAUTHOR_TAG followed a pattern similar to sequence - to - sequence models. they all implicitly or explicitly used distinct subm', '##odules to encode the definiendum and to', 'generate the definientia. in the case of  #TAUTHOR_TAG, the encoding was the concatenation of the embedding of the definiendum, a vector representation of its sequence of characters', 'derived from a characterlevel cnn, and its "" hypernym embedding ""', '.  #AUTHOR_TAG used a sigmoid - based gating module to tweak the defin', '##iendum embedding. the architecture proposed by is comprised of four modules, only one of which is used as a decoder :', '']",3
"['provides out - of - context definitions, like in  #TAUTHOR_TAG where the definition is not linked to the context of a word but to its defin']","['provides out - of - context definitions, like in  #TAUTHOR_TAG where the definition is not linked to the context of a word but to its definiendum only.', 'for context']","['is passed into the encoder : the resulting system provides out - of - context definitions, like in  #TAUTHOR_TAG where the definition is not linked to the context of a word but to its defin']","['', 'when to apply marking, as introduced by eq. 4, is crucial when using the multiplicative marking scheme select.', 'should we mark the definiendum before encoding, then only the definiendum embedding is passed into the encoder : the resulting system provides out - of - context definitions, like in  #TAUTHOR_TAG where the definition is not linked to the context of a word but to its definiendum only.', 'for context to be taken into account under the multiplicative strategy, tokens w k must be encoded and contextualized before integration with the indicator i k.', 'figure 1a presents the contextual select mechanism visually.', 'it consists in coercing the decoder to attend only to the contextualized representation for the definiendum.', 'to do so, we encode the full context and then select only the encoded representation of the definiendum, dropping the rest of the context, before running the decoder.', 'in the case of the transformer architecture, this is equivalent to using a multiplicative marking on the encoded representations : vectors that have been zeroed out are ignored during attention and thus cannot influence the behavior of the decoder.', 'this select approach may seem intuitive and naturally interpretable, as it directly controls what information is passed to the decoder - we carefully select only the contextualized definiendum, thus the only remaining zone of uncertainty would be how exactly contextualization is performed.', 'it also seems to provide a strong and reasonable bias for training the definition generation system.', 'such an approach, however, is not guaranteed to excel : forcibly omitted context could contain important information that might not be easily incorporated in the definiendum embedding.', 'being simple and natural, the select approach resembles architectures like that of  #AUTHOR_TAG and : the full encoder is dedicated to altering the embedding of the definiendum on the basis of its context ; in that, the encoder may be seen as a dedicated']",3
['note that the work of  #TAUTHOR_TAG had a much'],"['pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much']","['pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much']","['the full context may confuse it. on the other hand, the lower number of self -', 'referring definitions may also be linked to this richer, more varied input : this would allow the model not to fall 7 self -', 'referring definitions are those where a definiendum is used as', 'a definiens for itself. dictionaries are expected to be exempt of such definitions : as readers are assumed not to know the meaning of the definiendum when looking it up. back on simply', 'reusing the definiendum as its own definiens. self - referring definitions highlight that our', 'models equate the meaning of the definiendum to the composed meaning of its definientia. simply masking the corresponding output embedding might suffice to prevent this specific problem ; preliminary experiments in that direction suggest that this may also help decrease perplexity further. as for pos - mismatches, we do note that the work of  #TAUTHOR_TAG had a much lower rate of', '']",3
"['between the encoder and the decoder : therefore output tokens can only correspond to words attested as definientia.', '4 the dropout rate and warmup steps number were set using a hyperparameter search on the dataset from  #TAUTHOR_TAG, during which encoder and decoder vocabulary were merged']","['not share vocabularies between the encoder and the decoder : therefore output tokens can only correspond to words attested as definientia.', '4 the dropout rate and warmup steps number were set using a hyperparameter search on the dataset from  #TAUTHOR_TAG, during which encoder and decoder vocabulary were merged']","['non - contextual definition modeling, definienda are mapped directly to definitions.', 'as the source corresponds only to the definiendum, we conjecture that few parameters are required for the encoder.', 'we use 1 layer for the encoder, 6 for the decoder, 300 dimensions per hidden representations and 6 heads for multi - head attention.', 'we do not share vocabularies between the encoder and the decoder : therefore output tokens can only correspond to words attested as definientia.', '4 the dropout rate and warmup steps number were set using a hyperparameter search on the dataset from  #TAUTHOR_TAG, during which encoder and decoder vocabulary were merged']","['non - contextual definition modeling, definienda are mapped directly to definitions.', 'as the source corresponds only to the definiendum, we conjecture that few parameters are required for the encoder.', 'we use 1 layer for the encoder, 6 for the decoder, 300 dimensions per hidden representations and 6 heads for multi - head attention.', 'we do not share vocabularies between the encoder and the decoder : therefore output tokens can only correspond to words attested as definientia.', '4 the dropout rate and warmup steps number were set using a hyperparameter search on the dataset from  #TAUTHOR_TAG, during which encoder and decoder vocabulary were merged for computational simplicity and models stopped after 12, 000 steps.', 'we first fixed dropout to 0. 1 and tested warmup step values between 1000 and 10, 000 by increments of 1000, then focused on the most promising span ( 1000 - 4000 steps ) and exhaustively tested dropout rates from 0. 2 to 0. 8 by increments of 0. 1']",5
['of  #TAUTHOR_TAG ('],['of  #TAUTHOR_TAG'],"[', our experiments focus on the english language.', 'the dataset of  #TAUTHOR_TAG (']","['train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling.', 'as a consequence, our experiments focus on the english language.', 'the dataset of  #TAUTHOR_TAG ( henceforth d nor ) maps definienda to their respective definientia, as well as additional information not used here.', 'in the dataset of  #AUTHOR_TAG ( henceforth d gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence.', 'd nor contains on average shorter definitions than d gad.', '']",5
['bert  #TAUTHOR_TAG and open - gpt'],"['bert  #TAUTHOR_TAG and open - gpt [ 3 ],']","['bert  #TAUTHOR_TAG and open - gpt [ 3 ],']","['introduction of pre - trained language models, such as bert  #TAUTHOR_TAG and open - gpt [ 3 ], among many others, has brought tremendous progress to the nlp research and industrial communities.', 'the contribution of these models can be categorized into two aspects.', 'first, pre - trained language models allow modelers to achieve reasonable accuracy without the need an excessive amount of manually labeled data.', 'this strategy is in contrast with the classical deep learning methods, which requires a multitude more data to reach comparable results.', 'second, for many nlp tasks, including but not limited to, squad [ 4 ], coqa [ 5 ], named entity recognition [ 6 ], glue [ 7 ], machine translation [ 8 ], pre - trained model allows the creation of new state - of - art, given a reasonable amount of labelled data.', 'in the post pre - trained language model era, to pursue new state - of - art, two directions can be followed.', 'the first method, is to improve the pre - training process, such as in the work of ernie [ 9 ], gpt2. 0 [ 3 ] and mt - dnn [ 10 ].', 'the second method is to stand on the shoulder of the pre - trained language models.', 'among the many possibilities, one of them is to build new neural network structures on top of pre - trained language models.', 'in principles, there are three ways to train the networks with stacked neural networks on top of pre - trained language models, as shown in table 1.', 'in peters et al. [ 1 ], the authors compare the possibility of option stack - only and finetune - only, and conclude that option finetune - only is better than option stack - only.', 'more specifically, peter et al. [ 1 ] arxiv : 1907. 05338v1 [ argue that it is better to add a task - specific head on top of bert than to freeze the weights of bert and add more complex network structures.', 'however, peters et al. [ 1 ] did not compare option stack - and - finetune and finetune - only.', 'on the other hand, before pre - trained deep language models became popular, researchers often use a strategy analog to option stack - and - finetune.', 'that is, modelers first train the model until convergence, and then fine - tune the word embeddings with a few epochs.', 'if pre - trained language models can be understood as at least partially resemblance of word embeddings, then it will be imprudent not']",0
"['bert  #TAUTHOR_TAG, it is']","['bert  #TAUTHOR_TAG, it is']","['of pre - trained language models, especially bert  #TAUTHOR_TAG, it is']",[' #TAUTHOR_TAG'],0
"['bert  #TAUTHOR_TAG, it is']","['bert  #TAUTHOR_TAG, it is']","['of pre - trained language models, especially bert  #TAUTHOR_TAG, it is']",[' #TAUTHOR_TAG'],0
"['bert  #TAUTHOR_TAG, it is']","['bert  #TAUTHOR_TAG, it is']","['of pre - trained language models, especially bert  #TAUTHOR_TAG, it is']",[' #TAUTHOR_TAG'],0
"['.  #TAUTHOR_TAG, the author directly trained bert along with with a light - weighted']","['al.  #TAUTHOR_TAG, the author directly trained bert along with with a light - weighted task - specific head.', 'in our case though, we']","['.  #TAUTHOR_TAG, the author directly trained bert along with with a light - weighted task - specific head.', 'in our case though, we top bert with a more complex network structure, using kaiming initialization [ 29 ].', 'if one would fine - tune directly the top models']","['our strategy stack - and - finetune, the model training process is divided into two phases, which are described in detail below.', 'in the first phase, the parameters of the pre - training model are fixed, and only the upper - level models added for a specific task is learned.', 'in the second phase, we fine - tune the upper - level models together with the pre - trained language models.', 'we choose this strategy for the following reasons.', 'pre - training models have been used to obtain more effective word representations through the study of a large number of corpora.', 'in the paradigm proposed in the original work by devlin et al.  #TAUTHOR_TAG, the author directly trained bert along with with a light - weighted task - specific head.', 'in our case though, we top bert with a more complex network structure, using kaiming initialization [ 29 ].', 'if one would fine - tune directly the top models along with the weights in bert, one is faced with the following dilemma : on the one hand, if the learning rate is too large, it is likely to disturb the structure innate to the pre - trained language models ; on the other hand, if the learning rate is too small, since we top bert with relatively complex models, the convergence of the top models might be impeded.', 'therefore, in the first phase we fix the weights in the pre - training language models, and only train the model on top of it.', 'another aspect that is worth commenting in the first phase is that it is most beneficial that one does not train the top model until it reaches the highest accuracy on the training or validation data sets, but rather only up to a point where the prediction accuracy of the training and validation data sets do not differ much.', '']",0
"['bert  #TAUTHOR_TAG, it is']","['bert  #TAUTHOR_TAG, it is']","['of pre - trained language models, especially bert  #TAUTHOR_TAG, it is']",[' #TAUTHOR_TAG'],1
"['bert  #TAUTHOR_TAG, it is']","['bert  #TAUTHOR_TAG, it is']","['of pre - trained language models, especially bert  #TAUTHOR_TAG, it is']",[' #TAUTHOR_TAG'],1
"['.', 'since bert - adam  #TAUTHOR_TAG has excellent']","['−5 in the later stage.', 'since bert - adam  #TAUTHOR_TAG has excellent performance, in our experiments, we use']","['##e −5 in the later stage.', 'since bert - adam  #TAUTHOR_TAG has excellent performance, in our experiments, we use']","['', 'after our experiments, we found that it gets better results while the learning rate is set to 0. 001 in the stage of training only the upper model and set to 5e −5 in the later stage.', 'since bert - adam  #TAUTHOR_TAG has excellent performance, in our experiments, we use it as an optimizer with β 1 = 0. 9, β 2 = 0. 999, l 2 - weight decay of 0. 01. we apply a dropout trick on all layers and set the dropout probability as 0. 1']",3
"['their proposed methods [ 30, 31, 32, 33,  #TAUTHOR_TAG.', 'for strategy finetun']","['their proposed methods [ 30, 31, 32, 33,  #TAUTHOR_TAG.', 'for strategy finetune - only and strategy stack - and - finetune, we implemented two models : one with bert']","['their proposed methods [ 30, 31, 32, 33,  #TAUTHOR_TAG.', 'for strategy finetun']","['the sequence labeling task, we explore sub - task named entity recognition using conll03 dataset [ 6 ], which is a public available used in many studies to test the accuracy of their proposed methods [ 30, 31, 32, 33,  #TAUTHOR_TAG.', 'for strategy finetune - only and strategy stack - and - finetune, we implemented two models : one with bert and the other with bert adding a bi - lstm on top.', 'eval measure is accuracy and f1 score']",3
"['.', 'since bert - adam  #TAUTHOR_TAG has excellent']","['−5 in the later stage.', 'since bert - adam  #TAUTHOR_TAG has excellent performance, in our experiments, we use']","['##e −5 in the later stage.', 'since bert - adam  #TAUTHOR_TAG has excellent performance, in our experiments, we use']","['', 'after our experiments, we found that it gets better results while the learning rate is set to 0. 001 in the stage of training only the upper model and set to 5e −5 in the later stage.', 'since bert - adam  #TAUTHOR_TAG has excellent performance, in our experiments, we use it as an optimizer with β 1 = 0. 9, β 2 = 0. 999, l 2 - weight decay of 0. 01. we apply a dropout trick on all layers and set the dropout probability as 0. 1']",5
"['their proposed methods [ 30, 31, 32, 33,  #TAUTHOR_TAG.', 'for strategy finetun']","['their proposed methods [ 30, 31, 32, 33,  #TAUTHOR_TAG.', 'for strategy finetune - only and strategy stack - and - finetune, we implemented two models : one with bert']","['their proposed methods [ 30, 31, 32, 33,  #TAUTHOR_TAG.', 'for strategy finetun']","['the sequence labeling task, we explore sub - task named entity recognition using conll03 dataset [ 6 ], which is a public available used in many studies to test the accuracy of their proposed methods [ 30, 31, 32, 33,  #TAUTHOR_TAG.', 'for strategy finetune - only and strategy stack - and - finetune, we implemented two models : one with bert and the other with bert adding a bi - lstm on top.', 'eval measure is accuracy and f1 score']",5
"['', 'very recently,  #TAUTHOR_TAG']","['explored yet.', 'very recently,  #TAUTHOR_TAG that, given']","['', 'very recently,  #TAUTHOR_TAG that, given an image caption,']","['', 'very recently,  #TAUTHOR_TAG that, given an image caption, jointly predicts another caption and the features of associated image.', '']",0
"['', 'very recently,  #TAUTHOR_TAG']","['explored yet.', 'very recently,  #TAUTHOR_TAG that, given']","['', 'very recently,  #TAUTHOR_TAG that, given an image caption,']","['', 'very recently,  #TAUTHOR_TAG that, given an image caption, jointly predicts another caption and the features of associated image.', '']",0
"['base our model on the  #TAUTHOR_TAG.', 'a bidirectional long short - term memory ( lstm ) [ 24 ] encodes an input sentence and produces a sentence representation for the input.', 'a pair of lstm cells encodes']","['base our model on the  #TAUTHOR_TAG.', 'a bidirectional long short - term memory ( lstm ) [ 24 ] encodes an input sentence and produces a sentence representation for the input.', 'a pair of lstm cells encodes']","['base our model on the  #TAUTHOR_TAG.', 'a bidirectional long short - term memory ( lstm ) [ 24 ] encodes an input sentence and produces a sentence representation for the input.', 'a pair of lstm cells encodes the input sequence in both directions and produce two final hidden states : h t and h t.', 'the hidden representation of the entire sequence is produced by selecting maximum elements between the two hidden states : h s = max ( h t, h t ).', 'the decoder calculates the probability of a target word y t at each time step t, conditional to the sentence representation h s and all target words before t. p ( y t | y < t, h s ).', 'the objective of the basic encoder - decoder model is thus the negative log - likelihood of the target sentence given all model parameters : l c = − x, y∈d yt∈y log p ( y t | y < t, x, θ )']","['base our model on the  #TAUTHOR_TAG.', 'a bidirectional long short - term memory ( lstm ) [ 24 ] encodes an input sentence and produces a sentence representation for the input.', 'a pair of lstm cells encodes the input sequence in both directions and produce two final hidden states : h t and h t.', 'the hidden representation of the entire sequence is produced by selecting maximum elements between the two hidden states : h s = max ( h t, h t ).', 'the decoder calculates the probability of a target word y t at each time step t, conditional to the sentence representation h s and all target words before t. p ( y t | y < t, h s ).', 'the objective of the basic encoder - decoder model is thus the negative log - likelihood of the target sentence given all model parameters : l c = − x, y∈d yt∈y log p ( y t | y < t, x, θ )']",5
"['the experimental design of  #TAUTHOR_TAG, we conduct experiments on three different learning objectives : cap2all, cap2cap, cap2img.', 'under cap2all, the model is trained to predict both']","['the experimental design of  #TAUTHOR_TAG, we conduct experiments on three different learning objectives : cap2all, cap2cap, cap2img.', 'under cap2all, the model is trained to predict both']","['the experimental design of  #TAUTHOR_TAG, we conduct experiments on three different learning objectives : cap2all, cap2cap, cap2img.', 'under cap2all, the model is trained to predict both the target caption and']","['the experimental design of  #TAUTHOR_TAG, we conduct experiments on three different learning objectives : cap2all, cap2cap, cap2img.', 'under cap2all, the model is trained to predict both the target caption and the associated image : l = l c + l v g.', 'under cap2cap, the model is trained to predict only the target caption ( l = l c ) and, under cap2img, only the associated image ( l = l v g )']",5
"['32 ].', 'we evaluate sentence representation quality using senteval 2  #TAUTHOR_TAG, 10 ] scripts.', 'mini - batch size is 128 and negative samples are prepared from remaining data samples in the same mini - batch']","['of resnet - 101 [ 32 ].', 'we evaluate sentence representation quality using senteval 2  #TAUTHOR_TAG, 10 ] scripts.', 'mini - batch size is 128 and negative samples are prepared from remaining data samples in the same mini - batch']","['- 101 [ 32 ].', 'we evaluate sentence representation quality using senteval 2  #TAUTHOR_TAG, 10 ] scripts.', 'mini - batch size is 128 and negative samples are prepared from remaining data samples in the same mini - batch']","['embeddings w e are initialized with glove [ 27 ].', 'the hidden dimension of each encoder and decoder lstm cell ( d h ) is 1024 1.', 'we use adam optimizer [ 28 ] and clip the gradients to between - 5 and 5.', 'number of layers, dropout, and non - linearity for image feature prediction layers are 4, 0. 3 and relu [ 29 ] respectively.', 'dimensionality of hidden attention layers ( d a ) is 350 and number of attentions ( n a ) is 30.', 'we employ orthogonal initialization [ 30 ] for recurrent weights and xavier initialization [ 31 ] for all others.', ""for the datasets, we use karpathy and fei - fei's split for ms - coco dataset [ 13 ]."", 'image features are prepared by extracting hidden representations at the final layer of resnet - 101 [ 32 ].', 'we evaluate sentence representation quality using senteval 2  #TAUTHOR_TAG, 10 ] scripts.', 'mini - batch size is 128 and negative samples are prepared from remaining data samples in the same mini - batch']",5
"['##ring to the experimental settings of  #TAUTHOR_TAG, we concaten']","['##ring to the experimental settings of  #TAUTHOR_TAG, we concatenate sentence representations']","['##ring to the experimental settings of  #TAUTHOR_TAG, we concatenate sentence representations']","['##ring to the experimental settings of  #TAUTHOR_TAG, we concatenate sentence representations produced from our model with those obtained from the state - of - the - art unsupervised learning model ( layer normalized skip - thoughts, st - ln ) [ 33 ].', 'we evaluate the quality of sentence representations produced from different variants of our encoders on well - known transfer tasks : movie review sentiment ( mr ) [ 34 ], customer reviews ( cr ) [ 35 ], subjectivity ( subj ) [ 36 ], opinion polarity ( mpqa ) [ 37 ], paraphrase identification ( msrp ) [ 38 ], binary sentiment classification ( sst ) [ 39 ], sick entailment and sick relatedness [ 40 ]']",5
"['training cross - modal feature matching  #TAUTHOR_TAG, 20, 25 ], we find that log - exp - sum pairwise ranking']","['training cross - modal feature matching  #TAUTHOR_TAG, 20, 25 ], we find that log - exp - sum pairwise ranking [ 26 ] yields better results in']","['training cross - modal feature matching  #TAUTHOR_TAG, 20, 25 ], we find that log - exp - sum pairwise ranking']","['the source caption representation h s and the relevant image representation h i, we associate the two representations by projecting h s into image feature space.', 'we train the model to rank the similarity between predicted image featuresh i and the target image features h i higher than other pairs, which is achieved by ranking loss functions.', 'although margin ranking loss has been the dominant choice for training cross - modal feature matching  #TAUTHOR_TAG, 20, 25 ], we find that log - exp - sum pairwise ranking [ 26 ] yields better results in terms of evaluation performance and efficiency.', 'thus, the objective for ranking', 'where n is the set of negative examples and sim is cosine similarity']",4
"['- level visual features onto sentence representations  #TAUTHOR_TAG.', 'figure 1 : activated attention weights on two samples from ms - coco dataset.', '']","['expose word - level visual features onto sentence representations  #TAUTHOR_TAG.', 'figure 1 : activated attention weights on two samples from ms - coco dataset.', '']","['- level visual features onto sentence representations  #TAUTHOR_TAG.', 'figure 1 : activated attention weights on two samples from ms - coco dataset.', 'vertical axis shows attention vectors']","['order to study the effects of incorporating self - attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from ms - coco dataset and compare them to associated images ( figure 1 ).', 'for example, given the sentence "" man in black shirt is playing guitar "", our model identifies words that have association with strong visual imagery, such as "" man "", "" black "" and "" guitar "".', 'given the second sentence, our model learned to attend to visually significant words such as "" cat "" and "" bowl "".', 'these findings show that visually grounding self - attended sentence representations helps to expose word - level visual features onto sentence representations  #TAUTHOR_TAG.', 'figure 1 : activated attention weights on two samples from ms - coco dataset.', 'vertical axis shows attention vectors learned by our model ( compressed due to space limit ).', 'note how the sentence encoder learned to identify words with strong visual associations']",4
"['- level visual features onto sentence representations  #TAUTHOR_TAG.', 'figure 1 : activated attention weights on two samples from ms - coco dataset.', '']","['expose word - level visual features onto sentence representations  #TAUTHOR_TAG.', 'figure 1 : activated attention weights on two samples from ms - coco dataset.', '']","['- level visual features onto sentence representations  #TAUTHOR_TAG.', 'figure 1 : activated attention weights on two samples from ms - coco dataset.', 'vertical axis shows attention vectors']","['order to study the effects of incorporating self - attention mechanism in joint prediction of image and language features, we examine attention vectors for selected samples from ms - coco dataset and compare them to associated images ( figure 1 ).', 'for example, given the sentence "" man in black shirt is playing guitar "", our model identifies words that have association with strong visual imagery, such as "" man "", "" black "" and "" guitar "".', 'given the second sentence, our model learned to attend to visually significant words such as "" cat "" and "" bowl "".', 'these findings show that visually grounding self - attended sentence representations helps to expose word - level visual features onto sentence representations  #TAUTHOR_TAG.', 'figure 1 : activated attention weights on two samples from ms - coco dataset.', 'vertical axis shows attention vectors learned by our model ( compressed due to space limit ).', 'note how the sentence encoder learned to identify words with strong visual associations']",6
"[') based sequence to sequence models  #TAUTHOR_TAG.', 'convolutional sequence to sequence based models have been used in']","['( rnn ) based sequence to sequence models  #TAUTHOR_TAG.', 'convolutional sequence to sequence based models have been used in']","[') based sequence to sequence models  #TAUTHOR_TAG.', 'convolutional sequence to sequence based models have been used in']","['language generation ( nlg ) is an important component in spoken dialog systems ( sdss ).', 'a model for nlg involves sequence to sequence learning.', 'state - of - the - art nlg models are built using recurrent neural network ( rnn ) based sequence to sequence models  #TAUTHOR_TAG.', 'convolutional sequence to sequence based models have been used in the domain of machine translation but their application as natural language generators in dialogue systems is still unexplored.', 'in this work, we propose a novel approach to nlg using convolutional neural network ( cnn ) based sequence to sequence learning.', 'cnn - based approach allows to build a hierarchical model which encapsulates dependencies between words via shorter path unlike rnns.', 'in contrast to recurrent models, convolutional approach allows for efficient utilization of computational resources by parallelizing computations over all elements, and eases the learning process by applying constant number of nonlinearities.', 'we also propose to use cnn - based reranker for obtaining responses having semantic correspondence with input dialogue acts.', 'the proposed model is capable of entrainment.', 'studies using a standard dataset shows the effectiveness of the proposed cnn - based approach to nlg']",0
['model  #TAUTHOR_TAG'],['context aware nlg model  #TAUTHOR_TAG'],"['of developing a fully trainable context aware nlg model  #TAUTHOR_TAG.', 'however,']","['task - specific spoken dialogue systems ( sds ), the function of natural language generation ( nlg ) components is to generate natural language response from a dialogue act ( da )  #AUTHOR_TAG.', 'da is a meaning representation specifying actions along with various attributes and their values.', 'nlg plays a very important role in realizing the overall quality of the sds.', 'entrainment to users way of speaking is essential for generating more natural and high quality natural language responses.', 'most of the approaches for incorporating entrainment are rule - based models.', 'recent advances have been in the direction of developing a fully trainable context aware nlg model  #TAUTHOR_TAG.', 'however, all these approaches are based on recurrent sequence to sequence architecture.', 'convolutional neural networks are largely unexplored in the domain of nlg for sds inspite of having several advantages  #AUTHOR_TAG le  #AUTHOR_TAG.', 'recurrent networks depend on the computations of previous time step and thus inhibits parallelization within a sequence.', 'convolutional networks on the other hand, allows parallelization within a sequence resulting in efficient use of gpus and other computational resources  #AUTHOR_TAG.', 'multi - block ( multilayer ) convolutional networks enable controlling the upper bound on the effective context size and form a hierarchical structure.', '']",0
[' #TAUTHOR_TAG serves as a baseline sequence'],[' #TAUTHOR_TAG serves as a baseline sequence'],"['of more informative response. model proposed by  #TAUTHOR_TAG serves as a baseline sequence to sequence generation model ( tg', '##en model ) for sds which takes into account the context. the model', 'takes into account the preceding user utterance while generating natural language output. the model implemented three modifications to the model proposed']","['', 'deep syntax trees which can be converted to output utterance using a surface realization mechanism. this model is context unaware because it takes into account only the input', 'da and no preceding user utterance ( s ). this leads to generation of very rigid responses and also inhibits flexible interactions. context', ""awareness adapts / entrains to the user's way of speaking and thereby generates responses of high quality and naturalness. the semantic meaning"", 'which is required to be given in response to a query is very well modelled if context awareness is taken into account. this', 'leads to generation of more informative response. model proposed by  #TAUTHOR_TAG serves as a baseline sequence to sequence generation model ( tg', '##en model ) for sds which takes into account the context. the model', 'takes into account the preceding user utterance while generating natural language output. the model implemented three modifications to the model proposed by dusek and', ' #AUTHOR_TAG b ). the first modification was prepending context to the input das. the second modification was implementing a separate encoder for user', 'utterances / contexts. the third modification was implementing', 'a n - gram match reranker. this reranker is based on n - gram precision scores and promotes responses having phrase overlaps', 'with user utterances  #TAUTHOR_TAG. in the next section, we present the proposed cnn', '- based sequence to sequence generator for nlg']",0
['model  #TAUTHOR_TAG'],['context aware nlg model  #TAUTHOR_TAG'],"['of developing a fully trainable context aware nlg model  #TAUTHOR_TAG.', 'however,']","['task - specific spoken dialogue systems ( sds ), the function of natural language generation ( nlg ) components is to generate natural language response from a dialogue act ( da )  #AUTHOR_TAG.', 'da is a meaning representation specifying actions along with various attributes and their values.', 'nlg plays a very important role in realizing the overall quality of the sds.', 'entrainment to users way of speaking is essential for generating more natural and high quality natural language responses.', 'most of the approaches for incorporating entrainment are rule - based models.', 'recent advances have been in the direction of developing a fully trainable context aware nlg model  #TAUTHOR_TAG.', 'however, all these approaches are based on recurrent sequence to sequence architecture.', 'convolutional neural networks are largely unexplored in the domain of nlg for sds inspite of having several advantages  #AUTHOR_TAG le  #AUTHOR_TAG.', 'recurrent networks depend on the computations of previous time step and thus inhibits parallelization within a sequence.', 'convolutional networks on the other hand, allows parallelization within a sequence resulting in efficient use of gpus and other computational resources  #AUTHOR_TAG.', 'multi - block ( multilayer ) convolutional networks enable controlling the upper bound on the effective context size and form a hierarchical structure.', '']",5
[' #TAUTHOR_TAG serves as a baseline sequence'],[' #TAUTHOR_TAG serves as a baseline sequence'],"['of more informative response. model proposed by  #TAUTHOR_TAG serves as a baseline sequence to sequence generation model ( tg', '##en model ) for sds which takes into account the context. the model', 'takes into account the preceding user utterance while generating natural language output. the model implemented three modifications to the model proposed']","['', 'deep syntax trees which can be converted to output utterance using a surface realization mechanism. this model is context unaware because it takes into account only the input', 'da and no preceding user utterance ( s ). this leads to generation of very rigid responses and also inhibits flexible interactions. context', ""awareness adapts / entrains to the user's way of speaking and thereby generates responses of high quality and naturalness. the semantic meaning"", 'which is required to be given in response to a query is very well modelled if context awareness is taken into account. this', 'leads to generation of more informative response. model proposed by  #TAUTHOR_TAG serves as a baseline sequence to sequence generation model ( tg', '##en model ) for sds which takes into account the context. the model', 'takes into account the preceding user utterance while generating natural language output. the model implemented three modifications to the model proposed by dusek and', ' #AUTHOR_TAG b ). the first modification was prepending context to the input das. the second modification was implementing a separate encoder for user', 'utterances / contexts. the third modification was implementing', 'a n - gram match reranker. this reranker is based on n - gram precision scores and promotes responses having phrase overlaps', 'with user utterances  #TAUTHOR_TAG. in the next section, we present the proposed cnn', '- based sequence to sequence generator for nlg']",5
[' #TAUTHOR_TAG serves as a baseline sequence'],[' #TAUTHOR_TAG serves as a baseline sequence'],"['of more informative response. model proposed by  #TAUTHOR_TAG serves as a baseline sequence to sequence generation model ( tg', '##en model ) for sds which takes into account the context. the model', 'takes into account the preceding user utterance while generating natural language output. the model implemented three modifications to the model proposed']","['', 'deep syntax trees which can be converted to output utterance using a surface realization mechanism. this model is context unaware because it takes into account only the input', 'da and no preceding user utterance ( s ). this leads to generation of very rigid responses and also inhibits flexible interactions. context', ""awareness adapts / entrains to the user's way of speaking and thereby generates responses of high quality and naturalness. the semantic meaning"", 'which is required to be given in response to a query is very well modelled if context awareness is taken into account. this', 'leads to generation of more informative response. model proposed by  #TAUTHOR_TAG serves as a baseline sequence to sequence generation model ( tg', '##en model ) for sds which takes into account the context. the model', 'takes into account the preceding user utterance while generating natural language output. the model implemented three modifications to the model proposed by dusek and', ' #AUTHOR_TAG b ). the first modification was prepending context to the input das. the second modification was implementing a separate encoder for user', 'utterances / contexts. the third modification was implementing', 'a n - gram match reranker. this reranker is based on n - gram precision scores and promotes responses having phrase overlaps', 'with user utterances  #TAUTHOR_TAG. in the next section, we present the proposed cnn', '- based sequence to sequence generator for nlg']",5
['as given by  #TAUTHOR_TAG'],['as given by  #TAUTHOR_TAG'],"['n - gram match reranker and misfit penalties from cnn reranker.', 'here, ω and w are constants.', 'we implement the n - gram match reranker as given by  #TAUTHOR_TAG.', 'we describe the proposed convolutional sequence to sequence generator in section 3. 1 and']","['', 'here, we get log probabilities from convseq2seq generator, bigram precision scores from n - gram match reranker and misfit penalties from cnn reranker.', 'here, ω and w are constants.', 'we implement the n - gram match reranker as given by  #TAUTHOR_TAG.', 'we describe the proposed convolutional sequence to sequence generator in section 3. 1 and convolutional reranker in section 3. 2']",5
"['have used  #TAUTHOR_TAG, there are 19 such classes of da types and slot - value combinations.', 'these 19']","['have used  #TAUTHOR_TAG, there are 19 such classes of da types and slot - value combinations.', 'these 19']","['of binary vector is a binary decision on the presence of da type or slot - value combinations.', 'for the dataset which we have used  #TAUTHOR_TAG, there are 19 such classes of da types and slot - value combinations.', 'these 19 classes are shown in figure 3']","['n - best beam search responses from convseq2seq model may have missing information and / or irrelevant information.', 'cnn reranker reranks the n - best beam search responses and heavily penalizes those responses which are not semantically in correspondence with the input da.', 'responses having missing information and / or irrelevant information are heavily penalized.', 'convolutional networks are excellent feature extractors and have achieved state - of - the - art results in many text classification and sentence - level classification tasks such as sentiment analysis, question classification, etc  #AUTHOR_TAG.', 'this classifier takes as input a natural language response and outputs a binary vector.', 'each element of binary vector is a binary decision on the presence of da type or slot - value combinations.', 'for the dataset which we have used  #TAUTHOR_TAG, there are 19 such classes of da types and slot - value combinations.', 'these 19 classes are shown in figure 3.', 'input das are converted to similar binary vector.', 'hamming distance between the classifier output and binary vector representation of input da is considered as reranking penalty.', 'the weighted reranking penalties of all the n - best responses are subtracted from their log - probabilities similar to  #TAUTHOR_TAG.', '']",5
"['have used  #TAUTHOR_TAG, there are 19 such classes of da types and slot - value combinations.', 'these 19']","['have used  #TAUTHOR_TAG, there are 19 such classes of da types and slot - value combinations.', 'these 19']","['of binary vector is a binary decision on the presence of da type or slot - value combinations.', 'for the dataset which we have used  #TAUTHOR_TAG, there are 19 such classes of da types and slot - value combinations.', 'these 19 classes are shown in figure 3']","['n - best beam search responses from convseq2seq model may have missing information and / or irrelevant information.', 'cnn reranker reranks the n - best beam search responses and heavily penalizes those responses which are not semantically in correspondence with the input da.', 'responses having missing information and / or irrelevant information are heavily penalized.', 'convolutional networks are excellent feature extractors and have achieved state - of - the - art results in many text classification and sentence - level classification tasks such as sentiment analysis, question classification, etc  #AUTHOR_TAG.', 'this classifier takes as input a natural language response and outputs a binary vector.', 'each element of binary vector is a binary decision on the presence of da type or slot - value combinations.', 'for the dataset which we have used  #TAUTHOR_TAG, there are 19 such classes of da types and slot - value combinations.', 'these 19 classes are shown in figure 3.', 'input das are converted to similar binary vector.', 'hamming distance between the classifier output and binary vector representation of input da is considered as reranking penalty.', 'the weighted reranking penalties of all the n - best responses are subtracted from their log - probabilities similar to  #TAUTHOR_TAG.', '']",5
"['tgen model  #TAUTHOR_TAG. for comparison,']","['tgen model  #TAUTHOR_TAG. for comparison,']","['embedding dimension 128, filter sizes', '( 3, 5, 7, 9 ), number of filters 64, dropout keep probability 0. 5, batch', 'size 100, number of epochs 100 and l2 regularization', ', λ = 0. 05. the performance of the proposed convseq2seq model for nlg is compared with that of tgen model  #TAUTHOR_TAG. for comparison,']","['. 00001, maximum number of epochs 2000, learning rate shrink factor 0. 5, clip', '- norm 0. 5, encoder embedding dimension 100, decoder embedding dimension 100, decoder output embedding dimension 100 and', 'dropout 0. 3. encoder part includes 10 layers / blocks, each having 100 units and kernel width of 7. decoder part includes 10 layers, each having 100', 'units and kernel width of 7. for generating outputs on test', 'set, we choose batch size 128 and beam size 20. for our cnn reranker, all the possible combinations of', 'da tokens and its values are considered as classes. we have 19 such classes. each input is a', 'natural language sentence and each output is a set of class labels. training is done', 'by minimizing cross - entropy loss using adam optimizer', ' #AUTHOR_TAG. cross - entropy error is measured on validation set after every 100 steps. misclass', '##ification penalty for cnn reranker is set to 100. based', 'on our experiments, we choose embedding dimension 128, filter sizes', '( 3, 5, 7, 9 ), number of filters 64, dropout keep probability 0. 5, batch', 'size 100, number of epochs 100 and l2 regularization', ', λ = 0. 05. the performance of the proposed convseq2seq model for nlg is compared with that of tgen model  #TAUTHOR_TAG. for comparison, we have considered nist  #AUTHOR_TAG, bleu  #AUTHOR_TAG, meteor  #AUTHOR_TAG, rouge l  #AUTHOR_TAG and cider metrics  #AUTHOR_TAG. for this study, we have considered script "" mtevalv13a -', 'sig. pl "" ( version 13a ) that implements these metrics. this script was used', 'for e2e nlg challenge  #AUTHOR_TAG. we focus on the evaluations using this', 'version. our model has also been evaluated using the metric script "" mtevalv', '##11b. pl "" ( version 11b ) to compare our results with those stated in  #TAUTHOR_TAG. the 13', '##a version takes into account the', 'closest reference length with respect to candidate length for calculation of brevity penalty. this is', 'in accordance with ibm bleu. on the contrary, 11b version takes', 'shortest reference length for measuring brevity penalty. this is the reason behind', 'higher bleu scores in the 11b version when compared to 13', '##a version. both the models have been evaluated on five different metrics, with nist and bleu scores being of atmost importance. we have used n - gram match reranker with the weight ω set to 1 based on experiments done on validation set. when using 11', '##b version for evaluating automatic metrics, weight ω is set to 5']",5
"['tgen model  #TAUTHOR_TAG. for comparison,']","['tgen model  #TAUTHOR_TAG. for comparison,']","['embedding dimension 128, filter sizes', '( 3, 5, 7, 9 ), number of filters 64, dropout keep probability 0. 5, batch', 'size 100, number of epochs 100 and l2 regularization', ', λ = 0. 05. the performance of the proposed convseq2seq model for nlg is compared with that of tgen model  #TAUTHOR_TAG. for comparison,']","['. 00001, maximum number of epochs 2000, learning rate shrink factor 0. 5, clip', '- norm 0. 5, encoder embedding dimension 100, decoder embedding dimension 100, decoder output embedding dimension 100 and', 'dropout 0. 3. encoder part includes 10 layers / blocks, each having 100 units and kernel width of 7. decoder part includes 10 layers, each having 100', 'units and kernel width of 7. for generating outputs on test', 'set, we choose batch size 128 and beam size 20. for our cnn reranker, all the possible combinations of', 'da tokens and its values are considered as classes. we have 19 such classes. each input is a', 'natural language sentence and each output is a set of class labels. training is done', 'by minimizing cross - entropy loss using adam optimizer', ' #AUTHOR_TAG. cross - entropy error is measured on validation set after every 100 steps. misclass', '##ification penalty for cnn reranker is set to 100. based', 'on our experiments, we choose embedding dimension 128, filter sizes', '( 3, 5, 7, 9 ), number of filters 64, dropout keep probability 0. 5, batch', 'size 100, number of epochs 100 and l2 regularization', ', λ = 0. 05. the performance of the proposed convseq2seq model for nlg is compared with that of tgen model  #TAUTHOR_TAG. for comparison, we have considered nist  #AUTHOR_TAG, bleu  #AUTHOR_TAG, meteor  #AUTHOR_TAG, rouge l  #AUTHOR_TAG and cider metrics  #AUTHOR_TAG. for this study, we have considered script "" mtevalv13a -', 'sig. pl "" ( version 13a ) that implements these metrics. this script was used', 'for e2e nlg challenge  #AUTHOR_TAG. we focus on the evaluations using this', 'version. our model has also been evaluated using the metric script "" mtevalv', '##11b. pl "" ( version 11b ) to compare our results with those stated in  #TAUTHOR_TAG. the 13', '##a version takes into account the', 'closest reference length with respect to candidate length for calculation of brevity penalty. this is', 'in accordance with ibm bleu. on the contrary, 11b version takes', 'shortest reference length for measuring brevity penalty. this is the reason behind', 'higher bleu scores in the 11b version when compared to 13', '##a version. both the models have been evaluated on five different metrics, with nist and bleu scores being of atmost importance. we have used n - gram match reranker with the weight ω set to 1 based on experiments done on validation set. when using 11', '##b version for evaluating automatic metrics, weight ω is set to 5']",5
"['tgen model  #TAUTHOR_TAG. for comparison,']","['tgen model  #TAUTHOR_TAG. for comparison,']","['embedding dimension 128, filter sizes', '( 3, 5, 7, 9 ), number of filters 64, dropout keep probability 0. 5, batch', 'size 100, number of epochs 100 and l2 regularization', ', λ = 0. 05. the performance of the proposed convseq2seq model for nlg is compared with that of tgen model  #TAUTHOR_TAG. for comparison,']","['. 00001, maximum number of epochs 2000, learning rate shrink factor 0. 5, clip', '- norm 0. 5, encoder embedding dimension 100, decoder embedding dimension 100, decoder output embedding dimension 100 and', 'dropout 0. 3. encoder part includes 10 layers / blocks, each having 100 units and kernel width of 7. decoder part includes 10 layers, each having 100', 'units and kernel width of 7. for generating outputs on test', 'set, we choose batch size 128 and beam size 20. for our cnn reranker, all the possible combinations of', 'da tokens and its values are considered as classes. we have 19 such classes. each input is a', 'natural language sentence and each output is a set of class labels. training is done', 'by minimizing cross - entropy loss using adam optimizer', ' #AUTHOR_TAG. cross - entropy error is measured on validation set after every 100 steps. misclass', '##ification penalty for cnn reranker is set to 100. based', 'on our experiments, we choose embedding dimension 128, filter sizes', '( 3, 5, 7, 9 ), number of filters 64, dropout keep probability 0. 5, batch', 'size 100, number of epochs 100 and l2 regularization', ', λ = 0. 05. the performance of the proposed convseq2seq model for nlg is compared with that of tgen model  #TAUTHOR_TAG. for comparison, we have considered nist  #AUTHOR_TAG, bleu  #AUTHOR_TAG, meteor  #AUTHOR_TAG, rouge l  #AUTHOR_TAG and cider metrics  #AUTHOR_TAG. for this study, we have considered script "" mtevalv13a -', 'sig. pl "" ( version 13a ) that implements these metrics. this script was used', 'for e2e nlg challenge  #AUTHOR_TAG. we focus on the evaluations using this', 'version. our model has also been evaluated using the metric script "" mtevalv', '##11b. pl "" ( version 11b ) to compare our results with those stated in  #TAUTHOR_TAG. the 13', '##a version takes into account the', 'closest reference length with respect to candidate length for calculation of brevity penalty. this is', 'in accordance with ibm bleu. on the contrary, 11b version takes', 'shortest reference length for measuring brevity penalty. this is the reason behind', 'higher bleu scores in the 11b version when compared to 13', '##a version. both the models have been evaluated on five different metrics, with nist and bleu scores being of atmost importance. we have used n - gram match reranker with the weight ω set to 1 based on experiments done on validation set. when using 11', '##b version for evaluating automatic metrics, weight ω is set to 5']",5
"['tgen model  #TAUTHOR_TAG. for comparison,']","['tgen model  #TAUTHOR_TAG. for comparison,']","['embedding dimension 128, filter sizes', '( 3, 5, 7, 9 ), number of filters 64, dropout keep probability 0. 5, batch', 'size 100, number of epochs 100 and l2 regularization', ', λ = 0. 05. the performance of the proposed convseq2seq model for nlg is compared with that of tgen model  #TAUTHOR_TAG. for comparison,']","['. 00001, maximum number of epochs 2000, learning rate shrink factor 0. 5, clip', '- norm 0. 5, encoder embedding dimension 100, decoder embedding dimension 100, decoder output embedding dimension 100 and', 'dropout 0. 3. encoder part includes 10 layers / blocks, each having 100 units and kernel width of 7. decoder part includes 10 layers, each having 100', 'units and kernel width of 7. for generating outputs on test', 'set, we choose batch size 128 and beam size 20. for our cnn reranker, all the possible combinations of', 'da tokens and its values are considered as classes. we have 19 such classes. each input is a', 'natural language sentence and each output is a set of class labels. training is done', 'by minimizing cross - entropy loss using adam optimizer', ' #AUTHOR_TAG. cross - entropy error is measured on validation set after every 100 steps. misclass', '##ification penalty for cnn reranker is set to 100. based', 'on our experiments, we choose embedding dimension 128, filter sizes', '( 3, 5, 7, 9 ), number of filters 64, dropout keep probability 0. 5, batch', 'size 100, number of epochs 100 and l2 regularization', ', λ = 0. 05. the performance of the proposed convseq2seq model for nlg is compared with that of tgen model  #TAUTHOR_TAG. for comparison, we have considered nist  #AUTHOR_TAG, bleu  #AUTHOR_TAG, meteor  #AUTHOR_TAG, rouge l  #AUTHOR_TAG and cider metrics  #AUTHOR_TAG. for this study, we have considered script "" mtevalv13a -', 'sig. pl "" ( version 13a ) that implements these metrics. this script was used', 'for e2e nlg challenge  #AUTHOR_TAG. we focus on the evaluations using this', 'version. our model has also been evaluated using the metric script "" mtevalv', '##11b. pl "" ( version 11b ) to compare our results with those stated in  #TAUTHOR_TAG. the 13', '##a version takes into account the', 'closest reference length with respect to candidate length for calculation of brevity penalty. this is', 'in accordance with ibm bleu. on the contrary, 11b version takes', 'shortest reference length for measuring brevity penalty. this is the reason behind', 'higher bleu scores in the 11b version when compared to 13', '##a version. both the models have been evaluated on five different metrics, with nist and bleu scores being of atmost importance. we have used n - gram match reranker with the weight ω set to 1 based on experiments done on validation set. when using 11', '##b version for evaluating automatic metrics, weight ω is set to 5']",5
['model  #TAUTHOR_TAG'],['context aware nlg model  #TAUTHOR_TAG'],"['of developing a fully trainable context aware nlg model  #TAUTHOR_TAG.', 'however,']","['task - specific spoken dialogue systems ( sds ), the function of natural language generation ( nlg ) components is to generate natural language response from a dialogue act ( da )  #AUTHOR_TAG.', 'da is a meaning representation specifying actions along with various attributes and their values.', 'nlg plays a very important role in realizing the overall quality of the sds.', 'entrainment to users way of speaking is essential for generating more natural and high quality natural language responses.', 'most of the approaches for incorporating entrainment are rule - based models.', 'recent advances have been in the direction of developing a fully trainable context aware nlg model  #TAUTHOR_TAG.', 'however, all these approaches are based on recurrent sequence to sequence architecture.', 'convolutional neural networks are largely unexplored in the domain of nlg for sds inspite of having several advantages  #AUTHOR_TAG le  #AUTHOR_TAG.', 'recurrent networks depend on the computations of previous time step and thus inhibits parallelization within a sequence.', 'convolutional networks on the other hand, allows parallelization within a sequence resulting in efficient use of gpus and other computational resources  #AUTHOR_TAG.', 'multi - block ( multilayer ) convolutional networks enable controlling the upper bound on the effective context size and form a hierarchical structure.', '']",4
['represented in  #TAUTHOR_TAG'],['represented in  #TAUTHOR_TAG'],"['represented in  #TAUTHOR_TAG.', 'the scores of our model shows slight improvement over tgen model.', 'the studies done to compare']","['comparison of the performance of the proposed model with that of tgen model using the 11b version of the metric implementation is given in table 2.', 'a slight improvement in the scores of our convseq2seq generator after using cnn reranker is seen in table 2 except for bleu score.', 'we see an improvement of 6. 7 bleu points when using n - gram match reranker with ω set to 5.', 'a decrease in scores of other metrics is seen.', 'these inconsistencies are due to the way brevity penalty is calculated for computing bleu scores in 11b version of metric implementation.', 'bleu and nist scores of the tgen model given in table 2 match with that represented in  #TAUTHOR_TAG.', 'the scores of our model shows slight improvement over tgen model.', 'the studies done to compare the proposed model with the tgen model, show the effectiveness of considering the cnn - based approach to nlg.', 'studies also show that cnn reranker outperforms the rnn reranker.', 'further, cnn - based model is expected to take less time to train when compared to rnn - based model.', 'we compare the time taken by the models in the next section']",3
"['second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus']","['second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which']","['a second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which']","['', 'his algorithm starts with a few positive and negative seeds, and then uses data from the web together with a similarity method ( pointwise mutual information ) to automatically grow this seed list.', 'our approach differs from  #AUTHOR_TAG in two important ways : first, we do not address the task of polarity lexicon construction, but instead we focus on the acquisition of subjectivity lexicons.', 'second, turney assumes a very large corpus such as the terabyte corpus of english documents available on the web, whereas we rely on fewer, smaller - scale resources, namely a basic dictionary and a small raw corpus.', 'the problem of distinguishing subjective versus objective instances has often proven to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification.', 'this is reported in studies of manual annotation of phrases  #AUTHOR_TAG, recognizing contextual polarity of expressions  #AUTHOR_TAG, and sentiment tagging of words and word senses  #AUTHOR_TAG b ).', 'another closely related work is our own previously proposed method for leveraging on resources available for english to construct resources for a second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which may not always be available']",0
"[') objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence']","[') objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence']","['% ) objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence - level subjectivity classification results are shown in']","['discussions. the agreement of the two annotators is 0. 83 % ( κ = 0. 67 ) ; when the uncertain annotations are removed, the agreement rises to', '0. 89 ( κ = 0. 77 ). the two annotators reached consensus on all sentences for which they disagreed, resulting in', 'a gold standard dataset with 272 ( 54 % ) subjective sentences and 232 ( 46 % ) objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence - level subjectivity classification results are shown in table 2. by using the extracted lexicon alone, we were able to obtain a rule - based subjectivity classifier with an', '']",0
"['second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus']","['second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which']","['a second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which']","['', 'his algorithm starts with a few positive and negative seeds, and then uses data from the web together with a similarity method ( pointwise mutual information ) to automatically grow this seed list.', 'our approach differs from  #AUTHOR_TAG in two important ways : first, we do not address the task of polarity lexicon construction, but instead we focus on the acquisition of subjectivity lexicons.', 'second, turney assumes a very large corpus such as the terabyte corpus of english documents available on the web, whereas we rely on fewer, smaller - scale resources, namely a basic dictionary and a small raw corpus.', 'the problem of distinguishing subjective versus objective instances has often proven to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification.', 'this is reported in studies of manual annotation of phrases  #AUTHOR_TAG, recognizing contextual polarity of expressions  #AUTHOR_TAG, and sentiment tagging of words and word senses  #AUTHOR_TAG b ).', 'another closely related work is our own previously proposed method for leveraging on resources available for english to construct resources for a second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which may not always be available']",1
"['second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus']","['second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which']","['a second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which']","['', 'his algorithm starts with a few positive and negative seeds, and then uses data from the web together with a similarity method ( pointwise mutual information ) to automatically grow this seed list.', 'our approach differs from  #AUTHOR_TAG in two important ways : first, we do not address the task of polarity lexicon construction, but instead we focus on the acquisition of subjectivity lexicons.', 'second, turney assumes a very large corpus such as the terabyte corpus of english documents available on the web, whereas we rely on fewer, smaller - scale resources, namely a basic dictionary and a small raw corpus.', 'the problem of distinguishing subjective versus objective instances has often proven to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification.', 'this is reported in studies of manual annotation of phrases  #AUTHOR_TAG, recognizing contextual polarity of expressions  #AUTHOR_TAG, and sentiment tagging of words and word senses  #AUTHOR_TAG b ).', 'another closely related work is our own previously proposed method for leveraging on resources available for english to construct resources for a second language  #TAUTHOR_TAG.', 'that method assumed the availability of a bridge between languages, such as a bilingual lexicon or a parallel corpus.', 'instead, in the method proposed here, we rely exclusively on language - specific resources, and do not make use of any such bilingual resources which may not always be available']",4
"[') objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence']","[') objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence']","['% ) objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence - level subjectivity classification results are shown in']","['discussions. the agreement of the two annotators is 0. 83 % ( κ = 0. 67 ) ; when the uncertain annotations are removed, the agreement rises to', '0. 89 ( κ = 0. 77 ). the two annotators reached consensus on all sentences for which they disagreed, resulting in', 'a gold standard dataset with 272 ( 54 % ) subjective sentences and 232 ( 46 % ) objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence - level subjectivity classification results are shown in table 2. by using the extracted lexicon alone, we were able to obtain a rule - based subjectivity classifier with an', '']",4
"[') objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence']","[') objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence']","['% ) objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence - level subjectivity classification results are shown in']","['discussions. the agreement of the two annotators is 0. 83 % ( κ = 0. 67 ) ; when the uncertain annotations are removed, the agreement rises to', '0. 89 ( κ = 0. 77 ). the two annotators reached consensus on all sentences for which they disagreed, resulting in', 'a gold standard dataset with 272 ( 54 % ) subjective sentences and 232 ( 46 % ) objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence - level subjectivity classification results are shown in table 2. by using the extracted lexicon alone, we were able to obtain a rule - based subjectivity classifier with an', '']",5
"[') objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence']","[') objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence']","['% ) objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence - level subjectivity classification results are shown in']","['discussions. the agreement of the two annotators is 0. 83 % ( κ = 0. 67 ) ; when the uncertain annotations are removed, the agreement rises to', '0. 89 ( κ = 0. 77 ). the two annotators reached consensus on all sentences for which they disagreed, resulting in', 'a gold standard dataset with 272 ( 54 % ) subjective sentences and 232 ( 46 % ) objective sentences. more details about this data set are available', 'in  #TAUTHOR_TAG. the sentence - level subjectivity classification results are shown in table 2. by using the extracted lexicon alone, we were able to obtain a rule - based subjectivity classifier with an', '']",7
"['the added paths, instead of the unlexicalized labels,  #TAUTHOR_TAG augmented']","['the expressivity of the added paths, instead of the unlexicalized labels,  #TAUTHOR_TAG augmented']","['the added paths, instead of the unlexicalized labels,  #TAUTHOR_TAG augmented the kb graph with verbs ( surface relations )', 'from a corpus containing over 600']","['', 'external corpus. to improve the expressivity of the added paths, instead of the unlexicalized labels,  #TAUTHOR_TAG augmented the kb graph with verbs ( surface relations )', 'from a corpus containing over 600 million subject - verb - object ( svo ) triples. these verbs act as', '']",0
"['the added paths, instead of the unlexicalized labels,  #TAUTHOR_TAG augmented']","['the expressivity of the added paths, instead of the unlexicalized labels,  #TAUTHOR_TAG augmented']","['the added paths, instead of the unlexicalized labels,  #TAUTHOR_TAG augmented the kb graph with verbs ( surface relations )', 'from a corpus containing over 600']","['', 'external corpus. to improve the expressivity of the added paths, instead of the unlexicalized labels,  #TAUTHOR_TAG augmented the kb graph with verbs ( surface relations )', 'from a corpus containing over 600 million subject - verb - object ( svo ) triples. these verbs act as', '']",0
['was explored in  #TAUTHOR_TAG'],['was explored in  #TAUTHOR_TAG'],"['performing pca on the surface relations was explored in  #TAUTHOR_TAG.', '']","['', 'augmenting the kb for improving pra inference using surface relations mined from an external corpus and using latent edge labels obtained by performing pca on the surface relations was explored in  #TAUTHOR_TAG.', ""instead of hard mapping of surface relations to latent embeddings,  #AUTHOR_TAG perform a'soft'mapping using vector space random walks."", 'this allows the random walker to traverse an edge semantically similar to the current edge type more frequently than other edges.', 'although, like others, we too use an external corpus to augment the kb, the crucial difference in our approach is that apart from adding surface relations, we also add bridging entities that enable us to create new paths in the kb.', '']",0
"['and  #AUTHOR_TAG respectively, where']","['and  #AUTHOR_TAG respectively, where']","['and  #AUTHOR_TAG respectively, where the kb graph is augmented with edges']","['and  #AUTHOR_TAG respectively, where the kb graph is augmented with edges mined from a large subject - verb - object ( svo ) triple corpus.', 'in these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting.', 'in contrast, pra - oda, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on - demand manner']",0
"['and  #AUTHOR_TAG respectively, where']","['and  #AUTHOR_TAG respectively, where']","['and  #AUTHOR_TAG respectively, where the kb graph is augmented with edges']","['and  #AUTHOR_TAG respectively, where the kb graph is augmented with edges mined from a large subject - verb - object ( svo ) triple corpus.', 'in these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting.', 'in contrast, pra - oda, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on - demand manner']",1
"['as used in  #TAUTHOR_TAG, viz.,', '']","['same values as used in  #TAUTHOR_TAG, viz.,', '']","['as used in  #TAUTHOR_TAG, viz.,', 'l 1 = 0. 005, and l 2 = 1. 0. this is because the parameters were reported to be robust,', '']","['', '##tion ). the hyperparameter values d max = 2,', 'k = 10 reported the highest mrr and were used for the rest of the relations. for the l 1 and l 2 regularization parameters in the logistic regression classifier, we used', 'the same values as used in  #TAUTHOR_TAG, viz.,', 'l 1 = 0. 005, and l 2 = 1. 0. this is because the parameters were reported to be robust,', 'and seemed to work well even when the knowledge base was augmented. we compare the results ( pra - oda ) with the pra algorithm executed on the nell kb, nell kb augmented with surface relations  #TAUTHOR_TAG and vector', 'space random walk pra ( pra - vs )  #AUTHOR_TAG. the run times, i. e, the time taken to perform', 'an entire experiment for  #TAUTHOR_TAG and pra - vs includes the time taken to augment nell kb with svo edges. the pra - vs runtime also includes the time taken for generating embeddings to perform the vector space random', 'walk. as can be seen from table 2 and table 3, our scheme, pra - oda, provides performance equivalent to pra - vs with faster running time ( speed up of', '']",5
"['as used in  #TAUTHOR_TAG, viz.,', '']","['same values as used in  #TAUTHOR_TAG, viz.,', '']","['as used in  #TAUTHOR_TAG, viz.,', 'l 1 = 0. 005, and l 2 = 1. 0. this is because the parameters were reported to be robust,', '']","['', '##tion ). the hyperparameter values d max = 2,', 'k = 10 reported the highest mrr and were used for the rest of the relations. for the l 1 and l 2 regularization parameters in the logistic regression classifier, we used', 'the same values as used in  #TAUTHOR_TAG, viz.,', 'l 1 = 0. 005, and l 2 = 1. 0. this is because the parameters were reported to be robust,', 'and seemed to work well even when the knowledge base was augmented. we compare the results ( pra - oda ) with the pra algorithm executed on the nell kb, nell kb augmented with surface relations  #TAUTHOR_TAG and vector', 'space random walk pra ( pra - vs )  #AUTHOR_TAG. the run times, i. e, the time taken to perform', 'an entire experiment for  #TAUTHOR_TAG and pra - vs includes the time taken to augment nell kb with svo edges. the pra - vs runtime also includes the time taken for generating embeddings to perform the vector space random', 'walk. as can be seen from table 2 and table 3, our scheme, pra - oda, provides performance equivalent to pra - vs with faster running time ( speed up of', '']",3
"['as used in  #TAUTHOR_TAG, viz.,', '']","['same values as used in  #TAUTHOR_TAG, viz.,', '']","['as used in  #TAUTHOR_TAG, viz.,', 'l 1 = 0. 005, and l 2 = 1. 0. this is because the parameters were reported to be robust,', '']","['', '##tion ). the hyperparameter values d max = 2,', 'k = 10 reported the highest mrr and were used for the rest of the relations. for the l 1 and l 2 regularization parameters in the logistic regression classifier, we used', 'the same values as used in  #TAUTHOR_TAG, viz.,', 'l 1 = 0. 005, and l 2 = 1. 0. this is because the parameters were reported to be robust,', 'and seemed to work well even when the knowledge base was augmented. we compare the results ( pra - oda ) with the pra algorithm executed on the nell kb, nell kb augmented with surface relations  #TAUTHOR_TAG and vector', 'space random walk pra ( pra - vs )  #AUTHOR_TAG. the run times, i. e, the time taken to perform', 'an entire experiment for  #TAUTHOR_TAG and pra - vs includes the time taken to augment nell kb with svo edges. the pra - vs runtime also includes the time taken for generating embeddings to perform the vector space random', 'walk. as can be seen from table 2 and table 3, our scheme, pra - oda, provides performance equivalent to pra - vs with faster running time ( speed up of', '']",7
['representations by optimizing a ranking cost function  #TAUTHOR_TAG or by aligning'],['representations by optimizing a ranking cost function  #TAUTHOR_TAG or by aligning'],['representations by optimizing a ranking cost function  #TAUTHOR_TAG or by aligning image regions ( objects ) and segments of the description  #AUTHOR_TAG in'],"['recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images.', 'examples include text - based image retrieval, image description and visual question answering.', 'an increasing number of large image description datasets has become available  #AUTHOR_TAG and various systems have been proposed to handle the image description task as a generation problem  #AUTHOR_TAG.', 'there has also been a great deal of work on sentence - based image search or cross - modal retrieval where the objective is to learn a joint space for images and text  #AUTHOR_TAG.', 'previous work on image description generation or learning a joint space for images and text has mostly focused on english due to the availability of english datasets.', 'recently there have been attempts to create image descriptions and models for other languages  #AUTHOR_TAG.', 'most work on learning a joint space for images and their descriptions is based on canonical correlation analysis ( cca ) or neural variants of cca over representations of image and its descriptions  #AUTHOR_TAG.', 'besides cca, a few others learn a visual - semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function  #TAUTHOR_TAG or by aligning image regions ( objects ) and segments of the description  #AUTHOR_TAG in a common space.', '']",0
['models of  #AUTHOR_TAG and  #TAUTHOR_TAG and'],['models of  #AUTHOR_TAG and  #TAUTHOR_TAG and'],['of  #AUTHOR_TAG and  #TAUTHOR_TAG and'],"['', 'we report results for both english and german descriptions.', 'note that we have one single model for both languages.', 'in tables 1 and 2 we present the ranking results of the baseline models of  #AUTHOR_TAG and  #TAUTHOR_TAG and our proposed pivot and parallel models.', 'we do not compare our image - description ranking results with  #AUTHOR_TAG since they report results on half of validation set of multi30k whereas our results are on the publicly available test set of multi30k.', 'for english, pivot with asymmetric similarity is either competitive or better than monolingual models 4 https : / / github. com / ivendrov / order - embedding and symmetric similarity, especially in the r @ 10 category it obtains state - of - the - art.', 'for german, both pivot and parallel with the asymmetric scoring function outperform monolingual models and symmetric similarity.', 'we also observe that the german ranking experiments benefit the most from the multilingual signal.', 'a reason for this could be that the german description corpus has many singleton words ( more than 50 % of the vocabulary ) and english description mapping might have helped in learning better semantic embeddings.', 'these results suggest that the multilingual signal could be used to learn better multimodal embeddings, irrespective of the language.', 'our results also show that the asymmetric scoring function can help learn better embeddings.', 'in table 3 we present a few examples where pivot - asym and parallel - asym models performed better on both the languages compared to baseline order embedding model even using descriptions of very different lengths as queries']",0
"['for the latter we use the metric of  #TAUTHOR_TAG which is useful for learning embeddings that maintain an order,']","['for the latter we use the metric of  #TAUTHOR_TAG which is useful for learning embeddings that maintain an order, e. g., dog and cat are more closer to pet than animal while being distinct.', 'such ordering is shown to be useful in building effective multimodal space of images and texts.', 'an analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each ( which is useful']","['for the latter we use the metric of  #TAUTHOR_TAG which is useful for learning embeddings that maintain an order, e. g., dog and cat are more closer to pet than animal while being distinct.', 'such ordering is shown to be useful in building effective multimodal space of images and texts.', 'an analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each ( which is useful']","['both pivot and parallel we use a deep convolutional neural network architecture ( cnn ) to represent the image i denoted by f i ( i ) = w i · cnn ( i ) where w i is a learned weight matrix and cnn ( i ) is the image vector representation.', 'for each language we define a recurrent neural network encoder f c ( c k ) = gru ( c k ) with gated recurrent units ( gru ) activations to encode the description c k.', 'in pivot, we use monolingual corpora from multiple languages of sentences aligned with images to learn the joint space.', 'the intuition of this model is that an image is a universal representation across all languages, and if we constrain a sentence representation to be closer to image, sentences in different languages may also come closer.', 'accordingly we design a loss function as follows :', 'where k stands for each language.', 'this loss function encourages the similarity s ( c k, i ) between gold - standard description c k and image i to be greater than any other irrelevant description c k by a margin α.', 'a similar loss function is useful for learning multimodal embeddings in a single language  #AUTHOR_TAG.', 'for each minibatch, we obtain invalid descriptions by selecting descriptions of other images except the current image of interest and vice - versa.', 'in parallel, in addition to making an image similar to a description, we make multiple descriptions of the same image in different languages similar to each other, based on the assumption that these descriptions, although not parallel, share some commonalities.', 'accordingly we enhance the previous loss function with an additional term :', 'note that we are iterating over all pairs of descriptions ( c 1, c 2 ), and maximizing the similarity between descriptions of the same image and at the same time minimizing the similarity between descriptions of different images.', 'we learn models using two similarity functions : symmetric and asymmetric.', 'for the former we use cosine similarity and for the latter we use the metric of  #TAUTHOR_TAG which is useful for learning embeddings that maintain an order, e. g., dog and cat are more closer to pet than animal while being distinct.', 'such ordering is shown to be useful in building effective multimodal space of images and texts.', 'an analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each ( which is useful when sentences describe two different aspects of the image ).', 'the similarity metric is defined as :', 'where a and b are embeddings of image and description.', 'we call the symmetric similarity variants of our models as pivot - sym and parallel -']",5
"['', 'following  #TAUTHOR_TAG we']","['descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we']","['descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we set']","['test our model on the tasks of imagedescription ranking and semantic textual similarity.', 'we work with each language separately.', 'since we learn embeddings for images and languages in the same semantic space, our hope is that the training data for each modality or language acts complementary data for the another modality or language, and thus helps us learn better embeddings.', 'experiment setup we sampled minibatches of size 64 images and their descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we set the dimensionality of the embedding space and the gru hidden layer n to 1024 for both english and german.', 'we set the dimensionality of the learned word embeddings to 300 for both languages, and the margin α to 0. 05 and 0. 2, respectively, to learn asymmetric and symmetric similarity - based embeddings.', '1 we keep all hyperparameters constant across all models.', 'we used the l2 norm to mitigate over - fitting  #AUTHOR_TAG.', 'we tokenize and truecase both english and german descriptions using the moses decoder scripts.', '']",5
"['', 'following  #TAUTHOR_TAG we']","['descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we']","['descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we set']","['test our model on the tasks of imagedescription ranking and semantic textual similarity.', 'we work with each language separately.', 'since we learn embeddings for images and languages in the same semantic space, our hope is that the training data for each modality or language acts complementary data for the another modality or language, and thus helps us learn better embeddings.', 'experiment setup we sampled minibatches of size 64 images and their descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we set the dimensionality of the embedding space and the gru hidden layer n to 1024 for both english and german.', 'we set the dimensionality of the learned word embeddings to 300 for both languages, and the margin α to 0. 05 and 0. 2, respectively, to learn asymmetric and symmetric similarity - based embeddings.', '1 we keep all hyperparameters constant across all models.', 'we used the l2 norm to mitigate over - fitting  #AUTHOR_TAG.', 'we tokenize and truecase both english and german descriptions using the moses decoder scripts.', '']",5
"['for the latter we use the metric of  #TAUTHOR_TAG which is useful for learning embeddings that maintain an order,']","['for the latter we use the metric of  #TAUTHOR_TAG which is useful for learning embeddings that maintain an order, e. g., dog and cat are more closer to pet than animal while being distinct.', 'such ordering is shown to be useful in building effective multimodal space of images and texts.', 'an analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each ( which is useful']","['for the latter we use the metric of  #TAUTHOR_TAG which is useful for learning embeddings that maintain an order, e. g., dog and cat are more closer to pet than animal while being distinct.', 'such ordering is shown to be useful in building effective multimodal space of images and texts.', 'an analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each ( which is useful']","['both pivot and parallel we use a deep convolutional neural network architecture ( cnn ) to represent the image i denoted by f i ( i ) = w i · cnn ( i ) where w i is a learned weight matrix and cnn ( i ) is the image vector representation.', 'for each language we define a recurrent neural network encoder f c ( c k ) = gru ( c k ) with gated recurrent units ( gru ) activations to encode the description c k.', 'in pivot, we use monolingual corpora from multiple languages of sentences aligned with images to learn the joint space.', 'the intuition of this model is that an image is a universal representation across all languages, and if we constrain a sentence representation to be closer to image, sentences in different languages may also come closer.', 'accordingly we design a loss function as follows :', 'where k stands for each language.', 'this loss function encourages the similarity s ( c k, i ) between gold - standard description c k and image i to be greater than any other irrelevant description c k by a margin α.', 'a similar loss function is useful for learning multimodal embeddings in a single language  #AUTHOR_TAG.', 'for each minibatch, we obtain invalid descriptions by selecting descriptions of other images except the current image of interest and vice - versa.', 'in parallel, in addition to making an image similar to a description, we make multiple descriptions of the same image in different languages similar to each other, based on the assumption that these descriptions, although not parallel, share some commonalities.', 'accordingly we enhance the previous loss function with an additional term :', 'note that we are iterating over all pairs of descriptions ( c 1, c 2 ), and maximizing the similarity between descriptions of the same image and at the same time minimizing the similarity between descriptions of different images.', 'we learn models using two similarity functions : symmetric and asymmetric.', 'for the former we use cosine similarity and for the latter we use the metric of  #TAUTHOR_TAG which is useful for learning embeddings that maintain an order, e. g., dog and cat are more closer to pet than animal while being distinct.', 'such ordering is shown to be useful in building effective multimodal space of images and texts.', 'an analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each ( which is useful when sentences describe two different aspects of the image ).', 'the similarity metric is defined as :', 'where a and b are embeddings of image and description.', 'we call the symmetric similarity variants of our models as pivot - sym and parallel -']",3
"['', 'following  #TAUTHOR_TAG we']","['descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we']","['descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we set']","['test our model on the tasks of imagedescription ranking and semantic textual similarity.', 'we work with each language separately.', 'since we learn embeddings for images and languages in the same semantic space, our hope is that the training data for each modality or language acts complementary data for the another modality or language, and thus helps us learn better embeddings.', 'experiment setup we sampled minibatches of size 64 images and their descriptions, and drew all negative samples from the minibatch.', 'we trained using the adam optimizer with learning rate 0. 001, and early stopping on the validation set.', 'following  #TAUTHOR_TAG we set the dimensionality of the embedding space and the gru hidden layer n to 1024 for both english and german.', 'we set the dimensionality of the learned word embeddings to 300 for both languages, and the margin α to 0. 05 and 0. 2, respectively, to learn asymmetric and symmetric similarity - based embeddings.', '1 we keep all hyperparameters constant across all models.', 'we used the l2 norm to mitigate over - fitting  #AUTHOR_TAG.', 'we tokenize and truecase both english and german descriptions using the moses decoder scripts.', '']",3
[' #TAUTHOR_TAG vg'],"[' #TAUTHOR_TAG vgg19 82.', 'tences ( image descriptions in this case ).', 'we evaluate on video task from']","['. 7 79. 7 vse  #AUTHOR_TAG vgg19 80. 6 82. 7 89. 6 oe  #TAUTHOR_TAG vgg19 82.', 'tences ( image descriptions in this case ).', 'we evaluate on video task from']","['the semantic textual similarity task ( sts ), we use the textual embeddings from our model to compute the similarity between a pair of sen -  #AUTHOR_TAG − 83. 7 84. 5 85. 0 mlmme  #AUTHOR_TAG vgg19 − 72. 7 79. 7 vse  #AUTHOR_TAG vgg19 80. 6 82. 7 89. 6 oe  #TAUTHOR_TAG vgg19 82.', 'tences ( image descriptions in this case ).', '']",3
[' #TAUTHOR_TAG vg'],"[' #TAUTHOR_TAG vgg19 82.', 'tences ( image descriptions in this case ).', 'we evaluate on video task from']","['. 7 79. 7 vse  #AUTHOR_TAG vgg19 80. 6 82. 7 89. 6 oe  #TAUTHOR_TAG vgg19 82.', 'tences ( image descriptions in this case ).', 'we evaluate on video task from']","['the semantic textual similarity task ( sts ), we use the textual embeddings from our model to compute the similarity between a pair of sen -  #AUTHOR_TAG − 83. 7 84. 5 85. 0 mlmme  #AUTHOR_TAG vgg19 − 72. 7 79. 7 vse  #AUTHOR_TAG vgg19 80. 6 82. 7 89. 6 oe  #TAUTHOR_TAG vgg19 82.', 'tences ( image descriptions in this case ).', '']",3
"['through questions  #TAUTHOR_TAG, c ) and']","['through questions  #TAUTHOR_TAG, c ) and']","['use a single reference summary, either directly  #AUTHOR_TAG or through questions  #TAUTHOR_TAG, c ) and']","['all salient information can fit in a given summary length, while even extractive methods that select complete sentences are not guaranteed to produce a coherent summary overall. the most consistently used evaluation approach is comparison of the summaries produces against reference summaries via automatic measures such as rouge  #AUTHOR_TAG and its variants.', 'however, automatic measures are unlikely to be sufficient to measure performance in summarization  #AUTHOR_TAG, also known for', 'other tasks in which the goal is to generate natural language  #AUTHOR_TAG. furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against', 'them is likely to exhibit reference bias  #AUTHOR_TAG, penalizing summaries containing salient content different from the reference. for the above reasons manual evaluation is considered necessary for measuring progress in summarization. however, the intrinsic difficulty of the task has led', 'to research without manual evaluation or only fluency being assessed manually. those that conduct manual assessment of the content, typically use a single reference summary, either directly  #AUTHOR_TAG or through questions  #TAUTHOR_TAG, c ) and thus are also likely to', '']",0
"['through questions  #TAUTHOR_TAG, c ) and']","['through questions  #TAUTHOR_TAG, c ) and']","['use a single reference summary, either directly  #AUTHOR_TAG or through questions  #TAUTHOR_TAG, c ) and']","['all salient information can fit in a given summary length, while even extractive methods that select complete sentences are not guaranteed to produce a coherent summary overall. the most consistently used evaluation approach is comparison of the summaries produces against reference summaries via automatic measures such as rouge  #AUTHOR_TAG and its variants.', 'however, automatic measures are unlikely to be sufficient to measure performance in summarization  #AUTHOR_TAG, also known for', 'other tasks in which the goal is to generate natural language  #AUTHOR_TAG. furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against', 'them is likely to exhibit reference bias  #AUTHOR_TAG, penalizing summaries containing salient content different from the reference. for the above reasons manual evaluation is considered necessary for measuring progress in summarization. however, the intrinsic difficulty of the task has led', 'to research without manual evaluation or only fluency being assessed manually. those that conduct manual assessment of the content, typically use a single reference summary, either directly  #AUTHOR_TAG or through questions  #TAUTHOR_TAG, c ) and thus are also likely to', '']",0
"['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","[' #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in']","['annotators. finally, a small number of work evaluates the "" correctness ""', ' #AUTHOR_TAG b ;  #AUTHOR_TAG of the summary, similar to fact checking  #AUTHOR_TAG, which can be a challenging task in its own right. the', 'linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, format', '##ting, naturalness and coherence. most recent work uses a single human judgment to capture all linguistic qualities of the summary  #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in table 1 with an exception of "" clarity "" which was evaluated in the duc evaluation campaigns  #AUTHOR_TAG. the "" clarity "" metric puts emphasis in easy identification of noun and pronoun phrases in the summary which is a different', 'dimension than "" fluency "", as a summary may be fluent but difficult to be understood due to poor clarity. absolute vs relative summary ranking. in relative assessment of summar', '##ization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the relative assessment is often done using the paired comparison  #AUTHOR_TAG or the best - worst scaling ( woodworth and g, 1991 ;  #AUTHOR_TAG, to improve inter - annotator agreement. on the other hand, absolute assessment of summarization  #AUTHOR_TAG b ;  #AUTHOR_TAG kryscinski et al., 2018 ;  #AUTHOR_TAG is often done using the likert rating scale  #AUTHOR_TAG where', 'a summary is assessed on a numerical scale. absolute assessment was also employed in combination with the question answering approach for content evaluation  #TAUTHOR_TAG. both approaches', ', relative ranking and absolute assessment, have been investigated extensively in machine translation  #AUTHOR_TAG ( bojar et al.,, 2017. absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models  #AUTHOR_TAG. choice of reference. the most convenient way to evaluate a system summary is to assess it against the reference summary  #AUTHOR_TAG, as this typically requires less effort than reading the source document. the question answering approach of  #AUTHOR_TAG b, c ) also falls in this category, as the questions were written using the reference summary. however, summarization datasets are limited to a single reference summary per document  #TAUTHOR_TAG thus evaluations using them is prone to reference bias', ' #AUTHOR_TAG, also a known issue in machine', 'translation evaluation  #AUTHOR_TAG. a circumvention for this issue is to evaluate it against the source document  #AUTHOR_TAG a ;  #AUTHOR_TAG kryscinski et al., 2018 ), asking judges to assess the summary', 'after reading the source document. however this requires more effort and is known to lead to low inter - annotator agreement  #AUTHOR_TAG']",0
"['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","[' #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in']","['annotators. finally, a small number of work evaluates the "" correctness ""', ' #AUTHOR_TAG b ;  #AUTHOR_TAG of the summary, similar to fact checking  #AUTHOR_TAG, which can be a challenging task in its own right. the', 'linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, format', '##ting, naturalness and coherence. most recent work uses a single human judgment to capture all linguistic qualities of the summary  #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in table 1 with an exception of "" clarity "" which was evaluated in the duc evaluation campaigns  #AUTHOR_TAG. the "" clarity "" metric puts emphasis in easy identification of noun and pronoun phrases in the summary which is a different', 'dimension than "" fluency "", as a summary may be fluent but difficult to be understood due to poor clarity. absolute vs relative summary ranking. in relative assessment of summar', '##ization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the relative assessment is often done using the paired comparison  #AUTHOR_TAG or the best - worst scaling ( woodworth and g, 1991 ;  #AUTHOR_TAG, to improve inter - annotator agreement. on the other hand, absolute assessment of summarization  #AUTHOR_TAG b ;  #AUTHOR_TAG kryscinski et al., 2018 ;  #AUTHOR_TAG is often done using the likert rating scale  #AUTHOR_TAG where', 'a summary is assessed on a numerical scale. absolute assessment was also employed in combination with the question answering approach for content evaluation  #TAUTHOR_TAG. both approaches', ', relative ranking and absolute assessment, have been investigated extensively in machine translation  #AUTHOR_TAG ( bojar et al.,, 2017. absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models  #AUTHOR_TAG. choice of reference. the most convenient way to evaluate a system summary is to assess it against the reference summary  #AUTHOR_TAG, as this typically requires less effort than reading the source document. the question answering approach of  #AUTHOR_TAG b, c ) also falls in this category, as the questions were written using the reference summary. however, summarization datasets are limited to a single reference summary per document  #TAUTHOR_TAG thus evaluations using them is prone to reference bias', ' #AUTHOR_TAG, also a known issue in machine', 'translation evaluation  #AUTHOR_TAG. a circumvention for this issue is to evaluate it against the source document  #AUTHOR_TAG a ;  #AUTHOR_TAG kryscinski et al., 2018 ), asking judges to assess the summary', 'after reading the source document. however this requires more effort and is known to lead to low inter - annotator agreement  #AUTHOR_TAG']",0
"['through questions  #TAUTHOR_TAG, c ) and']","['through questions  #TAUTHOR_TAG, c ) and']","['use a single reference summary, either directly  #AUTHOR_TAG or through questions  #TAUTHOR_TAG, c ) and']","['all salient information can fit in a given summary length, while even extractive methods that select complete sentences are not guaranteed to produce a coherent summary overall. the most consistently used evaluation approach is comparison of the summaries produces against reference summaries via automatic measures such as rouge  #AUTHOR_TAG and its variants.', 'however, automatic measures are unlikely to be sufficient to measure performance in summarization  #AUTHOR_TAG, also known for', 'other tasks in which the goal is to generate natural language  #AUTHOR_TAG. furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against', 'them is likely to exhibit reference bias  #AUTHOR_TAG, penalizing summaries containing salient content different from the reference. for the above reasons manual evaluation is considered necessary for measuring progress in summarization. however, the intrinsic difficulty of the task has led', 'to research without manual evaluation or only fluency being assessed manually. those that conduct manual assessment of the content, typically use a single reference summary, either directly  #AUTHOR_TAG or through questions  #TAUTHOR_TAG, c ) and thus are also likely to', '']",4
"['through questions  #TAUTHOR_TAG, c ) and']","['through questions  #TAUTHOR_TAG, c ) and']","['use a single reference summary, either directly  #AUTHOR_TAG or through questions  #TAUTHOR_TAG, c ) and']","['all salient information can fit in a given summary length, while even extractive methods that select complete sentences are not guaranteed to produce a coherent summary overall. the most consistently used evaluation approach is comparison of the summaries produces against reference summaries via automatic measures such as rouge  #AUTHOR_TAG and its variants.', 'however, automatic measures are unlikely to be sufficient to measure performance in summarization  #AUTHOR_TAG, also known for', 'other tasks in which the goal is to generate natural language  #AUTHOR_TAG. furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against', 'them is likely to exhibit reference bias  #AUTHOR_TAG, penalizing summaries containing salient content different from the reference. for the above reasons manual evaluation is considered necessary for measuring progress in summarization. however, the intrinsic difficulty of the task has led', 'to research without manual evaluation or only fluency being assessed manually. those that conduct manual assessment of the content, typically use a single reference summary, either directly  #AUTHOR_TAG or through questions  #TAUTHOR_TAG, c ) and thus are also likely to', '']",5
"['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","[' #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in']","['annotators. finally, a small number of work evaluates the "" correctness ""', ' #AUTHOR_TAG b ;  #AUTHOR_TAG of the summary, similar to fact checking  #AUTHOR_TAG, which can be a challenging task in its own right. the', 'linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, format', '##ting, naturalness and coherence. most recent work uses a single human judgment to capture all linguistic qualities of the summary  #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in table 1 with an exception of "" clarity "" which was evaluated in the duc evaluation campaigns  #AUTHOR_TAG. the "" clarity "" metric puts emphasis in easy identification of noun and pronoun phrases in the summary which is a different', 'dimension than "" fluency "", as a summary may be fluent but difficult to be understood due to poor clarity. absolute vs relative summary ranking. in relative assessment of summar', '##ization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the relative assessment is often done using the paired comparison  #AUTHOR_TAG or the best - worst scaling ( woodworth and g, 1991 ;  #AUTHOR_TAG, to improve inter - annotator agreement. on the other hand, absolute assessment of summarization  #AUTHOR_TAG b ;  #AUTHOR_TAG kryscinski et al., 2018 ;  #AUTHOR_TAG is often done using the likert rating scale  #AUTHOR_TAG where', 'a summary is assessed on a numerical scale. absolute assessment was also employed in combination with the question answering approach for content evaluation  #TAUTHOR_TAG. both approaches', ', relative ranking and absolute assessment, have been investigated extensively in machine translation  #AUTHOR_TAG ( bojar et al.,, 2017. absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models  #AUTHOR_TAG. choice of reference. the most convenient way to evaluate a system summary is to assess it against the reference summary  #AUTHOR_TAG, as this typically requires less effort than reading the source document. the question answering approach of  #AUTHOR_TAG b, c ) also falls in this category, as the questions were written using the reference summary. however, summarization datasets are limited to a single reference summary per document  #TAUTHOR_TAG thus evaluations using them is prone to reference bias', ' #AUTHOR_TAG, also a known issue in machine', 'translation evaluation  #AUTHOR_TAG. a circumvention for this issue is to evaluate it against the source document  #AUTHOR_TAG a ;  #AUTHOR_TAG kryscinski et al., 2018 ), asking judges to assess the summary', 'after reading the source document. however this requires more effort and is known to lead to low inter - annotator agreement  #AUTHOR_TAG']",5
"['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","[' #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in']","['annotators. finally, a small number of work evaluates the "" correctness ""', ' #AUTHOR_TAG b ;  #AUTHOR_TAG of the summary, similar to fact checking  #AUTHOR_TAG, which can be a challenging task in its own right. the', 'linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, format', '##ting, naturalness and coherence. most recent work uses a single human judgment to capture all linguistic qualities of the summary  #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in table 1 with an exception of "" clarity "" which was evaluated in the duc evaluation campaigns  #AUTHOR_TAG. the "" clarity "" metric puts emphasis in easy identification of noun and pronoun phrases in the summary which is a different', 'dimension than "" fluency "", as a summary may be fluent but difficult to be understood due to poor clarity. absolute vs relative summary ranking. in relative assessment of summar', '##ization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the relative assessment is often done using the paired comparison  #AUTHOR_TAG or the best - worst scaling ( woodworth and g, 1991 ;  #AUTHOR_TAG, to improve inter - annotator agreement. on the other hand, absolute assessment of summarization  #AUTHOR_TAG b ;  #AUTHOR_TAG kryscinski et al., 2018 ;  #AUTHOR_TAG is often done using the likert rating scale  #AUTHOR_TAG where', 'a summary is assessed on a numerical scale. absolute assessment was also employed in combination with the question answering approach for content evaluation  #TAUTHOR_TAG. both approaches', ', relative ranking and absolute assessment, have been investigated extensively in machine translation  #AUTHOR_TAG ( bojar et al.,, 2017. absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models  #AUTHOR_TAG. choice of reference. the most convenient way to evaluate a system summary is to assess it against the reference summary  #AUTHOR_TAG, as this typically requires less effort than reading the source document. the question answering approach of  #AUTHOR_TAG b, c ) also falls in this category, as the questions were written using the reference summary. however, summarization datasets are limited to a single reference summary per document  #TAUTHOR_TAG thus evaluations using them is prone to reference bias', ' #AUTHOR_TAG, also a known issue in machine', 'translation evaluation  #AUTHOR_TAG. a circumvention for this issue is to evaluate it against the source document  #AUTHOR_TAG a ;  #AUTHOR_TAG kryscinski et al., 2018 ), asking judges to assess the summary', 'after reading the source document. however this requires more effort and is known to lead to low inter - annotator agreement  #AUTHOR_TAG']",5
"[',  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","['( xsum,  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","[',  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","['use the extreme summarization dataset ( xsum,  #TAUTHOR_TAG 2 which comprises bbc articles paired with their singlesentence summaries, provided by the journalists writing the articles.', 'the summary in the xsum dataset demonstrates a larger number of novel ngrams compared to other popular datasets such as cnn / dailymail  #AUTHOR_TAG or ny times  #AUTHOR_TAG as such it is suitable to be used for our experiment since the more abstractive nature of the summary renders automatic methods such as rouge less accurate as they rely on string matching, and thus calls for human evaluation for more accurate system comparisons.', ""following  #TAUTHOR_TAG, we didn't use the whole test set portion, but sampled 50 articles from it for our highlight - based evaluation."", ""we assessed summaries from two state - ofthe - art abstractive summarization systems using our highlight - based evaluation : ( i ) the pointergenerator model ( ptgen ) introduced by  #AUTHOR_TAG is an rnn - based abstractive systems which allows to copy words from the source text, and ( ii ) the topic - aware convolutional sequence to sequence model ( tconvs2s ) introduced by  #TAUTHOR_TAG is an abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks."", 'we used the pre - trained models 3 provided by the authors to obtain summaries from both systems for the documents in our test set']",5
"[',  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","['( xsum,  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","[',  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","['use the extreme summarization dataset ( xsum,  #TAUTHOR_TAG 2 which comprises bbc articles paired with their singlesentence summaries, provided by the journalists writing the articles.', 'the summary in the xsum dataset demonstrates a larger number of novel ngrams compared to other popular datasets such as cnn / dailymail  #AUTHOR_TAG or ny times  #AUTHOR_TAG as such it is suitable to be used for our experiment since the more abstractive nature of the summary renders automatic methods such as rouge less accurate as they rely on string matching, and thus calls for human evaluation for more accurate system comparisons.', ""following  #TAUTHOR_TAG, we didn't use the whole test set portion, but sampled 50 articles from it for our highlight - based evaluation."", ""we assessed summaries from two state - ofthe - art abstractive summarization systems using our highlight - based evaluation : ( i ) the pointergenerator model ( ptgen ) introduced by  #AUTHOR_TAG is an rnn - based abstractive systems which allows to copy words from the source text, and ( ii ) the topic - aware convolutional sequence to sequence model ( tconvs2s ) introduced by  #TAUTHOR_TAG is an abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks."", 'we used the pre - trained models 3 provided by the authors to obtain summaries from both systems for the documents in our test set']",5
"[',  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","['( xsum,  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","[',  #TAUTHOR_TAG 2 which comprises bbc articles paired with']","['use the extreme summarization dataset ( xsum,  #TAUTHOR_TAG 2 which comprises bbc articles paired with their singlesentence summaries, provided by the journalists writing the articles.', 'the summary in the xsum dataset demonstrates a larger number of novel ngrams compared to other popular datasets such as cnn / dailymail  #AUTHOR_TAG or ny times  #AUTHOR_TAG as such it is suitable to be used for our experiment since the more abstractive nature of the summary renders automatic methods such as rouge less accurate as they rely on string matching, and thus calls for human evaluation for more accurate system comparisons.', ""following  #TAUTHOR_TAG, we didn't use the whole test set portion, but sampled 50 articles from it for our highlight - based evaluation."", ""we assessed summaries from two state - ofthe - art abstractive summarization systems using our highlight - based evaluation : ( i ) the pointergenerator model ( ptgen ) introduced by  #AUTHOR_TAG is an rnn - based abstractive systems which allows to copy words from the source text, and ( ii ) the topic - aware convolutional sequence to sequence model ( tconvs2s ) introduced by  #TAUTHOR_TAG is an abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks."", 'we used the pre - trained models 3 provided by the authors to obtain summaries from both systems for the documents in our test set']",5
"['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","[' #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency']","['., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in']","['annotators. finally, a small number of work evaluates the "" correctness ""', ' #AUTHOR_TAG b ;  #AUTHOR_TAG of the summary, similar to fact checking  #AUTHOR_TAG, which can be a challenging task in its own right. the', 'linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, format', '##ting, naturalness and coherence. most recent work uses a single human judgment to capture all linguistic qualities of the summary  #AUTHOR_TAG kryscinski', 'et al., 2018 ;  #TAUTHOR_TAG ; we group them under "" fluency "" in table 1 with an exception of "" clarity "" which was evaluated in the duc evaluation campaigns  #AUTHOR_TAG. the "" clarity "" metric puts emphasis in easy identification of noun and pronoun phrases in the summary which is a different', 'dimension than "" fluency "", as a summary may be fluent but difficult to be understood due to poor clarity. absolute vs relative summary ranking. in relative assessment of summar', '##ization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the relative assessment is often done using the paired comparison  #AUTHOR_TAG or the best - worst scaling ( woodworth and g, 1991 ;  #AUTHOR_TAG, to improve inter - annotator agreement. on the other hand, absolute assessment of summarization  #AUTHOR_TAG b ;  #AUTHOR_TAG kryscinski et al., 2018 ;  #AUTHOR_TAG is often done using the likert rating scale  #AUTHOR_TAG where', 'a summary is assessed on a numerical scale. absolute assessment was also employed in combination with the question answering approach for content evaluation  #TAUTHOR_TAG. both approaches', ', relative ranking and absolute assessment, have been investigated extensively in machine translation  #AUTHOR_TAG ( bojar et al.,, 2017. absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models  #AUTHOR_TAG. choice of reference. the most convenient way to evaluate a system summary is to assess it against the reference summary  #AUTHOR_TAG, as this typically requires less effort than reading the source document. the question answering approach of  #AUTHOR_TAG b, c ) also falls in this category, as the questions were written using the reference summary. however, summarization datasets are limited to a single reference summary per document  #TAUTHOR_TAG thus evaluations using them is prone to reference bias', ' #AUTHOR_TAG, also a known issue in machine', 'translation evaluation  #AUTHOR_TAG. a circumvention for this issue is to evaluate it against the source document  #AUTHOR_TAG a ;  #AUTHOR_TAG kryscinski et al., 2018 ), asking judges to assess the summary', 'after reading the source document. however this requires more effort and is known to lead to low inter - annotator agreement  #AUTHOR_TAG']",1
"['hr', '##ouge confirm that the reference summaries are rather abstractive as reported by  #TAUTHOR_TAG', ', and']","[', both rouge and hr', '##ouge confirm that the reference summaries are rather abstractive as reported by  #TAUTHOR_TAG', ', and']","[', both rouge and hr', '##ouge confirm that the reference summaries are rather abstractive as reported by  #TAUTHOR_TAG', ', and']","['', 'ptgen, on both measures. the pearson correlation between fluency and clarity evaluation is 0. 68', 'which shows a weak correlation ; it confirms our hypothesis that the "" clarity ""', 'captures different aspects from "" fluency "" and they should not be combined as it is commonly done. in the latter', 'case, hrouge becomes the standard rouge metric with β n g = 1 for all n - grams g. both rouge and hrouge favour method of copying content from the original document and penalizes abstractive methods, thus it is not surprising that pt', '##gen is superior to tconvs2s, as the former has an explicit copy mechanism. the fact that ptgen is better in', 'terms of hrouge is also an evidence that the copying done by pt - gen selects salient content, thus confirming that the copying mechanism works as intended. when comparing the reference summaries against the original documents, both rouge and hr', '##ouge confirm that the reference summaries are rather abstractive as reported by  #TAUTHOR_TAG', ', and they in fact score below the system summaries. recall scores are very low in all cases which is expected, since the 10 highlights obtained per document or the', 'documents themselves, taken together, are much longer than any of the summaries']",3
"['hr', '##ouge confirm that the reference summaries are rather abstractive as reported by  #TAUTHOR_TAG', ', and']","[', both rouge and hr', '##ouge confirm that the reference summaries are rather abstractive as reported by  #TAUTHOR_TAG', ', and']","[', both rouge and hr', '##ouge confirm that the reference summaries are rather abstractive as reported by  #TAUTHOR_TAG', ', and']","['', 'ptgen, on both measures. the pearson correlation between fluency and clarity evaluation is 0. 68', 'which shows a weak correlation ; it confirms our hypothesis that the "" clarity ""', 'captures different aspects from "" fluency "" and they should not be combined as it is commonly done. in the latter', 'case, hrouge becomes the standard rouge metric with β n g = 1 for all n - grams g. both rouge and hrouge favour method of copying content from the original document and penalizes abstractive methods, thus it is not surprising that pt', '##gen is superior to tconvs2s, as the former has an explicit copy mechanism. the fact that ptgen is better in', 'terms of hrouge is also an evidence that the copying done by pt - gen selects salient content, thus confirming that the copying mechanism works as intended. when comparing the reference summaries against the original documents, both rouge and hr', '##ouge confirm that the reference summaries are rather abstractive as reported by  #TAUTHOR_TAG', ', and they in fact score below the system summaries. recall scores are very low in all cases which is expected, since the 10 highlights obtained per document or the', 'documents themselves, taken together, are much longer than any of the summaries']",3
"['nlp tasks  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consequently, word embeddings have been']","['nlp tasks  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consequently, word embeddings have been']","['nlp tasks  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks']","['is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of nlp tasks  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, nlp applications.', 'however, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory.', 'for example, for 1 million words, loading 200 dimensional vectors takes up to 1. 6 gb memory on a 64 - bit system.', 'considering applications that make use of billions of tokens and multiple languages, size issues impose significant limitations on the practical use of word embeddings.', 'this paper presents the question of whether it is possible to significantly reduce the memory needs for the use and training of word embeddings.', 'specifically, we ask "" what is the impact of representing each dimension of a dense representation with significantly fewer bits than the standard 64 bits? "" moreover, we investigate the possibility of directly training dense embedding vectors using significantly fewer bits than typically used.', 'the results we present are quite surprising.', '']",0
"[' #TAUTHOR_TAG b ).', 'however, it has been proven that brown clusters as discrete features']","[' #TAUTHOR_TAG b ).', 'however, it has been proven that brown clusters as discrete features']","['generalization ability  #TAUTHOR_TAG b ).', 'however, it has been proven that brown clusters as discrete features are even']","['we consider traditional cluster encoded word representation, e. g., brown clusters  #AUTHOR_TAG, it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word.', 'in fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability  #TAUTHOR_TAG b ).', 'however, it has been proven that brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks  #AUTHOR_TAG.', 'guo et al.  #AUTHOR_TAG further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension.', 'they have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks.', 'moreover, faruqui et al.  #AUTHOR_TAG showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets.', 'these works indicate that, for some tasks, we do not need all the information encoded in "" standard "" word embeddings.', 'nonetheless, it is clear that binarization loses a lot of information, and this calls for a systematic comparison of how many bits are needed to maintain the expressivity needed from word embeddings for different tasks']",0
"['nlp tasks  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consequently, word embeddings have been']","['nlp tasks  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consequently, word embeddings have been']","['nlp tasks  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks']","['is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports better performance on a range of nlp tasks  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as features in multiple, syntactic and semantic, nlp applications.', 'however, keeping embedding vectors for hundreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory.', 'for example, for 1 million words, loading 200 dimensional vectors takes up to 1. 6 gb memory on a 64 - bit system.', 'considering applications that make use of billions of tokens and multiple languages, size issues impose significant limitations on the practical use of word embeddings.', 'this paper presents the question of whether it is possible to significantly reduce the memory needs for the use and training of word embeddings.', 'specifically, we ask "" what is the impact of representing each dimension of a dense representation with significantly fewer bits than the standard 64 bits? "" moreover, we investigate the possibility of directly training dense embedding vectors using significantly fewer bits than typically used.', 'the results we present are quite surprising.', '']",6
"[' #TAUTHOR_TAG b ).', 'however, it has been proven that brown clusters as discrete features']","[' #TAUTHOR_TAG b ).', 'however, it has been proven that brown clusters as discrete features']","['generalization ability  #TAUTHOR_TAG b ).', 'however, it has been proven that brown clusters as discrete features are even']","['we consider traditional cluster encoded word representation, e. g., brown clusters  #AUTHOR_TAG, it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word.', 'in fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability  #TAUTHOR_TAG b ).', 'however, it has been proven that brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks  #AUTHOR_TAG.', 'guo et al.  #AUTHOR_TAG further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension.', 'they have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks.', 'moreover, faruqui et al.  #AUTHOR_TAG showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets.', 'these works indicate that, for some tasks, we do not need all the information encoded in "" standard "" word embeddings.', 'nonetheless, it is clear that binarization loses a lot of information, and this calls for a systematic comparison of how many bits are needed to maintain the expressivity needed from word embeddings for different tasks']",1
"[') iteration in word2vec algorithms  #TAUTHOR_TAG b ),']","['( sgd ) iteration in word2vec algorithms  #TAUTHOR_TAG b ),']","["") iteration in word2vec algorithms  #TAUTHOR_TAG b ), the updating vector's values""]","['the memory for training word embedding is also limited, we need to modify the training algorithms by introducing new data structures to reduce the bits used to encode the values.', ""in practice, we found that in the stochastic gradient descent ( sgd ) iteration in word2vec algorithms  #TAUTHOR_TAG b ), the updating vector's values are often very small numbers ( e. g., < 10 −5 )."", 'in this case, if we directly apply the rounding method to certain precisions ( e. g., 8 bits ), the update of word vectors will always be zero.', 'for example, the 8 - bit precision is 2 −7 = 0. 0078, so 10 −5 is not significant enough to update the vector with 8 - bit values.', 'therefore, we consider the following two ways to improve this.', 'stochastic rounding.', 'we first consider using stochastic rounding  #AUTHOR_TAG to train word embedding.', 'stochastic rounding introduces some randomness into the rounding mechanism, which has been proven to be helpful when there are many parameters in the learning system, such as deep learning systems  #AUTHOR_TAG.', 'here we also introduce this approach to update word embedding vectors in sgd.', 'the probability of rounding x to x is proportional to the proximity of x to x :', '']",7
"['evaluating word embeddings.', 'we train the word embedding algorithms, word2vec  #TAUTHOR_TAG b ), based on the oct. 2013 wikipedia dump.', '1 we first compare levels of truncation of word2vec embeddings,']","['evaluating word embeddings.', 'we train the word embedding algorithms, word2vec  #TAUTHOR_TAG b ), based on the oct. 2013 wikipedia dump.', '1 we first compare levels of truncation of word2vec embeddings,']","['evaluating word embeddings.', 'we train the word embedding algorithms, word2vec  #TAUTHOR_TAG b ), based on the oct. 2013 wikipedia dump.', '1 we first compare levels of truncation of word2vec embeddings,']","['this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings.', 'we train the word embedding algorithms, word2vec  #TAUTHOR_TAG b ), based on the oct. 2013 wikipedia dump.', '1 we first compare levels of truncation of word2vec embeddings, and then evaluate the stochastic rounding and the auxiliary vectors based methods for training word2vec vectors']",5
['cbow and skipgram with negative sampling  #TAUTHOR_TAG b ) on the wikipedia dump'],"['cbow and skipgram with negative sampling  #TAUTHOR_TAG b ) on the wikipedia dump data, and set the window size of context to be five.', 'then we performed value truncation with 4 bits, 6 bits, and 8 bits.', 'the results are shown in fig. 1, and the numbers of the averaged results are shown in table 1.', '']","['ran both cbow and skipgram with negative sampling  #TAUTHOR_TAG b ) on the wikipedia dump data, and set the window size of context to be five.', 'then we performed value truncation with 4 bits, 6 bits, and 8 bits.', 'the results are shown in fig. 1, and the numbers of the averaged results are shown in table 1.', '']","['ran both cbow and skipgram with negative sampling  #TAUTHOR_TAG b ) on the wikipedia dump data, and set the window size of context to be five.', 'then we performed value truncation with 4 bits, 6 bits, and 8 bits.', 'the results are shown in fig. 1, and the numbers of the averaged results are shown in table 1.', 'we also used the binarization algorithm  #AUTHOR_TAG to truncate each dimension to three values ; these experiments are is denoted using the suffix "" binary "" in the figure.', 'for both cbow and skipgram models, we train the vectors with 25 and 200 dimensions respectively.', 'the representations used in our experiments were trained using the whole wikipedia dump.', '']",5
"['', 'recently,  #TAUTHOR_TAG proposed a public dataset to explore the complete']","['', 'recently,  #TAUTHOR_TAG proposed a public dataset to explore the complete']","['', 'recently,  #TAUTHOR_TAG proposed a public dataset to explore the complete process of the large - scale fact - checking.', 'it is designed not only']","['', 'recently,  #TAUTHOR_TAG proposed a public dataset to explore the complete process of the large - scale fact - checking.', '']",0
"['text as in  #TAUTHOR_TAG,']","['text as in  #TAUTHOR_TAG,']","['and document text as in  #TAUTHOR_TAG,']","['this module, l sentences are extracted as possible evidence for the claim.', 'instead of selecting the sentences by recomputing sentence - level tf - idf features between claim and document text as in  #TAUTHOR_TAG, we propose a neural ranker using decomposable attention ( da ) model  #AUTHOR_TAG to perform evidence selection.', 'da model does not require the input text to be parsed syntactically, nor is an ensemble, and it is faster without any recurrent structure.', 'in general, using neural methods is better for the following reasons : 1 ) the tf - idf may have limited ability to capture semantics compared to word representation learning 2 ) faster inference time compared to tf - idf methods that need real - time reconstruction.', 'the neural ranker da rank is trained using a fake task, which is to classify whether a given sentence is an evidence of a given claim or not.', 'the output of da rank is considered as the evidence probability.', 'the training samples are unbalanced since there are more unrelated sentences than evidence sentences.', '']",4
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG, we use the decomposable attention (']","['a claim and l possible evidence, a da rte classifier is trained to recognize the textual entailment to be support, refute or not enough information to verify ( nei ).', 'same as  #TAUTHOR_TAG, we use the decomposable attention ( da ) between the claim and the evidence for rte.', 'da model decomposes the rte problem into subproblems, which can be considered as bi - direction wordlevel attention features.', 'note that the da model is utilized over other models such as as  #AUTHOR_TAG b ) ;  #AUTHOR_TAG, because it is a simple but effective model.', 'our da rte model must correctly decide whether a claim is nei, when the evidence retrieved is irrelevant and insufficient.', 'however, nei claims have no annotated evidence, thus cannot be used to train rte.', 'to overcome this issue, same as  #TAUTHOR_TAG, the most probable nei evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module.', 'mlp da rte da rte + ner accuracy ( % ) 63. 2 78. 4 79. 9 table 4 : oracle rte classification accuracy in the test set using gold evidence']",3
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG, we use the decomposable attention (']","['a claim and l possible evidence, a da rte classifier is trained to recognize the textual entailment to be support, refute or not enough information to verify ( nei ).', 'same as  #TAUTHOR_TAG, we use the decomposable attention ( da ) between the claim and the evidence for rte.', 'da model decomposes the rte problem into subproblems, which can be considered as bi - direction wordlevel attention features.', 'note that the da model is utilized over other models such as as  #AUTHOR_TAG b ) ;  #AUTHOR_TAG, because it is a simple but effective model.', 'our da rte model must correctly decide whether a claim is nei, when the evidence retrieved is irrelevant and insufficient.', 'however, nei claims have no annotated evidence, thus cannot be used to train rte.', 'to overcome this issue, same as  #TAUTHOR_TAG, the most probable nei evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module.', 'mlp da rte da rte + ner accuracy ( % ) 63. 2 78. 4 79. 9 table 4 : oracle rte classification accuracy in the test set using gold evidence']",3
"[',  #TAUTHOR_TAG addresses large -']","['works have investigated fact verification using politifact data  #AUTHOR_TAG or fakenews challenge ( pomerleau and rao ).', 'most closely related to our work,  #TAUTHOR_TAG addresses large - scale fact extraction']","[',  #TAUTHOR_TAG addresses large - scale fact extraction']","['work  #AUTHOR_TAG have proposed fact - checking through entailment from knowledge bases.', 'some works have investigated fact verification using politifact data  #AUTHOR_TAG or fakenews challenge ( pomerleau and rao ).', 'most closely related to our work,  #TAUTHOR_TAG addresses large - scale fact extraction and verification task using a pipeline approach.', 'in addition, question answering  #AUTHOR_TAG a ;  #AUTHOR_TAG and task - oriented dialog systems  #AUTHOR_TAG table 7 : full - pipeline evaluation on the test set using k = 2 and th = 0. 6.', 'the first and the second one ( with * ) are the baselines from  #AUTHOR_TAG.', ' #AUTHOR_TAG also have similar aspects to these works, although aiming at a different goal.', 'other fields that are related to the particular individual modules of our system are the following : document and evidence retrieval for identifying text segments and documents to support a given claim  #AUTHOR_TAG.', 'recognizing textual entailment that aims to determine whether a hypothesis h can justifiably be inferred from a premise  #AUTHOR_TAG b ;  #AUTHOR_TAG.', 'in some of these work  #AUTHOR_TAG, the lexical and linguistic features are leveraged to further improve the performance']",3
[': fever dataset  #TAUTHOR_TAG is a relatively large -'],[': fever dataset  #TAUTHOR_TAG is a relatively large - scale'],"[': fever dataset  #TAUTHOR_TAG is a relatively large - scale dataset compared to other previous fact extraction and verification works, with around']","[': fever dataset  #TAUTHOR_TAG is a relatively large - scale dataset compared to other previous fact extraction and verification works, with around 5. 4m wikipedia documents and 185k samples.', 'the claims are generated by altering sentences extracted from wikipedia, with humanannotated evidence sentences and verification labels ( e. g. table 1 ).', 'the training / validation / test sets of these three datasets are split in advance by the providers.', 'note that the test - set was equally split into 3 classes : supported ( 3333 ), refuted ( 3333 ), nei ( 3333 ).', 'training : we trained our models end - to - end using adagrad optimizer  #AUTHOR_TAG.', 'the embedding size is set to 200 and initialized with glove  #AUTHOR_TAG.', 'the dropout rate is set to 0. 2.', 'in all the datasets, we tuned the hyper - parameters with grid - search over the validation set.', 'evaluation : for each module, we independently measure oracle performance, where we assume gold standard documents and set of evidence are provided ( oracle evaluation ).', 'for the final fullpipeline, we compare to and follow the metric defined in  #TAUTHOR_TAG.', 'noscoreev is a simple classification accuracy that only considers the correctness of the verification label.', 'on the other hand, scoreev is a stricter measure that also considers the correctness of the retrieved evidence.', 'hence, it is a more meaningful measure because it considers the classification to be correct only if appropriate evidence is provided to justify the classification']",5
[': fever dataset  #TAUTHOR_TAG is a relatively large -'],[': fever dataset  #TAUTHOR_TAG is a relatively large - scale'],"[': fever dataset  #TAUTHOR_TAG is a relatively large - scale dataset compared to other previous fact extraction and verification works, with around']","[': fever dataset  #TAUTHOR_TAG is a relatively large - scale dataset compared to other previous fact extraction and verification works, with around 5. 4m wikipedia documents and 185k samples.', 'the claims are generated by altering sentences extracted from wikipedia, with humanannotated evidence sentences and verification labels ( e. g. table 1 ).', 'the training / validation / test sets of these three datasets are split in advance by the providers.', 'note that the test - set was equally split into 3 classes : supported ( 3333 ), refuted ( 3333 ), nei ( 3333 ).', 'training : we trained our models end - to - end using adagrad optimizer  #AUTHOR_TAG.', 'the embedding size is set to 200 and initialized with glove  #AUTHOR_TAG.', 'the dropout rate is set to 0. 2.', 'in all the datasets, we tuned the hyper - parameters with grid - search over the validation set.', 'evaluation : for each module, we independently measure oracle performance, where we assume gold standard documents and set of evidence are provided ( oracle evaluation ).', 'for the final fullpipeline, we compare to and follow the metric defined in  #TAUTHOR_TAG.', 'noscoreev is a simple classification accuracy that only considers the correctness of the verification label.', 'on the other hand, scoreev is a stricter measure that also considers the correctness of the retrieved evidence.', 'hence, it is a more meaningful measure because it considers the classification to be correct only if appropriate evidence is provided to justify the classification']",5
['between the claim and evidence as features as shown in  #TAUTHOR_TAG'],['between the claim and evidence as features as shown in  #TAUTHOR_TAG'],"['', 'the mlp is a simple multi - layer perceptron using tf and tf - idf cosine similarity between the claim and evidence as features as shown in  #TAUTHOR_TAG.', 'the highest accuracy achieved is 79']","['', 'this is because providing a succinct set of evidence makes the verification task easier for the rte module.', 'therefore, we choose da rank + ner model with th = 0. 6 for the fullpipeline.', 'recognizing textual entailment : the oracle classification accuracy for rte is shown in table 4.', 'the mlp is a simple multi - layer perceptron using tf and tf - idf cosine similarity between the claim and evidence as features as shown in  #TAUTHOR_TAG.', 'the highest accuracy achieved is 79. 9 % using da rte with ner information, thus, we further evaluate the']",5
"['14 ].', ' #TAUTHOR_TAG.', 'traditional methods assess readability using surface features and shallow linguistic features']","['formality score ( [ 10 ] ) in four different domains [ 14 ].', ' #TAUTHOR_TAG.', 'traditional methods assess readability using surface features and shallow linguistic features']","['14 ].', ' #TAUTHOR_TAG.', 'traditional methods assess readability using surface features and shallow linguistic features']","['##ability scoring in english has a long and rich history, starting with the work of l. a. sherman in the late nineteenth century [ 20 ].', 'among the early readability formulas were flesch reading ease [ 7 ], dale - chall formula [ 5 ], automated readability index [ 19 ], gunning fog index [ 9 ], smog score [ 16 ], and coleman - liau index [ 2 ].', 'these early indices were based on simple features like average number of characters, words, syllables and sentences, number of difficult and polysyllabic words, etc.', ""albeit simple, these readability indices were surprisingly good predictors of a reader's grade level."", 'two different lines of work focused on children and adult readability formulas.', 'recently lahiri et al. showed moderate correlation between readability indices and formality score ( [ 10 ] ) in four different domains [ 14 ].', ' #TAUTHOR_TAG.', 'traditional methods assess readability using surface features and shallow linguistic features such as the ones mentioned in the preceding paragraph.', ""cognitively motivated methods take into account the cohesion and coherence of text, its latent topic structure, kintsch's propositions, etc [ 1 ], [ 8 ], [ 13 ]."", 'finally, machine learning methods utilize sophisticated structures such as language models [ 3 ], [ 4 ], [ 18 ], query logs [ 15 ], and several other features to predict the readability of open - domain text data.', 'there are very few studies on readability assessment in bengali texts.', 'we found only three lines of work that specifically looked into bengali readability [ 6 ], [ 11 ],  #TAUTHOR_TAG.', 'das and roychoudhury worked with a miniature model of two parameters in their pioneering study [ 6 ].', 'they found that the two - parameter model was a better predictor of readability than the one - parameter model.', ""note, however, that das and roychoudhury's corpus was small ( only seven documents ), thereby calling into question the validity of their results."", ' #TAUTHOR_TAG.', ' #TAUTHOR_TAG own readability model on 16 texts.', 'around the same time, islam et al. independently reached the same conclusion [ 11 ].', 'they designed a bengali readability classifier on lexical and information - theoretic features, resulting in an f - score 50 % higher than that from traditional scoring approaches.', 'while all the above studies are very important and insightful, none of them explicitly performed an inter - rater agreement study.', 'for reasons mentioned in section 1, an inter - rate']",1
"["".'s dataset is not annotated in as fine - grained a fashion as ours."", ""note also that our dataset is larger than both  #TAUTHOR_TAG, and das and roychoudhury's seven document dataset""]","['research.', 'we are working on readability modeling in bengali, and this dataset will be very helpful.', 'an important limitation of our study is the small corpus size.', ""we only have 30 annotated passages at our disposal, whereas islam et al. [ 11 ] had around 300. but islam et al.'s dataset is not annotated in as fine - grained a fashion as ours."", ""note also that our dataset is larger than both  #TAUTHOR_TAG, and das and roychoudhury's seven document dataset [ 6 ]."", 'we plan to increase the size of our dataset in future']","["".'s dataset is not annotated in as fine - grained a fashion as ours."", ""note also that our dataset is larger than both  #TAUTHOR_TAG, and das and roychoudhury's seven document dataset [ 6 ]."", 'we plan to increase the size of our dataset in']","['', 'we are working on readability modeling in bengali, and this dataset will be very helpful.', 'an important limitation of our study is the small corpus size.', ""we only have 30 annotated passages at our disposal, whereas islam et al. [ 11 ] had around 300. but islam et al.'s dataset is not annotated in as fine - grained a fashion as ours."", ""note also that our dataset is larger than both  #TAUTHOR_TAG, and das and roychoudhury's seven document dataset [ 6 ]."", 'we plan to increase the size of our dataset in future']",4
['result by  #TAUTHOR_TAG'],['result by  #TAUTHOR_TAG'],"['', '- of - the - art result by  #TAUTHOR_TAG']","['', '- of - the - art result by  #TAUTHOR_TAG']",4
['based decoder with dynamic attention mechanism similar to  #TAUTHOR_TAG with modifications'],"['th record and q and p are weights to be learnt.', 'we do not have any constraints on static attention vector.', 'dynamic record attention for decoder : our decoder is a gru based decoder with dynamic attention mechanism similar to  #TAUTHOR_TAG with modifications']","['##t.', 'we do not have any constraints on static attention vector.', 'dynamic record attention for decoder : our decoder is a gru based decoder with dynamic attention mechanism similar to  #TAUTHOR_TAG with modifications']","['a table, a subset of record types can always be more salient compared to other record types.', 'this is captured by learning a static set of weights over all the records.', 'these weights regulate the dynamic attention weights computed during decoding at each time step.', 'equation 7 performs this step where g r is the static record attention weight for r th record and q and p are weights to be learnt.', 'we do not have any constraints on static attention vector.', 'dynamic record attention for decoder : our decoder is a gru based decoder with dynamic attention mechanism similar to  #TAUTHOR_TAG with modifications to modulate attention weights at each time step using static record attentions.', 'at each time step t attention weights are calculated using 8, 9, 10, where γ r t is the aggregated attention weight of record r at time step t. we use the soft attention over input encoder sequences c r to calculate the weighted average, which is passed to the gru.', 'gru hidden state s t is used to calculate output probabilities p t by using a softmax as described by equation 11, 12, 13, which is then used to get output word y t.', 'due to the static attention at attribute level, the time complexity of a single pass is o ( t m + t t ), where t is the number of records, m is the number of attributes and t is the number of decoder steps.', 'in case of dynamic attention at both levels ( as in  #AUTHOR_TAG ), the time complexity is much higher o ( t m t ).', 'thus, mixed hierarchical attention model is faster than fully dynamic hierarchical attention.', 'for better understanding of the contribution of hierarchical attention ( mham ), we propose a simpler non - hierarchical ( nhm ) architecture with attention only at record level.', 'in nhm, b r is calculated by concatenating all the record attributes along with corresponding record type.', 'input table generated output type time min max mean mode mb100 - 4 mb20 - 2 temperature 17 - 30 27 36 29 - - - windchill 17 - 30 14 26 18 - - - windspeed 17 - 30 14 20 16 - - 10 - 20 winddir 17 - 30 - - - ssw - - gust 17 - 30 0 0 0 - - - skycover 17 - 30 - - - - 75 - 100 - skycover 17 - 21 - - - - 75 - 100 - skycover 17 - 26 - - - - 75 - 100 - skycover 21 - 30 - - - - 75 - 100 - skycover 26 - 30 - - - - 75 - 100']",6
"['work of mbw  #TAUTHOR_TAG, as']","['of our model against the state - of - the - art work of mbw  #TAUTHOR_TAG, as']","['and methodology : to evaluate our model we have used weathergov dataset  #AUTHOR_TAG which is the standard benchmark dataset to evaluate tabular data summarization techniques.', 'we compared the performance of our model against the state - of - the - art work of mbw  #TAUTHOR_TAG, as']","['and methodology : to evaluate our model we have used weathergov dataset  #AUTHOR_TAG which is the standard benchmark dataset to evaluate tabular data summarization techniques.', ""we compared the performance of our model against the state - of - the - art work of mbw  #TAUTHOR_TAG, as well as two other baseline models kl  #AUTHOR_TAG and alk  #AUTHOR_TAG'00010000000000'and'00001000000000'respectively."", ""similar works for directions, for example'nw ','nne'and'ne'are encoded as'00000100000000 ','00000011000000'and'00000001000000'resp."", ""time interval were also encoded as ordinal encodings, for example'6 - 21'is encoded as'111100'and'6 - 13'is encoded as'110000 ', the six bits corresponding to six atomic time intervals available in the dataset."", 'other attributes and words were encoded as one - hot vectors']",5
"['( cbleu )  #TAUTHOR_TAG has also been reported.', 'cbleu does not penalize numbers which differ']","['standard bleu ( sbleu )  #AUTHOR_TAG, a customized bleu ( cbleu )  #TAUTHOR_TAG has also been reported.', 'cbleu does not penalize numbers which differ']","['( sbleu )  #AUTHOR_TAG, a customized bleu ( cbleu )  #TAUTHOR_TAG has also been reported.', 'cbleu does not penalize numbers which differ']","['', 'in addition to the standard bleu ( sbleu )  #AUTHOR_TAG, a customized bleu ( cbleu )  #TAUTHOR_TAG has also been reported.', 'cbleu does not penalize numbers which differ by at most five ; hence 20 and 18 will be considered same.', 'table 2 describes the results of our proposed models ( mham and nhm ) along with the aforementioned baseline models.', '']",5
['.  #TAUTHOR_TAG'],['.  #TAUTHOR_TAG for kg completion do not capture the entity'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
['.  #TAUTHOR_TAG'],['.  #TAUTHOR_TAG for kg completion do not capture the entity'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
['.  #TAUTHOR_TAG'],['.  #TAUTHOR_TAG for kg completion do not capture the entity'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['- making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['', 'in knowledge graphs by framing it as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '. deeppath [ 30 ] was the first method that used rl to find relation paths between two', 'entities in kgs. it walks from the source entity, chooses a relation and translates to any entity in the tail entity set of the relation. minerva [ 3 ], on the other hand, learns to do multi - hop reasoning by jointly selecting an entity - relation pair via a policy network. marlpar', '[ 12 ] uses a multi - agent rl approach where two agents are used to perform the relation selection and entity selection iteratively.  #TAUTHOR_TAG implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect', '']",0
"['3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['- making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['', 'in knowledge graphs by framing it as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '. deeppath [ 30 ] was the first method that used rl to find relation paths between two', 'entities in kgs. it walks from the source entity, chooses a relation and translates to any entity in the tail entity set of the relation. minerva [ 3 ], on the other hand, learns to do multi - hop reasoning by jointly selecting an entity - relation pair via a policy network. marlpar', '[ 12 ] uses a multi - agent rl approach where two agents are used to perform the relation selection and entity selection iteratively.  #TAUTHOR_TAG implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect', '']",0
"['3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['- making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['', 'in knowledge graphs by framing it as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '. deeppath [ 30 ] was the first method that used rl to find relation paths between two', 'entities in kgs. it walks from the source entity, chooses a relation and translates to any entity in the tail entity set of the relation. minerva [ 3 ], on the other hand, learns to do multi - hop reasoning by jointly selecting an entity - relation pair via a policy network. marlpar', '[ 12 ] uses a multi - agent rl approach where two agents are used to perform the relation selection and entity selection iteratively.  #TAUTHOR_TAG implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect', '']",0
"['3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['- making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['', 'in knowledge graphs by framing it as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '. deeppath [ 30 ] was the first method that used rl to find relation paths between two', 'entities in kgs. it walks from the source entity, chooses a relation and translates to any entity in the tail entity set of the relation. minerva [ 3 ], on the other hand, learns to do multi - hop reasoning by jointly selecting an entity - relation pair via a policy network. marlpar', '[ 12 ] uses a multi - agent rl approach where two agents are used to perform the relation selection and entity selection iteratively.  #TAUTHOR_TAG implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect', '']",0
"['3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['- making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['', 'in knowledge graphs by framing it as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '. deeppath [ 30 ] was the first method that used rl to find relation paths between two', 'entities in kgs. it walks from the source entity, chooses a relation and translates to any entity in the tail entity set of the relation. minerva [ 3 ], on the other hand, learns to do multi - hop reasoning by jointly selecting an entity - relation pair via a policy network. marlpar', '[ 12 ] uses a multi - agent rl approach where two agents are used to perform the relation selection and entity selection iteratively.  #TAUTHOR_TAG implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect', '']",0
"['3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['- making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '']","['', 'in knowledge graphs by framing it as a sequential decision - making process [ 3', ', 23, 30, 22, 12,  #TAUTHOR_TAG ]', '. deeppath [ 30 ] was the first method that used rl to find relation paths between two', 'entities in kgs. it walks from the source entity, chooses a relation and translates to any entity in the tail entity set of the relation. minerva [ 3 ], on the other hand, learns to do multi - hop reasoning by jointly selecting an entity - relation pair via a policy network. marlpar', '[ 12 ] uses a multi - agent rl approach where two agents are used to perform the relation selection and entity selection iteratively.  #TAUTHOR_TAG implement reward shaping to address the problem of the sparse reward signal and action dropout to reduce the effect', '']",0
[' #TAUTHOR_TAG ] pre - computes pagerank scores for'],"[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","['', 'action space and do so via a pre - computed heuristic. for example, [  #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows the action space to a fixed number of highest - ranking neighbors. in this work, we use entity type information to limit the search to the entities with best - matching types, given the previous actions. we call the reduced action space a t. we provide more details in section 3. 3. rewards. the agent evaluates each action and chooses', 'the one that will maximize a reward. previous works [ 30, 3 ] define only a terminal reward of + 1 if the agent reaches the correct answer. however, since knowledge graphs are incomplete, a binary reward cannot model the potentially missing', 'facts. as a result, the agent receives low - quality rewards as it explores the environment. inspired by [  #TAUTHOR_TAG ], we use pre - trained kg embeddings based on existing kg embedding methods to design a soft reward', '']",0
"[': new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['the path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['this section we present a few case studies that show the strength of our proposed method.', 'in the nell - 995 dataset, our method is more successful when the agent encounters an entity with a high out - degree.', 'as an example, for the query ( buffalo bills ( sports team ), [ d : 315 ], they are not able to navigate properly after reaching the nfl entity, due to its high out - degree.', 'as a result, they are unable to discover the answer mike mularkey.', 'as another example, we consider the query : ( new york ( city ), organization hired person,? ).', 'our method discovers the path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with finding the next best step after entity new york.', 'our method uses the location information to find the answer michael bloomberg.', 'in the amazon datasets, there are fewer entity and relation types.', 'as a result, we observe many frequent patterns that  #TAUTHOR_TAG are able to discover.', 'therefore, we focus on the diversity of the relations used in our method and the best performing baseline [  #TAUTHOR_TAG ] for the discovered paths in the development set.', 'figure 3 displays the inference results.', 'on the amazon cellphones data, our method discovers fewer < null >, produced - by and also - bought relations while it utilizes more of other relations, in particular, belongs - to relations.', 'similarly, on the amazon beauty data, our method utilizes fewer also - bought, produced - by and < null > relations, while it uses other relations more frequently, especially the bought - together relation.', 'we believe one reason for the success of our method is the diverse use of different relation types for discovering new path types']",0
['.  #TAUTHOR_TAG'],['.  #TAUTHOR_TAG for kg completion do not capture the entity'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],1
[' #TAUTHOR_TAG ] pre - computes pagerank scores for'],"[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","['', 'action space and do so via a pre - computed heuristic. for example, [  #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows the action space to a fixed number of highest - ranking neighbors. in this work, we use entity type information to limit the search to the entities with best - matching types, given the previous actions. we call the reduced action space a t. we provide more details in section 3. 3. rewards. the agent evaluates each action and chooses', 'the one that will maximize a reward. previous works [ 30, 3 ] define only a terminal reward of + 1 if the agent reaches the correct answer. however, since knowledge graphs are incomplete, a binary reward cannot model the potentially missing', 'facts. as a result, the agent receives low - quality rewards as it explores the environment. inspired by [  #TAUTHOR_TAG ], we use pre - trained kg embeddings based on existing kg embedding methods to design a soft reward', '']",1
[' #TAUTHOR_TAG ] pre - computes pagerank scores for'],"[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","['', 'action space and do so via a pre - computed heuristic. for example, [  #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows the action space to a fixed number of highest - ranking neighbors. in this work, we use entity type information to limit the search to the entities with best - matching types, given the previous actions. we call the reduced action space a t. we provide more details in section 3. 3. rewards. the agent evaluates each action and chooses', 'the one that will maximize a reward. previous works [ 30, 3 ] define only a terminal reward of + 1 if the agent reaches the correct answer. however, since knowledge graphs are incomplete, a binary reward cannot model the potentially missing', 'facts. as a result, the agent receives low - quality rewards as it explores the environment. inspired by [  #TAUTHOR_TAG ], we use pre - trained kg embeddings based on existing kg embedding methods to design a soft reward', '']",5
[' #TAUTHOR_TAG ] pre - computes pagerank scores for'],"[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","['', 'action space and do so via a pre - computed heuristic. for example, [  #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows the action space to a fixed number of highest - ranking neighbors. in this work, we use entity type information to limit the search to the entities with best - matching types, given the previous actions. we call the reduced action space a t. we provide more details in section 3. 3. rewards. the agent evaluates each action and chooses', 'the one that will maximize a reward. previous works [ 30, 3 ] define only a terminal reward of + 1 if the agent reaches the correct answer. however, since knowledge graphs are incomplete, a binary reward cannot model the potentially missing', 'facts. as a result, the agent receives low - quality rewards as it explores the environment. inspired by [  #TAUTHOR_TAG ], we use pre - trained kg embeddings based on existing kg embedding methods to design a soft reward', '']",5
[' #TAUTHOR_TAG ] pre - computes pagerank scores for'],"[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","[' #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows']","['', 'action space and do so via a pre - computed heuristic. for example, [  #TAUTHOR_TAG ] pre - computes pagerank scores for each node, and narrows the action space to a fixed number of highest - ranking neighbors. in this work, we use entity type information to limit the search to the entities with best - matching types, given the previous actions. we call the reduced action space a t. we provide more details in section 3. 3. rewards. the agent evaluates each action and chooses', 'the one that will maximize a reward. previous works [ 30, 3 ] define only a terminal reward of + 1 if the agent reaches the correct answer. however, since knowledge graphs are incomplete, a binary reward cannot model the potentially missing', 'facts. as a result, the agent receives low - quality rewards as it explores the environment. inspired by [  #TAUTHOR_TAG ], we use pre - trained kg embeddings based on existing kg embedding methods to design a soft reward', '']",5
"['pre - trained embeddings [  #TAUTHOR_TAG ].', 'the experiments utilize the 3 data sets presented in']","['pre - trained embeddings [  #TAUTHOR_TAG ].', 'the experiments utilize the 3 data sets presented in']","['pre - trained embeddings [  #TAUTHOR_TAG ].', 'the experiments utilize the 3 data sets presented in table 1.', 'of the standard data sets used in kg reasoning tasks, nell - 995 is']","['this section, we describe and discuss the experimental results of our proposed approach.', 'we compare against several baseline methods : conve ( embeddingbased ) [ 4 ], complex ( embedding - based ) [ 27 ], minerva ( agent - based ) [ 3 ], and multihopkg ( agent - based ) using both conve and complex for pre - trained embeddings [  #TAUTHOR_TAG ].', 'the experiments utilize the 3 data sets presented in table 1.', 'of the standard data sets used in kg reasoning tasks, nell - 995 is the only one that explicitly encodes entity types.', 'therefore, in addition to nell, we incorporated two datasets from the amazon e - commerce collection [ 6 ].', 'each amazon data contains a set of users, products, brands, and other information, which the authors of [ 29 ] use to make product recommendations to users.', 'this task is a specialized instance of kg completion that only focuses on user - product relations, so we do not include it in our baseline results.', 'additionally, we found these data sets were too large for efficient computation in the broader kg completion task, so we shrunk them for our experiments.', 'to do this, we randomly chose 20 % of the nodes and induced a subgraph on those nodes.', 'while this might result in a sparser graph that makes predictions more difficult, this was the best option given the lack of other relevant data containing type information']",5
"['3,  #TAUTHOR_TAG ], we measure hits @ k and mrr only for']","['work [ 3,  #TAUTHOR_TAG ], we measure hits @ k and mrr only for']","['of these models generalize to unknown entities, followed by previous work [ 3,  #TAUTHOR_TAG ], we measure hits @ k and mrr only for']","['{ ( e s, r, e d ) ∈ q : rank ( e d, f ( e s, r ) ) ≤ k } | | q | × 100 ( 8 ) where q = queries and rank ( e d, e d ) is a function that returns the position of entity e d in the set of ordered predictionse d.', 'mrr is a related metric, defined as the multiplicative inverse of the rank of the correct answer, i. e. :', 'because none of these models generalize to unknown entities, followed by previous work [ 3,  #TAUTHOR_TAG ], we measure hits @ k and mrr only for queries for which both e s and e d have already been seen at least once by the model during training.', 'in other words, if either of the query entities is missing from the training set ( f acts − queries ), we discard it from testing.', 'additionally, we reserve a small portion of the f acts as a development set to estimate performance during training']",5
"[' #TAUTHOR_TAG baselines.', 'for minerva, we utilize the same hyperparameters described in']","[' #TAUTHOR_TAG baselines.', 'for minerva, we utilize the same hyperparameters described in [ 3 ] and train the model for 3, 000 epochs.', 'for the']","['- 995, utilize the same hyperparameters described in [  #TAUTHOR_TAG baselines.', 'for minerva, we utilize the same hyperparameters described in']","['nell - 995, utilize the same hyperparameters described in [  #TAUTHOR_TAG baselines.', 'for minerva, we utilize the same hyperparameters described in [ 3 ] and train the model for 3, 000 epochs.', 'for the two amazon datasets, we perform a grid search for our method and all  #TAUTHOR_TAG and report the best performance for each.', 'for all datasets, we train the kg embedding models ( conve and complex ) for 1000 epochs each.', 'these embeddings are then used to make predictions directly but also serve as pre - trained inputs for the rl agent, which we train for 30 epochs per experiment for all datasets.', 'we tried different embedding methods for the pretrained embeddings and eventually used complex for nell - 995 and distmult for amazon cellphones and amazon beauty as they resulted in the best performance in each data']",5
"[' #TAUTHOR_TAG baselines.', 'for minerva, we utilize the same hyperparameters described in']","[' #TAUTHOR_TAG baselines.', 'for minerva, we utilize the same hyperparameters described in [ 3 ] and train the model for 3, 000 epochs.', 'for the']","['- 995, utilize the same hyperparameters described in [  #TAUTHOR_TAG baselines.', 'for minerva, we utilize the same hyperparameters described in']","['nell - 995, utilize the same hyperparameters described in [  #TAUTHOR_TAG baselines.', 'for minerva, we utilize the same hyperparameters described in [ 3 ] and train the model for 3, 000 epochs.', 'for the two amazon datasets, we perform a grid search for our method and all  #TAUTHOR_TAG and report the best performance for each.', 'for all datasets, we train the kg embedding models ( conve and complex ) for 1000 epochs each.', 'these embeddings are then used to make predictions directly but also serve as pre - trained inputs for the rl agent, which we train for 30 epochs per experiment for all datasets.', 'we tried different embedding methods for the pretrained embeddings and eventually used complex for nell - 995 and distmult for amazon cellphones and amazon beauty as they resulted in the best performance in each data']",5
"['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in']","['experimental results are described in table 2.', 'for nell - 995 data, we quote the results reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in all three datasets, our results outperform both rl baselines  #TAUTHOR_TAG and minerva [ 3 ] ).', 'amazon datasets, on the other hand, are far more challenging.', 'we notice that even the embedding based methods are struggling with low performance on these datasets.', 'on the amazon data, the performance of all methods is significantly lower.', '']",5
"['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in']","['experimental results are described in table 2.', 'for nell - 995 data, we quote the results reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in all three datasets, our results outperform both rl baselines  #TAUTHOR_TAG and minerva [ 3 ] ).', 'amazon datasets, on the other hand, are far more challenging.', 'we notice that even the embedding based methods are struggling with low performance on these datasets.', 'on the amazon data, the performance of all methods is significantly lower.', '']",5
"['rl baseline by  #TAUTHOR_TAG.', '']","['rl baseline by  #TAUTHOR_TAG.', '']","['by  #TAUTHOR_TAG.', '']","['compare the number of unique paths discovered from the development set during the training procedure.', 'figure 2 shows that path diversity ( top row ) improves across all models as the model performance ( bottom row ) improves.', 'for this analysis, we compare our ablation models ( ours ( - n ) and ours ( - t ) ) with the best performing rl baseline by  #TAUTHOR_TAG.', 'our method is more successful in discovering novel paths and obtains a better hit ratio on the development set.', 'on the amazon beauty data, the number of unique paths discovered by ours ( - t ) is higher than both combined ( ours ) while in amazon cellphones the combined model performs better, but similar to amazon beauty, ours ( - n ) performs better than ours ( - t ).', 'nell - 955 shows a different trend where removing the type information results in a larger drop in the number of unique paths, compared to the heterogeneous neighbor encoder.', 'this is intuitive, since nell - 955 contains far more entity type than amazon datasets, and inclusion of type information may be a positive factor for discovering new paths.', 'in terms of convergence, amazon beauty and amazon cellphones show a similar trend, and removing the type information significantly reduces the hit ratio.', 'this gap is smaller for nell - 995 data, though our model still shows improvement in hit ratio on this dataset']",5
['with the  #TAUTHOR_TAG performance on seen and'],['with the  #TAUTHOR_TAG performance on seen and'],['with the  #TAUTHOR_TAG performance on seen and'],"['compare the ablation models along with the  #TAUTHOR_TAG performance on seen and unseen queries.', 'note that, percentage of unseen queries is much lower in the amazon datasets compared to the nell - 995 dataset.', '']",5
"['with the  #TAUTHOR_TAG.', 'we take a']","['with the  #TAUTHOR_TAG.', 'we take a']","['with the  #TAUTHOR_TAG.', 'we take a similar approach as [  #TAUTHOR_TAG ] to extract']","['evaluate our proposed model on different relation types and compare our results with the  #TAUTHOR_TAG.', 'we take a similar approach as [  #TAUTHOR_TAG ] to extract to - many and to - one relations.', '']",5
"[': new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['the path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['this section we present a few case studies that show the strength of our proposed method.', 'in the nell - 995 dataset, our method is more successful when the agent encounters an entity with a high out - degree.', 'as an example, for the query ( buffalo bills ( sports team ), [ d : 315 ], they are not able to navigate properly after reaching the nfl entity, due to its high out - degree.', 'as a result, they are unable to discover the answer mike mularkey.', 'as another example, we consider the query : ( new york ( city ), organization hired person,? ).', 'our method discovers the path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with finding the next best step after entity new york.', 'our method uses the location information to find the answer michael bloomberg.', 'in the amazon datasets, there are fewer entity and relation types.', 'as a result, we observe many frequent patterns that  #TAUTHOR_TAG are able to discover.', 'therefore, we focus on the diversity of the relations used in our method and the best performing baseline [  #TAUTHOR_TAG ] for the discovered paths in the development set.', 'figure 3 displays the inference results.', 'on the amazon cellphones data, our method discovers fewer < null >, produced - by and also - bought relations while it utilizes more of other relations, in particular, belongs - to relations.', 'similarly, on the amazon beauty data, our method utilizes fewer also - bought, produced - by and < null > relations, while it uses other relations more frequently, especially the bought - together relation.', 'we believe one reason for the success of our method is the diverse use of different relation types for discovering new path types']",5
"['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in']","['experimental results are described in table 2.', 'for nell - 995 data, we quote the results reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in all three datasets, our results outperform both rl baselines  #TAUTHOR_TAG and minerva [ 3 ] ).', 'amazon datasets, on the other hand, are far more challenging.', 'we notice that even the embedding based methods are struggling with low performance on these datasets.', 'on the amazon data, the performance of all methods is significantly lower.', '']",4
"['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in']","['experimental results are described in table 2.', 'for nell - 995 data, we quote the results reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in all three datasets, our results outperform both rl baselines  #TAUTHOR_TAG and minerva [ 3 ] ).', 'amazon datasets, on the other hand, are far more challenging.', 'we notice that even the embedding based methods are struggling with low performance on these datasets.', 'on the amazon data, the performance of all methods is significantly lower.', '']",4
"['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in']","['experimental results are described in table 2.', 'for nell - 995 data, we quote the results reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in all three datasets, our results outperform both rl baselines  #TAUTHOR_TAG and minerva [ 3 ] ).', 'amazon datasets, on the other hand, are far more challenging.', 'we notice that even the embedding based methods are struggling with low performance on these datasets.', 'on the amazon data, the performance of all methods is significantly lower.', '']",4
"['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher']","['3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in']","['experimental results are described in table 2.', 'for nell - 995 data, we quote the results reported in [ 3,  #TAUTHOR_TAG ].', 'embedding - based methods show an overall higher performance compared to the  #TAUTHOR_TAG.', 'we can see that in all three datasets, our results outperform both rl baselines  #TAUTHOR_TAG and minerva [ 3 ] ).', 'amazon datasets, on the other hand, are far more challenging.', 'we notice that even the embedding based methods are struggling with low performance on these datasets.', 'on the amazon data, the performance of all methods is significantly lower.', '']",4
"['rl baseline by  #TAUTHOR_TAG.', '']","['rl baseline by  #TAUTHOR_TAG.', '']","['by  #TAUTHOR_TAG.', '']","['compare the number of unique paths discovered from the development set during the training procedure.', 'figure 2 shows that path diversity ( top row ) improves across all models as the model performance ( bottom row ) improves.', 'for this analysis, we compare our ablation models ( ours ( - n ) and ours ( - t ) ) with the best performing rl baseline by  #TAUTHOR_TAG.', 'our method is more successful in discovering novel paths and obtains a better hit ratio on the development set.', 'on the amazon beauty data, the number of unique paths discovered by ours ( - t ) is higher than both combined ( ours ) while in amazon cellphones the combined model performs better, but similar to amazon beauty, ours ( - n ) performs better than ours ( - t ).', 'nell - 955 shows a different trend where removing the type information results in a larger drop in the number of unique paths, compared to the heterogeneous neighbor encoder.', 'this is intuitive, since nell - 955 contains far more entity type than amazon datasets, and inclusion of type information may be a positive factor for discovering new paths.', 'in terms of convergence, amazon beauty and amazon cellphones show a similar trend, and removing the type information significantly reduces the hit ratio.', 'this gap is smaller for nell - 995 data, though our model still shows improvement in hit ratio on this dataset']",4
['with the  #TAUTHOR_TAG performance on seen and'],['with the  #TAUTHOR_TAG performance on seen and'],['with the  #TAUTHOR_TAG performance on seen and'],"['compare the ablation models along with the  #TAUTHOR_TAG performance on seen and unseen queries.', 'note that, percentage of unseen queries is much lower in the amazon datasets compared to the nell - 995 dataset.', '']",4
"['with the  #TAUTHOR_TAG.', 'we take a']","['with the  #TAUTHOR_TAG.', 'we take a']","['with the  #TAUTHOR_TAG.', 'we take a similar approach as [  #TAUTHOR_TAG ] to extract']","['evaluate our proposed model on different relation types and compare our results with the  #TAUTHOR_TAG.', 'we take a similar approach as [  #TAUTHOR_TAG ] to extract to - many and to - one relations.', '']",4
"[': new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['the path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['this section we present a few case studies that show the strength of our proposed method.', 'in the nell - 995 dataset, our method is more successful when the agent encounters an entity with a high out - degree.', 'as an example, for the query ( buffalo bills ( sports team ), [ d : 315 ], they are not able to navigate properly after reaching the nfl entity, due to its high out - degree.', 'as a result, they are unable to discover the answer mike mularkey.', 'as another example, we consider the query : ( new york ( city ), organization hired person,? ).', 'our method discovers the path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with finding the next best step after entity new york.', 'our method uses the location information to find the answer michael bloomberg.', 'in the amazon datasets, there are fewer entity and relation types.', 'as a result, we observe many frequent patterns that  #TAUTHOR_TAG are able to discover.', 'therefore, we focus on the diversity of the relations used in our method and the best performing baseline [  #TAUTHOR_TAG ] for the discovered paths in the development set.', 'figure 3 displays the inference results.', 'on the amazon cellphones data, our method discovers fewer < null >, produced - by and also - bought relations while it utilizes more of other relations, in particular, belongs - to relations.', 'similarly, on the amazon beauty data, our method utilizes fewer also - bought, produced - by and < null > relations, while it uses other relations more frequently, especially the bought - together relation.', 'we believe one reason for the success of our method is the diverse use of different relation types for discovering new path types']",4
"[': new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['the path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with']","['this section we present a few case studies that show the strength of our proposed method.', 'in the nell - 995 dataset, our method is more successful when the agent encounters an entity with a high out - degree.', 'as an example, for the query ( buffalo bills ( sports team ), [ d : 315 ], they are not able to navigate properly after reaching the nfl entity, due to its high out - degree.', 'as a result, they are unable to discover the answer mike mularkey.', 'as another example, we consider the query : ( new york ( city ), organization hired person,? ).', 'our method discovers the path : new york ( city ).', 'again, other  #TAUTHOR_TAG struggle with finding the next best step after entity new york.', 'our method uses the location information to find the answer michael bloomberg.', 'in the amazon datasets, there are fewer entity and relation types.', 'as a result, we observe many frequent patterns that  #TAUTHOR_TAG are able to discover.', 'therefore, we focus on the diversity of the relations used in our method and the best performing baseline [  #TAUTHOR_TAG ] for the discovered paths in the development set.', 'figure 3 displays the inference results.', 'on the amazon cellphones data, our method discovers fewer < null >, produced - by and also - bought relations while it utilizes more of other relations, in particular, belongs - to relations.', 'similarly, on the amazon beauty data, our method utilizes fewer also - bought, produced - by and < null > relations, while it uses other relations more frequently, especially the bought - together relation.', 'we believe one reason for the success of our method is the diverse use of different relation types for discovering new path types']",7
['the scalability of  #TAUTHOR_TAG'],['the scalability of  #TAUTHOR_TAG'],"['the scalability of  #TAUTHOR_TAG.', 'furthermore,']","['', 'in the future, we plan to explore more efficient strategies for action - space pruning to improve the scalability of  #TAUTHOR_TAG.', 'furthermore, we plan to develop more effective type embeddings considering the hierarchical structure of the type information']",2
[' #TAUTHOR_TAG problem : given a'],[' #TAUTHOR_TAG problem : given a'],"['', 'in this paper, we study the critical yet under - addressed answer triggering  #TAUTHOR_TAG problem : given a question and a set of']",[' #TAUTHOR_TAG'],5
[' #TAUTHOR_TAG problem : given a'],[' #TAUTHOR_TAG problem : given a'],"['', 'in this paper, we study the critical yet under - addressed answer triggering  #TAUTHOR_TAG problem : given a question and a set of']",[' #TAUTHOR_TAG'],5
"['dataset  #TAUTHOR_TAG for evaluation.', 'it contains 3']","['use the wikiqa dataset  #TAUTHOR_TAG for evaluation.', 'it contains 3, 047 questions from bing']","['use the wikiqa dataset  #TAUTHOR_TAG for evaluation.', 'it contains 3']","['use the wikiqa dataset  #TAUTHOR_TAG for evaluation.', 'it contains 3, 047 questions from bing query logs, each associated with a group of candidate answer sentences from wikipedia and manually labeled via crowdsourcing.', '']",5
['1 score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],['1 score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],['score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],[' #TAUTHOR_TAG'],5
"['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to assist our analysis.', '']",5
"['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to assist our analysis.', '']",5
"['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to assist our analysis.', '']",5
"['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to assist our analysis.', '']",5
[' #TAUTHOR_TAG problem : given a'],[' #TAUTHOR_TAG problem : given a'],"['', 'in this paper, we study the critical yet under - addressed answer triggering  #TAUTHOR_TAG problem : given a question and a set of']",[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG problem : given a'],[' #TAUTHOR_TAG problem : given a'],"['', 'in this paper, we study the critical yet under - addressed answer triggering  #TAUTHOR_TAG problem : given a question and a set of']",[' #TAUTHOR_TAG'],6
['1 score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],['1 score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],['score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],[' #TAUTHOR_TAG'],4
['1 score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],['1 score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],['score from  #TAUTHOR_TAG and  #AUTHOR_TAG'],[' #TAUTHOR_TAG'],4
"['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to assist our analysis.', '']",4
"['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available,']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to']","['', 'since the code from  #TAUTHOR_TAG is available, we use it ( rather than  #AUTHOR_TAG ) to assist our analysis.', '']",4
['1 on penn treebank constituency parsing and  #TAUTHOR_TAG with 92'],['1 on penn treebank constituency parsing and  #TAUTHOR_TAG with 92. 8'],['4 f 1 on penn treebank constituency parsing and  #TAUTHOR_TAG with 92'],"['work on deep learning syntactic parsing models has achieved notably good results, e. g.,  #AUTHOR_TAG with 92. 4 f 1 on penn treebank constituency parsing and  #TAUTHOR_TAG with 92. 8 f 1.', 'in this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results, 93. 8 f 1, with a comparatively simple architecture.', 'in the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.', 'section 2 looks more closely at three of the most relevant previous papers.', 'we then describe our exact model ( section 3 ), followed by the experimental setup and results ( sections 4 and 5 ).', 'there is a one - to - one mapping between a tree and its sequential form.', '( part - of - speech tags are not used.']",0
[') as a sequence ( z )  #TAUTHOR_TAG as illustrated in'],"['of a tree ( x, y ) as a sequence ( z )  #TAUTHOR_TAG as illustrated in']","[', y ) as a sequence ( z )  #TAUTHOR_TAG as illustrated in figure 1, we can define a probability distribution over (']","['generative parsing model parses a sentence ( x ) into its phrasal structure ( y ) according to', 'where y ( x ) lists all possible structures of x. if we think of a tree ( x, y ) as a sequence ( z )  #TAUTHOR_TAG as illustrated in figure 1, we can define a probability distribution over ( x, y ) as follows :', 'which is equivalent to equation ( 1 ).', 'we have reduced parsing to language modeling and can use language modeling techniques of estimating p ( z t | z 1, · · ·, z t−1 ) for parsing']",0
['1 on penn treebank constituency parsing and  #TAUTHOR_TAG with 92'],['1 on penn treebank constituency parsing and  #TAUTHOR_TAG with 92. 8'],['4 f 1 on penn treebank constituency parsing and  #TAUTHOR_TAG with 92'],"['work on deep learning syntactic parsing models has achieved notably good results, e. g.,  #AUTHOR_TAG with 92. 4 f 1 on penn treebank constituency parsing and  #TAUTHOR_TAG with 92. 8 f 1.', 'in this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results, 93. 8 f 1, with a comparatively simple architecture.', 'in the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem.', 'section 2 looks more closely at three of the most relevant previous papers.', 'we then describe our exact model ( section 3 ), followed by the experimental setup and results ( sections 4 and 5 ).', 'there is a one - to - one mapping between a tree and its sequential form.', '( part - of - speech tags are not used.']",5
"['basic language modeling architecture that we have adopted, while the other two  #TAUTHOR_TAG are parsing models that have the current best results in nn parsing']","['basic language modeling architecture that we have adopted, while the other two  #TAUTHOR_TAG are parsing models that have the current best results in nn parsing']","['gives the basic language modeling architecture that we have adopted, while the other two  #TAUTHOR_TAG are parsing models that have the current best results in nn parsing']","['look here at three neural net ( nn ) models closest to our research along various dimensions.', 'the first  #AUTHOR_TAG gives the basic language modeling architecture that we have adopted, while the other two  #TAUTHOR_TAG are parsing models that have the current best results in nn parsing']",5
"['silver "" trees  #TAUTHOR_TAG']","['training ( 2 - 21 ), development ( 24 ) and testing ( 23 ) and millions of auto - parsed "" silver "" trees  #TAUTHOR_TAG']","['##d "" silver "" trees  #TAUTHOR_TAG']","['use the wall street journal ( wsj ) of the penn treebank  #AUTHOR_TAG for training ( 2 - 21 ), development ( 24 ) and testing ( 23 ) and millions of auto - parsed "" silver "" trees  #TAUTHOR_TAG for tritraining.', 'to obtain silver trees, we parse the entire section of the new york times ( nyt ) of the fifth gigaword  #AUTHOR_TAG with a product of eight berkeley parsers  #AUTHOR_TAG 2 and zpar  #AUTHOR_TAG and select 24 million trees on which both parsers agree  #AUTHOR_TAG.', 'we do not resample trees to match the sentence length distribution of the nyt to that of the wsj ( vinyals et 1 the code and trained models used for experiments are available at github. com / cdg720 / emnlp2016.', '2 we use the reimplementation by  #AUTHOR_TAG.', ' #AUTHOR_TAG performed better when trained on all of 24 million trees than when trained on resampled two million trees.', 'given x, we produce y ( x ), 50 - best trees, with charniak parser and find y with lstm - lm as  #AUTHOR_TAG do with their discriminative and generative models.', '']",5
['corpus 4 ( hc )  #TAUTHOR_TAG ; and an'],['trees of the highconfidence corpus 4 ( hc )  #TAUTHOR_TAG ; and an'],['of the highconfidence corpus 4 ( hc )  #TAUTHOR_TAG ; and an ensemble of six one - to - many sequence models'],"['compare lstm - lm ( gs ) to two very strong semi - supervised nn parsers : an ensemble of five mtps trained on 11 million trees of the highconfidence corpus 4 ( hc )  #TAUTHOR_TAG ; and an ensemble of six one - to - many sequence models trained on the hc and 4. 5 millions of englishgerman translation sentence pairs  #AUTHOR_TAG.', 'we also compare lstm - lm ( gs ) to best performing non - nn parsers in the literature.', ""parsers'parsing performance along with their training data is reported in table 3."", 'lstm - lm ( gs ) outperforms all the other parsers with 93. 1 f 1']",5
"['mtps  #TAUTHOR_TAG and rnng  #AUTHOR_TAG, both of which are trained on the wsj only']","['mtps  #TAUTHOR_TAG and rnng  #AUTHOR_TAG, both of which are trained on the wsj only']","['6 f 1 lstm - lm ( g ) outperforms an ensemble of five mtps  #TAUTHOR_TAG and rnng  #AUTHOR_TAG, both of which are trained on the wsj only']","['shown in table 2, with 92. 6 f 1 lstm - lm ( g ) outperforms an ensemble of five mtps  #TAUTHOR_TAG and rnng  #AUTHOR_TAG, both of which are trained on the wsj only']",4
"['on the wsj and additional resources.', 'note that the numbers of  #TAUTHOR_TAG and  #AUTHOR_TAG are not directly comparable as their models']","['on the wsj and additional resources.', 'note that the numbers of  #TAUTHOR_TAG and  #AUTHOR_TAG are not directly comparable as their models']","['on the wsj and additional resources.', 'note that the numbers of  #TAUTHOR_TAG and  #AUTHOR_TAG are not directly comparable as their models']","['', 'table 3 : evaluation of models trained on the wsj and additional resources.', 'note that the numbers of  #TAUTHOR_TAG and  #AUTHOR_TAG are not directly comparable as their models are evaluated on ontonotesstyle trees instead of ptb - style trees.', 'e ( lstm - lms ( gs ) ) is an ensemble of eight lstm - lms ( gs ).', 'x / y in silver column indicates the number of silver trees used to train charniak parser and lstm - lm.', 'for the ensemble model, we report the maximum number of trees used to train one of lstm - lms ( gs ).', 'at brown university for setting up gpu machines and david mcclosky for helping us train charniak parser on millions trees']",4
"['presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both']","['presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both']","['sparse word - based translation correspondences from supervised ranking signals have been presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both approaches']","['', 'alternative approaches avoid to solve the hard problem of word reordering, and instead rely on token - to - token translations that are used to project the query terms into the target language with a probabilistic weighting of the standard term tfidf scheme.', ' #AUTHOR_TAG termed this method the probabilistic structured query approach ( psq ).', 'the advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations  #AUTHOR_TAG.', ' #AUTHOR_TAG brought smt back into this paradigm by projecting terms from n - best translations from synchronous context - free grammars.', 'ranking approaches have been presented by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'their method is a classical learning - to - rank setup where pairwise ranking is applied to a few hundred dense features.', 'methods to learn sparse word - based translation correspondences from supervised ranking signals have been presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both approaches work in a cross - lingual setting, the former on wikipedia data, the latter on patents.', 'our approach extends the work of  #TAUTHOR_TAG by presenting an alternative learningto - rank approach that can be used for supervised model combination to integrate dense and sparse features, and by evaluating both approaches on cross - lingual retrieval for patents and wikipedia.', 'this relates our work to supervised model merging approaches  #AUTHOR_TAG']",0
"['', 'the algorithm of  #TAUTHOR_TAG combines batch boosting with bagging over a number of independently']","['on tuples.', 'the algorithm of  #TAUTHOR_TAG combines batch boosting with bagging over a number of independently']","['', 'the algorithm of  #TAUTHOR_TAG combines batch boosting with bagging over a number of independently']","['', 'is violated for the fewest number of tuples from r. we present two methods for optimizing w in the following.', 'pairwise ranking using boosting ( bm ).', 'the boosting - based ranking baseline  #AUTHOR_TAG optimizes an exponential loss :', 'where', 'is a non - negative importance function on tuples.', 'the algorithm of  #TAUTHOR_TAG combines batch boosting with bagging over a number of independently drawn bootstrap data samples from r. in each step, the single word pair feature is selected that provides the largest decrease of l exp.', 'the found corresponding models are averaged.', 'to reduce memory requirements we used random feature hashing with the size of the hash of 30 bits  #AUTHOR_TAG.', 'for regularization we rely on early stopping.', 'pairwise ranking with sgd ( vw ).', 'the second objective is an 1 - regularized hinge loss :', 'where ( x ) + = max ( 0, 1 − x ) and λ is the regularization parameter.', 'this newly added model utilizes the standard implementation of online sgd from the vowpal wabbit ( vw ) toolkit  #AUTHOR_TAG and was run on a data sample of 5m to 10m tuples from r. for wikipedia, we implemented features that compare the relative length of documents, number of links and images, the number of common links and common images, and wikipedia categories : given the categories associated with a foreign query, we use the language links on the wikipedia category pages to generate a set of "" translated "" english categories s. the englishside category graph is used to construct sets of super - and subcategories related to the candidate document\'s categories.', 'this expansion is done in both directions for two levels resulting in 5 category sets.', 'the intersection between']",0
"['of voting points which he is free to distribute among the scored documents  #TAUTHOR_TAG.', 'the']","['of voting points which he is free to distribute among the scored documents  #TAUTHOR_TAG.', 'the']","['by borda counts.', 'the baseline consensus - based voting borda count procedure endows each voter with a fixed amount of voting points which he is free to distribute among the scored documents  #TAUTHOR_TAG.', '']","['by borda counts.', 'the baseline consensus - based voting borda count procedure endows each voter with a fixed amount of voting points which he is free to distribute among the scored documents  #TAUTHOR_TAG.', '']",0
"['presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both']","['presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both']","['sparse word - based translation correspondences from supervised ranking signals have been presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both approaches']","['', 'alternative approaches avoid to solve the hard problem of word reordering, and instead rely on token - to - token translations that are used to project the query terms into the target language with a probabilistic weighting of the standard term tfidf scheme.', ' #AUTHOR_TAG termed this method the probabilistic structured query approach ( psq ).', 'the advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations  #AUTHOR_TAG.', ' #AUTHOR_TAG brought smt back into this paradigm by projecting terms from n - best translations from synchronous context - free grammars.', 'ranking approaches have been presented by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'their method is a classical learning - to - rank setup where pairwise ranking is applied to a few hundred dense features.', 'methods to learn sparse word - based translation correspondences from supervised ranking signals have been presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both approaches work in a cross - lingual setting, the former on wikipedia data, the latter on patents.', 'our approach extends the work of  #TAUTHOR_TAG by presenting an alternative learningto - rank approach that can be used for supervised model combination to integrate dense and sparse features, and by evaluating both approaches on cross - lingual retrieval for patents and wikipedia.', 'this relates our work to supervised model merging approaches  #AUTHOR_TAG']",4
"['presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both']","['presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both']","['sparse word - based translation correspondences from supervised ranking signals have been presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both approaches']","['', 'alternative approaches avoid to solve the hard problem of word reordering, and instead rely on token - to - token translations that are used to project the query terms into the target language with a probabilistic weighting of the standard term tfidf scheme.', ' #AUTHOR_TAG termed this method the probabilistic structured query approach ( psq ).', 'the advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations  #AUTHOR_TAG.', ' #AUTHOR_TAG brought smt back into this paradigm by projecting terms from n - best translations from synchronous context - free grammars.', 'ranking approaches have been presented by  #AUTHOR_TAG and  #AUTHOR_TAG.', 'their method is a classical learning - to - rank setup where pairwise ranking is applied to a few hundred dense features.', 'methods to learn sparse word - based translation correspondences from supervised ranking signals have been presented by  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'both approaches work in a cross - lingual setting, the former on wikipedia data, the latter on patents.', 'our approach extends the work of  #TAUTHOR_TAG by presenting an alternative learningto - rank approach that can be used for supervised model combination to integrate dense and sparse features, and by evaluating both approaches on cross - lingual retrieval for patents and wikipedia.', 'this relates our work to supervised model merging approaches  #AUTHOR_TAG']",6
"['from the marec and ntcir data  #TAUTHOR_TAG.', 'it contains automatically induced']","['from the marec and ntcir data  #TAUTHOR_TAG.', 'it contains automatically induced']","['from the marec and ntcir data  #TAUTHOR_TAG.', 'it contains automatically']","['use boostclir 1, a japanese - english ( jp - en ) corpus of patent abstracts from the marec and ntcir data  #TAUTHOR_TAG.', '']",5
"['by  #TAUTHOR_TAG, consisting of']","['by  #TAUTHOR_TAG, consisting of']","['by  #TAUTHOR_TAG, consisting of']","['', 'statistics are given in table 1.', 'preprocessing ranking data.', 'in addition to lowercasing and punctuation removal, we applied correlated feature hashing ( cfh ), that makes collisions more likely for words with close meaning  #AUTHOR_TAG.', 'for patents, vocabularies contained 60k and 365k words for jp and en.', 'filtering special symbols and stopwords reduced the jp vocabulary size to 50k ( small enough not to resort to cfh ).', 'to reduce the en vocabulary to a comparable size, we applied similar preprocessing and cfh with f = 30k and k = 5.', 'since for wikipedia data, the de and en vocabularies were both large ( 6. 7m and 6m ), we used the same filtering and preprocessing as for the patent data before applying cfh with f = 40k and k = 5 on both sides.', 'parallel data for smt - based clir.', 'for both tasks, dt and psq require an smt baseline system trained on parallel corpora that are disjunct from the ranking data.', 'a jp - en system was trained on data described and preprocessed by  #TAUTHOR_TAG, consisting of 1. 8m parallel sentences from the ntcir - 7 jp - en patentmt subtask  #AUTHOR_TAG and 2']",5
"['by  #TAUTHOR_TAG ; settings for wikipedia were adjusted on its dev set ( n = 1000, λ =']","['by  #TAUTHOR_TAG ; settings for wikipedia were adjusted on its dev set ( n = 1000, λ = 0. 4, l = 0, c = 1 ).', 'patent retrieval for dt was']","['by  #TAUTHOR_TAG ; settings for wikipedia were adjusted on its dev set ( n = 1000, λ = 0. 4, l = 0, c = 1 ).', 'patent retrieval for dt was done by sentencewise translation and']","['settings.', 'the smt - based models use cdec  #AUTHOR_TAG.', 'word alignments were created with mgiza ( jp - en ) and fast align  #AUTHOR_TAG ( de - en ).', 'language models were trained with the kenlm toolkit  #AUTHOR_TAG.', 'the jp - en system uses a 5 - gram language model from the en side of the training data.', 'for the de - en system, a 4 - gram model was built on the en side of the training data and the en wikipedia documents.', ""weights for the standard feature set were optimized using cdec's mert ( jp - en ) and mira ( de - en ) implementations  #AUTHOR_TAG."", 'psq on patents reuses settings found by  #TAUTHOR_TAG ; settings for wikipedia were adjusted on its dev set ( n = 1000, λ = 0. 4, l = 0, c = 1 ).', 'patent retrieval for dt was done by sentencewise translation and subsequent re - joining to form one query per patent, which was ranked against the documents using bm25.', 'for psq, bm25 is computed on expected term and document frequencies.', 'for ranking - based retrieval, we compare several combinations of learners and features ( table 2 ).', '']",5
"[',  #TAUTHOR_TAG.', 'li has reached a great']","['languages ( dsl,  #TAUTHOR_TAG.', 'li has reached a great']","[',  #TAUTHOR_TAG.', 'li has reached a great success in discriminating between languages with unique character sets and languages belonging to different language groups or typologically distant.', 'however, according to  #AUTHOR_TAG, multilingualism, noisy or non - standard features in text and discrimination between similar languages, varieties']","['identification ( li ) can be defined as the task of determining the language of a written text.', 'li is also a cross - cutting technology supporting many other text analysis tasks : sentiment analysis, political tendency or topic classification.', 'there are some interesting problems around written language identification that have attracted some attention recently, as native language identification ( nli,  #AUTHOR_TAG, the identification of the country of origin or the discrimination between similar or closely related languages ( dsl,  #TAUTHOR_TAG.', 'li has reached a great success in discriminating between languages with unique character sets and languages belonging to different language groups or typologically distant.', 'however, according to  #AUTHOR_TAG, multilingualism, noisy or non - standard features in text and discrimination between similar languages, varieties or dialects remain as the major known bottlenecks in language identification.', 'for this reason, dsl can be considered as a sub - task in language identification.', 'interestingly enough, li seems to work well with what  #AUTHOR_TAG called abstandsprache or language by distance ( because basque is an isolate, it is generally regarded as a distant language ) but fails in dealing with ausbausprache or language by development ( a standard variety together with all varieties heteronomous with respect to it, e. g. basque batua koine and the various vernacular dialects ).', 'mass media, educational centres, administrations and communications favour standard languages instead of other varieties.', 'standard varieties of languages are then seen by sociolinguists and dialectologists as political and cultural constructs  #AUTHOR_TAG.', 'however, languages and varieties are not just systems for communication between individuals, they are also used by groups and they are a crucial part of their identity and culture.', 'language variation is systematic, both inter - and intra - personal.', 'it can be related to political, social, geographical, situational, communicative or instrumental factors.', 'variation within a language can be found at different levels : alphabet, orthography ( diacritics ), word structure ( syllable composition, morphology ), lexical choice or even syntax.', 'similar or closely related languages often reflect a common origin and are members of a dialect continuum  #AUTHOR_TAG.', 'solutions to language identification are often based either on generative or discriminative character n - gram language models.', 'while character - based methods provide a means to distinguish between different languages on the basis of coarse - grained statistics on n - grams, it seems that discriminating between similar languages needs more fine - grained distinctions not always reflected by n - gram character distributions.', 'according to  #TAUTHOR_TAG, character - based n - gram methods fail for languages with']",0
"[',  #TAUTHOR_TAG.', 'li has reached a great']","['languages ( dsl,  #TAUTHOR_TAG.', 'li has reached a great']","[',  #TAUTHOR_TAG.', 'li has reached a great success in discriminating between languages with unique character sets and languages belonging to different language groups or typologically distant.', 'however, according to  #AUTHOR_TAG, multilingualism, noisy or non - standard features in text and discrimination between similar languages, varieties']","['identification ( li ) can be defined as the task of determining the language of a written text.', 'li is also a cross - cutting technology supporting many other text analysis tasks : sentiment analysis, political tendency or topic classification.', 'there are some interesting problems around written language identification that have attracted some attention recently, as native language identification ( nli,  #AUTHOR_TAG, the identification of the country of origin or the discrimination between similar or closely related languages ( dsl,  #TAUTHOR_TAG.', 'li has reached a great success in discriminating between languages with unique character sets and languages belonging to different language groups or typologically distant.', 'however, according to  #AUTHOR_TAG, multilingualism, noisy or non - standard features in text and discrimination between similar languages, varieties or dialects remain as the major known bottlenecks in language identification.', 'for this reason, dsl can be considered as a sub - task in language identification.', 'interestingly enough, li seems to work well with what  #AUTHOR_TAG called abstandsprache or language by distance ( because basque is an isolate, it is generally regarded as a distant language ) but fails in dealing with ausbausprache or language by development ( a standard variety together with all varieties heteronomous with respect to it, e. g. basque batua koine and the various vernacular dialects ).', 'mass media, educational centres, administrations and communications favour standard languages instead of other varieties.', 'standard varieties of languages are then seen by sociolinguists and dialectologists as political and cultural constructs  #AUTHOR_TAG.', 'however, languages and varieties are not just systems for communication between individuals, they are also used by groups and they are a crucial part of their identity and culture.', 'language variation is systematic, both inter - and intra - personal.', 'it can be related to political, social, geographical, situational, communicative or instrumental factors.', 'variation within a language can be found at different levels : alphabet, orthography ( diacritics ), word structure ( syllable composition, morphology ), lexical choice or even syntax.', 'similar or closely related languages often reflect a common origin and are members of a dialect continuum  #AUTHOR_TAG.', 'solutions to language identification are often based either on generative or discriminative character n - gram language models.', 'while character - based methods provide a means to distinguish between different languages on the basis of coarse - grained statistics on n - grams, it seems that discriminating between similar languages needs more fine - grained distinctions not always reflected by n - gram character distributions.', 'according to  #TAUTHOR_TAG, character - based n - gram methods fail for languages with']",0
['up in  #TAUTHOR_TAG where 9'],['up in  #TAUTHOR_TAG where 9'],['work is followed up in  #TAUTHOR_TAG where 9'],"['work is followed up in  #TAUTHOR_TAG where 9 % of improvement over standard approaches is reported and where', 'support for bosnian discrimination is included.  #AUTHOR_TAG use a bag of the most frequent words to build a voting identifier for three chinese varieties with a top', 'accuracy of 0. 929. more recently,  #AUTHOR_TAG compares the performance of n - gram based models to machine learning', 'methods using bag of words when discriminating similar languages and varieties obtaining comparable performance with both approaches.  #AUTHOR_TAG present the', 'shared task deft 2010. participants were challenged to identify the decade, country', '( france and canada ) and newspaper for a set of journalistic texts. as far as the country labeling is concerned, they report an upper 0. 964', 'f 1 - measure and an average of 0. 767. very brief descriptions of the systems are also offered.  #AUTHOR_TAG present a log - likelihood estimation method for language models built on orthographical ( character n - grams ), lexical ( word unigrams', ') and lexico - syntactic ( word bigrams ) features. they report a 0. 998 accuracy distinguishing european and brazilian portuguese with a language model based on character 4 - grams. this approach is adapted in to deal with spanish varieties, where the role of knowledge - rich features ( pos tags )', 'is also explored. they report a 0. 99 accuracy when binarily distinguishing argentinean and mexican spanish with single words or bigrams.  #AUTHOR_TAG compare the performance of textcat to the nearest neighbour and nearest prototype in combination with a cosine distance when distinguishing among sixteen varieties of dutch.', 'they report a micro - average f 1 - score of 0. 799 ( and a macro - average f 1 - score of 0. 527 ) with a top f 1 - score', 'of 0. 987 when dealing with frisian.  #AUTHOR_TAG report experiments with different classifiers to map english documents to their country of origin. an svm classifier with bag of words', 'is top ranked with a macro - average 0. 911 f 1 - score in a cross -', 'domain setting and 0. 975 in an in - domain setting. all these previous works ( with the sole', 'exception of  #AUTHOR_TAG, where a general purpose li system yields a satisfactory performance ) agree in the specificity of dsl regarding li. maybe', 'because of that, two level approaches are not uncommon. features used to discriminate seem to be languagegroup specific, altough word rather than character features seem to perform', 'better (  #AUTHOR_TAG report best results for character 4 - grams, however, given that european and brazilian portuguese do not completely share ortography )']",0
['up in  #TAUTHOR_TAG where 9'],['up in  #TAUTHOR_TAG where 9'],['work is followed up in  #TAUTHOR_TAG where 9'],"['work is followed up in  #TAUTHOR_TAG where 9 % of improvement over standard approaches is reported and where', 'support for bosnian discrimination is included.  #AUTHOR_TAG use a bag of the most frequent words to build a voting identifier for three chinese varieties with a top', 'accuracy of 0. 929. more recently,  #AUTHOR_TAG compares the performance of n - gram based models to machine learning', 'methods using bag of words when discriminating similar languages and varieties obtaining comparable performance with both approaches.  #AUTHOR_TAG present the', 'shared task deft 2010. participants were challenged to identify the decade, country', '( france and canada ) and newspaper for a set of journalistic texts. as far as the country labeling is concerned, they report an upper 0. 964', 'f 1 - measure and an average of 0. 767. very brief descriptions of the systems are also offered.  #AUTHOR_TAG present a log - likelihood estimation method for language models built on orthographical ( character n - grams ), lexical ( word unigrams', ') and lexico - syntactic ( word bigrams ) features. they report a 0. 998 accuracy distinguishing european and brazilian portuguese with a language model based on character 4 - grams. this approach is adapted in to deal with spanish varieties, where the role of knowledge - rich features ( pos tags )', 'is also explored. they report a 0. 99 accuracy when binarily distinguishing argentinean and mexican spanish with single words or bigrams.  #AUTHOR_TAG compare the performance of textcat to the nearest neighbour and nearest prototype in combination with a cosine distance when distinguishing among sixteen varieties of dutch.', 'they report a micro - average f 1 - score of 0. 799 ( and a macro - average f 1 - score of 0. 527 ) with a top f 1 - score', 'of 0. 987 when dealing with frisian.  #AUTHOR_TAG report experiments with different classifiers to map english documents to their country of origin. an svm classifier with bag of words', 'is top ranked with a macro - average 0. 911 f 1 - score in a cross -', 'domain setting and 0. 975 in an in - domain setting. all these previous works ( with the sole', 'exception of  #AUTHOR_TAG, where a general purpose li system yields a satisfactory performance ) agree in the specificity of dsl regarding li. maybe', 'because of that, two level approaches are not uncommon. features used to discriminate seem to be languagegroup specific, altough word rather than character features seem to perform', 'better (  #AUTHOR_TAG report best results for character 4 - grams, however, given that european and brazilian portuguese do not completely share ortography )']",0
"['', ') or clitic dropping ( lembro ). despite findings by', ' #TAUTHOR_TAG, character n - grams performed better during tenfold']","['brazilian preference for passive voice ( foram rebaixados ), auxiliary + gerund chunks ( estamos utilizando', ') or clitic dropping ( lembro ). despite findings by', ' #TAUTHOR_TAG, character n - grams performed better during tenfold cross - validation on the training dataset for']","['##s utilizando', ') or clitic dropping ( lembro ). despite findings by', ' #TAUTHOR_TAG, character n - grams performed better during tenfold cross - validation on the training dataset for']","['e ) finally, there is at least one text misclassified in the gold standard : it is labeled as argent', '##inian but it was written by the spanish efe news agency. some of these difficulties', 'cross - cut all language groups and are not specific to spanish but rather to dsl as a task. in contrast to what  #AUTHOR_TAG found, ten - fold cross - validation on the training dataset for different', 'feature settings on the dsl dataset did not find character n - grams to outperform word ngrams for group d ( portuguese ). it could be hypothesized', 'that they used a unique source ( newspaper ) for each variety and therefore rigid editorial conventions could be at play ; moreover, the collections were three years distant,', 'so topic consistency could also be compromised 6. manual inspection of mislabeled sentences shows some already known categories : evidence diluted by foreign words ( red brick warehouse, meszaros, fat duc', '##k ), poor evidence ( valongo, sao paulo ) or cross - information ( tap', ', brasilia ). there is, however, a portuguese -', 'specific issue : some texts obey the 1990 orthographic agreement 7 which blurs the orthographic distinctions regarding diacritics', 'or consonant clusters ; in fact, one sentence contains words following both standards ( perspectiva and reproducao ). it remains unexplained why word bigrams did not', 'capture the brazilian preference for passive voice ( foram rebaixados ), auxiliary + gerund chunks ( estamos utilizando', ') or clitic dropping ( lembro ). despite findings by', ' #TAUTHOR_TAG, character n - grams performed better during tenfold cross - validation on the training dataset for different feature settings on the dsl', 'dataset for group a ( bosnian, croatian and serbian ). misclassified sentences involve failing to capture adapted place names ( belgiji, svedskoj ) or derivational choices ( organiziranog ). results of ten - fold cross - validation on the training dataset for different feature settings for', 'group b ( indonesian and malay ) top ranked word unigrams. ranaivo - malancon ( 2006 ) uses number formatting and exclusive word lists. it can', ""be hypothesized that lexical overlap is low ( see table 2 ) and / or frequency distributions are dissimilar thus allowing word unigrams to perform as well as'white lists '"", '. languages of group c ( czech and slovak ) are dissimilar both orthographically and lexically. these dissimilarities are surprisingly well captured by the top', '10, 000 most frequent words']",3
"[' #TAUTHOR_TAG suggest, to']","[' #TAUTHOR_TAG suggest, to']","[' #TAUTHOR_TAG suggest, to']","['this paper, we have shown that a hierarchical classifier is well suited to discriminate among different language groups and languages or varieties therein.', 'different features are shown to better suit typological traits of supported languages.', 'a comparison to previous approaches is provided, when available.', 'in a multilingual setting, the effect of adding galician to group d could be investigated.', 'focusing on spanish language, we plan to geographically expand the classifier to deal with all national varieties, a much harder task as both  #AUTHOR_TAG and remark.', 'moreover, the classifier could be used, as  #TAUTHOR_TAG suggest, to learn varieties discriminators to label texts beyond national classes ( e. g. both caribbean and andean spanish cross - cut national borders and, conversely, nations involved are known not to be dialectally uniform ).', 'given that error analysis showed that word bigrams fail to capture certain syntactical idiosyncrasies, a model with longer n - grams and / or knowledge - richer features such as pos sequences could also be explored, although report lower performance than knowledge - poor features.', 'finally, classification techniques such as those described in  #AUTHOR_TAG may be used to discard translations when building monolingual, vernacular corpora.', 'a diachronic expansion, such as  #AUTHOR_TAG, is also in mind.', 'medieval castilian coexisted with other romance varieties such as leonese or aragonese whose features permeated castilian texts.', 'researchers are in need of a tool to properly classify diachronic texts to accurately describe older stages of spanish.', 'following the suggestion of  #TAUTHOR_TAG, we envisage the use of parallel texts such as versions of the bible from different areas to learn the differences among varieties']",3
"['', ') or clitic dropping ( lembro ). despite findings by', ' #TAUTHOR_TAG, character n - grams performed better during tenfold']","['brazilian preference for passive voice ( foram rebaixados ), auxiliary + gerund chunks ( estamos utilizando', ') or clitic dropping ( lembro ). despite findings by', ' #TAUTHOR_TAG, character n - grams performed better during tenfold cross - validation on the training dataset for']","['##s utilizando', ') or clitic dropping ( lembro ). despite findings by', ' #TAUTHOR_TAG, character n - grams performed better during tenfold cross - validation on the training dataset for']","['e ) finally, there is at least one text misclassified in the gold standard : it is labeled as argent', '##inian but it was written by the spanish efe news agency. some of these difficulties', 'cross - cut all language groups and are not specific to spanish but rather to dsl as a task. in contrast to what  #AUTHOR_TAG found, ten - fold cross - validation on the training dataset for different', 'feature settings on the dsl dataset did not find character n - grams to outperform word ngrams for group d ( portuguese ). it could be hypothesized', 'that they used a unique source ( newspaper ) for each variety and therefore rigid editorial conventions could be at play ; moreover, the collections were three years distant,', 'so topic consistency could also be compromised 6. manual inspection of mislabeled sentences shows some already known categories : evidence diluted by foreign words ( red brick warehouse, meszaros, fat duc', '##k ), poor evidence ( valongo, sao paulo ) or cross - information ( tap', ', brasilia ). there is, however, a portuguese -', 'specific issue : some texts obey the 1990 orthographic agreement 7 which blurs the orthographic distinctions regarding diacritics', 'or consonant clusters ; in fact, one sentence contains words following both standards ( perspectiva and reproducao ). it remains unexplained why word bigrams did not', 'capture the brazilian preference for passive voice ( foram rebaixados ), auxiliary + gerund chunks ( estamos utilizando', ') or clitic dropping ( lembro ). despite findings by', ' #TAUTHOR_TAG, character n - grams performed better during tenfold cross - validation on the training dataset for different feature settings on the dsl', 'dataset for group a ( bosnian, croatian and serbian ). misclassified sentences involve failing to capture adapted place names ( belgiji, svedskoj ) or derivational choices ( organiziranog ). results of ten - fold cross - validation on the training dataset for different feature settings for', 'group b ( indonesian and malay ) top ranked word unigrams. ranaivo - malancon ( 2006 ) uses number formatting and exclusive word lists. it can', ""be hypothesized that lexical overlap is low ( see table 2 ) and / or frequency distributions are dissimilar thus allowing word unigrams to perform as well as'white lists '"", '. languages of group c ( czech and slovak ) are dissimilar both orthographically and lexically. these dissimilarities are surprisingly well captured by the top', '10, 000 most frequent words']",4
"[' #TAUTHOR_TAG suggest, to']","[' #TAUTHOR_TAG suggest, to']","[' #TAUTHOR_TAG suggest, to']","['this paper, we have shown that a hierarchical classifier is well suited to discriminate among different language groups and languages or varieties therein.', 'different features are shown to better suit typological traits of supported languages.', 'a comparison to previous approaches is provided, when available.', 'in a multilingual setting, the effect of adding galician to group d could be investigated.', 'focusing on spanish language, we plan to geographically expand the classifier to deal with all national varieties, a much harder task as both  #AUTHOR_TAG and remark.', 'moreover, the classifier could be used, as  #TAUTHOR_TAG suggest, to learn varieties discriminators to label texts beyond national classes ( e. g. both caribbean and andean spanish cross - cut national borders and, conversely, nations involved are known not to be dialectally uniform ).', 'given that error analysis showed that word bigrams fail to capture certain syntactical idiosyncrasies, a model with longer n - grams and / or knowledge - richer features such as pos sequences could also be explored, although report lower performance than knowledge - poor features.', 'finally, classification techniques such as those described in  #AUTHOR_TAG may be used to discard translations when building monolingual, vernacular corpora.', 'a diachronic expansion, such as  #AUTHOR_TAG, is also in mind.', 'medieval castilian coexisted with other romance varieties such as leonese or aragonese whose features permeated castilian texts.', 'researchers are in need of a tool to properly classify diachronic texts to accurately describe older stages of spanish.', 'following the suggestion of  #TAUTHOR_TAG, we envisage the use of parallel texts such as versions of the bible from different areas to learn the differences among varieties']",5
"['as well  #TAUTHOR_TAG.', ""we'll see how learning the referents of words and learning the roles of social cues in""]","['as well  #TAUTHOR_TAG.', ""we'll see how learning the referents of words and learning the roles of social cues in""]","['as well  #TAUTHOR_TAG.', ""we'll see how learning the referents of words and learning the roles of social cues in language acquisition  #AUTHOR_TAG can be viewed as a kind of topic modelling problem that can be reduced to a grammatical inference problem using the techniques described in this talk""]","['- free grammars have been a cornerstone of theoretical computer science and computational linguistics since their inception over half a century ago.', 'topic models are a newer development in machine learning that play an important role in document analysis and information retrieval.', 'it turns out there is a surprising connection between the two that suggests novel ways of extending both grammars and topic models.', 'after explaining this connection, i go on to describe extensions which identify topical multiword collocations and automatically learn the internal structure of namedentity phrases.', 'the adaptor grammar framework is a nonparametric extension of probabilistic context - free grammars  #AUTHOR_TAG, which was initially intended to allow fast prototyping of models of unsupervised language acquisition  #AUTHOR_TAG, but it has been shown to have applications in text data mining and information retrieval as well  #TAUTHOR_TAG.', ""we'll see how learning the referents of words and learning the roles of social cues in language acquisition  #AUTHOR_TAG can be viewed as a kind of topic modelling problem that can be reduced to a grammatical inference problem using the techniques described in this talk""]",0
"['##uan chen and etc.', ' #TAUTHOR_TAG ever proposed a']","['without teaching.', 'zhiyuan chen and etc.', ' #TAUTHOR_TAG ever proposed a']","['##uan chen and etc.', ' #TAUTHOR_TAG ever proposed a approach to close the goal.', 'they made a big progress but the supervised learning still is needed.', 'guangyi lv and etc.', '']","['', 'if we can achieve this learning goal, the algorithms are able to solve new tasks without teaching.', 'zhiyuan chen and etc.', ' #TAUTHOR_TAG ever proposed a approach to close the goal.', 'they made a big progress but the supervised learning still is needed.', 'guangyi lv and etc.', '[ 4 ] extend the work of  #TAUTHOR_TAG with a neural network based approach.', 'however, the supervised learning still is necessary under their setting and huge volume of labeled data are required.', 'hence, this paper aims to decrease the usage of labeled data while maintain the performance']",0
"['##uan chen and etc.', ' #TAUTHOR_TAG ever proposed a']","['without teaching.', 'zhiyuan chen and etc.', ' #TAUTHOR_TAG ever proposed a']","['##uan chen and etc.', ' #TAUTHOR_TAG ever proposed a approach to close the goal.', 'they made a big progress but the supervised learning still is needed.', 'guangyi lv and etc.', '']","['', 'if we can achieve this learning goal, the algorithms are able to solve new tasks without teaching.', 'zhiyuan chen and etc.', ' #TAUTHOR_TAG ever proposed a approach to close the goal.', 'they made a big progress but the supervised learning still is needed.', 'guangyi lv and etc.', '[ 4 ] extend the work of  #TAUTHOR_TAG with a neural network based approach.', 'however, the supervised learning still is necessary under their setting and huge volume of labeled data are required.', 'hence, this paper aims to decrease the usage of labeled data while maintain the performance']",0
"['is much more efficient.', 'zhiyuan and etc.', ' #TAUTHOR_TAG improved']","['[ 1 ], ella is much more efficient.', 'zhiyuan and etc.', ' #TAUTHOR_TAG improved']","['is much more efficient.', 'zhiyuan and etc.', ' #TAUTHOR_TAG improved']","['was firstly called as lifelong machine learning since 1995 by thrun [ 7, 9 ].', 'efficient lifelong machine learning ( ella ) [ 6 ] raised by ruvolo and eaton.', 'comparing with the multi - task learning [ 1 ], ella is much more efficient.', 'zhiyuan and etc.', ' #TAUTHOR_TAG improved the sentiment classification by involving knowledge.', 'the object function was modified with two penalty terms which corresponding with previous tasks.', 'the knowledge system contains the following components']",0
[' #TAUTHOR_TAG mainly'],[' #TAUTHOR_TAG mainly'],[' #TAUTHOR_TAG mainly'],[' #TAUTHOR_TAG'],0
[' #TAUTHOR_TAG mainly'],[' #TAUTHOR_TAG mainly'],[' #TAUTHOR_TAG mainly'],[' #TAUTHOR_TAG'],0
"['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the']","['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the']","['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose']","['and etc. [ 3 ] had discussed that the nlp field is most suitable for the lifelong machine learning researches due to its knowledge is easy to extract and to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub - tasks in the different domains.', '']",0
"['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the']","['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the']","['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose']","['and etc. [ 3 ] had discussed that the nlp field is most suitable for the lifelong machine learning researches due to its knowledge is easy to extract and to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub - tasks in the different domains.', '']",0
"['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the']","['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the']","['to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose']","['and etc. [ 3 ] had discussed that the nlp field is most suitable for the lifelong machine learning researches due to its knowledge is easy to extract and to be understood by human.', 'previous classical paper  #TAUTHOR_TAG chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub - tasks in the different domains.', '']",5
"['below as in the lsc  #TAUTHOR_TAG.', 'p (']","['below as in the lsc  #TAUTHOR_TAG.', 'p ( w']","['below as in the lsc  #TAUTHOR_TAG.', 'p (']","['this paper, we define a word has sentiment polarity by calculating the probability that it appears in a positive or negative content ( sentence or document ).', 'if a word has a high probability with sentiment polarity, it also will leads to the document have higher probability of sentiment probability based on the naive bayesian ( nb ) formula.', 'hence, to determine the words with polarity is the key to predict the sentiment.', 'naive bayesian ( nb ) classifier [ 5 ] calculates the probability of each word w in a document d and then to predict the sentiment polarity ( positive or negative ).', 'we use the same formula below as in the lsc  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG used.', 'it contains the reviews']","['datasets as lsc  #TAUTHOR_TAG used.', 'it contains the reviews']","['the experiment, we use the same datasets as lsc  #TAUTHOR_TAG used.', 'it contains the reviews']","['the experiment, we use the same datasets as lsc  #TAUTHOR_TAG used.', 'it contains the reviews from 20 domains crawled from the amazon. com and each domain has 1, 000 reviews ( the distribution of positive and negative reviews is imbalanced )']",5
"['lsc  #TAUTHOR_TAG already raised a lifelong approach,']","['lsc  #TAUTHOR_TAG already raised a lifelong approach,']","['lsc  #TAUTHOR_TAG already raised a lifelong approach,']","['lsc  #TAUTHOR_TAG already raised a lifelong approach, it only aims to improve the classification accuracy.', 'it still is under the setting of the supervised learning and also is unable to deliver an explicit knowledge to guild further learning.', 'based on the lsc, this paper advances the lifelong learning in sentiment classification and have two main contributions :', '• a improved lifelong learning paradigm is proposed to solve the sentiment classification problem under unsupervised learning setting with previous knowledge.', '• we introduce a novel approach to discover and store the words with sentiment polarity for reuse']",6
"[',  #TAUTHOR_TAG have shown in the past that deep linguistic parsing outputs']","['language processing techniques.', 'as an example of such work,  #TAUTHOR_TAG have shown in the past that deep linguistic parsing outputs']","['various language processing techniques.', 'as an example of such work,  #TAUTHOR_TAG have shown in the past that deep linguistic parsing outputs']","['', 'as an example of such work,  #TAUTHOR_TAG have shown in the past that deep linguistic parsing outputs can be integrated to help improve the performance of the english semantic role labeling task.', 'but several questions remain unanswered.', 'first, the integration only experimented with the semantic role labeling part of the task.', 'it is not clear whether syntactic dependency parsing can also benefit from grammar - based parsing results.', 'second, the english grammar used to achieve the improvement is one of the largest and most mature hand - crafted linguistic grammars.', 'it is not clear whether similar improvements can be achieved with less developed grammars.', 'more specifically, the lack of coverage of hand - crafted linguistic grammars is a major concern.', 'on the other hand, the conll task is also a good opportunity for the deep processing community to ( re - ) evaluate their resources and software']",0
['of  #TAUTHOR_TAG has been'],['of  #TAUTHOR_TAG has been'],"['task brings substantial performance improvement.', 'the conclusion of  #TAUTHOR_TAG has been reconfirmed on multiple languages for which we handbuilt hpsg grammars exist, even where grammatical coverage is low.', 'also, the gain is more significant on out - of - domain tests, indicating that']","['this paper, we described our syntactic parsing and semantic role labeling system participated in both closed and open challenge of the ( joint ) conll 2009 shared task.', 'four hand - written hpsg grammars of a variety of scale have been applied to parse the datasets, and the outcomes were integrated as features into the semantic role labeler of the system.', 'the results clearly show that the integration of hpsg parsing results in the semantic role labeling task brings substantial performance improvement.', 'the conclusion of  #TAUTHOR_TAG has been reconfirmed on multiple languages for which we handbuilt hpsg grammars exist, even where grammatical coverage is low.', 'also, the gain is more significant on out - of - domain tests, indicating that the hybrid system is more robust to cross - domain variation']",0
"[' #TAUTHOR_TAG.', 'three major components were involved.', '']","[' #TAUTHOR_TAG.', 'three major components were involved.', '']","[' #TAUTHOR_TAG.', 'three major components were involved.', 'the hpsg parsing']","['overall system architecture is shown in figure 1.', 'it is similar to the architecture used by  #TAUTHOR_TAG.', 'three major components were involved.', 'the hpsg parsing component utilizes several handcrafted grammars for deep linguistic parsing.', '']",3
"[""semantic role labeling modules for the'open'challenge."", 'deep semantic features similar to  #TAUTHOR_TAG, we']","[""semantic role labeling modules for the'open'challenge."", 'deep semantic features similar to  #TAUTHOR_TAG, we']","[""semantic role labeling modules for the'open'challenge."", 'deep semantic features similar to  #TAUTHOR_TAG, we extract a set of features from']","['##sg parsing coverage and average cpu time per input for the four languages with delph - in grammars are summarized in table 1.', 'the pos - based unknown word mechanism was active for all grammars but no other robustness measures ( which tend to lower the quality of results ) were used, i. e. only complete spanning hpsg analyses were accepted.', 'parse times are for 1 - best parsing, using selective unpacking  #AUTHOR_TAG.', 'hpsg parsing outputs are available in several different forms.', 'we investigated two types of structures : syntactic derivations and mrs meaningrepresentations.', ""representative features were extracted from both structures and selectively used in the statistical syntactic dependency parsing and semantic role labeling modules for the'open'challenge."", 'deep semantic features similar to  #TAUTHOR_TAG, we extract a set of features from the semantic outputs ( mrs ) of the hpsg parses.', 'these features represent the basic predicate - argument structure, and provides a simplified semantic view on the target sentence.', 'deep syntactic dependency features a hpsg derivation is a tree structure.', 'the internal nodes are labeled with identifiers of grammar rules, and leaves with lexical entries.', 'the derivation tree provides complete information about the actual hpsg analysis, and can be used together with the grammar to reproduce complete feature structure and / or mrs.', 'given that the shared task adopts dependency representation, we further map the derivation trees into token - token dependencies, labeled by corresponding hpsg rules, by defining a set of head - finding rules for each grammar.', 'this dependency structure is different from the dependencies in conll dataset, and provides an alternative hpsg view on the sentences.', 'we refer to this structure as the dependency backbone ( db ) of the hpsg anaylsis.', 'a set of features were extracted from the deep syntactic dependency structures.', ""this includes : i ) the pos of the db parent from the predicate and / or argument ; ii ) db label of the argument to its parent ( only for ai / ac ) ; iii ) labeled path from predicate to argument in db ( only for ai / ac ) ; iv ) poses of the predicate's db dependent""]",3
"['described by  #TAUTHOR_TAG.', ""since predicates are indicated in the data, the predicate identification module is removed from this year's system."", 'argument identification, argument']","['described by  #TAUTHOR_TAG.', ""since predicates are indicated in the data, the predicate identification module is removed from this year's system."", 'argument identification, argument']","['described by  #TAUTHOR_TAG.', ""since predicates are indicated in the data, the predicate identification module is removed from this year's system."", 'argument identification, argument classification and predicate classification are the three sub - components in the pipeline.', '']","['semantic role labeling component used in the submitted system is similar to the one described by  #TAUTHOR_TAG.', ""since predicates are indicated in the data, the predicate identification module is removed from this year's system."", 'argument identification, argument classification and predicate classification are the three sub - components in the pipeline.', 'all of them are maxent - based classifiers.', 'for parameter estimation, we use the open source tadm system  #AUTHOR_TAG.', 'the active features used in various steps of srl are fine tuned separately for different languages using development datasets.', 'the significance of feature types varies across languages and datasets.', 'in the open challenge, two groups of extra features from hpsg parsing outputs, as described in section 3. 3, were used on languages for which we have hpsg grammars, that is english, german, japanese, and spanish']",3
"['of  #TAUTHOR_TAG, the gain with hpsg features is more significant']","['of  #TAUTHOR_TAG, the gain with hpsg features is more significant']","['of  #TAUTHOR_TAG, the gain with hpsg features is more significant']","['', 'while the system achieves mediocre performance, the clear performance difference between the closed and open challenges of the semantic role labeler indicates a substantial gain from the integration of hpsg parsing outputs.', 'the most interesting observation is that even with grammars which only achieve very limited coverage, noticeable srl improvements are obtained.', 'confirming the observation of  #TAUTHOR_TAG, the gain with hpsg features is more significant on outdomain tests, this time on german as well.', 'the training of the syntactic parsing models for all seven languages with mst parser takes about 100 cpu hours with 10 iterations.', 'the dependency parsing takes 6 - 7 cpu hours.', 'the training and testing of the semantic role labeler is much more efficient, thanks to the use of maxent models and the efficient parameter estimation software.', 'the training of all srl models for 7 languages takes about 3 cpu hours in total.', 'the total time for semantic role labeling on test datasets is less than 1 hour.', 'figure 2 shows the learning curve of the syntactic parser and semantic role labeler on the czech and english datasets.', '']",3
"[' #TAUTHOR_TAG.', 'three major components were involved.', '']","[' #TAUTHOR_TAG.', 'three major components were involved.', '']","[' #TAUTHOR_TAG.', 'three major components were involved.', 'the hpsg parsing']","['overall system architecture is shown in figure 1.', 'it is similar to the architecture used by  #TAUTHOR_TAG.', 'three major components were involved.', 'the hpsg parsing component utilizes several handcrafted grammars for deep linguistic parsing.', '']",4
"[' #TAUTHOR_TAG.', 'three major components were involved.', '']","[' #TAUTHOR_TAG.', 'three major components were involved.', '']","[' #TAUTHOR_TAG.', 'three major components were involved.', 'the hpsg parsing']","['overall system architecture is shown in figure 1.', 'it is similar to the architecture used by  #TAUTHOR_TAG.', 'three major components were involved.', 'the hpsg parsing component utilizes several handcrafted grammars for deep linguistic parsing.', '']",4
['graph  #TAUTHOR_TAG'],['graph  #TAUTHOR_TAG'],['incorporates this information in assessing node relatedness in the graph  #TAUTHOR_TAG. pc'],"['', 'varying importance for different types of similarity sought. for example, in the parsed text domain, noun similarity and verb similarity', 'are associated with different syntactic phenomena  #AUTHOR_TAG. to this end, we consider a path constrained graph walk ( pc', '##w ) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph  #TAUTHOR_TAG. pcw have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire', 'text  #TAUTHOR_TAG, where the specialized measures learned outperformed the state - ofthe - art dependency vectors method ( pado and  #AUTHOR_TAG for small - and medium - sized', 'corpora. in this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text', ', conducting a set of experiments on the task of synonym extraction. while the tasks of named entity extraction and synonym extraction', 'from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general', 'framework. our results are encouraging : the pcw model yields superior results to the dependency vectors approach. further, we show that learning specialized similarity measures per word type ( nouns, verbs and adjectives ) is preferable to applying', 'a uniform model for all word types']",0
['graph  #TAUTHOR_TAG'],['graph  #TAUTHOR_TAG'],['incorporates this information in assessing node relatedness in the graph  #TAUTHOR_TAG. pc'],"['', 'varying importance for different types of similarity sought. for example, in the parsed text domain, noun similarity and verb similarity', 'are associated with different syntactic phenomena  #AUTHOR_TAG. to this end, we consider a path constrained graph walk ( pc', '##w ) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph  #TAUTHOR_TAG. pcw have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire', 'text  #TAUTHOR_TAG, where the specialized measures learned outperformed the state - ofthe - art dependency vectors method ( pado and  #AUTHOR_TAG for small - and medium - sized', 'corpora. in this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text', ', conducting a set of experiments on the task of synonym extraction. while the tasks of named entity extraction and synonym extraction', 'from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general', 'framework. our results are encouraging : the pcw model yields superior results to the dependency vectors approach. further, we show that learning specialized similarity measures per word type ( nouns, verbs and adjectives ) is preferable to applying', 'a uniform model for all word types']",0
"['##w is a graph walk variant proposed recently that is intended to bias the random walk process to follow meaningful edge sequences ( paths )  #TAUTHOR_TAG.', 'in this approach, rather than assume fixed ( possibly, uniform ) edge weight parameters θ for the']","['##w is a graph walk variant proposed recently that is intended to bias the random walk process to follow meaningful edge sequences ( paths )  #TAUTHOR_TAG.', 'in this approach, rather than assume fixed ( possibly, uniform ) edge weight parameters θ for the']","['##w is a graph walk variant proposed recently that is intended to bias the random walk process to follow meaningful edge sequences ( paths )  #TAUTHOR_TAG.', 'in this approach, rather than assume fixed ( possibly, uniform ) edge weight parameters θ for the various edge types in the graph, the probability of following an edge of type [UNK] from node x is evaluated dynamically, based on the history']","['##w is a graph walk variant proposed recently that is intended to bias the random walk process to follow meaningful edge sequences ( paths )  #TAUTHOR_TAG.', 'in this approach, rather than assume fixed ( possibly, uniform ) edge weight parameters θ for the various edge types in the graph, the probability of following an edge of type [UNK] from node x is evaluated dynamically, based on the history of the walk up to x.', 'the pcw algorithm includes two components.', 'first, it should provide estimates of edge weights conditioned on the history of a walk, based on training examples.', 'second, the random walk algorithm has to be modified to maintain historical information about the walk compactly.', 'in learning, a dataset of n labelled example queries is provided.', 'the labeling schema is binary, where a set of nodes considered as relevant answers to an example query e i, denoted as r i, is specified, and graph nodes that are not explicitly included in r i are assumed irrelevant to e i.', 'as a starting point, an initial graph walk is applied to generate a ranked list of graph nodes l i for every example query e i.', '']",0
['graph  #TAUTHOR_TAG'],['graph  #TAUTHOR_TAG'],['incorporates this information in assessing node relatedness in the graph  #TAUTHOR_TAG. pc'],"['', 'varying importance for different types of similarity sought. for example, in the parsed text domain, noun similarity and verb similarity', 'are associated with different syntactic phenomena  #AUTHOR_TAG. to this end, we consider a path constrained graph walk ( pc', '##w ) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph  #TAUTHOR_TAG. pcw have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire', 'text  #TAUTHOR_TAG, where the specialized measures learned outperformed the state - ofthe - art dependency vectors method ( pado and  #AUTHOR_TAG for small - and medium - sized', 'corpora. in this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text', ', conducting a set of experiments on the task of synonym extraction. while the tasks of named entity extraction and synonym extraction', 'from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general', 'framework. our results are encouraging : the pcw model yields superior results to the dependency vectors approach. further, we show that learning specialized similarity measures per word type ( nouns, verbs and adjectives ) is preferable to applying', 'a uniform model for all word types']",5
['graph  #TAUTHOR_TAG'],['graph  #TAUTHOR_TAG'],['incorporates this information in assessing node relatedness in the graph  #TAUTHOR_TAG. pc'],"['', 'varying importance for different types of similarity sought. for example, in the parsed text domain, noun similarity and verb similarity', 'are associated with different syntactic phenomena  #AUTHOR_TAG. to this end, we consider a path constrained graph walk ( pc', '##w ) algorithm, which allows one to learn meaningful paths given a small number of labeled examples and incorporates this information in assessing node relatedness in the graph  #TAUTHOR_TAG. pcw have been successfully applied to the extraction of named entity coordinate terms, including city and person names, from graphs representing newswire', 'text  #TAUTHOR_TAG, where the specialized measures learned outperformed the state - ofthe - art dependency vectors method ( pado and  #AUTHOR_TAG for small - and medium - sized', 'corpora. in this work, we apply the path constrained graph walk method to the task of eliciting general word relatedness from parsed text', ', conducting a set of experiments on the task of synonym extraction. while the tasks of named entity extraction and synonym extraction', 'from text have been treated separately in the literature, this work shows that both tasks can be addressed using the same general', 'framework. our results are encouraging : the pcw model yields superior results to the dependency vectors approach. further, we show that learning specialized similarity measures per word type ( nouns, verbs and adjectives ) is preferable to applying', 'a uniform model for all word types']",5
"[' #TAUTHOR_TAG.', 'we implemented dv using code']","[' #TAUTHOR_TAG.', 'we implemented dv using code']","['with lower probability of reaching a relevant response, following on previous work  #TAUTHOR_TAG.', 'we implemented dv using code']","['a query like { term = "" movie "" }, we would like to get synonymous words, such as film, to appear at the top of the retrieved list.', 'in our experimental setting, we assume that the word type of the query term is known.', 'rather than rank all words ( terms ) in response to a query, we use available ( noisy ) part of speech information to narrow down the search to the terms of the same type as the query term, e. g. for the query "" film "" we retrieve nodes of type τ = noun.', 'we applied the pcw method to learn separate models for noun, verb and adjective queries.', 'the path trees were constructed using the paths leading to the node known to be a correct answer, as well as to the otherwise irrelevant top - ranked 10 terms.', 'we required the paths considered by pcw to include exactly 6 segments ( edges ).', 'such paths represent distributional similarity phenomena, allowing a direct comparison against the dv method.', 'in conducting the constrained walk, we applied a threshold of 0. 5 to truncate paths associated with lower probability of reaching a relevant response, following on previous work  #TAUTHOR_TAG.', 'we implemented dv using code made available by its authors, 3 where we converted the syntactic patterns specified to stanford dependency parser conventions.', 'the parameters of the dv method were set to medium context and oblique edge weighting scheme, which were found to perform best ( pado and  #AUTHOR_TAG.', 'in applying a vector - space based method, a similarity score needs to be computed between every candidate from the corpus and the query term to construct a ranked list.', 'in practice, we used the union of the top 300 words retrieved by pcw as candidate terms for dv.', ""we evaluate the following variants of dv : hav - table 2 : 5 - fold cross validation results : map ing inter - word similarity computed using lin's measure  #AUTHOR_TAG ( dv - lin ), or using cosine similarity ( dv - cos )."", ""in addition, we consider a non - syntactic variant, where a word's vector consists of its cooccurrence counts with other terms ( using a window of two words ) ; that is, ignoring the dependency structure ( co - lin )."", 'finally, in addition to the pcw model described above ( pcw ), we evaluate the pcw approach in settings where random, noisy, edges have been eliminated from the underlying graph.', 'specifically, dependency links in the graph may be associated with pointwise mutual information ( pmi ) scores of the linked word mention pairs ( manning and schutze, 1999 )']",5
['( mt ) has shown steady improvements  #TAUTHOR_TAG'],['( mt ) has shown steady improvements  #TAUTHOR_TAG'],"['to recent advances in deep learning over the last two decades, machine translation ( mt ) has shown steady improvements  #TAUTHOR_TAG']","['to recent advances in deep learning over the last two decades, machine translation ( mt ) has shown steady improvements  #TAUTHOR_TAG.', 'nevertheless, even the state - of - the - art mt systems are still often far from perfect.', 'translations provided by mt systems may contain incorrect lexical choices or word ordering.', '']",0
['( mt ) has shown steady improvements  #TAUTHOR_TAG'],['( mt ) has shown steady improvements  #TAUTHOR_TAG'],"['to recent advances in deep learning over the last two decades, machine translation ( mt ) has shown steady improvements  #TAUTHOR_TAG']","['to recent advances in deep learning over the last two decades, machine translation ( mt ) has shown steady improvements  #TAUTHOR_TAG.', 'nevertheless, even the state - of - the - art mt systems are still often far from perfect.', 'translations provided by mt systems may contain incorrect lexical choices or word ordering.', '']",0
"['', 'as described in  #TAUTHOR_TAG,']","['1, is an extension of transformer.', 'as described in  #TAUTHOR_TAG,']","['', 'as described in  #TAUTHOR_TAG,']","['the sequences src x = (, [UNK], ), mt y = (, [UNK], ), and pe z = ( z, [UNK], ) with the length of the sequence,, and, respectively, the model is trained to learn probabilities conditioning on two sources x, y and target history :', 'the model architecture, as shown in figure 1, is an extension of transformer.', 'as described in  #TAUTHOR_TAG, each stacked layer is composed of multi - head attention networks and position - wise fully connected feed - forward networks.', '']",0
"['', 'as described in  #TAUTHOR_TAG,']","['1, is an extension of transformer.', 'as described in  #TAUTHOR_TAG,']","['', 'as described in  #TAUTHOR_TAG,']","['the sequences src x = (, [UNK], ), mt y = (, [UNK], ), and pe z = ( z, [UNK], ) with the length of the sequence,, and, respectively, the model is trained to learn probabilities conditioning on two sources x, y and target history :', 'the model architecture, as shown in figure 1, is an extension of transformer.', 'as described in  #TAUTHOR_TAG, each stacked layer is composed of multi - head attention networks and position - wise fully connected feed - forward networks.', '']",5
"['1 play an important role in constructing joint representations.', 'as described in  #TAUTHOR_TAG, we utilize the same multihead attention with h - heads based on scaled dotproduct attention']","['1 play an important role in constructing joint representations.', 'as described in  #TAUTHOR_TAG, we utilize the same multihead attention with h - heads based on scaled dotproduct attention']","['multi - head attention layers marked with dashed lines in figure 1 play an important role in constructing joint representations.', 'as described in  #TAUTHOR_TAG, we utilize the same multihead attention with h - heads based on scaled dotproduct attention']","['multi - head attention layers marked with dashed lines in figure 1 play an important role in constructing joint representations.', 'as described in  #TAUTHOR_TAG, we utilize the same multihead attention with h - heads based on scaled dotproduct attention to get matrix c composed of context vectors as follows :', 'where', 'attn (,, ) = softmax ( 4 ) the output is then transferred to the layer normalization layer with residual connection.', '']",5
['rates  #TAUTHOR_TAG with a size'],['rates  #TAUTHOR_TAG with a size'],"['employed the opennmt - py  #AUTHOR_TAG framework to modify the original transformer network.', 'we trained our model for ~ 14k update steps with the adam optimizer  #AUTHOR_TAG, warm up learning rates  #TAUTHOR_TAG with a size']","['employed the opennmt - py  #AUTHOR_TAG framework to modify the original transformer network.', 'we trained our model for ~ 14k update steps with the adam optimizer  #AUTHOR_TAG, warm up learning rates  #TAUTHOR_TAG with a size of 12, 000, and batch size of approximately 17, 000 tokens for each triplet.', 'other detailed settings were the same as  #AUTHOR_TAG.', 'to obtain a single trained model, it consumed ~ 14k update steps until convergence on the development set']",5
"['1 play an important role in constructing joint representations.', 'as described in  #TAUTHOR_TAG, we utilize the same multihead attention with h - heads based on scaled dotproduct attention']","['1 play an important role in constructing joint representations.', 'as described in  #TAUTHOR_TAG, we utilize the same multihead attention with h - heads based on scaled dotproduct attention']","['multi - head attention layers marked with dashed lines in figure 1 play an important role in constructing joint representations.', 'as described in  #TAUTHOR_TAG, we utilize the same multihead attention with h - heads based on scaled dotproduct attention']","['multi - head attention layers marked with dashed lines in figure 1 play an important role in constructing joint representations.', 'as described in  #TAUTHOR_TAG, we utilize the same multihead attention with h - heads based on scaled dotproduct attention to get matrix c composed of context vectors as follows :', 'where', 'attn (,, ) = softmax ( 4 ) the output is then transferred to the layer normalization layer with residual connection.', '']",3
[' #TAUTHOR_TAG include words'],[' #TAUTHOR_TAG include words'],[' #TAUTHOR_TAG include words'],"['for emotions. they expanded then the coverage of the lexicon by checking semantically related synsets compared to the core set.', 'they were able to annotate 2, 874 synsets and 4, 787 words. wordnet', 'affect was also tested in different applications such as affective text sensing systems and computational humor. wordnet affect is of good quality given that it was manually created and validated, however, it is', 'of limited size.  #AUTHOR_TAG presented challenges that researchers face for developing emotion lexicon', ""##s and devised an annotation strategy to create a good quality and inexpensive emotion lexicon, emolex, by utilizing crowdsourcing. to create emolex, the authors first identified target terms for annotation extracted from macquarie thesaurus  #AUTHOR_TAG, wordnet affect and the general inquirer  #AUTHOR_TAG. then, they launched the annotation task on amazon's mechanical turk. emolex has around 10k terms annotated for emotions as well as for sentiment polarities."", 'they evaluated the annotation quality using different techniques such as computing inter - annotator agreement and comparing a subsample of emolex with existing gold data. affectnet  #AUTHOR_TAG, part of the senticnet project, includes', 'also around 10k terms extracted from conceptnet  #AUTHOR_TAG and aligned with wordnet affect. they extended wordnet affect using the concepts in conceptnet. while wordnet affect, emolex and affectnet include terms with emotion labels, affect database  #AUTHOR_TAG and depechemo', '##od  #TAUTHOR_TAG include words that have emotion scores instead, which can be useful for compositional computations of emotion scores. affect database extends sentiful and covers around 2. 5k words presented in their lemma form along with the corresponding part of speech ( pos ) tag. depechemood was automatically built by harvesting social media data that', ""were implicitly annotated with emotions.  #TAUTHOR_TAG utilized news articles from rappler. com. the articles are accompanied by rappler's mood meter, which"", 'allows readers to express their emotions about the article they are reading. depechemood includes around 37k lemmas along with their part of speech tags and the lemmas are aligned with ewn. staiano and guerini also evaluated depechemood in emotion regression and classification tasks in unsupervised settings. they claim that although they utilized a naive unsupervised model, they', 'were able to outperform existing lexicons when applied on semeval 2007 dataset  #AUTHOR_TAG. since depechemood is aligned with ewn, is publicly available and has', 'a better coverage and claimed performance compared to existing emotion lexicons, we decide to expand it using ewn semantic relations as described below in section 3']",0
[' #TAUTHOR_TAG include words'],[' #TAUTHOR_TAG include words'],[' #TAUTHOR_TAG include words'],"['for emotions. they expanded then the coverage of the lexicon by checking semantically related synsets compared to the core set.', 'they were able to annotate 2, 874 synsets and 4, 787 words. wordnet', 'affect was also tested in different applications such as affective text sensing systems and computational humor. wordnet affect is of good quality given that it was manually created and validated, however, it is', 'of limited size.  #AUTHOR_TAG presented challenges that researchers face for developing emotion lexicon', ""##s and devised an annotation strategy to create a good quality and inexpensive emotion lexicon, emolex, by utilizing crowdsourcing. to create emolex, the authors first identified target terms for annotation extracted from macquarie thesaurus  #AUTHOR_TAG, wordnet affect and the general inquirer  #AUTHOR_TAG. then, they launched the annotation task on amazon's mechanical turk. emolex has around 10k terms annotated for emotions as well as for sentiment polarities."", 'they evaluated the annotation quality using different techniques such as computing inter - annotator agreement and comparing a subsample of emolex with existing gold data. affectnet  #AUTHOR_TAG, part of the senticnet project, includes', 'also around 10k terms extracted from conceptnet  #AUTHOR_TAG and aligned with wordnet affect. they extended wordnet affect using the concepts in conceptnet. while wordnet affect, emolex and affectnet include terms with emotion labels, affect database  #AUTHOR_TAG and depechemo', '##od  #TAUTHOR_TAG include words that have emotion scores instead, which can be useful for compositional computations of emotion scores. affect database extends sentiful and covers around 2. 5k words presented in their lemma form along with the corresponding part of speech ( pos ) tag. depechemood was automatically built by harvesting social media data that', ""were implicitly annotated with emotions.  #TAUTHOR_TAG utilized news articles from rappler. com. the articles are accompanied by rappler's mood meter, which"", 'allows readers to express their emotions about the article they are reading. depechemood includes around 37k lemmas along with their part of speech tags and the lemmas are aligned with ewn. staiano and guerini also evaluated depechemood in emotion regression and classification tasks in unsupervised settings. they claim that although they utilized a naive unsupervised model, they', 'were able to outperform existing lexicons when applied on semeval 2007 dataset  #AUTHOR_TAG. since depechemood is aligned with ewn, is publicly available and has', 'a better coverage and claimed performance compared to existing emotion lexicons, we decide to expand it using ewn semantic relations as described below in section 3']",0
[' #TAUTHOR_TAG include words'],[' #TAUTHOR_TAG include words'],[' #TAUTHOR_TAG include words'],"['for emotions. they expanded then the coverage of the lexicon by checking semantically related synsets compared to the core set.', 'they were able to annotate 2, 874 synsets and 4, 787 words. wordnet', 'affect was also tested in different applications such as affective text sensing systems and computational humor. wordnet affect is of good quality given that it was manually created and validated, however, it is', 'of limited size.  #AUTHOR_TAG presented challenges that researchers face for developing emotion lexicon', ""##s and devised an annotation strategy to create a good quality and inexpensive emotion lexicon, emolex, by utilizing crowdsourcing. to create emolex, the authors first identified target terms for annotation extracted from macquarie thesaurus  #AUTHOR_TAG, wordnet affect and the general inquirer  #AUTHOR_TAG. then, they launched the annotation task on amazon's mechanical turk. emolex has around 10k terms annotated for emotions as well as for sentiment polarities."", 'they evaluated the annotation quality using different techniques such as computing inter - annotator agreement and comparing a subsample of emolex with existing gold data. affectnet  #AUTHOR_TAG, part of the senticnet project, includes', 'also around 10k terms extracted from conceptnet  #AUTHOR_TAG and aligned with wordnet affect. they extended wordnet affect using the concepts in conceptnet. while wordnet affect, emolex and affectnet include terms with emotion labels, affect database  #AUTHOR_TAG and depechemo', '##od  #TAUTHOR_TAG include words that have emotion scores instead, which can be useful for compositional computations of emotion scores. affect database extends sentiful and covers around 2. 5k words presented in their lemma form along with the corresponding part of speech ( pos ) tag. depechemood was automatically built by harvesting social media data that', ""were implicitly annotated with emotions.  #TAUTHOR_TAG utilized news articles from rappler. com. the articles are accompanied by rappler's mood meter, which"", 'allows readers to express their emotions about the article they are reading. depechemood includes around 37k lemmas along with their part of speech tags and the lemmas are aligned with ewn. staiano and guerini also evaluated depechemood in emotion regression and classification tasks in unsupervised settings. they claim that although they utilized a naive unsupervised model, they', 'were able to outperform existing lexicons when applied on semeval 2007 dataset  #AUTHOR_TAG. since depechemood is aligned with ewn, is publicly available and has', 'a better coverage and claimed performance compared to existing emotion lexicons, we decide to expand it using ewn semantic relations as described below in section 3']",0
[' #TAUTHOR_TAG include words'],[' #TAUTHOR_TAG include words'],[' #TAUTHOR_TAG include words'],"['for emotions. they expanded then the coverage of the lexicon by checking semantically related synsets compared to the core set.', 'they were able to annotate 2, 874 synsets and 4, 787 words. wordnet', 'affect was also tested in different applications such as affective text sensing systems and computational humor. wordnet affect is of good quality given that it was manually created and validated, however, it is', 'of limited size.  #AUTHOR_TAG presented challenges that researchers face for developing emotion lexicon', ""##s and devised an annotation strategy to create a good quality and inexpensive emotion lexicon, emolex, by utilizing crowdsourcing. to create emolex, the authors first identified target terms for annotation extracted from macquarie thesaurus  #AUTHOR_TAG, wordnet affect and the general inquirer  #AUTHOR_TAG. then, they launched the annotation task on amazon's mechanical turk. emolex has around 10k terms annotated for emotions as well as for sentiment polarities."", 'they evaluated the annotation quality using different techniques such as computing inter - annotator agreement and comparing a subsample of emolex with existing gold data. affectnet  #AUTHOR_TAG, part of the senticnet project, includes', 'also around 10k terms extracted from conceptnet  #AUTHOR_TAG and aligned with wordnet affect. they extended wordnet affect using the concepts in conceptnet. while wordnet affect, emolex and affectnet include terms with emotion labels, affect database  #AUTHOR_TAG and depechemo', '##od  #TAUTHOR_TAG include words that have emotion scores instead, which can be useful for compositional computations of emotion scores. affect database extends sentiful and covers around 2. 5k words presented in their lemma form along with the corresponding part of speech ( pos ) tag. depechemood was automatically built by harvesting social media data that', ""were implicitly annotated with emotions.  #TAUTHOR_TAG utilized news articles from rappler. com. the articles are accompanied by rappler's mood meter, which"", 'allows readers to express their emotions about the article they are reading. depechemood includes around 37k lemmas along with their part of speech tags and the lemmas are aligned with ewn. staiano and guerini also evaluated depechemood in emotion regression and classification tasks in unsupervised settings. they claim that although they utilized a naive unsupervised model, they', 'were able to outperform existing lexicons when applied on semeval 2007 dataset  #AUTHOR_TAG. since depechemood is aligned with ewn, is publicly available and has', 'a better coverage and claimed performance compared to existing emotion lexicons, we decide to expand it using ewn semantic relations as described below in section 3']",0
"['this variation performed best according to the presented results in  #TAUTHOR_TAG.', 'in fig. 1, we show an']","['this variation performed best according to the presented results in  #TAUTHOR_TAG.', 'in fig. 1, we show an']","['this variation performed best according to the presented results in  #TAUTHOR_TAG.', 'in fig. 1, we show an overview of the steps']","['this section, we describe the approach we followed in order to expand depechemood and build emowordnet.', ""depechemood consists of 37, 771 lemmas along with their corresponding pos tags where each entry is appended with scores for 8 emotion labels : afraid, amused, angry, annoyed, don't care, happy, inspired and sad."", 'three variations of score representations exist for depechemood.', 'we select to expand the depechemood variation with normalized scores since this variation performed best according to the presented results in  #TAUTHOR_TAG.', 'in fig. 1, we show an overview of the steps followed to expand depechemood.', 'step 1 : ewn synsets that include lemmas of depechemood were retrieved.', 'a score was then computed for each retrieved synset, s. let s denotes the set of all such synsets.', '']",6
"['this variation performed best according to the presented results in  #TAUTHOR_TAG.', 'in fig. 1, we show an']","['this variation performed best according to the presented results in  #TAUTHOR_TAG.', 'in fig. 1, we show an']","['this variation performed best according to the presented results in  #TAUTHOR_TAG.', 'in fig. 1, we show an overview of the steps']","['this section, we describe the approach we followed in order to expand depechemood and build emowordnet.', ""depechemood consists of 37, 771 lemmas along with their corresponding pos tags where each entry is appended with scores for 8 emotion labels : afraid, amused, angry, annoyed, don't care, happy, inspired and sad."", 'three variations of score representations exist for depechemood.', 'we select to expand the depechemood variation with normalized scores since this variation performed best according to the presented results in  #TAUTHOR_TAG.', 'in fig. 1, we show an overview of the steps followed to expand depechemood.', 'step 1 : ewn synsets that include lemmas of depechemood were retrieved.', 'a score was then computed for each retrieved synset, s. let s denotes the set of all such synsets.', '']",4
['of  #TAUTHOR_TAG : fear'],['of  #TAUTHOR_TAG : fear'],"['provided by rappler mood meter.', 'we considered the same emotion mapping assumptions presented in the work of  #TAUTHOR_TAG : fear']","['utilized the dataset provided publicly by semeval 2007 task on affective text  #AUTHOR_TAG.', 'the dataset consists of one thousand news headlines annotated with six emotion scores : anger, disgust, fear, joy, sadness and surprise.', 'for the regression task, a score between 0 and 1 is provided for each emotion.', 'for the classification task, a threshold is applied on the emotion scores to get a binary representation of the emotions : if the score of a certain emotion is greater than 0. 5, the corresponding emotion label is set to 1, otherwise it is 0.', 'the emotion labels used in the dataset correspond to the six emotions of the ekman model  #AUTHOR_TAG while those in emowordnet, as well as depechemood, follow the ones provided by rappler mood meter.', 'we considered the same emotion mapping assumptions presented in the work of  #TAUTHOR_TAG : fear → afraid, anger → angry, joy → happy, sadness → sad and surprise → inspired.', 'disgust was not aligned with any emotion in emowordnet and hence was discarded as also assumed in  #TAUTHOR_TAG.', 'one important aspect of the extrinsic evaluation was checking the coverage of emowordnet against semeval dataset.', 'in order to compute coverage, we performed lemmatization of the news headlines using wordnet lemmatizer available through python nltk package.', 'we excluded all words with pos tags different than noun, verb, adjective and adverb.', '']",3
['of  #TAUTHOR_TAG : fear'],['of  #TAUTHOR_TAG : fear'],"['provided by rappler mood meter.', 'we considered the same emotion mapping assumptions presented in the work of  #TAUTHOR_TAG : fear']","['utilized the dataset provided publicly by semeval 2007 task on affective text  #AUTHOR_TAG.', 'the dataset consists of one thousand news headlines annotated with six emotion scores : anger, disgust, fear, joy, sadness and surprise.', 'for the regression task, a score between 0 and 1 is provided for each emotion.', 'for the classification task, a threshold is applied on the emotion scores to get a binary representation of the emotions : if the score of a certain emotion is greater than 0. 5, the corresponding emotion label is set to 1, otherwise it is 0.', 'the emotion labels used in the dataset correspond to the six emotions of the ekman model  #AUTHOR_TAG while those in emowordnet, as well as depechemood, follow the ones provided by rappler mood meter.', 'we considered the same emotion mapping assumptions presented in the work of  #TAUTHOR_TAG : fear → afraid, anger → angry, joy → happy, sadness → sad and surprise → inspired.', 'disgust was not aligned with any emotion in emowordnet and hence was discarded as also assumed in  #TAUTHOR_TAG.', 'one important aspect of the extrinsic evaluation was checking the coverage of emowordnet against semeval dataset.', 'in order to compute coverage, we performed lemmatization of the news headlines using wordnet lemmatizer available through python nltk package.', 'we excluded all words with pos tags different than noun, verb, adjective and adverb.', '']",3
"['using sum for both lexicons.', ""as stated in  #TAUTHOR_TAG paper,'dis""]","['using sum for both lexicons.', ""as stated in  #TAUTHOR_TAG paper,'disgust'emotion was excluded""]","['using sum for both lexicons.', ""as stated in  #TAUTHOR_TAG paper,'dis""]","['followed an approach similar to the one presented for evaluating depechemood.', 'for preprocessing, we first lemmatized the headlines using wordnet lemmatizer available in python nltk package.', 'we also accounted for multi - word terms that were solely available in emowordnet by looking at n - grams ( up to n = 3 ) after lemmatization.', 'we then removed all terms that did not belong to any of the four pos tags : noun, verb, adjective and adverbs.', 'for features computation, we considered two variations : the sum and the average of the emotion scores for the five emotion labels that overlapped between emowordnet and semeval dataset.', 'using average turned out to perform better than when using sum for both lexicons.', ""as stated in  #TAUTHOR_TAG paper,'disgust'emotion was excluded since there was no corresponding mapping in emowordnet / depechemood."", 'the first evaluation consisted of measuring pearson correlation between the scores computed using the lexicons and those provided in semeval.', 'the results are reported in table 1.', 'we could see that the results are relatively close to each other : emowordnet slightly outperformed depechemood for the five different emotions.', 'it was expected to have close results given that the coverage of emowordnet is very close to depechemood.', 'given the slight improvement, we expect emowordnet to perform much better on larger datasets.', 'for the classification task, we first transformed the numerical emotion scores of the headlines to a binary representation.', ""we applied min - max normalization on the computed emotion scores per headline, and then assigned a'1'for the emotion label with score greater than'0. 5 ', and a'0'otherwise."", 'we used f1 measure for evaluation.', 'results are shown in table 2.', 'more significant improvement was observed in classification task compared to regression task when using emowordnet']",3
"['with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in']","['with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in']","[""with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in the'multi - author document'context""]","['authorial clustering is the process of partitioning n documents written by k distinct authors into k groups of documents segmented by authorship.', 'nothing is assumed about each document except that it was written by a single author.', "" #AUTHOR_TAG formulated this problem in a paper focused on clustering five books from the hebrew bible. they also consider a'multi - author document'version of the problem : decomposing sentences from a single composite document generated by merging randomly sampled chunks of text from k authors."", "" #AUTHOR_TAG followed that work with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in the'multi - author document'context by exploiting posterior probabilities of a naive - bayesian model."", ""we consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text ( a'document'), for which it can be reliably asserted that only a single author is present."", 'furthermore, this formulation precludes results dependent on a random document generation procedure.', 'in this paper, we argue that the biblical clustering done by  #AUTHOR_TAG and by  #TAUTHOR_TAG do not represent a grouping around true authorship within the bible, but rather around common topics or shared style.', 'we demonstrate a general technique that can accurately discern multiple authors contained within the books of ezekiel and jeremiah.', 'prior work assumes that each prophetic book reflects a single source, and does not consider the consensus among modern biblical scholars that the books of ezekiel and jeremiah were written by multiple individuals.', 'to cluster documents by true authorship, we propose that considering part - of - speech ( pos ) ngrams as features most distinctly identifies an individual writer.', 'the use of syntactic structure in authorial research has been studied before.', ' #AUTHOR_TAG introduced syntactic information measures for authorship attribution and  #AUTHOR_TAG argued that pos information could reflect a more reliable authorial fingerprint than lexical information.', "" #AUTHOR_TAG and  #AUTHOR_TAG propose that syntactic feature sets are reliable predictors for authorial attribution, and  #AUTHOR_TAG demonstrates, with modest success, authorial decomposition using pq - grams extracted from sentences'syntax trees."", ""we found that by combining the feature set of pos n - grams with a clustering approach similar to the one presented by  #AUTHOR_TAG, our method of decomposition attains higher accuracy than tschuggnall's method, which also considers grammatical style."", 'additionally, in cases where authors are rhetorically similar, our framework outperforms techniques outlined by  #AUTHOR_TAG and  #TAUTHOR_TAG, which both rely on word occurrences as features.', 'this paper is organized as follows : section 2 outlines our proposed framework']",0
"['with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in']","['with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in']","[""with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in the'multi - author document'context""]","['authorial clustering is the process of partitioning n documents written by k distinct authors into k groups of documents segmented by authorship.', 'nothing is assumed about each document except that it was written by a single author.', "" #AUTHOR_TAG formulated this problem in a paper focused on clustering five books from the hebrew bible. they also consider a'multi - author document'version of the problem : decomposing sentences from a single composite document generated by merging randomly sampled chunks of text from k authors."", "" #AUTHOR_TAG followed that work with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in the'multi - author document'context by exploiting posterior probabilities of a naive - bayesian model."", ""we consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text ( a'document'), for which it can be reliably asserted that only a single author is present."", 'furthermore, this formulation precludes results dependent on a random document generation procedure.', 'in this paper, we argue that the biblical clustering done by  #AUTHOR_TAG and by  #TAUTHOR_TAG do not represent a grouping around true authorship within the bible, but rather around common topics or shared style.', 'we demonstrate a general technique that can accurately discern multiple authors contained within the books of ezekiel and jeremiah.', 'prior work assumes that each prophetic book reflects a single source, and does not consider the consensus among modern biblical scholars that the books of ezekiel and jeremiah were written by multiple individuals.', 'to cluster documents by true authorship, we propose that considering part - of - speech ( pos ) ngrams as features most distinctly identifies an individual writer.', 'the use of syntactic structure in authorial research has been studied before.', ' #AUTHOR_TAG introduced syntactic information measures for authorship attribution and  #AUTHOR_TAG argued that pos information could reflect a more reliable authorial fingerprint than lexical information.', "" #AUTHOR_TAG and  #AUTHOR_TAG propose that syntactic feature sets are reliable predictors for authorial attribution, and  #AUTHOR_TAG demonstrates, with modest success, authorial decomposition using pq - grams extracted from sentences'syntax trees."", ""we found that by combining the feature set of pos n - grams with a clustering approach similar to the one presented by  #AUTHOR_TAG, our method of decomposition attains higher accuracy than tschuggnall's method, which also considers grammatical style."", 'additionally, in cases where authors are rhetorically similar, our framework outperforms techniques outlined by  #AUTHOR_TAG and  #TAUTHOR_TAG, which both rely on word occurrences as features.', 'this paper is organized as follows : section 2 outlines our proposed framework']",4
"['with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in']","['with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in']","[""with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in the'multi - author document'context""]","['authorial clustering is the process of partitioning n documents written by k distinct authors into k groups of documents segmented by authorship.', 'nothing is assumed about each document except that it was written by a single author.', "" #AUTHOR_TAG formulated this problem in a paper focused on clustering five books from the hebrew bible. they also consider a'multi - author document'version of the problem : decomposing sentences from a single composite document generated by merging randomly sampled chunks of text from k authors."", "" #AUTHOR_TAG followed that work with an expanded method, and  #TAUTHOR_TAG have since presented an improved technique in the'multi - author document'context by exploiting posterior probabilities of a naive - bayesian model."", ""we consider only the case of clustering n documents written by k authors because we believe that, in most cases of authorial decomposition, there is some minimum size of text ( a'document'), for which it can be reliably asserted that only a single author is present."", 'furthermore, this formulation precludes results dependent on a random document generation procedure.', 'in this paper, we argue that the biblical clustering done by  #AUTHOR_TAG and by  #TAUTHOR_TAG do not represent a grouping around true authorship within the bible, but rather around common topics or shared style.', 'we demonstrate a general technique that can accurately discern multiple authors contained within the books of ezekiel and jeremiah.', 'prior work assumes that each prophetic book reflects a single source, and does not consider the consensus among modern biblical scholars that the books of ezekiel and jeremiah were written by multiple individuals.', 'to cluster documents by true authorship, we propose that considering part - of - speech ( pos ) ngrams as features most distinctly identifies an individual writer.', 'the use of syntactic structure in authorial research has been studied before.', ' #AUTHOR_TAG introduced syntactic information measures for authorship attribution and  #AUTHOR_TAG argued that pos information could reflect a more reliable authorial fingerprint than lexical information.', "" #AUTHOR_TAG and  #AUTHOR_TAG propose that syntactic feature sets are reliable predictors for authorial attribution, and  #AUTHOR_TAG demonstrates, with modest success, authorial decomposition using pq - grams extracted from sentences'syntax trees."", ""we found that by combining the feature set of pos n - grams with a clustering approach similar to the one presented by  #AUTHOR_TAG, our method of decomposition attains higher accuracy than tschuggnall's method, which also considers grammatical style."", 'additionally, in cases where authors are rhetorically similar, our framework outperforms techniques outlined by  #AUTHOR_TAG and  #TAUTHOR_TAG, which both rely on word occurrences as features.', 'this paper is organized as follows : section 2 outlines our proposed framework']",4
"['is a canonical dataset that has served as a benchmark for both  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the corpus is comprised of texts from four columnists : gail collins ( 274']","['is a canonical dataset that has served as a benchmark for both  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the corpus is comprised of texts from four columnists : gail collins ( 274']","['it is a canonical dataset that has served as a benchmark for both  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the corpus is comprised of texts from four columnists : gail collins ( 27']","['shall describe a clustering of new york times columns to clarify our framework.', 'the nyt cor - pus is used both because the author of each document is known with certainty and because it is a canonical dataset that has served as a benchmark for both  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'the corpus is comprised of texts from four columnists : gail collins ( 274 documents ), maureen dowd ( 298 documents ), thomas friedman ( 279 documents ), and paul krugman ( 331 documents ).', 'each document is approximately the same length and the columnists discuss a variety of topics.', 'here we consider the binary ( k = 2 ) case of clustering the set of 629 dowd and krugman documents into two groups.', ""in step one, the documents are converted into their'pos - translated'form as previously outlined."", ""each document is represented as a frequency vector that reflects all 3, 4, and 5 - grams that appear in the'pos - translated'corpus."", 'this range of ngrams was determined through validation of different values for n across several datasets.', 'results of this validation for the two way split over nyt columnists is displayed in table 1.', 'these results are consistent when validating against other datasets.', '']",3
['presented in  #AUTHOR_TAG and  #TAUTHOR_TAG -'],"['presented in  #AUTHOR_TAG and  #TAUTHOR_TAG - by replacing pos n - grams with ordinary word occurrences in step one - our framework performed very well, clustering at']",['presented in  #AUTHOR_TAG and  #TAUTHOR_TAG - by replacing pos n - grams with ordinary word occurrences in step one - our framework performed very'],"['order to confirm our framework is accurate over a variety of documents, we considered campaign speeches from the 2008 presidential election.', 'collecting 27 speeches from president obama and 20 from senator mccain, we expected our technique to excel in this context.', 'we found instead that our method performed exceptionally poorly, clustering these speeches with only 74. 2 % accuracy.', 'indeed, we were further surprised to discover that by adjusting our framework to be similar to that presented in  #AUTHOR_TAG and  #TAUTHOR_TAG - by replacing pos n - grams with ordinary word occurrences in step one - our framework performed very well, clustering at 95. 3 %.', 'similarly, our framework performed poorly on the books of ezekiel and jeremiah from the hebrew bible.', 'using the english - translated king james version, and considering each chapter as an individual document, our framework clusters the 48 chapters of ezekiel and the 52 chapters of jeremiah at 54. 7 %.', ' #AUTHOR_TAG reports 98. 0 % on this dataset, and when considering the original english text instead of the pos - translated text, our framework achieves 99. 0 %.', 'the simultaneous success of word features and failure of pos features on these two datasets seemed to completely contradict our previous results.', 'we propose two explanations.', '']",3
['embeddings  #TAUTHOR_TAG'],['embeddings  #TAUTHOR_TAG'],"['even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings  #TAUTHOR_TAG.', 'in this paper, we proposed a rescoring approach']","['', 'the dependency - based word embedding can relieve the problem of data sparseness, since even without occurrence of dependency word pairs in a corpus, dependency scores can be still calculated by word embeddings  #TAUTHOR_TAG.', 'in this paper, we proposed a rescoring approach for parsing, based on a combination of original parsing scores and dependency word embedding scores to assist the determination of the best parse tree among the n - best parse trees.', 'there are three main steps in our rescoring approach.', 'the first step is to have the parser to produce n - best parse trees with their structural scores.', 'for each parsed tree including words, part - of - speech ( pos ) and semantic role labels.', 'second, we extract word - to - word associations ( or called word dependency, a dependency implies its close association with other words in either syntactic or semantic perspective ) from large amounts of auto - parsed data and adopt word2vecf [ 13 ] to train dependency - based word embeddings.', 'the last step is to build a structural rescoring method to find the best tree structure from the n - best candidates.', 'we conduct experiments on the standard data sets of the chinese treebank.', 'we also study how different types of embeddings influence on rescoring, including word, word with semantic role labels, and word senses ( concepts )']",1
['- lstm model  #TAUTHOR_TAG for language modeling to'],['of the awd - lstm model  #TAUTHOR_TAG for language modeling to'],"['multiple languages co - exist.', 'in this work we assess the performance of the awd - lstm model  #TAUTHOR_TAG for language modeling to']","['parameter tuning is an integral part of building deep learning models.', 'state of the art models are often benchmarked on a small set of datasets such as penn treebank [ 1 ], wikitext, gigaword, mnist, cifar10 to name a few of the limited set.', 'the hyper parameters values on these datasets are however not directly applicable to other use case specific datasets.', 'advances in deep learning research including its applications to natural language processing ( nlp ) is correlated to the introduction of new increasing strategies for regularization and optimization of neural networks.', 'these strategies, more often than not introduce new hyper parameters, thus, compounding the challenge of hyper parameter tuning ; even more so if hyper parameter values are overly sensitive to the dataset.', 'the effect of this would be that reproducing state of the art neural models on a unique dataset would require significant hyper parameter search thus limiting the reach of these models to parties with significant computing resources.', 'we present work done to understand the effect of the set of parameters selected on the perplexity ( the exponential of the average negative log - likelihood of prediction of the next word in a sequence [ 2 ] ) of a neural language model ( nlm ).', 'we apply hyper parameter search methods given baseline hyper parameter values for benchmark datasets to modeling codemixed text.', 'code - mixed text is text which draws from elements of two or more grammatical systems [ 3 ] ).', 'code - mixed text is common in countries in which multiple languages co - exist.', 'in this work we assess the performance of the awd - lstm model  #TAUTHOR_TAG for language modeling to better understand how relevant the published hyper parameters may be for a codemixed corpus and to isolate which hyper parameters could be further tuned to improve performance.', 'our results show that as a whole, the set of hyperparameters considered the best  #TAUTHOR_TAG are reasonably good, however ther are better sets hyperparamers for the code - mixed corpora.', 'moreover, even with the best set of hyper parameters, the perplexity observed for our data are significantly higher ( i. e. performance is worse at the task of language modeling ) than the performance demonstrated in the literature.', 'finally, our implemented approach is one that not only enables confirmation of the goodness of the hyper parameters values, but we can also develop inferences about which hyper parameter values would yield better results']",5
['8 ] datasets as reported in  #TAUTHOR_TAG shows little'],['wikitext 2 [ 8 ] datasets as reported in  #TAUTHOR_TAG shows little'],['8 ] datasets as reported in  #TAUTHOR_TAG shows little sensitivity'],"['##learning has found sucess in various applications including natural language processing tasks such as language modeling, parts of speech tagging, summarization and many others.', 'the learning performance of deep neural networks however depends on systematic tuning of the hyper parameters.', 'as such finding optimal hyper parameters is an integral part of building neural models including neural language models.', 'recurrent neural networks ( rnns ) being well suited to dealing with sequences of vectors, have found much success in nlp by leveraging the sequential structure of language, a variant of rnns known as long short - term memory networks ( lstms ) [ 5 ] have particularly been widely used and stands as the state of the art technique for language modeling on benchmark datasets such as penn treebank ( ptb ) [ 1 ] and one billion words [ 6 ] among others.', 'language models ( lms ) by themselves are valuable because well trained lms improve the underlying metrics of downstream tasks such as word error rate for speech recognition, bleu score for translation.', 'in addition, lms compactly extract knowledge encoded in training data [ 7 ].', 'the current state of the art on modeling both ptb and wikitext 2 [ 8 ] datasets as reported in  #TAUTHOR_TAG shows little sensitivity to hyper parameters ; sharing almost all hyper parameters values between both datasets.', 'in [ 9 ], its is also shown that deep learning model can jointly learn a number of large - scale tasks from multiple domains by designing a multi - modal architecture in which as many parameters as possible are shared.', 'training and evaluating a neural network involves mapping the hyper parameter configuration ( set of values for each hyper parameter ) used in training the network to the validation error obtained at the end.', 'strategies for searching and obtaining an optimal configuration that have been applied and found considerable success include grid search, random search, bayesian optimization, sequential model - based bayesian optimization ( smbo ) [ 10 ], deterministic hyperparameter optimization methods that employs radial basis functions as error surrogates proposed by [ 11 ], gaussian process batch upper confidence bound ( gp - bucb ) [ 12 ] ; an upper confidence bound - based algorithm, which models the reward function as a sample from a gaussian process.', 'in [ 13 ], the authors propose initializing bayesian hyper parameters using meta - learning.', 'the idea being initializing the configuration space for a novel dataset based on configurations that are known to perform well on similar, previously evaluated, datasets.', 'following a meta - learning approach, we apply a genetic algorithm and a sequential search algorithm, described in the next section, initialized using the best configuration reported in  #TAUTHOR_TAG to']",5
['- lstm model  #TAUTHOR_TAG for language modeling to'],['of the awd - lstm model  #TAUTHOR_TAG for language modeling to'],"['multiple languages co - exist.', 'in this work we assess the performance of the awd - lstm model  #TAUTHOR_TAG for language modeling to']","['parameter tuning is an integral part of building deep learning models.', 'state of the art models are often benchmarked on a small set of datasets such as penn treebank [ 1 ], wikitext, gigaword, mnist, cifar10 to name a few of the limited set.', 'the hyper parameters values on these datasets are however not directly applicable to other use case specific datasets.', 'advances in deep learning research including its applications to natural language processing ( nlp ) is correlated to the introduction of new increasing strategies for regularization and optimization of neural networks.', 'these strategies, more often than not introduce new hyper parameters, thus, compounding the challenge of hyper parameter tuning ; even more so if hyper parameter values are overly sensitive to the dataset.', 'the effect of this would be that reproducing state of the art neural models on a unique dataset would require significant hyper parameter search thus limiting the reach of these models to parties with significant computing resources.', 'we present work done to understand the effect of the set of parameters selected on the perplexity ( the exponential of the average negative log - likelihood of prediction of the next word in a sequence [ 2 ] ) of a neural language model ( nlm ).', 'we apply hyper parameter search methods given baseline hyper parameter values for benchmark datasets to modeling codemixed text.', 'code - mixed text is text which draws from elements of two or more grammatical systems [ 3 ] ).', 'code - mixed text is common in countries in which multiple languages co - exist.', 'in this work we assess the performance of the awd - lstm model  #TAUTHOR_TAG for language modeling to better understand how relevant the published hyper parameters may be for a codemixed corpus and to isolate which hyper parameters could be further tuned to improve performance.', 'our results show that as a whole, the set of hyperparameters considered the best  #TAUTHOR_TAG are reasonably good, however ther are better sets hyperparamers for the code - mixed corpora.', 'moreover, even with the best set of hyper parameters, the perplexity observed for our data are significantly higher ( i. e. performance is worse at the task of language modeling ) than the performance demonstrated in the literature.', 'finally, our implemented approach is one that not only enables confirmation of the goodness of the hyper parameters values, but we can also develop inferences about which hyper parameter values would yield better results']",7
"['- lstm model as reported in  #TAUTHOR_TAG.', 'we evaluate the sensitivity of']","['up the configuration with the best result for the awd - lstm model as reported in  #TAUTHOR_TAG.', 'we evaluate the sensitivity of']","['refering to an individual hyper parameter value that makes up the configuration with the best result for the awd - lstm model as reported in  #TAUTHOR_TAG.', 'we evaluate the sensitivity of the hyper parameters by observing the test perplexity distribution comparing it with the default values']","['use the term default value when refering to an individual hyper parameter value that makes up the configuration with the best result for the awd - lstm model as reported in  #TAUTHOR_TAG.', 'we evaluate the sensitivity of the hyper parameters by observing the test perplexity distribution comparing it with the default values']",7
['8 ] datasets as reported in  #TAUTHOR_TAG shows little'],['wikitext 2 [ 8 ] datasets as reported in  #TAUTHOR_TAG shows little'],['8 ] datasets as reported in  #TAUTHOR_TAG shows little sensitivity'],"['##learning has found sucess in various applications including natural language processing tasks such as language modeling, parts of speech tagging, summarization and many others.', 'the learning performance of deep neural networks however depends on systematic tuning of the hyper parameters.', 'as such finding optimal hyper parameters is an integral part of building neural models including neural language models.', 'recurrent neural networks ( rnns ) being well suited to dealing with sequences of vectors, have found much success in nlp by leveraging the sequential structure of language, a variant of rnns known as long short - term memory networks ( lstms ) [ 5 ] have particularly been widely used and stands as the state of the art technique for language modeling on benchmark datasets such as penn treebank ( ptb ) [ 1 ] and one billion words [ 6 ] among others.', 'language models ( lms ) by themselves are valuable because well trained lms improve the underlying metrics of downstream tasks such as word error rate for speech recognition, bleu score for translation.', 'in addition, lms compactly extract knowledge encoded in training data [ 7 ].', 'the current state of the art on modeling both ptb and wikitext 2 [ 8 ] datasets as reported in  #TAUTHOR_TAG shows little sensitivity to hyper parameters ; sharing almost all hyper parameters values between both datasets.', 'in [ 9 ], its is also shown that deep learning model can jointly learn a number of large - scale tasks from multiple domains by designing a multi - modal architecture in which as many parameters as possible are shared.', 'training and evaluating a neural network involves mapping the hyper parameter configuration ( set of values for each hyper parameter ) used in training the network to the validation error obtained at the end.', 'strategies for searching and obtaining an optimal configuration that have been applied and found considerable success include grid search, random search, bayesian optimization, sequential model - based bayesian optimization ( smbo ) [ 10 ], deterministic hyperparameter optimization methods that employs radial basis functions as error surrogates proposed by [ 11 ], gaussian process batch upper confidence bound ( gp - bucb ) [ 12 ] ; an upper confidence bound - based algorithm, which models the reward function as a sample from a gaussian process.', 'in [ 13 ], the authors propose initializing bayesian hyper parameters using meta - learning.', 'the idea being initializing the configuration space for a novel dataset based on configurations that are known to perform well on similar, previously evaluated, datasets.', 'following a meta - learning approach, we apply a genetic algorithm and a sequential search algorithm, described in the next section, initialized using the best configuration reported in  #TAUTHOR_TAG to']",0
"['a language modeling task  #TAUTHOR_TAG.', 'applying the aw']","['a language modeling task  #TAUTHOR_TAG.', 'applying the awd - lstm model, based on the open sourced code and trained on code - mixed twitter data, we sample 84 different hyper parameter configurations for']","['a language modeling task  #TAUTHOR_TAG.', 'applying the awd - lstm model, based on the open sourced code and trained on code - mixed twitter data, we sample 84 different hyper parameter configurations for each dataset, and evaluate the resulting test perplexity distributions while varying individual hyperparameter values to understand the effect of the set of hyper parameter values selected on the model perplexity']","['begin our work by establishing what the baseline and current state of the art model is for a language modeling task  #TAUTHOR_TAG.', 'applying the awd - lstm model, based on the open sourced code and trained on code - mixed twitter data, we sample 84 different hyper parameter configurations for each dataset, and evaluate the resulting test perplexity distributions while varying individual hyperparameter values to understand the effect of the set of hyper parameter values selected on the model perplexity']",0
"['the population based and sequential search space were manually initialized with four ( 4 ) values of each hyperparameter in the neighbourhood of the best values reported in  #TAUTHOR_TAG as shown in table i.', 'it is important to note that the sampled configuration space is very small compared to']","['the population based and sequential search space were manually initialized with four ( 4 ) values of each hyperparameter in the neighbourhood of the best values reported in  #TAUTHOR_TAG as shown in table i.', 'it is important to note that the sampled configuration space is very small compared to']","['the population based and sequential search space were manually initialized with four ( 4 ) values of each hyperparameter in the neighbourhood of the best values reported in  #TAUTHOR_TAG as shown in table i.', 'it is important to note that the sampled configuration space is very small compared to the overall space']","['the population based and sequential search space were manually initialized with four ( 4 ) values of each hyperparameter in the neighbourhood of the best values reported in  #TAUTHOR_TAG as shown in table i.', 'it is important to note that the sampled configuration space is very small compared to the overall space which is of size 4', '11. 84 samples for each dataset constitute the set of sampled configurations which is a far cry from the size of the universal set']",3
"['on net - like lexical resources  #TAUTHOR_TAG.', 'our']","['on net - like lexical resources  #TAUTHOR_TAG.', 'our']","['s attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on net - like lexical resources  #TAUTHOR_TAG.', 'our']","[""title of our talk - an implicit reference to the english cliche like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on net - like lexical resources  #TAUTHOR_TAG."", 'our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use  #AUTHOR_TAG, but also lexicographic activity.', 'in that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.', 'after all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities : describing a natural phenomenon is a form of learning through explicit conceptualization.', 'lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word.', 'they do not merely transcribe word knowledge and observations made on word behavior in speech and texts : they "" acquire "" the word.', 'this makes them feel good and this explains why lexicography is indeed extremely addictive.', 'our talk title is also an implicit reference to the english collocation web of words, that is so often used to refer to natural language lexicons as messy and too big to be embraced entities - cf.', ' #AUTHOR_TAG, entitled caught in the web of words : james a. h. murray and the oxford english dictionary.', 'of course, webs can be seen as being essentially traps that one gets caught in.', 'this is so to speak the fly or innocent bug perspective.', 'however, lexicographers ought not be caught in the web : they can behave as spiders weaving the web.', 'this is possible if the model they are constructing is indeed a diagrammatic representation - in a semiotic sense  #AUTHOR_TAG - of the natural language lexicon that is being scrutinized.', 'it is when lexicographers run on pages, writing dictionary articles, like flies walking on a glass window, that they have the most chance to get caught in the web of words.', 'this is why lexicographers have long ago introduced systems of cards and records to help them compile data on lexical units.', 'lexicographic records helped lexicographers free themselves from the two - dimensional prison of the dictionary.', 'their knowledge about words occupied a "" volume, "" that of filing cabinets, which is more in line with the three - dimensional nature of the lexicon']",5
"['on net - like lexical resources  #TAUTHOR_TAG.', 'our']","['on net - like lexical resources  #TAUTHOR_TAG.', 'our']","['s attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on net - like lexical resources  #TAUTHOR_TAG.', 'our']","[""title of our talk - an implicit reference to the english cliche like a spider weaving her webintends to attract one's attention to the metaphor that can be drawn between the dance of a spider weaving her web and a new lexicographic gesture that is gradually emerging from the work on net - like lexical resources  #TAUTHOR_TAG."", 'our claim is that the inherent graph structure of natural language lexicons not only determine vocabulary acquisition and use  #AUTHOR_TAG, but also lexicographic activity.', 'in that respect, reflecting on new ways to implement the task of building lexical resources is essential for lexicographers themselves, but also for anyone interested is lexicons as mental structures.', 'after all, lexicographers and language learners are those who have the most direct contact with lexical structures, through closely related activities : describing a natural phenomenon is a form of learning through explicit conceptualization.', 'lexicographers often experience the fact that by completing the description of a word they achieve a form of understanding and mastering of this word.', 'they do not merely transcribe word knowledge and observations made on word behavior in speech and texts : they "" acquire "" the word.', 'this makes them feel good and this explains why lexicography is indeed extremely addictive.', 'our talk title is also an implicit reference to the english collocation web of words, that is so often used to refer to natural language lexicons as messy and too big to be embraced entities - cf.', ' #AUTHOR_TAG, entitled caught in the web of words : james a. h. murray and the oxford english dictionary.', 'of course, webs can be seen as being essentially traps that one gets caught in.', 'this is so to speak the fly or innocent bug perspective.', 'however, lexicographers ought not be caught in the web : they can behave as spiders weaving the web.', 'this is possible if the model they are constructing is indeed a diagrammatic representation - in a semiotic sense  #AUTHOR_TAG - of the natural language lexicon that is being scrutinized.', 'it is when lexicographers run on pages, writing dictionary articles, like flies walking on a glass window, that they have the most chance to get caught in the web of words.', 'this is why lexicographers have long ago introduced systems of cards and records to help them compile data on lexical units.', 'lexicographic records helped lexicographers free themselves from the two - dimensional prison of the dictionary.', 'their knowledge about words occupied a "" volume, "" that of filing cabinets, which is more in line with the three - dimensional nature of the lexicon']",5
"['lexical description.', 'work performed on the french lexical network  #TAUTHOR_TAG will serve to demonstrate how the lexicographic process']","['lexical description.', 'work performed on the french lexical network  #TAUTHOR_TAG will serve to demonstrate how the lexicographic process']","['incrementing the lexical description.', 'work performed on the french lexical network  #TAUTHOR_TAG will serve to demonstrate how the lexicographic process']","['our talk, we take the above observations as given, including the fact that lexicography should indeed be targeting virtual dictionaries, generated from non - textual lexical models.', 'we illustrate how the lexicographic process of building graph - based lexical models can benefit from tools that allow lexicographers to wade through the lexical web, following paradigmatic and syntagmatic paths, while simultaneously weaving new links and incrementing the lexical description.', 'work performed on the french lexical network  #TAUTHOR_TAG will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.', ""the main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the meaning - text linguistic approach ( mel'cuk, 1996 )."", 'it induces the multidimensional and non - hierarchical graph structure of the fln that, we believe, is far better suited for designing lexical resources than hyperonymy - based structures.', 'computational aspects of the work on the french lexical network are dealt with in  #TAUTHOR_TAG.', 'in our presentation, we focus on the actual process of weaving lexical relations']",2
"['lexical description.', 'work performed on the french lexical network  #TAUTHOR_TAG will serve to demonstrate how the lexicographic process']","['lexical description.', 'work performed on the french lexical network  #TAUTHOR_TAG will serve to demonstrate how the lexicographic process']","['incrementing the lexical description.', 'work performed on the french lexical network  #TAUTHOR_TAG will serve to demonstrate how the lexicographic process']","['our talk, we take the above observations as given, including the fact that lexicography should indeed be targeting virtual dictionaries, generated from non - textual lexical models.', 'we illustrate how the lexicographic process of building graph - based lexical models can benefit from tools that allow lexicographers to wade through the lexical web, following paradigmatic and syntagmatic paths, while simultaneously weaving new links and incrementing the lexical description.', 'work performed on the french lexical network  #TAUTHOR_TAG will serve to demonstrate how the lexicographic process can be made closer to actual navigation through lexical knowledge by the speaker.', ""the main theoretical and descriptive tool that makes such navigation possible is the system of lexical functions proposed by the meaning - text linguistic approach ( mel'cuk, 1996 )."", 'it induces the multidimensional and non - hierarchical graph structure of the fln that, we believe, is far better suited for designing lexical resources than hyperonymy - based structures.', 'computational aspects of the work on the french lexical network are dealt with in  #TAUTHOR_TAG.', 'in our presentation, we focus on the actual process of weaving lexical relations']",0
['answering  #TAUTHOR_TAG'],['answering  #TAUTHOR_TAG'],"['opendomain question answering  #TAUTHOR_TAG.', 'this']","['##ing structured knowledge from knowledge graphs into deep models using graph neural networks ( gnn ) [ 23, 30, 29 ] is shown to improve their performance on tasks such as visual question answering [ 12 ], object detection [ 9 ], natural language inference [ 2 ], neural machine translation [ 11 ], and opendomain question answering  #TAUTHOR_TAG.', 'this particularly helps question answering neural models such as memory networks [ 22 ] and key - value memory networks [ 10 ] by providing them with wellstructured knowledge on specific and open domains [ 28 ].', 'most models, however, answer questions using a single information source, usually either a text corpus, or a single knowledge graph.', 'text corpora have high coverage but extracting information from them is challenging whereas knowledge graphs are incomplete but are easier to extract answers from  #TAUTHOR_TAG.', 'in this paper, we propose a relational gnn for open - domain question answering that learns contextual knowledge graph embeddings by jointly updating the embeddings from a knowledge graph and a set of linked documents.', 'more specifically, our contributions are as follows : ( 1 ) we use documents as contextualized relations to augment the knowledge graph and co - train the knowledge graph and document representations, ( 2 ) we introduce a bi - directional graph attention mechanism for knowledge graphs, ( 3 ) we propose a simple graph coarsening method to fuse node and cluster representations, and ( 4 ) we show that our model achieves state - of - the - art results on the webquestionssp benchmark']",0
"['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre -']","['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model']","['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model']","['representation learning on knowledge graphs allows projecting high - level factual information into embedding spaces which can be fused with other learned structural representations to improve down - stream tasks.', 'pioneer works such as trans - e [ 1 ], complex - e [ 18 ], hole - e [ 13 ], and distmul [ 25 ] use unsupervised and mostly linear models to learn such pre - trained representations.', 'a few recent works, on the other hand, use gnns to compute the knowledge graph representation [ 24, 20, 27 ].', 'these high - level knowledge graph representations are particularly important for question answering task [ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model and then update them using a relational and bi - directional gnn model.', 'only a few works fuse knowledge graphs with text corpora to answer questions.', 'in [ 3 ] early fusion of knowledge graph facts and text is performed using key - value memory networks ( kvmn ).', 'this model, however, ignores relational structure between the text and knowledge graph.', 'our model links the knowledge graph and documents through document - contextualized edges and also links entities with their positions in the corpus.', 'this linking is used in graft - net as well which also performs question answering through fusing learned knowledge graph and linked document representations  #TAUTHOR_TAG.', 'unlike graft - net, our model uses variants of differential pooling [ 26 ] and bi - directional graph attention [ 19 ] for more powerful message passing.', 'our model also introduces trainable documentcontextualized relation embeddings instead of exclusively relying on fixed relation representations']",0
"['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre -']","['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model']","['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model']","['representation learning on knowledge graphs allows projecting high - level factual information into embedding spaces which can be fused with other learned structural representations to improve down - stream tasks.', 'pioneer works such as trans - e [ 1 ], complex - e [ 18 ], hole - e [ 13 ], and distmul [ 25 ] use unsupervised and mostly linear models to learn such pre - trained representations.', 'a few recent works, on the other hand, use gnns to compute the knowledge graph representation [ 24, 20, 27 ].', 'these high - level knowledge graph representations are particularly important for question answering task [ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model and then update them using a relational and bi - directional gnn model.', 'only a few works fuse knowledge graphs with text corpora to answer questions.', 'in [ 3 ] early fusion of knowledge graph facts and text is performed using key - value memory networks ( kvmn ).', 'this model, however, ignores relational structure between the text and knowledge graph.', 'our model links the knowledge graph and documents through document - contextualized edges and also links entities with their positions in the corpus.', 'this linking is used in graft - net as well which also performs question answering through fusing learned knowledge graph and linked document representations  #TAUTHOR_TAG.', 'unlike graft - net, our model uses variants of differential pooling [ 26 ] and bi - directional graph attention [ 19 ] for more powerful message passing.', 'our model also introduces trainable documentcontextualized relation embeddings instead of exclusively relying on fixed relation representations']",0
['answering  #TAUTHOR_TAG'],['answering  #TAUTHOR_TAG'],"['opendomain question answering  #TAUTHOR_TAG.', 'this']","['##ing structured knowledge from knowledge graphs into deep models using graph neural networks ( gnn ) [ 23, 30, 29 ] is shown to improve their performance on tasks such as visual question answering [ 12 ], object detection [ 9 ], natural language inference [ 2 ], neural machine translation [ 11 ], and opendomain question answering  #TAUTHOR_TAG.', 'this particularly helps question answering neural models such as memory networks [ 22 ] and key - value memory networks [ 10 ] by providing them with wellstructured knowledge on specific and open domains [ 28 ].', 'most models, however, answer questions using a single information source, usually either a text corpus, or a single knowledge graph.', 'text corpora have high coverage but extracting information from them is challenging whereas knowledge graphs are incomplete but are easier to extract answers from  #TAUTHOR_TAG.', 'in this paper, we propose a relational gnn for open - domain question answering that learns contextual knowledge graph embeddings by jointly updating the embeddings from a knowledge graph and a set of linked documents.', 'more specifically, our contributions are as follows : ( 1 ) we use documents as contextualized relations to augment the knowledge graph and co - train the knowledge graph and document representations, ( 2 ) we introduce a bi - directional graph attention mechanism for knowledge graphs, ( 3 ) we propose a simple graph coarsening method to fuse node and cluster representations, and ( 4 ) we show that our model achieves state - of - the - art results on the webquestionssp benchmark']",1
"['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre -']","['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model']","['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model']","['representation learning on knowledge graphs allows projecting high - level factual information into embedding spaces which can be fused with other learned structural representations to improve down - stream tasks.', 'pioneer works such as trans - e [ 1 ], complex - e [ 18 ], hole - e [ 13 ], and distmul [ 25 ] use unsupervised and mostly linear models to learn such pre - trained representations.', 'a few recent works, on the other hand, use gnns to compute the knowledge graph representation [ 24, 20, 27 ].', 'these high - level knowledge graph representations are particularly important for question answering task [ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model and then update them using a relational and bi - directional gnn model.', 'only a few works fuse knowledge graphs with text corpora to answer questions.', 'in [ 3 ] early fusion of knowledge graph facts and text is performed using key - value memory networks ( kvmn ).', 'this model, however, ignores relational structure between the text and knowledge graph.', 'our model links the knowledge graph and documents through document - contextualized edges and also links entities with their positions in the corpus.', 'this linking is used in graft - net as well which also performs question answering through fusing learned knowledge graph and linked document representations  #TAUTHOR_TAG.', 'unlike graft - net, our model uses variants of differential pooling [ 26 ] and bi - directional graph attention [ 19 ] for more powerful message passing.', 'our model also introduces trainable documentcontextualized relation embeddings instead of exclusively relying on fixed relation representations']",1
"['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre -']","['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model']","['[ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model']","['representation learning on knowledge graphs allows projecting high - level factual information into embedding spaces which can be fused with other learned structural representations to improve down - stream tasks.', 'pioneer works such as trans - e [ 1 ], complex - e [ 18 ], hole - e [ 13 ], and distmul [ 25 ] use unsupervised and mostly linear models to learn such pre - trained representations.', 'a few recent works, on the other hand, use gnns to compute the knowledge graph representation [ 24, 20, 27 ].', 'these high - level knowledge graph representations are particularly important for question answering task [ 22, 10,  #TAUTHOR_TAG.', 'we use pre - trained representations to initialize the model and then update them using a relational and bi - directional gnn model.', 'only a few works fuse knowledge graphs with text corpora to answer questions.', 'in [ 3 ] early fusion of knowledge graph facts and text is performed using key - value memory networks ( kvmn ).', 'this model, however, ignores relational structure between the text and knowledge graph.', 'our model links the knowledge graph and documents through document - contextualized edges and also links entities with their positions in the corpus.', 'this linking is used in graft - net as well which also performs question answering through fusing learned knowledge graph and linked document representations  #TAUTHOR_TAG.', 'unlike graft - net, our model uses variants of differential pooling [ 26 ] and bi - directional graph attention [ 19 ] for more powerful message passing.', 'our model also introduces trainable documentcontextualized relation embeddings instead of exclusively relying on fixed relation representations']",6
"['', 'following  #TAUTHOR_TAG, we']","['( i. e., binary cross entropy loss ).', 'following  #TAUTHOR_TAG, we']","['e., binary cross entropy loss ).', 'following  #TAUTHOR_TAG, we']","['', 'a query can have zero or multiple answers and hence the task is reduced to binary node classification on g ( i. e., binary cross entropy loss ).', 'following  #TAUTHOR_TAG, we first extract a subgraph g q ⊂ g which contains v aq with high probability.', 'this is done by linking the query entities to g and expanding their neighborhood using personalized pagerank ( ppr ) method.', 'we then initialize g q, q, d, and r in embedding space as follows.', 'each entity v ∈ v is initialized using pre - trained transe embeddings [ 1 ] : h', 'is the maximum number of tokens in documents, and h ( l ) d k, j ∈ r dw corresponds to the jth token embedding of dimension d w in the kth document.', 'a bidirectional lstm that takes in pre - trained glove embeddings [ 15 ] of tokens and computes a global sequence embedding is shared between query and documents to initialize their embeddings : h', '×dr is a trainable parameter, is the concatenation operator, and [UNK] glove ∈ r dw is the mean pool over glove embeddings of the relation tokens']",5
"['8 ].', 'following  #TAUTHOR_TAG, we']","['test questions ) posed over freebase entities [ 8 ].', 'following  #TAUTHOR_TAG, we']","['##9 test questions ) posed over freebase entities [ 8 ].', 'following  #TAUTHOR_TAG, we apply the same pre - processing']","['', 'following  #TAUTHOR_TAG, we apply the same pre - processing and report average f 1 and hits @ 1, as well as micro - average, and macro - average f 1 scores.', '[ 4 ] suggests that micro - averaged f 1 best represents the performance on imbalanced binary classification.', 'table 1 shows the performance of our model compared to other models that also feature early fusion of the knowledge graph and text.', 'these include key - value memory networks ( kvmn ) [ 3 ] and graft - net  #TAUTHOR_TAG.', 'the results suggest that our model outperforms graft - net with an absolute increase in all metrics.', 'to investigate the effect of the proposed methods we performed an ablation study by masking each introduced component and training and evaluating the model.', 'the results in table 2 ( appendix ) shows the effect of each component and suggest that all introduced components contribute to the performance']",5
"['h r.', 'this is distinct from previous approaches which only process incoming nodes [  #TAUTHOR_TAG.', 'the next step is']","['edges, we negate h r.', 'this is distinct from previous approaches which only process incoming nodes [  #TAUTHOR_TAG.', 'the next step is aggregating the embeddings']","['( l ) v are trainable parameters.', 'to distinguish inward edges from outward edges, we negate h r.', 'this is distinct from previous approaches which only process incoming nodes [  #TAUTHOR_TAG.', 'the next step is aggregating the embeddings']","['', 'to distinguish inward edges from outward edges, we negate h r.', 'this is distinct from previous approaches which only process incoming nodes [  #TAUTHOR_TAG.', 'the next step is aggregating the embeddings of the edges connecting to the node, i. e., h l eout and h l ein.', 'we apply two attention mechanisms to perform the aggregation and hence the model separately aggregates weighted sums of edge embeddings over each attention parameters : α gat is based on the similarity between a node and its neighbors with respect to the relationship between them.', 'assume ( v i, r, v j ) is an edge between nodes v i and v j with relationship of type r. the edge score is defined as the dot product between the edge embedding of the original direction and inverted direction ( v j, r, v i ) and then normalized over all inward edges : α ).', 'unlike graph attention [ 26 ] this method addresses heterogeneous directed knowledge graphs.', 'finally, the model updates the node embedding ( figure 2 )']",4
"['8 ].', 'following  #TAUTHOR_TAG, we']","['test questions ) posed over freebase entities [ 8 ].', 'following  #TAUTHOR_TAG, we']","['##9 test questions ) posed over freebase entities [ 8 ].', 'following  #TAUTHOR_TAG, we apply the same pre - processing']","['', 'following  #TAUTHOR_TAG, we apply the same pre - processing and report average f 1 and hits @ 1, as well as micro - average, and macro - average f 1 scores.', '[ 4 ] suggests that micro - averaged f 1 best represents the performance on imbalanced binary classification.', 'table 1 shows the performance of our model compared to other models that also feature early fusion of the knowledge graph and text.', 'these include key - value memory networks ( kvmn ) [ 3 ] and graft - net  #TAUTHOR_TAG.', 'the results suggest that our model outperforms graft - net with an absolute increase in all metrics.', 'to investigate the effect of the proposed methods we performed an ablation study by masking each introduced component and training and evaluating the model.', 'the results in table 2 ( appendix ) shows the effect of each component and suggest that all introduced components contribute to the performance']",4
"['in  #TAUTHOR_TAG.', 'our improved evaluation technique captures word relatedness based on the word context']","['in  #TAUTHOR_TAG.', 'our improved evaluation technique captures word relatedness based on the word context']","['in  #TAUTHOR_TAG.', 'our improved evaluation technique captures word relatedness based on the word context']","['this proposal track paper, we have presented a crowdsourcing - based word embedding evaluation technique that will be more reliable and linguistically justified.', 'the method is designed for intrinsic evaluation and extends the approach proposed in  #TAUTHOR_TAG.', 'our improved evaluation technique captures word relatedness based on the word context']",6
"[', we have first extended the work of  #TAUTHOR_TAG to include sentential context to']","['an evaluation, we have first extended the work of  #TAUTHOR_TAG to include sentential context to']","['such an evaluation, we have first extended the work of  #TAUTHOR_TAG to include sentential context to']","['efforts on multiple embeddings for words  #AUTHOR_TAG require a more sophisticated evaluation and further motivate our ideas.', 'there are existing works, such as  #AUTHOR_TAG, where the sense embedding was proposed as a remedy for the current word embedding limitation on ubiquitous polysemous words, and the method learns a vector for each sense of a word.', 'for words with multiple meanings, it is important to see how many senses a word embedding technique can represent through multiple vectors.', 'to achieve such an evaluation, we have first extended the work of  #TAUTHOR_TAG to include sentential context to avoid word sense ambiguity faced by a human tester.', 'in our method, every query word is accompanied by a context sentence.', 'we then extended  #TAUTHOR_TAG further so that it is more suitable to evaluate embedding techniques designed for polysemous words with regard to their ability to embed diverse senses']",6
"[', we have first extended the work of  #TAUTHOR_TAG to include sentential context to']","['an evaluation, we have first extended the work of  #TAUTHOR_TAG to include sentential context to']","['such an evaluation, we have first extended the work of  #TAUTHOR_TAG to include sentential context to']","['efforts on multiple embeddings for words  #AUTHOR_TAG require a more sophisticated evaluation and further motivate our ideas.', 'there are existing works, such as  #AUTHOR_TAG, where the sense embedding was proposed as a remedy for the current word embedding limitation on ubiquitous polysemous words, and the method learns a vector for each sense of a word.', 'for words with multiple meanings, it is important to see how many senses a word embedding technique can represent through multiple vectors.', 'to achieve such an evaluation, we have first extended the work of  #TAUTHOR_TAG to include sentential context to avoid word sense ambiguity faced by a human tester.', 'in our method, every query word is accompanied by a context sentence.', 'we then extended  #TAUTHOR_TAG further so that it is more suitable to evaluate embedding techniques designed for polysemous words with regard to their ability to embed diverse senses']",6
['chief idea is to extend the work of  #TAUTHOR_TAG by adding a context sentence for'],['chief idea is to extend the work of  #TAUTHOR_TAG by adding a context sentence for'],"['chief idea is to extend the work of  #TAUTHOR_TAG by adding a context sentence for each query term.', 'using a context sentence']","['chief idea is to extend the work of  #TAUTHOR_TAG by adding a context sentence for each query term.', 'using a context sentence for resolving word sense ambiguity is not a new concept, and it has been used by numerous researchers, such as  #AUTHOR_TAG.', 'in particular, human judgement based approaches, such as  #AUTHOR_TAG, have used the sentential context to determine the similarity between two words, and  #AUTHOR_TAG used sentential context for lexical substitution realising the importance of the word interpretation in the context for crowdsourcing - based evaluations.', ""due to limited and potentially insufficient embedded vocabulary used to identify a related sense of the query term, we are also proposing to provide another option of'none of the above'along with the six words."", ""in fact,  #TAUTHOR_TAG have already considered'i don't know the meaning of one ( or several ) of the words'; however, when the context is in place, there may be a situation when none of the embeddings make a good match for the query term, and in that case'none of the above'is more appropriate."", ""in this way, the user's response will be more justified, and a more reliable evaluation score will be retrieved."", 'our proposal is based on an observation that human reasoning about a word is based on the context, and in crowdsourcing evaluations, we use a human to interpret the meaning ; and based on their judgement, we evaluate embedding techniques.', 'so the human should be presented with the examples in the manner that is consistent with what humans see in real - life']",6
"['- based word embedding evaluation technique of  #TAUTHOR_TAG was extended to provide data - driven treatment of word sense ambiguity.', 'the method of  #TAUTHOR_TAG']","['this paper, a crowdsourcing - based word embedding evaluation technique of  #TAUTHOR_TAG was extended to provide data - driven treatment of word sense ambiguity.', 'the method of  #TAUTHOR_TAG']","['this paper, a crowdsourcing - based word embedding evaluation technique of  #TAUTHOR_TAG was extended to provide data - driven treatment of word sense ambiguity.', ""the method of  #TAUTHOR_TAG relies on user's subjective and knowledge dependent ability""]","['this paper, a crowdsourcing - based word embedding evaluation technique of  #TAUTHOR_TAG was extended to provide data - driven treatment of word sense ambiguity.', ""the method of  #TAUTHOR_TAG relies on user's subjective and knowledge dependent ability to select'preferred'meanings whereas our method would deal with this problem selecting explicit contexts for words."", 'the selection is according to the real frequencies of meanings computed from data.', 'with this data - driven feature, our method could be more appropriate to evaluate both methods that produce one embedding per word as well as methods that produce one embedding per word sense.', 'our method would provide scores that accommodate word sense frequencies in the real use of the language.', 'here, we assume that word embeddings should recover the most frequent senses with higher priority']",6
"['cost - effective way  #AUTHOR_TAG. in  #TAUTHOR_TAG, crowdsour']","['cost - effective way  #AUTHOR_TAG. in  #TAUTHOR_TAG, crowdsour']","['evaluation in a time and cost - effective way  #AUTHOR_TAG. in  #TAUTHOR_TAG, crowdsourcingbased evaluation was proposed for synonyms or a word relatedness task where six word embedding techniques were evaluated. the  #TAUTHOR_TAG which tests embeddings for semantic', 'relationship between words focuses on a direct comparison of word embeddings with']","['of the meanings of a word in a particular language. reference is used to deal with the relationship between a language and the real world knowledge about an object or entity. the context of a word can be understood', 'through a sentence, and thus understanding a word in a sentential context works as ambiguity resolution  #AUTHOR_TAG. the vector space representation', 'of words ( embeddings ) keeps related words nearby in the vector space. the word relatedness is', 'usually measured through synonyms, but synonyms can differ in at least one semantic feature. the feature can be', ""' denotative ', referring to some actual, real world difference in the object the language is dealing with, such as, walk, lum"", '##ber, stroll, meander, lurch, stagger. the feature', ""can be'connotative ', referring to how the user feels about the object rather than any real"", 'difference in the object itself, such as, die, pass away, give up the ghost, kick the bucket, croak. absolute synonyms are usually rare in a language. for example : sofa and couch are nearly absolute', 'synonyms, however based on the context, they have different meaning in at least one way, such as, couch potato, because there is no word sense available for sofa potato  #AUTHOR_TAG. crowdsourcing  #AUTHOR_TAG callison  #AUTHOR_TAG, which allows employing people', 'worldwide to perform short tasks via online platforms, can', 'be an effective tool for performing evaluation in a time and cost - effective way  #AUTHOR_TAG. in  #TAUTHOR_TAG, crowdsourcingbased evaluation was proposed for synonyms or a word relatedness task where six word embedding techniques were evaluated. the  #TAUTHOR_TAG which tests embeddings for semantic', 'relationship between words focuses on a direct comparison of word embeddings with respect to individual queries. although  #TAUTHOR_TAG has some shortcomings. specifically,  #TAUTHOR_TAG does not explicitly consider word context. as  #TAUTHOR_TAG relies on human interpretation', 'of words, it is important to take into account how humans interpret or understand the meaning of a word. humans usually understand semantic relatedness', 'between words based on the context. thus, if  #TAUTHOR_TAG is based only on the word without its context, it will be', 'difficult for humans to understand the', '']",5
"['ambiguity inherent in word sense and corresponding reference.', 'although the experiments in  #TAUTHOR_TAG incorporated participants with adequate knowledge of english, the ambiguity is']","['ambiguity inherent in word sense and corresponding reference.', 'although the experiments in  #TAUTHOR_TAG incorporated participants with adequate knowledge of english, the ambiguity is']","['ambiguity inherent in word sense and corresponding reference.', 'although the experiments in  #TAUTHOR_TAG incorporated participants with adequate knowledge of english, the ambiguity is']","['word relatedness evaluation task for word embeddings is challenging due to ambiguity inherent in word sense and corresponding reference.', 'although the experiments in  #TAUTHOR_TAG incorporated participants with adequate knowledge of english, the ambiguity is inherent in the language.', 'this means that evaluations that ignore the context may have impact on the evaluation result.', 'also, the evaluated word embedding techniques in  #TAUTHOR_TAG except tscca  #AUTHOR_TAG - generate one vector for each word, and that makes comparisons between two related words from two embedding techniques difficult.', '']",5
"['accommodated in  #TAUTHOR_TAG.', '']","['accommodated in  #TAUTHOR_TAG.', '']","['the end of sec. 2. 2, we explained how word sense ambiguity is accommodated in  #TAUTHOR_TAG.', 'we argued that  #TAUTHOR_TAG evaluation was in expectation with']","['the end of sec. 2. 2, we explained how word sense ambiguity is accommodated in  #TAUTHOR_TAG.', 'we argued that  #TAUTHOR_TAG evaluation was in expectation with respect to subjective preferences of the turkers.', 'additionally, when the context is not provided, the turkers may even forget about common senses of the query word.', 'in our proposal, we argue that query words should be presented in an appropriate context.', 'similar to sec. 2. 2, we can distinguish two ways in which we can apply our method :', '1. informed sampling sample senses according to their frequency in wordnet 2.', 'uniform sampling sample senses according to a uniform probability distribution if no frequency data is available ( e. g. wikipedia ) we can now draw a parallel with alternative ways that turkers may apply to solve the word sense ambiguity problem.', ""in particular, under certain conditions ( i. e. when word embeddings don't use sense frequency information ), the uniform sampling option in our method would be equivalent with the uniform sampling method in sec. 2. 2."", 'this means that asking the turkers to select senses randomly according to a uniform probability distribution is the same as sampling contexts according to a uniform distribution.', 'the two approaches differ, however, when non - uniform, informed probability distributions are used.', 'informed sampling in our approach is based on wordnet whose sense frequencies are based on data - driven research.', 'this means that the overall evaluation would be based on real frequencies coming from the data instead of subjective and idiosyncratic judgements by the turkers.', 'this probabilistic argument provides another justification for our approach']",5
"['- based word embedding evaluation technique of  #TAUTHOR_TAG was extended to provide data - driven treatment of word sense ambiguity.', 'the method of  #TAUTHOR_TAG']","['this paper, a crowdsourcing - based word embedding evaluation technique of  #TAUTHOR_TAG was extended to provide data - driven treatment of word sense ambiguity.', 'the method of  #TAUTHOR_TAG']","['this paper, a crowdsourcing - based word embedding evaluation technique of  #TAUTHOR_TAG was extended to provide data - driven treatment of word sense ambiguity.', ""the method of  #TAUTHOR_TAG relies on user's subjective and knowledge dependent ability""]","['this paper, a crowdsourcing - based word embedding evaluation technique of  #TAUTHOR_TAG was extended to provide data - driven treatment of word sense ambiguity.', ""the method of  #TAUTHOR_TAG relies on user's subjective and knowledge dependent ability to select'preferred'meanings whereas our method would deal with this problem selecting explicit contexts for words."", 'the selection is according to the real frequencies of meanings computed from data.', 'with this data - driven feature, our method could be more appropriate to evaluate both methods that produce one embedding per word as well as methods that produce one embedding per word sense.', 'our method would provide scores that accommodate word sense frequencies in the real use of the language.', 'here, we assume that word embeddings should recover the most frequent senses with higher priority']",4
"['normalizing temporal expressions by  #TAUTHOR_TAG there are also ( semi - ) automatic approaches to port a temporal tagger from one language to another.', 'for instance, terseo has been extended from spanish to english and italian by automatic']","['normalizing temporal expressions by  #TAUTHOR_TAG there are also ( semi - ) automatic approaches to port a temporal tagger from one language to another.', 'for instance, terseo has been extended from spanish to english and italian by automatic']","['normalizing temporal expressions by  #TAUTHOR_TAG there are also ( semi - ) automatic approaches to port a temporal tagger from one language to another.', 'for instance, terseo has been extended from spanish to english and italian by automatic ruletranslation and automatically developed parallel corpora.', 'however, the normalization quality of this approach was rather low compared to a']","['chinese temporal tagging, machine learning and rule - based approaches have been employed.', ' #AUTHOR_TAG a ) and  #AUTHOR_TAG report that machine learning techniques do not achieve as good results as rule - based approaches when processing chinese.', 'thus, it is reasonable to extend a rulebased system such as heideltime to chinese.', ""in general, temporal tagging approaches perform the extraction, the normalization, or both, and create tides timex2  #AUTHOR_TAG or timeml's timex3  #AUTHOR_TAG annotations."", 'for development and evaluation, there are two chinese temporally annotated corpora, the ace 2005 training corpus and tempeval - 2 ( c. f. section 3 ).', 'table 1 lists approaches to chinese temporal tagging with some further information.', 'the most recent work is the learningbased language - independent discriminative parsing approach for normalizing temporal expressions by  #TAUTHOR_TAG there are also ( semi - ) automatic approaches to port a temporal tagger from one language to another.', 'for instance, terseo has been extended from spanish to english and italian by automatic ruletranslation and automatically developed parallel corpora.', 'however, the normalization quality of this approach was rather low compared to a rulebased tagger manually developed for the specific language  #AUTHOR_TAG.', 'this finding encouraged us to manually create chinese heideltime resources instead of trying automatic methods']",0
"['2 participants addressed chinese and only  #TAUTHOR_TAG report evaluation results on this corpus.', 'since heideltime is time']","['participants addressed chinese and only  #TAUTHOR_TAG report evaluation results on this corpus.', 'since heideltime is timex3 - compliant, and we address the extraction and normalization subtasks, we use the tempeval - 2 corpus in our work']","['##2 extent annotations.', 'in contrast, the tempeval - 2 chinese data sets  #AUTHOR_TAG contain timex3 annotations with extent and normalization information.', 'however, no tempeval - 2 participants addressed chinese and only  #TAUTHOR_TAG report evaluation results on this corpus.', 'since heideltime is time']","['are two chinese temporally annotated corpora available : while the chinese part of the ace 2005 multilingual training corpus  #AUTHOR_TAG has been used by some approaches ( c. f. table 1 ), it only contains timex2 extent annotations.', 'in contrast, the tempeval - 2 chinese data sets  #AUTHOR_TAG contain timex3 annotations with extent and normalization information.', 'however, no tempeval - 2 participants addressed chinese and only  #TAUTHOR_TAG report evaluation results on this corpus.', 'since heideltime is timex3 - compliant, and we address the extraction and normalization subtasks, we use the tempeval - 2 corpus in our work']",0
"['type.', '1 this issue was also reported by  #TAUTHOR_TAG.', 'thus, they report evaluation results on two versions of the data sets,']","['type.', '1 this issue was also reported by  #TAUTHOR_TAG.', 'thus, they report evaluation results on two versions of the data sets,']","['in the test set ), others no type.', '1 this issue was also reported by  #TAUTHOR_TAG.', 'thus, they report evaluation results on two versions of the data sets,']","['chinese training and test sets consist of 44 and 15 documents with 746 and 190 temporal expressions, respectively.', 'however, several expressions have no normalized value information ( 85 in the training and 47 in the test set ), others no type.', '1 this issue was also reported by  #TAUTHOR_TAG.', 'thus, they report evaluation results on two versions of the data sets, the original version and a cleaned version, in which all expressions without value information were removed']",0
['- task to  #TAUTHOR_TAG'],['the normalization sub - task to  #TAUTHOR_TAG'],['the normalization sub - task to  #TAUTHOR_TAG'],"['this section, we present evaluation results of our newly developed chinese heideltime resources.', 'in addition, we compare our results for the normalization sub - task to  #TAUTHOR_TAG']",7
"[' #TAUTHOR_TAG.', 'however, their']","[' #TAUTHOR_TAG.', 'however, their']","[' #TAUTHOR_TAG.', 'however, their']","['3 ( top ) shows the evaluation results on the training set.', 'extraction and normalization quality are high, and value accuracies of over 90 % on the cleaned and improved versions are promising.', '4 the results on the test sets (  #AUTHOR_TAG.', 'are written in modern mandarin, some test documents contain taiwan - specific expressions ( c. f. section 4. 3 ) not covered by our rules yet.', 'finally, we compare the normalization quality of our approach to the multilingual parsing approach of  #TAUTHOR_TAG.', 'however, their approach performs only the normalization subtask assuming that the extents of temporal expressions are provided.', 'for this, they used gold extents for evaluation.', 'heideltime only normalizes those expressions that it knows how to extract.', 'thus, we run heideltime performing the extraction and the normalization.', 'however, since the accuracy measure used by the tempeval - 2 script calculates the ratio of correctly normalized expressions to all extracted expressions and not to all expressions in the gold standard, we additionally present the raw numbers of correctly normalized expressions for the two systems.', 'table 4 shows the comparison between our approach and the one by  #TAUTHOR_TAG.', 'we outperform their approach not only with respect to the accuracy but also with respect to the numbers of correctly normalized expressions ( 574 vs. 484 5 and 121 vs. 86 5 on the training and test sets, respectively ) - despite the fact that we perform the full task of temporal tagging and not only the normalization']",7
['cleaned versions also used by  #TAUTHOR_TAG in which temporal expressions without value information'],['cleaned versions also used by  #TAUTHOR_TAG in which temporal expressions without value information'],"['the original versions, ( ii ) the improved versions described in section 3. 3, and ( iii ) the cleaned versions also used by  #TAUTHOR_TAG in which temporal expressions without value information']","[': we use three versions of the tempeval - 2 training and test sets : ( i ) the original versions, ( ii ) the improved versions described in section 3. 3, and ( iii ) the cleaned versions also used by  #TAUTHOR_TAG in which temporal expressions without value information are removed.', 'setting : since the tempeval - 2 data already contains sentence and token information, we only had to perform part - of - speech tagging as linguistic preprocessing step.', 'for this, we used the treetagger  #AUTHOR_TAG with its chinese model']",5
['cleaned versions also used by  #TAUTHOR_TAG in which temporal expressions without value information'],['cleaned versions also used by  #TAUTHOR_TAG in which temporal expressions without value information'],"['the original versions, ( ii ) the improved versions described in section 3. 3, and ( iii ) the cleaned versions also used by  #TAUTHOR_TAG in which temporal expressions without value information']","[': we use three versions of the tempeval - 2 training and test sets : ( i ) the original versions, ( ii ) the improved versions described in section 3. 3, and ( iii ) the cleaned versions also used by  #TAUTHOR_TAG in which temporal expressions without value information are removed.', 'setting : since the tempeval - 2 data already contains sentence and token information, we only had to perform part - of - speech tagging as linguistic preprocessing step.', 'for this, we used the treetagger  #AUTHOR_TAG with its chinese model']",3
"[' #TAUTHOR_TAG.', 'however, their']","[' #TAUTHOR_TAG.', 'however, their']","[' #TAUTHOR_TAG.', 'however, their']","['3 ( top ) shows the evaluation results on the training set.', 'extraction and normalization quality are high, and value accuracies of over 90 % on the cleaned and improved versions are promising.', '4 the results on the test sets (  #AUTHOR_TAG.', 'are written in modern mandarin, some test documents contain taiwan - specific expressions ( c. f. section 4. 3 ) not covered by our rules yet.', 'finally, we compare the normalization quality of our approach to the multilingual parsing approach of  #TAUTHOR_TAG.', 'however, their approach performs only the normalization subtask assuming that the extents of temporal expressions are provided.', 'for this, they used gold extents for evaluation.', 'heideltime only normalizes those expressions that it knows how to extract.', 'thus, we run heideltime performing the extraction and the normalization.', 'however, since the accuracy measure used by the tempeval - 2 script calculates the ratio of correctly normalized expressions to all extracted expressions and not to all expressions in the gold standard, we additionally present the raw numbers of correctly normalized expressions for the two systems.', 'table 4 shows the comparison between our approach and the one by  #TAUTHOR_TAG.', 'we outperform their approach not only with respect to the accuracy but also with respect to the numbers of correctly normalized expressions ( 574 vs. 484 5 and 121 vs. 86 5 on the training and test sets, respectively ) - despite the fact that we perform the full task of temporal tagging and not only the normalization']",4
"['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance']","['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance']","['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance matrix of the word vectors in']","['documents that are similar to a query using vectors has a long history.', 'earlier methods modeled documents and queries using vector space models via bag - of - words ( bow ) representation  #AUTHOR_TAG.', 'other representations include latent semantic indexing ( lsi )  #AUTHOR_TAG, which can be used to define dense vector representation for documents and / or queries.', 'the past few years have witnessed a big interest in distributed representation for words, sentences, paragraphs and documents.', 'this was achieved by leveraging deep learning methods that learn word vector representation.', 'introduction of neural language models  #AUTHOR_TAG using deep learning allowed to learn word vector representation ( word embedding for simplicity ).', 'the seminal work of mikolov et al. introduced an efficient way to compute dense vectorized representation of words  #AUTHOR_TAG a, b ).', 'a more recent step was taken to move beyond distributed representation of words.', 'this is to find a distributed representation for sentences, paragraphs and documents.', 'most of the presented works study the interrelationship between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance matrix of the word vectors in some document to define a novel descriptor for a document.', 'we call our representation docov descriptor.', 'our descriptor obtains a fixed - length representation of the paragraph which captures the interrelationship between the dimensions of the word embedding via the covariance matrix elements.', 'this makes our work distinguished from to the work of  #TAUTHOR_TAG where they study the interrelationship of words in the text snippet']",0
"['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors  #TAUTHOR_TAG are positioned in the space as in figure 1']","['we describe our motivation towards the proposal of our novel representation :', '( 1 ) some neural - based paragraph representations such as paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors  #TAUTHOR_TAG are positioned in the space as in figure 1.', '( 2 ) the covariance matrix represents the second order summary statistic of multivariate data.', 'this distinguishes the covariance matrix from the mean vector.', 'in figure 1 we visualize the covariance matrix using confidence ellipse representation.', 'we see that the covariance encodes the shape of the density composed of the words of interest.', 'in the earlier example the mean vectors of two dissimilar documents are put close by the word embedding.', 'on the other hand, the covariance matrices capture the distinctness of the two documents.', '( 3 ) the use of the covariance as a spatial descriptor for multivariate data has a great success in different domains like computer vision  #AUTHOR_TAG and brain signal analysis  #AUTHOR_TAG.', 'with this global success of this representation, we believe this method can be useful for text - related tasks.', '( 4 ) the computation of the covariance descriptor is known to be fast and highly parallelizable.', 'moreover, there is no inference steps involved while computing the covariance matrix given its observations.', 'this is an advantage compared to existing methods for generating paragraph vectors, such as  #TAUTHOR_TAG.', 'our contribution in this work is two - fold : ( 1 ) we propose the document - covariance descriptor ( docov ) to represent every document as the covariance of the word embedding of its words.', 'to the best of our knowledge, we are the first to explicitly compute covariance descriptors on word embedding such as word2vec  #AUTHOR_TAG b ) or similar word vectors.', '( 2 ) we empirically show the effectiveness of our novel descriptor in comparison to the state - of - theart methods in various unsupervised and supervised classification tasks.', 'our results show that our descriptor can attain comparable accuracy to state - ofthe - art methods in a diverse set of tasks']",0
['paragraph vectors  #TAUTHOR_TAG'],['paragraph vectors  #TAUTHOR_TAG'],['paragraph vectors  #TAUTHOR_TAG'],"['show the correlation values between the similarities computed via docov and the human judgements.', 'we contrast the performance of other representations in table 2.', 'we observe that docov representation outperforms other representations in this task.', 'other models such as skipthought vectors  #AUTHOR_TAG and sdae  #AUTHOR_TAG requires building an encoder - decoder model which takes time 3 to learn.', 'for other models like paragraph vectors  #TAUTHOR_TAG and fastsent vectors  #AUTHOR_TAG, they require a gradient descent inference step to compute the paragraph / sentence vectors.', 'using the docov, we just require a pre - trained word embedding model and we do not need any additional training like encoder - decoder models or inference steps via gradient descent']",0
"['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance']","['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance']","['between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance matrix of the word vectors in']","['documents that are similar to a query using vectors has a long history.', 'earlier methods modeled documents and queries using vector space models via bag - of - words ( bow ) representation  #AUTHOR_TAG.', 'other representations include latent semantic indexing ( lsi )  #AUTHOR_TAG, which can be used to define dense vector representation for documents and / or queries.', 'the past few years have witnessed a big interest in distributed representation for words, sentences, paragraphs and documents.', 'this was achieved by leveraging deep learning methods that learn word vector representation.', 'introduction of neural language models  #AUTHOR_TAG using deep learning allowed to learn word vector representation ( word embedding for simplicity ).', 'the seminal work of mikolov et al. introduced an efficient way to compute dense vectorized representation of words  #AUTHOR_TAG a, b ).', 'a more recent step was taken to move beyond distributed representation of words.', 'this is to find a distributed representation for sentences, paragraphs and documents.', 'most of the presented works study the interrelationship between words in a text snip - pet  #TAUTHOR_TAG in an unsupervised fashion.', 'other methods build a task specific representation  #AUTHOR_TAG.', 'in this paper we propose to use the covariance matrix of the word vectors in some document to define a novel descriptor for a document.', 'we call our representation docov descriptor.', 'our descriptor obtains a fixed - length representation of the paragraph which captures the interrelationship between the dimensions of the word embedding via the covariance matrix elements.', 'this makes our work distinguished from to the work of  #TAUTHOR_TAG where they study the interrelationship of words in the text snippet']",4
"['paragraph vector  #TAUTHOR_TAG.', 'first, we used gensim library 1 to generate']","['paragraph vector  #TAUTHOR_TAG.', 'first, we used gensim library 1 to generate']","['paragraph vector  #TAUTHOR_TAG.', 'first, we used gensim library 1 to generate word vectors and paragraph vectors using a dummy training corpus.', 'next, we formed two hypothetical documents ;']","['show a toy example to highlight the differences between docov vector, the mean vector and paragraph vector  #TAUTHOR_TAG.', 'first, we used gensim library 1 to generate word vectors and paragraph vectors using a dummy training corpus.', 'next, we formed two hypothetical documents ; first document contains words about "" pets "" and second document contains words about "" travel "".', 'in figure 1 we show on the top part the first two dimensions of a word embedding for each document separately.', ""on the bottom left, we show embedding of the two documents'words in the same space."", '']",4
"['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors  #TAUTHOR_TAG are positioned in the space as in figure 1']","['we describe our motivation towards the proposal of our novel representation :', '( 1 ) some neural - based paragraph representations such as paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors  #TAUTHOR_TAG are positioned in the space as in figure 1.', '( 2 ) the covariance matrix represents the second order summary statistic of multivariate data.', 'this distinguishes the covariance matrix from the mean vector.', 'in figure 1 we visualize the covariance matrix using confidence ellipse representation.', 'we see that the covariance encodes the shape of the density composed of the words of interest.', 'in the earlier example the mean vectors of two dissimilar documents are put close by the word embedding.', 'on the other hand, the covariance matrices capture the distinctness of the two documents.', '( 3 ) the use of the covariance as a spatial descriptor for multivariate data has a great success in different domains like computer vision  #AUTHOR_TAG and brain signal analysis  #AUTHOR_TAG.', 'with this global success of this representation, we believe this method can be useful for text - related tasks.', '( 4 ) the computation of the covariance descriptor is known to be fast and highly parallelizable.', 'moreover, there is no inference steps involved while computing the covariance matrix given its observations.', 'this is an advantage compared to existing methods for generating paragraph vectors, such as  #TAUTHOR_TAG.', 'our contribution in this work is two - fold : ( 1 ) we propose the document - covariance descriptor ( docov ) to represent every document as the covariance of the word embedding of its words.', 'to the best of our knowledge, we are the first to explicitly compute covariance descriptors on word embedding such as word2vec  #AUTHOR_TAG b ) or similar word vectors.', '( 2 ) we empirically show the effectiveness of our novel descriptor in comparison to the state - of - theart methods in various unsupervised and supervised classification tasks.', 'our results show that our descriptor can attain comparable accuracy to state - ofthe - art methods in a diverse set of tasks']",4
"['is consistently better than the paragraph vectors  #TAUTHOR_TAG, fastsent']","['is consistently better than the paragraph vectors  #TAUTHOR_TAG, fastsent']","['is consistently better than the paragraph vectors  #TAUTHOR_TAG, fastsent']","['', 'all of our results are reported using the freely available gnews word2vec of dim = 300.', 'we use classification accuracy as the evaluation measure for this experiment as  #AUTHOR_TAG.', 'the subsets used in comparative benchmark evaluation are : movie reviews mr  #AUTHOR_TAG, subjectivity subj  #AUTHOR_TAG, customer reviews cr  #AUTHOR_TAG and trec question trec  #AUTHOR_TAG.', 'results and discussion table 3 shows the results of our variants against state - of - art algorithms that use unsupervised paragraph representation.', 'we observe that docov is consistently better than the mean vector and bow with tf - idf weights.', 'also, docov is improving consistently when concatenated with baselines such as mean vector and bow vectors.', 'this means each feature is capturing different discriminating information.', 'this justifies the choice of concatenating docov with other features.', 'we further observe that docov is consistently better than the paragraph vectors  #TAUTHOR_TAG, fastsent and sdae  #AUTHOR_TAG.', 'the overall accuracy of docov is highlighted and it outperforms other methods on the text classification benchmark']",4
"['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear']","['paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors  #TAUTHOR_TAG are positioned in the space as in figure 1']","['we describe our motivation towards the proposal of our novel representation :', '( 1 ) some neural - based paragraph representations such as paragraph vectors  #TAUTHOR_TAG, fastsent  #AUTHOR_TAG use a shared space between the words and paragraphs.', 'this is counter intuitive, as the paragraph is a different entity other than the words.', 'figure 1 illustrates that point, we do not see a clear interpretation of why the paragraph vectors  #TAUTHOR_TAG are positioned in the space as in figure 1.', '( 2 ) the covariance matrix represents the second order summary statistic of multivariate data.', 'this distinguishes the covariance matrix from the mean vector.', 'in figure 1 we visualize the covariance matrix using confidence ellipse representation.', 'we see that the covariance encodes the shape of the density composed of the words of interest.', 'in the earlier example the mean vectors of two dissimilar documents are put close by the word embedding.', 'on the other hand, the covariance matrices capture the distinctness of the two documents.', '( 3 ) the use of the covariance as a spatial descriptor for multivariate data has a great success in different domains like computer vision  #AUTHOR_TAG and brain signal analysis  #AUTHOR_TAG.', 'with this global success of this representation, we believe this method can be useful for text - related tasks.', '( 4 ) the computation of the covariance descriptor is known to be fast and highly parallelizable.', 'moreover, there is no inference steps involved while computing the covariance matrix given its observations.', 'this is an advantage compared to existing methods for generating paragraph vectors, such as  #TAUTHOR_TAG.', 'our contribution in this work is two - fold : ( 1 ) we propose the document - covariance descriptor ( docov ) to represent every document as the covariance of the word embedding of its words.', 'to the best of our knowledge, we are the first to explicitly compute covariance descriptors on word embedding such as word2vec  #AUTHOR_TAG b ) or similar word vectors.', '( 2 ) we empirically show the effectiveness of our novel descriptor in comparison to the state - of - theart methods in various unsupervised and supervised classification tasks.', 'our results show that our descriptor can attain comparable accuracy to state - ofthe - art methods in a diverse set of tasks']",3
"['paragraph vectors  #TAUTHOR_TAG.', 'recently other neural - based sentence and paragraph']","['paragraph vectors  #TAUTHOR_TAG.', 'recently other neural - based sentence and paragraph']","['paragraph vectors  #TAUTHOR_TAG.', 'recently other neural - based sentence and paragraph level representations appeared to provide a fixed length representation']",[' #TAUTHOR_TAG'],3
"['contrast our results against the methods reported in  #AUTHOR_TAG.', 'the competing methods are the paragraph vectors  #TAUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fast']","['contrast our results against the methods reported in  #AUTHOR_TAG.', 'the competing methods are the paragraph vectors  #TAUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fastsent  #AUTHOR_TAG, sequential ( denoising )']","['contrast our results against the methods reported in  #AUTHOR_TAG.', 'the competing methods are the paragraph vectors  #TAUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fast']","['contrast our results against the methods reported in  #AUTHOR_TAG.', 'the competing methods are the paragraph vectors  #TAUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fastsent  #AUTHOR_TAG, sequential ( denoising ) autoencoders ( sdae )  #AUTHOR_TAG.', 'the mean vector baseline is also implemented.', 'also, we use the sum of the similarities generated by the docov and the mean vectors.', 'all of our results are reported using the freely available gnews word2vec of dim = 300.', 'we use same evaluation measures  #AUTHOR_TAG.', 'we use the pearson correlation and spearman correlation with the manual relatedness judgements.', 'the semantic sentence relatedness datasets used in the comparative evaluation the sick dataset  #AUTHOR_TAG consists of 10, 000 pairs of sentences and relatedness judgements and the sts 2014 dataset  #AUTHOR_TAG consists of 3, 750 pairs and ratings from six linguistic domains']",3
"['contrast our results against the methods reported in  #AUTHOR_TAG.', 'the competing methods are the paragraph vectors  #TAUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fast']","['contrast our results against the methods reported in  #AUTHOR_TAG.', 'the competing methods are the paragraph vectors  #TAUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fastsent  #AUTHOR_TAG, sequential ( denoising )']","['contrast our results against the methods reported in  #AUTHOR_TAG.', 'the competing methods are the paragraph vectors  #TAUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fast']","['contrast our results against the methods reported in  #AUTHOR_TAG.', 'the competing methods are the paragraph vectors  #TAUTHOR_TAG, skip - thought vectors  #AUTHOR_TAG, fastsent  #AUTHOR_TAG, sequential ( denoising ) autoencoders ( sdae )  #AUTHOR_TAG.', 'the mean vector baseline is also implemented.', 'also, we use the sum of the similarities generated by the docov and the mean vectors.', 'all of our results are reported using the freely available gnews word2vec of dim = 300.', 'we use same evaluation measures  #AUTHOR_TAG.', 'we use the pearson correlation and spearman correlation with the manual relatedness judgements.', 'the semantic sentence relatedness datasets used in the comparative evaluation the sick dataset  #AUTHOR_TAG consists of 10, 000 pairs of sentences and relatedness judgements and the sts 2014 dataset  #AUTHOR_TAG consists of 3, 750 pairs and ratings from six linguistic domains']",5
"['the morphological divergence between the source and target language is high.', ' #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG have demonstrated ways to']","['the morphological divergence between the source and target language is high.', ' #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG have demonstrated ways to']","['the morphological divergence between the source and target language is high.', ' #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG have demonstrated ways to']","['this paper, we present our experiments on smt from bengali, marathi, tamil, telugu and english to hindi.', 'from the set of languages involved in the shared task, bengali, hindi and marathi belong to indoaryan family and tamil and telugu are from dravidian language family.', 'all languages except english, have the same flexibility towards word order, canonically following the sov structure.', 'with reference to the morphology, bengali, marathi, tamil, and telugu are more agglutinative compared to hindi.', 'it is known that smt produces more unknown words resulting in the bad translation quality if the morphological divergence between the source and target language is high.', ' #AUTHOR_TAG,  #AUTHOR_TAG and  #TAUTHOR_TAG have demonstrated ways to handle this issue with morphological segmentation of words before training the smt system.', 'to tackle the morphological divergence of hindi with these languages we have explored suffix separation ( ss ) and compound word splitting ( cs ) as a pre - processing step.', 'for english to hindi smt, better alignment is achieved through the use of preordering developed by  #AUTHOR_TAG and stem as an alignment factor.', 'the rest of the paper is organized as follows.', 'in section 2, we discuss our methodology, followed by data - set and experimental setup in section 3.', 'section 4 discusses experiments and results.', 'submitted systems to the shared task and error analysis are displayed in section 5 and 6 respectively, followed by conclusion and future work in section 7']",0
['##ordering of the source language sentence helps in the better alignment and decoding for english to indian language  #TAUTHOR_TAG sm'],"['##ordering of the source language sentence helps in the better alignment and decoding for english to indian language  #TAUTHOR_TAG smt.', 'table 4 details the results for the systems under study.', 'we can see that bl + ro shows significant']","['##ordering of the source language sentence helps in the better alignment and decoding for english to indian language  #TAUTHOR_TAG smt.', 'table 4 details the results for the systems under study.', 'we can see that bl + ro shows significant improvement over bl.', 'further, the']","['##ordering of the source language sentence helps in the better alignment and decoding for english to indian language  #TAUTHOR_TAG smt.', 'table 4 details the results for the systems under study.', 'we can see that bl + ro shows significant improvement over bl.', 'further, the factored smt system with stem as alignment factor shows slight improvement in bleu over the bl + ro, but other metrics show bl + ro is better compared to the factored system']",0
"['according to the target language ( hindi ) structure.', 'we have used source side reordering developed by  #AUTHOR_TAG, and  #TAUTHOR_TAG we now discuss training and testing corpus from health, tourism and general domains for be - hi, mrhi, ta - hi, te - hi, and en - hi language pairs, followed by preprocessing, smt system setup and evaluation metrics for experiments']","['according to the target language ( hindi ) structure.', 'we have used source side reordering developed by  #AUTHOR_TAG, and  #TAUTHOR_TAG we now discuss training and testing corpus from health, tourism and general domains for be - hi, mrhi, ta - hi, te - hi, and en - hi language pairs, followed by preprocessing, smt system setup and evaluation metrics for experiments']","['is based on the syntactic transformation of the english sentence parse tree according to the target language ( hindi ) structure.', 'we have used source side reordering developed by  #AUTHOR_TAG, and  #TAUTHOR_TAG we now discuss training and testing corpus from health, tourism and general domains for be - hi, mrhi, ta - hi, te - hi, and en - hi language pairs, followed by preprocessing, smt system setup and evaluation metrics for experiments']","['is based on the syntactic transformation of the english sentence parse tree according to the target language ( hindi ) structure.', 'we have used source side reordering developed by  #AUTHOR_TAG, and  #TAUTHOR_TAG we now discuss training and testing corpus from health, tourism and general domains for be - hi, mrhi, ta - hi, te - hi, and en - hi language pairs, followed by preprocessing, smt system setup and evaluation metrics for experiments']",5
"[', as explained in section 2.', 'to handle the structural divergence for english - hindi smt, we exploited source side preordering  #TAUTHOR_TAG']","['tackle the morphological divergence between the source and target languages ( bengali / marathi / tamil / telugu to hindi ), we used suffix separation and compound splitting, as explained in section 2.', 'to handle the structural divergence for english - hindi smt, we exploited source side preordering  #TAUTHOR_TAG']","['tackle the morphological divergence between the source and target languages ( bengali / marathi / tamil / telugu to hindi ), we used suffix separation and compound splitting, as explained in section 2.', 'to handle the structural divergence for english - hindi smt, we exploited source side preordering  #TAUTHOR_TAG']","['tackle the morphological divergence between the source and target languages ( bengali / marathi / tamil / telugu to hindi ), we used suffix separation and compound splitting, as explained in section 2.', 'to handle the structural divergence for english - hindi smt, we exploited source side preordering  #TAUTHOR_TAG']",5
"['##ed smt training, we used source and target side stem as an alignment factor.', 'stemming was done using lightweight stemmer  #TAUTHOR_TAG for hindi.', 'for english, we used porter stemmer  #AUTHOR_TAG']","['baseline system was setup using the phrasebased model (  #AUTHOR_TAG was used for factored model.', 'the language model was trained using kenlm  #AUTHOR_TAG toolkit with modified kneser - ney smoothing  #AUTHOR_TAG.', 'for factored smt training, we used source and target side stem as an alignment factor.', 'stemming was done using lightweight stemmer  #TAUTHOR_TAG for hindi.', 'for english, we used porter stemmer  #AUTHOR_TAG']","['##ed smt training, we used source and target side stem as an alignment factor.', 'stemming was done using lightweight stemmer  #TAUTHOR_TAG for hindi.', 'for english, we used porter stemmer  #AUTHOR_TAG']","['baseline system was setup using the phrasebased model (  #AUTHOR_TAG was used for factored model.', 'the language model was trained using kenlm  #AUTHOR_TAG toolkit with modified kneser - ney smoothing  #AUTHOR_TAG.', 'for factored smt training, we used source and target side stem as an alignment factor.', 'stemming was done using lightweight stemmer  #TAUTHOR_TAG for hindi.', 'for english, we used porter stemmer  #AUTHOR_TAG']",5
"[' #TAUTHOR_TAG. gss is a spatial mixture model based blind', 'source separation approach which exploits the annotation given in the chim']","[' #TAUTHOR_TAG. gss is a spatial mixture model based blind', 'source separation approach which exploits the annotation given in the chim']","[' #TAUTHOR_TAG. gss is a spatial mixture model based blind', 'source separation approach which exploits the annotation given in the chim']","[']. the chime - 5 data is heavily degraded by reverberation and overlapped speech. as much as 23 % of the', ""time more than one speaker is active at the same time [ 12 ]. the challenge's baseline system poor performance ( about 80 %"", 'wer ) is an indication that asr training did not work well. recently, guided source separation ( gss ) enhancement on the test data was shown to significantly', 'improve the performance of an acoustic model, which had been trained with a large amount of unprocessed and simulated noisy data  #TAUTHOR_TAG. gss is a spatial mixture model based blind', 'source separation approach which exploits the annotation given in the chime - 5 database for initialization and, in this way, avoids the frequency permutation problem [ 14 ]. we conjectured that cleaning up the training data would enable', 'a more effective acoustic model training for the chime - 5 scenario. we have therefore experimented with enhancement algorithms of various strengths, from relatively simple beamforming over singlearray gss to a quite sophisticated multi - array gss approach, and tested all combinations of training and test data enhancement methods. furthermore, compared to the initial gss approach', 'in [ 14 ], we describe here some modifications, which led to improved performance. we also propose an improved neural acoustic modeling structure compared to the chime - 5 baseline system described in [ 10 ]. it consists of initial convolutional neural network ( cnn ) layers followed by factorized', 'tdnn ( tdnn - f ) layers, instead of a homogeneous tdnn - f architecture. using a single acoustic model trained with 308 hrs of training data, which', 'resulted after applying multi - array gss data cleaning and a three - fold speed perturbation,', '']",0
"[' #TAUTHOR_TAG. gss is a spatial mixture model based blind', 'source separation approach which exploits the annotation given in the chim']","[' #TAUTHOR_TAG. gss is a spatial mixture model based blind', 'source separation approach which exploits the annotation given in the chim']","[' #TAUTHOR_TAG. gss is a spatial mixture model based blind', 'source separation approach which exploits the annotation given in the chim']","[']. the chime - 5 data is heavily degraded by reverberation and overlapped speech. as much as 23 % of the', ""time more than one speaker is active at the same time [ 12 ]. the challenge's baseline system poor performance ( about 80 %"", 'wer ) is an indication that asr training did not work well. recently, guided source separation ( gss ) enhancement on the test data was shown to significantly', 'improve the performance of an acoustic model, which had been trained with a large amount of unprocessed and simulated noisy data  #TAUTHOR_TAG. gss is a spatial mixture model based blind', 'source separation approach which exploits the annotation given in the chime - 5 database for initialization and, in this way, avoids the frequency permutation problem [ 14 ]. we conjectured that cleaning up the training data would enable', 'a more effective acoustic model training for the chime - 5 scenario. we have therefore experimented with enhancement algorithms of various strengths, from relatively simple beamforming over singlearray gss to a quite sophisticated multi - array gss approach, and tested all combinations of training and test data enhancement methods. furthermore, compared to the initial gss approach', 'in [ 14 ], we describe here some modifications, which led to improved performance. we also propose an improved neural acoustic modeling structure compared to the chime - 5 baseline system described in [ 10 ]. it consists of initial convolutional neural network ( cnn ) layers followed by factorized', 'tdnn ( tdnn - f ) layers, instead of a homogeneous tdnn - f architecture. using a single acoustic model trained with 308 hrs of training data, which', 'resulted after applying multi - array gss data cleaning and a three - fold speed perturbation,', '']",4
"['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in']","['facilitate comparison with the recently published top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in table 4.', 'as explained in section 5. 1, we opted for  #TAUTHOR_TAG instead of [ 14 ] as baseline because the former system is stronger.', 'the experiments include refining the gss enhancement using time annotations from asr output ( gss w / asr ), performing discriminative training on top of the ams trained with lf - mmi and performing rnn lm rescoring.', '']",4
"['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in']","['facilitate comparison with the recently published top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in table 4.', 'as explained in section 5. 1, we opted for  #TAUTHOR_TAG instead of [ 14 ] as baseline because the former system is stronger.', 'the experiments include refining the gss enhancement using time annotations from asr output ( gss w / asr ), performing discriminative training on top of the ams trained with lf - mmi and performing rnn lm rescoring.', '']",4
"['to the movement  #TAUTHOR_TAG. alternatively,']","['to the movement  #TAUTHOR_TAG. alternatively,']","['to the movement  #TAUTHOR_TAG. alternatively, one can run the em first with']","[""parameter estimation of the mm both during em initialization ( posterior masks set to one divided by the number of active speakers for active speakers'frames"", ', and zero for the non - active speakers ) and after each e - step ( posterior masks are clamped to zero', 'for non - active speakers ). the initialization of the em for each mixture component is very important for the correct convergence of the algorithm. if the em initialization', 'is close enough to the final solution, then it is expected that the algorithm will correctly', 'separate the sources and source indices are not permuted across frequency bins. this has a', 'major practical application, since frequency permutation solvers like [ 21 ] become obsolete. temporal context also plays an important role in the em initialization. simulations have shown that a', 'large context of 15 seconds left and right of the considered segment improves the mixture model estimation performance significantly for chime', '- 5 [ 14 ]. however, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement  #TAUTHOR_TAG. alternatively, one can run the em first with a larger temporal context until convergence, then drop', 'the context and re - run it for some more iterations. as shown later in the paper', '']",5
"['to the movement  #TAUTHOR_TAG. alternatively,']","['to the movement  #TAUTHOR_TAG. alternatively,']","['to the movement  #TAUTHOR_TAG. alternatively, one can run the em first with']","[""parameter estimation of the mm both during em initialization ( posterior masks set to one divided by the number of active speakers for active speakers'frames"", ', and zero for the non - active speakers ) and after each e - step ( posterior masks are clamped to zero', 'for non - active speakers ). the initialization of the em for each mixture component is very important for the correct convergence of the algorithm. if the em initialization', 'is close enough to the final solution, then it is expected that the algorithm will correctly', 'separate the sources and source indices are not permuted across frequency bins. this has a', 'major practical application, since frequency permutation solvers like [ 21 ] become obsolete. temporal context also plays an important role in the em initialization. simulations have shown that a', 'large context of 15 seconds left and right of the considered segment improves the mixture model estimation performance significantly for chime', '- 5 [ 14 ]. however, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement  #TAUTHOR_TAG. alternatively, one can run the em first with a larger temporal context until convergence, then drop', 'the context and re - run it for some more iterations. as shown later in the paper', '']",5
"['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in']","['facilitate comparison with the recently published top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in table 4.', 'as explained in section 5. 1, we opted for  #TAUTHOR_TAG instead of [ 14 ] as baseline because the former system is stronger.', 'the experiments include refining the gss enhancement using time annotations from asr output ( gss w / asr ), performing discriminative training on top of the ams trained with lf - mmi and performing rnn lm rescoring.', '']",5
"['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in']","['facilitate comparison with the recently published top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in table 4.', 'as explained in section 5. 1, we opted for  #TAUTHOR_TAG instead of [ 14 ] as baseline because the former system is stronger.', 'the experiments include refining the gss enhancement using time annotations from asr output ( gss w / asr ), performing discriminative training on top of the ams trained with lf - mmi and performing rnn lm rescoring.', '']",5
"['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in']","['facilitate comparison with the recently published top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in table 4.', 'as explained in section 5. 1, we opted for  #TAUTHOR_TAG instead of [ 14 ] as baseline because the former system is stronger.', 'the experiments include refining the gss enhancement using time annotations from asr output ( gss w / asr ), performing discriminative training on top of the ams trained with lf - mmi and performing rnn lm rescoring.', '']",5
"['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have']","['top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in']","['facilitate comparison with the recently published top - line in  #TAUTHOR_TAG ( h / upb ), we have conducted a more focused set of experiments whose results are depicted in table 4.', 'as explained in section 5. 1, we opted for  #TAUTHOR_TAG instead of [ 14 ] as baseline because the former system is stronger.', 'the experiments include refining the gss enhancement using time annotations from asr output ( gss w / asr ), performing discriminative training on top of the ams trained with lf - mmi and performing rnn lm rescoring.', '']",5
"['corresponds to the gss configuration in  #TAUTHOR_TAG.', 'first']","['corresponds to the gss configuration in  #TAUTHOR_TAG.', 'first']","['corresponds to the gss configuration in  #TAUTHOR_TAG.', '']","['experiments have shown that the temporal context of some gss components has a significant effect on the wer.', 'two cases are investigated : ( i ) partially dropping the temporal context for the em stage, and ( ii ) dropping the temporal context for beamforming.', 'the evaluation was conducted with an acoustic model trained on unprocessed speech and the enhancement was applied during test only.', 'results are depicted in table 6.', 'the first row corresponds to the gss configuration in [ 14 ] while the second one corresponds to the gss configuration in  #TAUTHOR_TAG.', 'first two rows show that dropping the temporal context for estimating statistics for beamforming improves asr accuracy.', 'for the last row, the em algorithm was run 20 iterations with temporal context, followed by another 10 without context.', '']",5
"['corresponds to the gss configuration in  #TAUTHOR_TAG.', 'first']","['corresponds to the gss configuration in  #TAUTHOR_TAG.', 'first']","['corresponds to the gss configuration in  #TAUTHOR_TAG.', '']","['experiments have shown that the temporal context of some gss components has a significant effect on the wer.', 'two cases are investigated : ( i ) partially dropping the temporal context for the em stage, and ( ii ) dropping the temporal context for beamforming.', 'the evaluation was conducted with an acoustic model trained on unprocessed speech and the enhancement was applied during test only.', 'results are depicted in table 6.', 'the first row corresponds to the gss configuration in [ 14 ] while the second one corresponds to the gss configuration in  #TAUTHOR_TAG.', 'first two rows show that dropping the temporal context for estimating statistics for beamforming improves asr accuracy.', 'for the last row, the em algorithm was run 20 iterations with temporal context, followed by another 10 without context.', '']",5
"['to the movement  #TAUTHOR_TAG. alternatively,']","['to the movement  #TAUTHOR_TAG. alternatively,']","['to the movement  #TAUTHOR_TAG. alternatively, one can run the em first with']","[""parameter estimation of the mm both during em initialization ( posterior masks set to one divided by the number of active speakers for active speakers'frames"", ', and zero for the non - active speakers ) and after each e - step ( posterior masks are clamped to zero', 'for non - active speakers ). the initialization of the em for each mixture component is very important for the correct convergence of the algorithm. if the em initialization', 'is close enough to the final solution, then it is expected that the algorithm will correctly', 'separate the sources and source indices are not permuted across frequency bins. this has a', 'major practical application, since frequency permutation solvers like [ 21 ] become obsolete. temporal context also plays an important role in the em initialization. simulations have shown that a', 'large context of 15 seconds left and right of the considered segment improves the mixture model estimation performance significantly for chime', '- 5 [ 14 ]. however, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement  #TAUTHOR_TAG. alternatively, one can run the em first with a larger temporal context until convergence, then drop', 'the context and re - run it for some more iterations. as shown later in the paper', '']",1
"['to the movement  #TAUTHOR_TAG. alternatively,']","['to the movement  #TAUTHOR_TAG. alternatively,']","['to the movement  #TAUTHOR_TAG. alternatively, one can run the em first with']","[""parameter estimation of the mm both during em initialization ( posterior masks set to one divided by the number of active speakers for active speakers'frames"", ', and zero for the non - active speakers ) and after each e - step ( posterior masks are clamped to zero', 'for non - active speakers ). the initialization of the em for each mixture component is very important for the correct convergence of the algorithm. if the em initialization', 'is close enough to the final solution, then it is expected that the algorithm will correctly', 'separate the sources and source indices are not permuted across frequency bins. this has a', 'major practical application, since frequency permutation solvers like [ 21 ] become obsolete. temporal context also plays an important role in the em initialization. simulations have shown that a', 'large context of 15 seconds left and right of the considered segment improves the mixture model estimation performance significantly for chime', '- 5 [ 14 ]. however, having such a large temporal context may become problematic when the speakers are moving, because the estimated spatial covariance matrix can become outdated due to the movement  #TAUTHOR_TAG. alternatively, one can run the em first with a larger temporal context until convergence, then drop', 'the context and re - run it for some more iterations. as shown later in the paper', '']",6
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG, the use of manual language - agnostic patterns on ud makes pred']","['', 'compared to other existing systems for predicate - argument extraction  #TAUTHOR_TAG, the use of manual language - agnostic patterns on ud makes predpatt a well - founded component across languages.', 'additionally, the underlying structure constructed by predpatt has been shown to be a well - formed syntax - semantics interface for nlp tasks :  #AUTHOR_TAG utilizes predpatt to extract possibilistic propositions in automatic common - sense inference generation.', '']",0
"['predicate - argument extraction  #TAUTHOR_TAG.', 'for ewt, we select 13, 583 sentences that have the version 2']","['predicate - argument extraction  #TAUTHOR_TAG.', 'for ewt, we select 13, 583 sentences that have the version 2. 0 of the gold ud annotations.', '5 the resulting annotations on these two corpora contain over 94k extractions']","['predicate - argument extraction  #TAUTHOR_TAG.', 'for ewt, we select 13, 583 sentences that have the version 2. 0 of the gold ud annotations.', '5 the resulting annotations on these two corpora contain over 94k extractions']","['', 'this makes propbank  #AUTHOR_TAG a natural choice from which we can create gold annotations for open ie, here, we choose to use expert annotations from propbank, as compared to the recent suggestion to employ non - expert annotations as a means of benchmarking systems  #AUTHOR_TAG.', 'another advantage of choosing propbank is that propbank has gold annotations for ud which lays the important groundwork for evaluating ud - based patterns in predpatt.', 'in this work, we create gold annotations for predicate - argument extraction by converting propbank annotations on english web treebank ( ewt ) ( ldc2012t13 ) and the penn treebank ii wall street journal corpus ( wsj )  #AUTHOR_TAG.', '3 these two corpora have all verbal predicates annotated, and are used to evaluate predpatt in different perspectives : ewt is the corpus where the gold standard english ud treebank is built over, which enables an evaluation and analysis of predpatt patterns ; wsj is used to evaluate predpatt in a real - world scenario where we run syntaxnet parser 4  #AUTHOR_TAG on the corpus to generate automated ud parses as input of predpatt.', 'table 1 shows the statistics of the auto - converted gold annotations for predicate - argument extraction on ewt and wsj.', ""we convert the propbank annotations for all verbal predicates in these two corpora, and ignore roles of directional ( dir ), manner ( mnr ), modals ( mod ), negation ( neg ) and adverbials ( adv ), as they aren't extracted as distinct argument but instead are folded into the complex predicate by predpatt and other systems for predicate - argument extraction  #TAUTHOR_TAG."", 'for ewt, we select 13, 583 sentences that have the version 2. 0 of the gold ud annotations.', '5 the resulting annotations on these two corpora contain over 94k extractions']",0
"['6 ollie  #AUTHOR_TAG, clausie  #AUTHOR_TAG, and stanford open ie  #TAUTHOR_TAG']","['prominent open ie systems : openie 4, 6 ollie  #AUTHOR_TAG, clausie  #AUTHOR_TAG, and stanford open ie  #TAUTHOR_TAG']","['with four prominent open ie systems : openie 4, 6 ollie  #AUTHOR_TAG, clausie  #AUTHOR_TAG, and stanford open ie  #TAUTHOR_TAG']","['this section, we evaluate the original predpatt ( predpatt v1 ) and the improved predpatt ( predpatt v2 ) on the english web treebank ( ewt ) and the wall street journal corpus ( wsj ), and compare their performance with four prominent open ie systems : openie 4, 6 ollie  #AUTHOR_TAG, clausie  #AUTHOR_TAG, and stanford open ie  #TAUTHOR_TAG']",5
"[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'however, it has been proven that brown clusters as discrete features']","[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'however, it has been proven that brown clusters as discrete features']","[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'however, it has been proven that brown clusters as discrete features are even']","['we consider traditional cluster encoded word representation, e. g., brown clusters  #AUTHOR_TAG, it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word.', 'in fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'however, it has been proven that brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks  #AUTHOR_TAG.', 'guo et al.  #AUTHOR_TAG further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension.', 'they have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks.', 'moreover, faruqui et al.  #AUTHOR_TAG showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets.', 'these works indicate that, for some tasks, we do not need all the information encoded in "" standard "" word embeddings.', 'nonetheless, it is clear that binarization loses a lot of information, and this calls for a systematic comparison of how many bits are needed to maintain the expressivity needed from word embeddings for different tasks']",0
"[') iteration in word2vec algorithms  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","['( sgd ) iteration in word2vec algorithms  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","["") iteration in word2vec algorithms  #AUTHOR_TAG a ;  #TAUTHOR_TAG, the updating vector's values are often very small numbers ( e.""]","['the memory for training word embedding is also limited, we need to modify the training algorithms by introducing new data structures to reduce the bits used to encode the values.', ""in practice, we found that in the stochastic gradient descent ( sgd ) iteration in word2vec algorithms  #AUTHOR_TAG a ;  #TAUTHOR_TAG, the updating vector's values are often very small numbers ( e. g., < 10 −5 )."", 'in this case, if we directly apply the rounding method to certain precisions ( e. g., 8 bits ), the update of word vectors will always be zero.', 'for example, the 8 - bit precision is 2 −7 = 0. 0078, so 10 −5 is not significant enough to update the vector with 8 - bit values.', 'therefore, we consider the following two ways to improve this.', 'stochastic rounding.', 'we first consider using stochastic rounding  #AUTHOR_TAG to train word embedding.', 'stochastic rounding introduces some randomness into the rounding mechanism, which has been proven to be helpful when there are many parameters in the learning system, such as deep learning systems  #AUTHOR_TAG.', 'here we also introduce this approach to update word embedding vectors in sgd.', 'the probability of rounding x to x is proportional to the proximity of x to x :', '']",0
"[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'however, it has been proven that brown clusters as discrete features']","[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'however, it has been proven that brown clusters as discrete features']","[' #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'however, it has been proven that brown clusters as discrete features are even']","['we consider traditional cluster encoded word representation, e. g., brown clusters  #AUTHOR_TAG, it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word.', 'in fact, word embedding generalized the idea of discrete clustering representation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and generalization ability  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', 'however, it has been proven that brown clusters as discrete features are even better than continuous word embedding as features for named entity recognition tasks  #AUTHOR_TAG.', 'guo et al.  #AUTHOR_TAG further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represent each dimension.', 'they have shown that binarization can be comparable to or even better than the original word embeddings when used as features for named entity recognition tasks.', 'moreover, faruqui et al.  #AUTHOR_TAG showed that imposing sparsity constraints over the embedding vectors can further improve the representation interpretability and performance on several word similarity and text classification benchmark datasets.', 'these works indicate that, for some tasks, we do not need all the information encoded in "" standard "" word embeddings.', 'nonetheless, it is clear that binarization loses a lot of information, and this calls for a systematic comparison of how many bits are needed to maintain the expressivity needed from word embeddings for different tasks']",1
"[') iteration in word2vec algorithms  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","['( sgd ) iteration in word2vec algorithms  #AUTHOR_TAG a ;  #TAUTHOR_TAG,']","["") iteration in word2vec algorithms  #AUTHOR_TAG a ;  #TAUTHOR_TAG, the updating vector's values are often very small numbers ( e.""]","['the memory for training word embedding is also limited, we need to modify the training algorithms by introducing new data structures to reduce the bits used to encode the values.', ""in practice, we found that in the stochastic gradient descent ( sgd ) iteration in word2vec algorithms  #AUTHOR_TAG a ;  #TAUTHOR_TAG, the updating vector's values are often very small numbers ( e. g., < 10 −5 )."", 'in this case, if we directly apply the rounding method to certain precisions ( e. g., 8 bits ), the update of word vectors will always be zero.', 'for example, the 8 - bit precision is 2 −7 = 0. 0078, so 10 −5 is not significant enough to update the vector with 8 - bit values.', 'therefore, we consider the following two ways to improve this.', 'stochastic rounding.', 'we first consider using stochastic rounding  #AUTHOR_TAG to train word embedding.', 'stochastic rounding introduces some randomness into the rounding mechanism, which has been proven to be helpful when there are many parameters in the learning system, such as deep learning systems  #AUTHOR_TAG.', 'here we also introduce this approach to update word embedding vectors in sgd.', 'the probability of rounding x to x is proportional to the proximity of x to x :', '']",1
"['evaluating word embeddings.', 'we train the word embedding algorithms, word2vec  #AUTHOR_TAG a ;  #TAUTHOR_TAG, based on the oct. 2013 wikipedia dump.', '1 we first compare levels of truncation of word2vec embeddings,']","['evaluating word embeddings.', 'we train the word embedding algorithms, word2vec  #AUTHOR_TAG a ;  #TAUTHOR_TAG, based on the oct. 2013 wikipedia dump.', '1 we first compare levels of truncation of word2vec embeddings,']","['evaluating word embeddings.', 'we train the word embedding algorithms, word2vec  #AUTHOR_TAG a ;  #TAUTHOR_TAG, based on the oct. 2013 wikipedia dump.', '1 we first compare levels of truncation of word2vec embeddings,']","['this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings.', 'we train the word embedding algorithms, word2vec  #AUTHOR_TAG a ;  #TAUTHOR_TAG, based on the oct. 2013 wikipedia dump.', '1 we first compare levels of truncation of word2vec embeddings, and then evaluate the stochastic rounding and the auxiliary vectors based methods for training word2vec vectors']",5
['cbow and skipgram with negative sampling  #AUTHOR_TAG a ;  #TAUTHOR_TAG on the wikipedia dump'],"['cbow and skipgram with negative sampling  #AUTHOR_TAG a ;  #TAUTHOR_TAG on the wikipedia dump data, and set the window size of context to be five.', 'then we performed value truncation with 4 bits, 6 bits, and 8 bits.', 'the results are shown in fig. 1, and the numbers of the averaged results are shown in table 1.', '']","['ran both cbow and skipgram with negative sampling  #AUTHOR_TAG a ;  #TAUTHOR_TAG on the wikipedia dump data, and set the window size of context to be five.', 'then we performed value truncation with 4 bits, 6 bits, and 8 bits.', 'the results are shown in fig. 1, and the numbers of the averaged results are shown in table 1.', '']","['ran both cbow and skipgram with negative sampling  #AUTHOR_TAG a ;  #TAUTHOR_TAG on the wikipedia dump data, and set the window size of context to be five.', 'then we performed value truncation with 4 bits, 6 bits, and 8 bits.', 'the results are shown in fig. 1, and the numbers of the averaged results are shown in table 1.', 'we also used the binarization algorithm  #AUTHOR_TAG to truncate each dimension to three values ; these experiments are is denoted using the suffix "" binary "" in the figure.', 'for both cbow and skipgram models, we train the vectors with 25 and 200 dimensions respectively.', 'the representations used in our experiments were trained using the whole wikipedia dump.', '']",5
"['##le ) ∧ mother ( michelle, sasha ) to train kb embeddings has been proposed very recently  #TAUTHOR_TAG']","['paths ( e. g., husband ( barack, michelle ) ∧ mother ( michelle, sasha ) to train kb embeddings has been proposed very recently  #TAUTHOR_TAG']","['g., husband ( barack, michelle ) ∧ mother ( michelle, sasha ) to train kb embeddings has been proposed very recently  #TAUTHOR_TAG.', 'while using relation paths']","['', 'early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities ( e. g., father ( barack, sasha ) )  #AUTHOR_TAG.', 'in contrast, using multi - step relation paths ( e. g., husband ( barack, michelle ) ∧ mother ( michelle, sasha ) to train kb embeddings has been proposed very recently  #TAUTHOR_TAG.', 'while using relation paths improves model performance, it also poses a critical technical challenge.', 'as the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply.', 'consequently, existing methods need to make approximations by sampling or pruning.', 'the problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance  #AUTHOR_TAG.', 'moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through ( e. g., michelle in our example ) ; all represent paths as a sequence of relation types.', 'in this work, we aim to develop a kb completion model that can incorporate relation paths efficiently.', 'we start from analyzing the procedures in existing approaches, focusing on their time and space complexity.', 'based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that enables efficient modeling of']",0
['training  #TAUTHOR_TAG and using relation paths as features'],['training  #TAUTHOR_TAG and using relation paths as features'],['training  #TAUTHOR_TAG and using relation paths as features'],"['two approaches we consider here are : using relation paths to generate new auxiliary triples for training  #TAUTHOR_TAG and using relation paths as features for scoring  #AUTHOR_TAG.', 'both approaches take into account embeddings of relation paths between entities, and both of them used vector space compositions to combine the embeddings of individual relation links r i into an embedding of the path π.', 'the intermediate nodes e i are neglected.', 'the natural composition function of a bilinear model is matrix multiplication  #TAUTHOR_TAG.', 'for this model, the embedding of a length - n path φ π ∈ r d×d is defined as the matrix product of the sequence of relation matrices for the relations in π.', 'for the bilinear - diag model, all the matrices are diagonal and the computation reduces to coordinate - wise product of vectors in r d.', ' #AUTHOR_TAG, information from relation paths was used to generate additional auxiliary terms in training, which serve to provide a compositional regularizer for the learned node and relation embeddings.', 'a more limited version of the same method was simultaneously proposed in garcia -  #AUTHOR_TAG']",0
['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],"['cutoff are pruned ( c = 1 in table 1 means that all sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation. we also included the bilinear - diag baseline. implementation details we used batch training with rprop  #AUTHOR_TAG. the l 2', 'penalty λ was set to 0. 1 for all models, and the entity', 'vectors x e were normalized to unit vectors. for each positive example we sample', '500 negative examples. for our implementation of  #TAUTHOR_TAG, we run 5 random walks of each length starting from each node and we found that adding a weight β to the multi - step path triples improves the results. after preliminary', 'experimentation, we fixed β to 0. 1. models using kb and textual relations were initialized', 'from models using kb relations only 7. model training was stopped when the development set map did not improve for 40 iterations ; the parameters with the best map on the development set were selected as output. finally, we used', '']",0
"['##le ) ∧ mother ( michelle, sasha ) to train kb embeddings has been proposed very recently  #TAUTHOR_TAG']","['paths ( e. g., husband ( barack, michelle ) ∧ mother ( michelle, sasha ) to train kb embeddings has been proposed very recently  #TAUTHOR_TAG']","['g., husband ( barack, michelle ) ∧ mother ( michelle, sasha ) to train kb embeddings has been proposed very recently  #TAUTHOR_TAG.', 'while using relation paths']","['', 'early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities ( e. g., father ( barack, sasha ) )  #AUTHOR_TAG.', 'in contrast, using multi - step relation paths ( e. g., husband ( barack, michelle ) ∧ mother ( michelle, sasha ) to train kb embeddings has been proposed very recently  #TAUTHOR_TAG.', 'while using relation paths improves model performance, it also poses a critical technical challenge.', 'as the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply.', 'consequently, existing methods need to make approximations by sampling or pruning.', 'the problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance  #AUTHOR_TAG.', 'moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through ( e. g., michelle in our example ) ; all represent paths as a sequence of relation types.', 'in this work, we aim to develop a kb completion model that can incorporate relation paths efficiently.', 'we start from analyzing the procedures in existing approaches, focusing on their time and space complexity.', 'based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that enables efficient modeling of']",1
['training  #TAUTHOR_TAG and using relation paths as features'],['training  #TAUTHOR_TAG and using relation paths as features'],['training  #TAUTHOR_TAG and using relation paths as features'],"['two approaches we consider here are : using relation paths to generate new auxiliary triples for training  #TAUTHOR_TAG and using relation paths as features for scoring  #AUTHOR_TAG.', 'both approaches take into account embeddings of relation paths between entities, and both of them used vector space compositions to combine the embeddings of individual relation links r i into an embedding of the path π.', 'the intermediate nodes e i are neglected.', 'the natural composition function of a bilinear model is matrix multiplication  #TAUTHOR_TAG.', 'for this model, the embedding of a length - n path φ π ∈ r d×d is defined as the matrix product of the sequence of relation matrices for the relations in π.', 'for the bilinear - diag model, all the matrices are diagonal and the computation reduces to coordinate - wise product of vectors in r d.', ' #AUTHOR_TAG, information from relation paths was used to generate additional auxiliary terms in training, which serve to provide a compositional regularizer for the learned node and relation embeddings.', 'a more limited version of the same method was simultaneously proposed in garcia -  #AUTHOR_TAG']",5
"['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non - zero.', 'this can be done in time o t where triples is the same quantity used in the analysis of  #TAUTHOR_TAG.', 'the memory requirements of this method are the same as these of  #TAUTHOR_TAG, up to a constant to store random - walk probabilities for paths.', 'the time requirements are different, however.', 'at training time, we compute scores and update gradients for triples corresponding to direct 5 the computation uses the fact that the number of path type sequences of length l is n l r.', 'we use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l.', 'knowledge base edges, whose number is e kb.', 'for each considered triple, however, we need to compute the sum of representations of path features that are active for the triple.', 'we estimate the average number of active paths per node pair as t ne 2.', 'therefore the overall time for this method per training iteration is o 2d ( η + 1 ) e kb', 'we should note that whether this method or the one of  #TAUTHOR_TAG will be faster in training depends on whether the average number of paths per node pair multiplied by e kb is bigger or smaller than the total number of triples t.', 'unlike the method of  #TAUTHOR_TAG, the evaluation - time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation - time memory requirements of all - paths, if these are lower as determined by the specific problem instance']",5
"['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non - zero.', 'this can be done in time o t where triples is the same quantity used in the analysis of  #TAUTHOR_TAG.', 'the memory requirements of this method are the same as these of  #TAUTHOR_TAG, up to a constant to store random - walk probabilities for paths.', 'the time requirements are different, however.', 'at training time, we compute scores and update gradients for triples corresponding to direct 5 the computation uses the fact that the number of path type sequences of length l is n l r.', 'we use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l.', 'knowledge base edges, whose number is e kb.', 'for each considered triple, however, we need to compute the sum of representations of path features that are active for the triple.', 'we estimate the average number of active paths per node pair as t ne 2.', 'therefore the overall time for this method per training iteration is o 2d ( η + 1 ) e kb', 'we should note that whether this method or the one of  #TAUTHOR_TAG will be faster in training depends on whether the average number of paths per node pair multiplied by e kb is bigger or smaller than the total number of triples t.', 'unlike the method of  #TAUTHOR_TAG, the evaluation - time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation - time memory requirements of all - paths, if these are lower as determined by the specific problem instance']",5
"['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non - zero.', 'this can be done in time o t where triples is the same quantity used in the analysis of  #TAUTHOR_TAG.', 'the memory requirements of this method are the same as these of  #TAUTHOR_TAG, up to a constant to store random - walk probabilities for paths.', 'the time requirements are different, however.', 'at training time, we compute scores and update gradients for triples corresponding to direct 5 the computation uses the fact that the number of path type sequences of length l is n l r.', 'we use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l.', 'knowledge base edges, whose number is e kb.', 'for each considered triple, however, we need to compute the sum of representations of path features that are active for the triple.', 'we estimate the average number of active paths per node pair as t ne 2.', 'therefore the overall time for this method per training iteration is o 2d ( η + 1 ) e kb', 'we should note that whether this method or the one of  #TAUTHOR_TAG will be faster in training depends on whether the average number of paths per node pair multiplied by e kb is bigger or smaller than the total number of triples t.', 'unlike the method of  #TAUTHOR_TAG, the evaluation - time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation - time memory requirements of all - paths, if these are lower as determined by the specific problem instance']",5
"['and d = 50, η = 50, and maximum path length of 5, the estimated memory for  #TAUTHOR_TAG']","['and d = 50, η = 50, and maximum path length of 5, the estimated memory for  #TAUTHOR_TAG']","['memory estimates for our nci + txt knowledge base.', 'given the values of the quantities from our knowledge graph and d = 50, η = 50, and maximum path length of 5, the estimated memory for  #TAUTHOR_TAG']","['', 'based on this analysis, we computed training time and memory estimates for our nci + txt knowledge base.', 'given the values of the quantities from our knowledge graph and d = 50, η = 50, and maximum path length of 5, the estimated memory for  #TAUTHOR_TAG and pruned - paths is 4. 0 × 10 18 and for all - paths the memory is 1. 9×10 9.', 'the time estimates are 2. 4×10 21, 2. 6 × 10 25, and 7. 3 × 10 15 for  #TAUTHOR_TAG, pruned - paths, and all - paths, respectively.', 'table 1 : kb completion results on nci - pid test : comparison of our compositional learning approach ( all - paths + nodes ) with baseline systems.', 'd is the embedding dimension ; sampled paths occurring less than c times were pruned in pruned - paths']",5
"['and d = 50, η = 50, and maximum path length of 5, the estimated memory for  #TAUTHOR_TAG']","['and d = 50, η = 50, and maximum path length of 5, the estimated memory for  #TAUTHOR_TAG']","['memory estimates for our nci + txt knowledge base.', 'given the values of the quantities from our knowledge graph and d = 50, η = 50, and maximum path length of 5, the estimated memory for  #TAUTHOR_TAG']","['', 'based on this analysis, we computed training time and memory estimates for our nci + txt knowledge base.', 'given the values of the quantities from our knowledge graph and d = 50, η = 50, and maximum path length of 5, the estimated memory for  #TAUTHOR_TAG and pruned - paths is 4. 0 × 10 18 and for all - paths the memory is 1. 9×10 9.', 'the time estimates are 2. 4×10 21, 2. 6 × 10 25, and 7. 3 × 10 15 for  #TAUTHOR_TAG, pruned - paths, and all - paths, respectively.', 'table 1 : kb completion results on nci - pid test : comparison of our compositional learning approach ( all - paths + nodes ) with baseline systems.', 'd is the embedding dimension ; sampled paths occurring less than c times were pruned in pruned - paths']",5
['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],"['cutoff are pruned ( c = 1 in table 1 means that all sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation. we also included the bilinear - diag baseline. implementation details we used batch training with rprop  #AUTHOR_TAG. the l 2', 'penalty λ was set to 0. 1 for all models, and the entity', 'vectors x e were normalized to unit vectors. for each positive example we sample', '500 negative examples. for our implementation of  #TAUTHOR_TAG, we run 5 random walks of each length starting from each node and we found that adding a weight β to the multi - step path triples improves the results. after preliminary', 'experimentation, we fixed β to 0. 1. models using kb and textual relations were initialized', 'from models using kb relations only 7. model training was stopped when the development set map did not improve for 40 iterations ; the parameters with the best map on the development set were selected as output. finally, we used', '']",5
['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],"['cutoff are pruned ( c = 1 in table 1 means that all sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation. we also included the bilinear - diag baseline. implementation details we used batch training with rprop  #AUTHOR_TAG. the l 2', 'penalty λ was set to 0. 1 for all models, and the entity', 'vectors x e were normalized to unit vectors. for each positive example we sample', '500 negative examples. for our implementation of  #TAUTHOR_TAG, we run 5 random walks of each length starting from each node and we found that adding a weight β to the multi - step path triples improves the results. after preliminary', 'experimentation, we fixed β to 0. 1. models using kb and textual relations were initialized', 'from models using kb relations only 7. model training was stopped when the development set map did not improve for 40 iterations ; the parameters with the best map on the development set were selected as output. finally, we used', '']",5
['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],"['cutoff are pruned ( c = 1 in table 1 means that all sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation. we also included the bilinear - diag baseline. implementation details we used batch training with rprop  #AUTHOR_TAG. the l 2', 'penalty λ was set to 0. 1 for all models, and the entity', 'vectors x e were normalized to unit vectors. for each positive example we sample', '500 negative examples. for our implementation of  #TAUTHOR_TAG, we run 5 random walks of each length starting from each node and we found that adding a weight β to the multi - step path triples improves the results. after preliminary', 'experimentation, we fixed β to 0. 1. models using kb and textual relations were initialized', 'from models using kb relations only 7. model training was stopped when the development set map did not improve for 40 iterations ; the parameters with the best map on the development set were selected as output. finally, we used', '']",5
['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],"['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly']","['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly worse than our re - implementation.', 'also, our re - implementation achieves only a slight gain over the bilinear - diag baseline, whereas the original implementation obtains substantial improvement over its own version of bilinear - diag.', 'these results underscore the importance of hyper - parameters and optimization, and invite future systematic research on the impact of such modeling choices.', '11', 'model map hits @ 10 bilinear - diag  #TAUTHOR_TAG n / a 12. 9 bilinear - diag 8. 0 12. 2 +  #AUTHOR_TAG n / a 14. 4 pruned - paths l = 3 c = 10 9. 5 14. 8 pruned - paths l = 3 c = 1 9. 5 14. 9 pruned - paths l = 5 c = 10 8. 9 14. 4 all - paths l = 3 9. 4 14. 7 all - paths + nodes l = 3 9. 4 15. 2 all - paths l = 5 9. 6 16. 6 all - paths + nodes l = 5 9. 8 16. 7 table 2 : kb completion results on the wordnet test set : comparison of our compositional learning approach ( all - paths ) with baseline systems.', 'the maximum length of paths is denoted by l. sampled paths occurring less than c times were pruned in pruned - paths.', ' #AUTHOR_TAG and our implementation.', 'the map results were not reported in  #TAUTHOR_TAG ; hence the na value for map in row one.', '12 on this dataset, our implementation of the baseline model does not have substantially different results than  #TAUTHOR_TAG and we use their reported results for the baseline and compositionally trained model.', 'compositional training improved performance in hits @ 10 from 12. 9 to 14. 4 in  #TAUTHOR_TAG, and we find that using pruned - paths as features gives similar, but a bit higher performance gains']",5
['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],"['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly']","['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly worse than our re - implementation.', 'also, our re - implementation achieves only a slight gain over the bilinear - diag baseline, whereas the original implementation obtains substantial improvement over its own version of bilinear - diag.', 'these results underscore the importance of hyper - parameters and optimization, and invite future systematic research on the impact of such modeling choices.', '11', 'model map hits @ 10 bilinear - diag  #TAUTHOR_TAG n / a 12. 9 bilinear - diag 8. 0 12. 2 +  #AUTHOR_TAG n / a 14. 4 pruned - paths l = 3 c = 10 9. 5 14. 8 pruned - paths l = 3 c = 1 9. 5 14. 9 pruned - paths l = 5 c = 10 8. 9 14. 4 all - paths l = 3 9. 4 14. 7 all - paths + nodes l = 3 9. 4 15. 2 all - paths l = 5 9. 6 16. 6 all - paths + nodes l = 5 9. 8 16. 7 table 2 : kb completion results on the wordnet test set : comparison of our compositional learning approach ( all - paths ) with baseline systems.', 'the maximum length of paths is denoted by l. sampled paths occurring less than c times were pruned in pruned - paths.', ' #AUTHOR_TAG and our implementation.', 'the map results were not reported in  #TAUTHOR_TAG ; hence the na value for map in row one.', '12 on this dataset, our implementation of the baseline model does not have substantially different results than  #TAUTHOR_TAG and we use their reported results for the baseline and compositionally trained model.', 'compositional training improved performance in hits @ 10 from 12. 9 to 14. 4 in  #TAUTHOR_TAG, and we find that using pruned - paths as features gives similar, but a bit higher performance gains']",5
['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],"['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly']","['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly worse than our re - implementation.', 'also, our re - implementation achieves only a slight gain over the bilinear - diag baseline, whereas the original implementation obtains substantial improvement over its own version of bilinear - diag.', 'these results underscore the importance of hyper - parameters and optimization, and invite future systematic research on the impact of such modeling choices.', '11', 'model map hits @ 10 bilinear - diag  #TAUTHOR_TAG n / a 12. 9 bilinear - diag 8. 0 12. 2 +  #AUTHOR_TAG n / a 14. 4 pruned - paths l = 3 c = 10 9. 5 14. 8 pruned - paths l = 3 c = 1 9. 5 14. 9 pruned - paths l = 5 c = 10 8. 9 14. 4 all - paths l = 3 9. 4 14. 7 all - paths + nodes l = 3 9. 4 15. 2 all - paths l = 5 9. 6 16. 6 all - paths + nodes l = 5 9. 8 16. 7 table 2 : kb completion results on the wordnet test set : comparison of our compositional learning approach ( all - paths ) with baseline systems.', 'the maximum length of paths is denoted by l. sampled paths occurring less than c times were pruned in pruned - paths.', ' #AUTHOR_TAG and our implementation.', 'the map results were not reported in  #TAUTHOR_TAG ; hence the na value for map in row one.', '12 on this dataset, our implementation of the baseline model does not have substantially different results than  #TAUTHOR_TAG and we use their reported results for the baseline and compositionally trained model.', 'compositional training improved performance in hits @ 10 from 12. 9 to 14. 4 in  #TAUTHOR_TAG, and we find that using pruned - paths as features gives similar, but a bit higher performance gains']",5
['with 12 we ran the trained model distributed by  #TAUTHOR_TAG and obtained'],['with 12 we ran the trained model distributed by  #TAUTHOR_TAG and obtained'],"['performed worse than paths up to length 3.', 'this performance degradation could be avoided with 12 we ran the trained model distributed by  #TAUTHOR_TAG and obtained a much lower hits @ 10 value']","['pruned - paths method is evaluated using count cutoffs of 1 and 10, and maximum path lengths of 3 and 5.', 'as can be seen, lower count cutoff performed better for paths up to length 3, but we could not run the method with path lengths up to 5 and count cutoff of 1, due to excessive memory requirements ( more than 248gb ).', 'when using count cutoff of 10, paths up to length 5 performed worse than paths up to length 3.', 'this performance degradation could be avoided with 12 we ran the trained model distributed by  #TAUTHOR_TAG and obtained a much lower hits @ 10 value of 6. 4 and map of of 3. 5.', ""due to the discrepancy, we report the original results from the authors'paper which lack map values instead."", 'a staged training regiment where models with shorter paths are first trained and used to initialize models using longer paths.', 'the performance of the all - paths method can be seen for maximum paths up to lengths 3 and 5, and with or without using features on intermediate path nodes.', '13 as shown in table 2, longer paths were useful, and features on intermediate nodes were also beneficial.', 'we tested the significance of the differences between several pairs of models and found that nodes led to significant improvement ( p <. 002 ) for paths of length up to 3, but not for the setting with longer paths.', 'all models using path features are significantly better than the baseline bilinear - diag model.', 'to summarize both sets of experiments, the all - paths approach allows us to efficiently include information from long kb relation paths as in wordnet, or paths including both text and kb relations as in nci - pid.', 'our dynamic programming algorithm considers relation paths efficiently, and is also straightforwardly generalizable to include modeling of intermediate path nodes, which would not be directly possible for the pruned - paths approach.', 'using intermediate nodes was beneficial on both datasets, and especially when paths could include textual relations as in the nci - pid dataset']",5
"['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non - zero.', 'this can be done in time o t where triples is the same quantity used in the analysis of  #TAUTHOR_TAG.', 'the memory requirements of this method are the same as these of  #TAUTHOR_TAG, up to a constant to store random - walk probabilities for paths.', 'the time requirements are different, however.', 'at training time, we compute scores and update gradients for triples corresponding to direct 5 the computation uses the fact that the number of path type sequences of length l is n l r.', 'we use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l.', 'knowledge base edges, whose number is e kb.', 'for each considered triple, however, we need to compute the sum of representations of path features that are active for the triple.', 'we estimate the average number of active paths per node pair as t ne 2.', 'therefore the overall time for this method per training iteration is o 2d ( η + 1 ) e kb', 'we should note that whether this method or the one of  #TAUTHOR_TAG will be faster in training depends on whether the average number of paths per node pair multiplied by e kb is bigger or smaller than the total number of triples t.', 'unlike the method of  #TAUTHOR_TAG, the evaluation - time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation - time memory requirements of all - paths, if these are lower as determined by the specific problem instance']",4
"['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['of  #TAUTHOR_TAG.', 'the memory requirements of this method are the']","['the values of the random walk probabilities for all pairs of nodes and relation paths, for which these probabilities are non - zero.', 'this can be done in time o t where triples is the same quantity used in the analysis of  #TAUTHOR_TAG.', 'the memory requirements of this method are the same as these of  #TAUTHOR_TAG, up to a constant to store random - walk probabilities for paths.', 'the time requirements are different, however.', 'at training time, we compute scores and update gradients for triples corresponding to direct 5 the computation uses the fact that the number of path type sequences of length l is n l r.', 'we use a, the average branching factor of nodes given relation types, to derive the estimated number of triples of a given relation type for a path of length l.', 'knowledge base edges, whose number is e kb.', 'for each considered triple, however, we need to compute the sum of representations of path features that are active for the triple.', 'we estimate the average number of active paths per node pair as t ne 2.', 'therefore the overall time for this method per training iteration is o 2d ( η + 1 ) e kb', 'we should note that whether this method or the one of  #TAUTHOR_TAG will be faster in training depends on whether the average number of paths per node pair multiplied by e kb is bigger or smaller than the total number of triples t.', 'unlike the method of  #TAUTHOR_TAG, the evaluation - time memory requirements of this approach are the same as its training memory requirements, or they could be reduced slightly to match the evaluation - time memory requirements of all - paths, if these are lower as determined by the specific problem instance']",4
['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],"['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly']","['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly worse than our re - implementation.', 'also, our re - implementation achieves only a slight gain over the bilinear - diag baseline, whereas the original implementation obtains substantial improvement over its own version of bilinear - diag.', 'these results underscore the importance of hyper - parameters and optimization, and invite future systematic research on the impact of such modeling choices.', '11', 'model map hits @ 10 bilinear - diag  #TAUTHOR_TAG n / a 12. 9 bilinear - diag 8. 0 12. 2 +  #AUTHOR_TAG n / a 14. 4 pruned - paths l = 3 c = 10 9. 5 14. 8 pruned - paths l = 3 c = 1 9. 5 14. 9 pruned - paths l = 5 c = 10 8. 9 14. 4 all - paths l = 3 9. 4 14. 7 all - paths + nodes l = 3 9. 4 15. 2 all - paths l = 5 9. 6 16. 6 all - paths + nodes l = 5 9. 8 16. 7 table 2 : kb completion results on the wordnet test set : comparison of our compositional learning approach ( all - paths ) with baseline systems.', 'the maximum length of paths is denoted by l. sampled paths occurring less than c times were pruned in pruned - paths.', ' #AUTHOR_TAG and our implementation.', 'the map results were not reported in  #TAUTHOR_TAG ; hence the na value for map in row one.', '12 on this dataset, our implementation of the baseline model does not have substantially different results than  #TAUTHOR_TAG and we use their reported results for the baseline and compositionally trained model.', 'compositional training improved performance in hits @ 10 from 12. 9 to 14. 4 in  #TAUTHOR_TAG, and we find that using pruned - paths as features gives similar, but a bit higher performance gains']",4
['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],['sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation'],"['cutoff are pruned ( c = 1 in table 1 means that all sampled paths are used ). the most relevant prior approach is  #TAUTHOR_TAG. we ran experiments using both their publicly available code and our re - implementation. we also included the bilinear - diag baseline. implementation details we used batch training with rprop  #AUTHOR_TAG. the l 2', 'penalty λ was set to 0. 1 for all models, and the entity', 'vectors x e were normalized to unit vectors. for each positive example we sample', '500 negative examples. for our implementation of  #TAUTHOR_TAG, we run 5 random walks of each length starting from each node and we found that adding a weight β to the multi - step path triples improves the results. after preliminary', 'experimentation, we fixed β to 0. 1. models using kb and textual relations were initialized', 'from models using kb relations only 7. model training was stopped when the development set map did not improve for 40 iterations ; the parameters with the best map on the development set were selected as output. finally, we used', '']",6
['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],['implementation of  #TAUTHOR_TAG with default parameters performed significantly'],"['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly']","['among the baselines also offers valuable insights.', 'the implementation of  #TAUTHOR_TAG with default parameters performed significantly worse than our re - implementation.', 'also, our re - implementation achieves only a slight gain over the bilinear - diag baseline, whereas the original implementation obtains substantial improvement over its own version of bilinear - diag.', 'these results underscore the importance of hyper - parameters and optimization, and invite future systematic research on the impact of such modeling choices.', '11', 'model map hits @ 10 bilinear - diag  #TAUTHOR_TAG n / a 12. 9 bilinear - diag 8. 0 12. 2 +  #AUTHOR_TAG n / a 14. 4 pruned - paths l = 3 c = 10 9. 5 14. 8 pruned - paths l = 3 c = 1 9. 5 14. 9 pruned - paths l = 5 c = 10 8. 9 14. 4 all - paths l = 3 9. 4 14. 7 all - paths + nodes l = 3 9. 4 15. 2 all - paths l = 5 9. 6 16. 6 all - paths + nodes l = 5 9. 8 16. 7 table 2 : kb completion results on the wordnet test set : comparison of our compositional learning approach ( all - paths ) with baseline systems.', 'the maximum length of paths is denoted by l. sampled paths occurring less than c times were pruned in pruned - paths.', ' #AUTHOR_TAG and our implementation.', 'the map results were not reported in  #TAUTHOR_TAG ; hence the na value for map in row one.', '12 on this dataset, our implementation of the baseline model does not have substantially different results than  #TAUTHOR_TAG and we use their reported results for the baseline and compositionally trained model.', 'compositional training improved performance in hits @ 10 from 12. 9 to 14. 4 in  #TAUTHOR_TAG, and we find that using pruned - paths as features gives similar, but a bit higher performance gains']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['', '##afor, a system described by  #AUTHOR_TAG. both systems use a rule - based module to identify targets. on the framenet 1. 5 data, presented additional semi - supervised experiments using gold targets, which was recently outperformed by an approach presented by  #TAUTHOR_TAG that made use of distributed word representations.  #AUTHOR_TAG presented the system', '']",0
"['presented by  #TAUTHOR_TAG', 'that made use of distributed word representations']","['presented by  #TAUTHOR_TAG', 'that made use of distributed word representations']","['presented by  #TAUTHOR_TAG', 'that made use of distributed word representations']","['been unaddressed. there are exceptions, where the verb frame has been taken into account during srl ( meza -  #AUTHOR_TAG. moreoever, the conll 2008 and 2009 shared tasks also include the verb and noun frame identification task in their evaluations', ', although the overall goal was to predict semantic dependencies based on propbank, and not full argument spans  #AUTHOR_TAG hajic et al.,', '2009 ). the semeval 2007 shared task  #AUTHOR_TAG attempted to revisit the frame - semantic analysis task based on framenet. it introduced a larger', 'framenet lexicon ( version 1. 3 ), and also a larger corpus with full - text annotations compared', 'to prior work, with multiple targets annotated per sentence. the corpus allowed words and phrases with noun, verb, adjective, adverb, number, determiner, conjunction and preposition syntactic categories to serve as targets and evoke frames, unlike any other single dataset ; it also allowed targets from different syntactic categories share', 'frames, and therefore roles. the repository of semantic role types was also much richer than propbankstyle lexicons, numbering in several hundreds. most systems participating in the task resorted to a cascade of classifiers and rule - based modules : identifying targets ( a non - trivial', 'subtask ), disambiguating frames, identifying potential arguments, and then labeling them with roles. the system described by  #AUTHOR_TAG performed the best in this shared task. next, we focus', 'on its performance, and subsequent improvements made by the research community on this task.. the words in bold correspond', ""to targets, which evoke semantic frames that are denoted in capital letters. above each target is shown the corresponding lexical unit, which is a lemma appended by a coarse part - of - speech tag. every frame is shown in a distinct color ; each frame's arguments"", 'are annotated with the same color, and are marked below the sentence, at different levels. for the cardinal numbers frame', ', "" m "" denotes the role multiplier and "" e "" denotes the role entity. table 1 : we show the current state of the art on the frame - semantic parsing task.', 'the first section shows results on the semeval 2007 shared task. the best system in the task, presented by  #AUTHOR_TAG was later outperformed by semafor, a system described by  #AUTHOR_TAG. both systems use a rule - based module to identify targets. on the framenet 1. 5', 'data, presented additional semi - supervised experiments using gold targets, which was recently outperformed by an approach presented by  #TAUTHOR_TAG', 'that made use of distributed word representations']",0
['of  #TAUTHOR_TAG contains only'],['of  #TAUTHOR_TAG contains only'],['of  #TAUTHOR_TAG contains only'],"['the wide body of work in frame - semantic analysis of text, and recent interest in using framesemantic parsers in nlp applications, the future directions of research look exciting.', 'first and foremost, to improve the quality of automatic frame - semantic parsers, the coverage of the framenet lexicon on free english text, and the number of annotated targets needs to increase.', 'for example, the training dataset used for the state - ofthe - art system of  #TAUTHOR_TAG contains only 4, 458 labeled targets, which is approximately 40 times less than the number of annotated targets in ontonotes 4. 0  #AUTHOR_TAG, a standard nlp dataset, containing propbank - style verb annotations.', 'this comparison is important because framenet covers many more syntactic categories than the propbank - style annotations, and features more than 1, 000 semantic role labels compared to 51 in ontonotes, but severely lacks annotations.', 'a machine learned system would find it very hard to generalize to new data given such data sparsity.', 'increasing the quantity of such annotations requires exhaustive inter - annotator agreement studies ( which has been rare in framenet corpora generation ) and the development of annotation guidenominal targets in nombank  #AUTHOR_TAG has been investigated recently  #AUTHOR_TAG.', 'lines, such that these annotations can be produced outside the framenet project.', 'other than increasing the amount of labeled data, there is a necessity of automatically aligning predicate - level semantic knowledge present in resources like framenet, propbank, nombank and verbnet  #AUTHOR_TAG.', 'these lexicons share a lot of knowledge about predicates and current resources like ontonotes do align some of the information, but a lot remains missing.', 'for example, alignment between these lexicons could be done within a statistical model for frame - semantic parsing, such that correlations between the coarse semantic role labels in propbank or nombank and the finer labels in framenet could be discovered automatically.', 'finally, the framenet data is an attractive test bed for semi - supervised learning techniques because of data sparsity ; distributed word representations, which often capture more semantic information than surface - form features could be exploited in various aspects of the frame - semantic parsing task']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['', '##afor, a system described by  #AUTHOR_TAG. both systems use a rule - based module to identify targets. on the framenet 1. 5 data, presented additional semi - supervised experiments using gold targets, which was recently outperformed by an approach presented by  #TAUTHOR_TAG that made use of distributed word representations.  #AUTHOR_TAG presented the system', '']",4
"['presented by  #TAUTHOR_TAG', 'that made use of distributed word representations']","['presented by  #TAUTHOR_TAG', 'that made use of distributed word representations']","['presented by  #TAUTHOR_TAG', 'that made use of distributed word representations']","['been unaddressed. there are exceptions, where the verb frame has been taken into account during srl ( meza -  #AUTHOR_TAG. moreoever, the conll 2008 and 2009 shared tasks also include the verb and noun frame identification task in their evaluations', ', although the overall goal was to predict semantic dependencies based on propbank, and not full argument spans  #AUTHOR_TAG hajic et al.,', '2009 ). the semeval 2007 shared task  #AUTHOR_TAG attempted to revisit the frame - semantic analysis task based on framenet. it introduced a larger', 'framenet lexicon ( version 1. 3 ), and also a larger corpus with full - text annotations compared', 'to prior work, with multiple targets annotated per sentence. the corpus allowed words and phrases with noun, verb, adjective, adverb, number, determiner, conjunction and preposition syntactic categories to serve as targets and evoke frames, unlike any other single dataset ; it also allowed targets from different syntactic categories share', 'frames, and therefore roles. the repository of semantic role types was also much richer than propbankstyle lexicons, numbering in several hundreds. most systems participating in the task resorted to a cascade of classifiers and rule - based modules : identifying targets ( a non - trivial', 'subtask ), disambiguating frames, identifying potential arguments, and then labeling them with roles. the system described by  #AUTHOR_TAG performed the best in this shared task. next, we focus', 'on its performance, and subsequent improvements made by the research community on this task.. the words in bold correspond', ""to targets, which evoke semantic frames that are denoted in capital letters. above each target is shown the corresponding lexical unit, which is a lemma appended by a coarse part - of - speech tag. every frame is shown in a distinct color ; each frame's arguments"", 'are annotated with the same color, and are marked below the sentence, at different levels. for the cardinal numbers frame', ', "" m "" denotes the role multiplier and "" e "" denotes the role entity. table 1 : we show the current state of the art on the frame - semantic parsing task.', 'the first section shows results on the semeval 2007 shared task. the best system in the task, presented by  #AUTHOR_TAG was later outperformed by semafor, a system described by  #AUTHOR_TAG. both systems use a rule - based module to identify targets. on the framenet 1. 5', 'data, presented additional semi - supervised experiments using gold targets, which was recently outperformed by an approach presented by  #TAUTHOR_TAG', 'that made use of distributed word representations']",4
"['improved inference using a dual decomposition algorithm.', 'subsequently,  #TAUTHOR_TAG used a very']","['improved inference using a dual decomposition algorithm.', 'subsequently,  #TAUTHOR_TAG used a very']","['improved inference using a dual decomposition algorithm.', 'subsequently,  #TAUTHOR_TAG used a very similar framework but presented a novel method using distributed word representations']","['', ' #AUTHOR_TAG presented a tool called se - mafor, 1 which improved upon this system with a similar framework for target identification, but only used two probabilistic models, one for frame identification, and one for predicting the arguments.', 'the frame identification subpart involved a latent - variable log - linear model, which intended to capture frames for unseen targets, many of which appeared in the test data.', 'moreover, the feature sets in both the models were sufficiently different from prior work, resulting in improvements.', 'table 1 shows results on the semeval 2007 data for these two systems.', 'the framenet project released more annotations and a larger frame lexicon in 2010 ; of this updated version of semafor involved handling unseen targets using a graph - based semisupervised learning approach and improved inference using a dual decomposition algorithm.', 'subsequently,  #TAUTHOR_TAG used a very similar framework but presented a novel method using distributed word representations for better frame identification, outperforming the aforementioned update to semafor.', '']",4
['of  #TAUTHOR_TAG contains only'],['of  #TAUTHOR_TAG contains only'],['of  #TAUTHOR_TAG contains only'],"['the wide body of work in frame - semantic analysis of text, and recent interest in using framesemantic parsers in nlp applications, the future directions of research look exciting.', 'first and foremost, to improve the quality of automatic frame - semantic parsers, the coverage of the framenet lexicon on free english text, and the number of annotated targets needs to increase.', 'for example, the training dataset used for the state - ofthe - art system of  #TAUTHOR_TAG contains only 4, 458 labeled targets, which is approximately 40 times less than the number of annotated targets in ontonotes 4. 0  #AUTHOR_TAG, a standard nlp dataset, containing propbank - style verb annotations.', 'this comparison is important because framenet covers many more syntactic categories than the propbank - style annotations, and features more than 1, 000 semantic role labels compared to 51 in ontonotes, but severely lacks annotations.', 'a machine learned system would find it very hard to generalize to new data given such data sparsity.', 'increasing the quantity of such annotations requires exhaustive inter - annotator agreement studies ( which has been rare in framenet corpora generation ) and the development of annotation guidenominal targets in nombank  #AUTHOR_TAG has been investigated recently  #AUTHOR_TAG.', 'lines, such that these annotations can be produced outside the framenet project.', 'other than increasing the amount of labeled data, there is a necessity of automatically aligning predicate - level semantic knowledge present in resources like framenet, propbank, nombank and verbnet  #AUTHOR_TAG.', 'these lexicons share a lot of knowledge about predicates and current resources like ontonotes do align some of the information, but a lot remains missing.', 'for example, alignment between these lexicons could be done within a statistical model for frame - semantic parsing, such that correlations between the coarse semantic role labels in propbank or nombank and the finer labels in framenet could be discovered automatically.', 'finally, the framenet data is an attractive test bed for semi - supervised learning techniques because of data sparsity ; distributed word representations, which often capture more semantic information than surface - form features could be exploited in various aspects of the frame - semantic parsing task']",2
"['for long textual sequences  #TAUTHOR_TAG.', 'in this paper, we propose a character - level attention - enhanced long short - term memory ( lstm ) model']","['for long textual sequences  #TAUTHOR_TAG.', 'in this paper, we propose a character - level attention - enhanced long short - term memory ( lstm ) model']","['to calculate the probability of the next character based on the given rating.', 'later, dong et.', 'al.', 'presented an efficient method to generate the next word in a sequence when it is added an attention mechanism, improving the performance for long textual sequences  #TAUTHOR_TAG.', 'in this paper, we propose a character - level attention - enhanced long short - term memory ( lstm ) model']","['', 'presented encoding rating vectors of reviews in the training phase, allowing the system to calculate the probability of the next character based on the given rating.', 'later, dong et.', 'al.', 'presented an efficient method to generate the next word in a sequence when it is added an attention mechanism, improving the performance for long textual sequences  #TAUTHOR_TAG.', 'in this paper, we propose a character - level attention - enhanced long short - term memory ( lstm ) model to generate personalized natural language explanations based on user - generated reviews.', 'the model is trained using two real - world datasets : beeradvocate [ 5 ] and amazon book reviews  #TAUTHOR_TAG.', 'the datasets present user reviews describing their opinion about items in natural language.', 'the explanations are adaptively composed by']",0
"['for long textual sequences  #TAUTHOR_TAG.', 'in this paper, we propose a character - level attention - enhanced long short - term memory ( lstm ) model']","['for long textual sequences  #TAUTHOR_TAG.', 'in this paper, we propose a character - level attention - enhanced long short - term memory ( lstm ) model']","['to calculate the probability of the next character based on the given rating.', 'later, dong et.', 'al.', 'presented an efficient method to generate the next word in a sequence when it is added an attention mechanism, improving the performance for long textual sequences  #TAUTHOR_TAG.', 'in this paper, we propose a character - level attention - enhanced long short - term memory ( lstm ) model']","['', 'presented encoding rating vectors of reviews in the training phase, allowing the system to calculate the probability of the next character based on the given rating.', 'later, dong et.', 'al.', 'presented an efficient method to generate the next word in a sequence when it is added an attention mechanism, improving the performance for long textual sequences  #TAUTHOR_TAG.', 'in this paper, we propose a character - level attention - enhanced long short - term memory ( lstm ) model to generate personalized natural language explanations based on user - generated reviews.', 'the model is trained using two real - world datasets : beeradvocate [ 5 ] and amazon book reviews  #TAUTHOR_TAG.', 'the datasets present user reviews describing their opinion about items in natural language.', 'the explanations are adaptively composed by']",5
"['h attention t  #TAUTHOR_TAG.', '• generating text the explanation is']","['h attention t  #TAUTHOR_TAG.', '• generating text the explanation is']","['character dependencies h t and attention inputs a. eq. 1 formally defines the new character dependencies using attention layer h attention t  #TAUTHOR_TAG.', '• generating text the explanation is generated character by character']","['', 'rating text samples, from poorly rated ( 1 ) to highly rated ( 5 ).', 'was introduced to solve the long - term dependency problem, which causes vanishing gradient in conventional rnn [ 2 ].', '• attention mechanism the attention mechanism, adaptively learns soft alignments c t between character dependencies h t and attention inputs a. eq. 1 formally defines the new character dependencies using attention layer h attention t  #TAUTHOR_TAG.', '• generating text the explanation is generated character by character.', 'the characters are given by maximizing the softmax conditional probability p, based on the new character dependencies h attention t  #TAUTHOR_TAG, as presented in eq. 2 p = softmax ( h attention t w + b ), char = arg max p ( 2']",5
"['h attention t  #TAUTHOR_TAG.', '• generating text the explanation is']","['h attention t  #TAUTHOR_TAG.', '• generating text the explanation is']","['character dependencies h t and attention inputs a. eq. 1 formally defines the new character dependencies using attention layer h attention t  #TAUTHOR_TAG.', '• generating text the explanation is generated character by character']","['', 'rating text samples, from poorly rated ( 1 ) to highly rated ( 5 ).', 'was introduced to solve the long - term dependency problem, which causes vanishing gradient in conventional rnn [ 2 ].', '• attention mechanism the attention mechanism, adaptively learns soft alignments c t between character dependencies h t and attention inputs a. eq. 1 formally defines the new character dependencies using attention layer h attention t  #TAUTHOR_TAG.', '• generating text the explanation is generated character by character.', 'the characters are given by maximizing the softmax conditional probability p, based on the new character dependencies h attention t  #TAUTHOR_TAG, as presented in eq. 2 p = softmax ( h attention t w + b ), char = arg max p ( 2']",5
"['recent works  #TAUTHOR_TAG 6 ],']","['recent works  #TAUTHOR_TAG 6 ],']","['recent works  #TAUTHOR_TAG 6 ],']","['work provides preliminary results in automatically generating natural language explanations.', 'the model differs from recent works  #TAUTHOR_TAG 6 ], due to the use of attention layer combined with character - level lstm.', ""the proposed model improves we would like to improve the model considering : ( 1 ) personalizing explanations to benefit the users'preferences based on their expressed sentiments ; and ( 2 ) testing the model in larger and more varied review domains such as hotels and restaurants""]",4
"['by  #TAUTHOR_TAG.', 'we propose a beam search algorithm that scores the entire candidate plaintext at']","['by  #TAUTHOR_TAG.', 'we propose a beam search algorithm that scores the entire candidate plaintext at']","['by  #TAUTHOR_TAG.', 'we propose a beam search algorithm that scores the entire candidate plaintext at each step of']","['##ipherment of homophonic substitution ciphers using language models ( lms ) is a wellstudied task in nlp.', 'previous work in this topic scores short local spans of possible plaintext decipherments using n - gram lms.', 'the most widely used technique is the use of beam search with n - gram lms proposed by  #TAUTHOR_TAG.', 'we propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural lm.', 'we augment beam search with a novel rest cost estimation that exploits the prediction power of a neural lm.', 'we compare against the state of the art n - gram based methods on many different decipherment tasks.', 'on challenging ciphers such as the beale cipher we provide significantly better error rates with much smaller beam sizes']",0
"['##ment use n - gram lms  #TAUTHOR_TAG.', '']","['substitution ciphers recovers the plaintext from a ciphertext that uses a 1 : 1 or homophonic cipher key.', 'previous work using pretrained language models ( lms ) for decipherment use n - gram lms  #TAUTHOR_TAG.', '']","['##ment use n - gram lms  #TAUTHOR_TAG.', 'some methods use']","['substitution ciphers recovers the plaintext from a ciphertext that uses a 1 : 1 or homophonic cipher key.', 'previous work using pretrained language models ( lms ) for decipherment use n - gram lms  #TAUTHOR_TAG.', 'some methods use the expectationmaximization ( em ) algorithm  #AUTHOR_TAG while most state - of - the - art approaches for decipherment of 1 : 1 and homophonic substitution ciphers use beam search and rely on the clever use of n - gram lms  #AUTHOR_TAG.', 'neural lms globally score the entire candidate plaintext sequence  #AUTHOR_TAG.', 'however, using a neural lm for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging.', 'we solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre - trained neural lms for the first time']",0
"['use the notation from  #TAUTHOR_TAG.', 'ciphertext']","['use the notation from  #TAUTHOR_TAG.', 'ciphertext']","['use the notation from  #TAUTHOR_TAG.', 'ciphertext f n 1 =']","['use the notation from  #TAUTHOR_TAG.', 'ciphertext f n 1 = f 1.. f i.. f n and plaintext e n 1 = e 1.. e i.. e n consist of vocabularies f i ∈ v f and e i ∈ v e respectively.', 'the beginning tokens in the ciphertext ( f 0 ) and plaintext ( e 0 ) are set to "" $ "" denoting the beginning of a sentence.', 'the substitutions are represented by a function φ : v f → v e such that 1 : 1 substitutions are bijective while homophonic substitutions are general.', 'a cipher function φ which does not have every φ ( f ) fixed is called a partial cipher function  #AUTHOR_TAG.', 'the number of f s that are fixed in φ is given by its cardinality.', 'φ is called an extension of φ, if f is fixed in φ such that δ ( φ ( f ), φ ( f ) ) yields true ∀f ∈ v f which are already fixed in φ where δ is kronecker delta.', 'decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.', 'where p (. ) is the language model ( lm ).', 'finding this argmax is solved using a beam search algorithm  #TAUTHOR_TAG which incrementally finds the most likely substitutions using the language model scores as the ranking']",0
"[' #TAUTHOR_TAG ( nuhn et al.,, 2014 hs']","[' #TAUTHOR_TAG ( nuhn et al.,, 2014 hs.', 'add ( ( ∅, 0 ) ) 5 :', 'while', '']","['1 is the beam search algorithm  #TAUTHOR_TAG ( nuhn et al.,, 2014 hs.', 'add']","['1 is the beam search algorithm  #TAUTHOR_TAG ( nuhn et al.,, 2014 hs.', 'add ( ( ∅, 0 ) ) 5 :', 'while', '']",0
['compared to the beam search algorithm  #TAUTHOR_TAG with beam size of'],"['##diac - 408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.', 'our neural lm model with global rest cost estimation and frequency matching heuristic with a beam size of 1m has ser of 1. 2 % compared to the beam search algorithm  #TAUTHOR_TAG with beam size of 10m with a 6 - gram lm which gives an ser of 2 %.', 'the improved beam search  #AUTHOR_TAG with an 8 - gram lm, however, gets 52 out of 54 mappings correct on the zodiac - 408 cipher']",['compared to the beam search algorithm  #TAUTHOR_TAG with beam size of'],"['##diac - 408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.', 'our neural lm model with global rest cost estimation and frequency matching heuristic with a beam size of 1m has ser of 1. 2 % compared to the beam search algorithm  #TAUTHOR_TAG with beam size of 10m with a 6 - gram lm which gives an ser of 2 %.', 'the improved beam search  #AUTHOR_TAG with an 8 - gram lm, however, gets 52 out of 54 mappings correct on the zodiac - 408 cipher']",0
"['iterative beam search algorithm.', ' #AUTHOR_TAG present various improvements to the beam search algorithm in  #TAUTHOR_TAG including improved rest cost estimation and an optimized']","['iterative beam search algorithm.', ' #AUTHOR_TAG present various improvements to the beam search algorithm in  #TAUTHOR_TAG including improved rest cost estimation and an optimized']","['employing a higher order language model and an iterative beam search algorithm.', ' #AUTHOR_TAG present various improvements to the beam search algorithm in  #TAUTHOR_TAG including improved rest cost estimation and an optimized strategy']","['decipherment for substitution ciphers started with dictionary attacks  #AUTHOR_TAG.', ' #AUTHOR_TAG frame the decipherment problem as an integer linear programming ( ilp ) problem.', ' #AUTHOR_TAG use an hmm - based em algorithm for solving a variety of decipherment problems.', ' #AUTHOR_TAG extend the hmm - based em approach with a bayesian approach, and report the first automatic decipherment of the zodiac - 408 cipher.', 'berg -  #AUTHOR_TAG show that a large number of random restarts can help the em approach.', ' #AUTHOR_TAG presented an efficient a * search algorithm to solve letter substitution ciphers.', ' #AUTHOR_TAG produce better results in faster time compared to ilp and em - based decipherment methods by employing a higher order language model and an iterative beam search algorithm.', ' #AUTHOR_TAG present various improvements to the beam search algorithm in  #TAUTHOR_TAG including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols.', ' #AUTHOR_TAG propose a novel approach for solving mono - alphabetic substitution ciphers which combines character - level and word - level language model.', 'they formulate decipherment as a tree search problem, and use monte carlo tree search ( mcts ) as an alternative to beam search.', 'their approach is the best for short ciphers.', ' #AUTHOR_TAG frames the decryption process as a sequence - to - sequence translation task and uses a deep lstm - based model to learn the decryption algorithms for three polyalphabetic ciphers including the enigma cipher.', 'however, this approach needs supervision compared to our approach which uses a pre - trained neural lm.', ' #AUTHOR_TAG ( ciphergan ) use a generative adversarial network to learn the mapping between the learned letter embedding distributions in the ciphertext and plaintext.', 'they apply this approach to shift ciphers ( including vigenere ciphers ).', 'their approach cannot be extended to homophonic ciphers and full message neural lms as in our work']",0
"['use the notation from  #TAUTHOR_TAG.', 'ciphertext']","['use the notation from  #TAUTHOR_TAG.', 'ciphertext']","['use the notation from  #TAUTHOR_TAG.', 'ciphertext f n 1 =']","['use the notation from  #TAUTHOR_TAG.', 'ciphertext f n 1 = f 1.. f i.. f n and plaintext e n 1 = e 1.. e i.. e n consist of vocabularies f i ∈ v f and e i ∈ v e respectively.', 'the beginning tokens in the ciphertext ( f 0 ) and plaintext ( e 0 ) are set to "" $ "" denoting the beginning of a sentence.', 'the substitutions are represented by a function φ : v f → v e such that 1 : 1 substitutions are bijective while homophonic substitutions are general.', 'a cipher function φ which does not have every φ ( f ) fixed is called a partial cipher function  #AUTHOR_TAG.', 'the number of f s that are fixed in φ is given by its cardinality.', 'φ is called an extension of φ, if f is fixed in φ such that δ ( φ ( f ), φ ( f ) ) yields true ∀f ∈ v f which are already fixed in φ where δ is kronecker delta.', 'decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.', 'where p (. ) is the language model ( lm ).', 'finding this argmax is solved using a beam search algorithm  #TAUTHOR_TAG which incrementally finds the most likely substitutions using the language model scores as the ranking']",5
"[': 1 letter substitution cipher dataset following  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'the text is from english wikipedia articles about history 3, preproc']","['this experiment we use a synthetic 1 : 1 letter substitution cipher dataset following  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'the text is from english wikipedia articles about history 3, preprocessed by stripping the text of all images, tables, then lower - casing all characters,']","['this experiment we use a synthetic 1 : 1 letter substitution cipher dataset following  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'the text is from english wikipedia articles about history 3, preproc']","['this experiment we use a synthetic 1 : 1 letter substitution cipher dataset following  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'the text is from english wikipedia articles about history 3, preprocessed by stripping the text of all images, tables, then lower - casing all characters, and removing all non - alphabetic and non - space characters.', 'we create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random caesar - cipher 1 : 1 substitution']",5
"[': 1 letter substitution cipher dataset following  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'the text is from english wikipedia articles about history 3, preproc']","['this experiment we use a synthetic 1 : 1 letter substitution cipher dataset following  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'the text is from english wikipedia articles about history 3, preprocessed by stripping the text of all images, tables, then lower - casing all characters,']","['this experiment we use a synthetic 1 : 1 letter substitution cipher dataset following  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'the text is from english wikipedia articles about history 3, preproc']","['this experiment we use a synthetic 1 : 1 letter substitution cipher dataset following  #AUTHOR_TAG,  #TAUTHOR_TAG and  #AUTHOR_TAG.', 'the text is from english wikipedia articles about history 3, preprocessed by stripping the text of all images, tables, then lower - casing all characters, and removing all non - alphabetic and non - space characters.', 'we create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random caesar - cipher 1 : 1 substitution']",3
['##ment from  #TAUTHOR_TAG ; and extend'],"['of large pre - trained neural lms to the decipherment problem.', 'we modify the beam search algorithm for decipherment from  #TAUTHOR_TAG ; and extend']",['##ment from  #TAUTHOR_TAG ; and extend'],"['paper presents, to our knowledge, the first application of large pre - trained neural lms to the decipherment problem.', 'we modify the beam search algorithm for decipherment from  #TAUTHOR_TAG ; and extend it to use global scoring of the plaintext message using neural lms.', 'to enable full plaintext scoring we use the neural lm to sample plaintext characters which reduces the beam size required.', 'for challenging ciphers such as beale pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers']",4
['##ment from  #TAUTHOR_TAG ; and extend'],"['of large pre - trained neural lms to the decipherment problem.', 'we modify the beam search algorithm for decipherment from  #TAUTHOR_TAG ; and extend']",['##ment from  #TAUTHOR_TAG ; and extend'],"['paper presents, to our knowledge, the first application of large pre - trained neural lms to the decipherment problem.', 'we modify the beam search algorithm for decipherment from  #TAUTHOR_TAG ; and extend it to use global scoring of the plaintext message using neural lms.', 'to enable full plaintext scoring we use the neural lm to sample plaintext characters which reduces the beam size required.', 'for challenging ciphers such as beale pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers']",6
['english and chinese were recently introduced  #TAUTHOR_TAG'],['english and chinese were recently introduced  #TAUTHOR_TAG'],"['driven models, large - scale datasets for both english and chinese were recently introduced  #TAUTHOR_TAG.', 'in']","['math word problems has been an interest of the natural language processing community since the 1960s  #AUTHOR_TAG.', 'more recently, algorithms for learning to solve algebra problems have gone in complementary directions : semantic and purely data - driven.', 'semantic methods learn from data how to map problem texts to a semantic representation which can then be converted to an equation.', 'these representations combine setlike constructs  #AUTHOR_TAG with hierarchical representations like equation trees ( koncel -  #AUTHOR_TAG.', 'such methods have the benefit of being interpretable, but no semantic representation general enough to solve all varieties of math word problems, including proportion problems and those that map to systems of equations, has been found.', 'another popular line of research is on purely data - driven solvers.', 'given enough training data, data - driven models can learn to map word problem texts to arbitrarily complex equations or systems of equations.', 'these models have the additional advantage of being more language - independent than semantic methods, which often rely on parsers and other nlp tools.', 'to train these fully data driven models, large - scale datasets for both english and chinese were recently introduced  #TAUTHOR_TAG.', 'in response to the success of representation learning elsewhere in nlp, sequence to sequence ( seq2seq ) models have been applied to algebra problem solving  #TAUTHOR_TAG.', '']",0
['english and chinese were recently introduced  #TAUTHOR_TAG'],['english and chinese were recently introduced  #TAUTHOR_TAG'],"['driven models, large - scale datasets for both english and chinese were recently introduced  #TAUTHOR_TAG.', 'in']","['math word problems has been an interest of the natural language processing community since the 1960s  #AUTHOR_TAG.', 'more recently, algorithms for learning to solve algebra problems have gone in complementary directions : semantic and purely data - driven.', 'semantic methods learn from data how to map problem texts to a semantic representation which can then be converted to an equation.', 'these representations combine setlike constructs  #AUTHOR_TAG with hierarchical representations like equation trees ( koncel -  #AUTHOR_TAG.', 'such methods have the benefit of being interpretable, but no semantic representation general enough to solve all varieties of math word problems, including proportion problems and those that map to systems of equations, has been found.', 'another popular line of research is on purely data - driven solvers.', 'given enough training data, data - driven models can learn to map word problem texts to arbitrarily complex equations or systems of equations.', 'these models have the additional advantage of being more language - independent than semantic methods, which often rely on parsers and other nlp tools.', 'to train these fully data driven models, large - scale datasets for both english and chinese were recently introduced  #TAUTHOR_TAG.', 'in response to the success of representation learning elsewhere in nlp, sequence to sequence ( seq2seq ) models have been applied to algebra problem solving  #TAUTHOR_TAG.', '']",0
"['for math23k, described in  #TAUTHOR_TAG, uses a hybrid jaccard retrieval and seq2seq model']","['for math23k, described in  #TAUTHOR_TAG, uses a hybrid jaccard retrieval and seq2seq model.', '']","['for the draw dataset is described in  #AUTHOR_TAG.', 'the state of the art for math23k, described in  #TAUTHOR_TAG, uses a hybrid jaccard retrieval and seq2seq model.', 'all models shown here fall well short of the highest possible classification / retrieval accuracy, shown in']","['', 'neither of these methods help over the english language data.', 'it appears that the elmo technique may require more training examples before it can improve solution accuracy.', 'the previous state of the art model for the draw dataset is described in  #AUTHOR_TAG.', 'the state of the art for math23k, described in  #TAUTHOR_TAG, uses a hybrid jaccard retrieval and seq2seq model.', 'all models shown here fall well short of the highest possible classification / retrieval accuracy, shown in table 2 as "" oracle "".', 'this gap invites a more detailed error analysis regarding the possible limitations of data - driven solvers']",0
['english and chinese were recently introduced  #TAUTHOR_TAG'],['english and chinese were recently introduced  #TAUTHOR_TAG'],"['driven models, large - scale datasets for both english and chinese were recently introduced  #TAUTHOR_TAG.', 'in']","['math word problems has been an interest of the natural language processing community since the 1960s  #AUTHOR_TAG.', 'more recently, algorithms for learning to solve algebra problems have gone in complementary directions : semantic and purely data - driven.', 'semantic methods learn from data how to map problem texts to a semantic representation which can then be converted to an equation.', 'these representations combine setlike constructs  #AUTHOR_TAG with hierarchical representations like equation trees ( koncel -  #AUTHOR_TAG.', 'such methods have the benefit of being interpretable, but no semantic representation general enough to solve all varieties of math word problems, including proportion problems and those that map to systems of equations, has been found.', 'another popular line of research is on purely data - driven solvers.', 'given enough training data, data - driven models can learn to map word problem texts to arbitrarily complex equations or systems of equations.', 'these models have the additional advantage of being more language - independent than semantic methods, which often rely on parsers and other nlp tools.', 'to train these fully data driven models, large - scale datasets for both english and chinese were recently introduced  #TAUTHOR_TAG.', 'in response to the success of representation learning elsewhere in nlp, sequence to sequence ( seq2seq ) models have been applied to algebra problem solving  #TAUTHOR_TAG.', '']",5
"[""' s equation template is then filled in with numbers from the test problem and solved."", 'following  #TAUTHOR_TAG, we use jaccard']","[""neighbor's equation template is then filled in with numbers from the test problem and solved."", 'following  #TAUTHOR_TAG, we use jaccard']","['methods map test word problem texts at inference time to the nearest training problem according to some similarity metric.', ""the nearest neighbor's equation template is then filled in with numbers from the test problem and solved."", 'following  #TAUTHOR_TAG, we use jaccard distance in this model.', 'for test problem s and training problem t, the jaccard similarity']","['methods map test word problem texts at inference time to the nearest training problem according to some similarity metric.', ""the nearest neighbor's equation template is then filled in with numbers from the test problem and solved."", 'following  #TAUTHOR_TAG, we use jaccard distance in this model.', 'for test problem s and training problem t, the jaccard similarity is computed as : jacc ( s, t ) = s∩t s∪t.', 'we also evaluate the use of a cosine similarity metric.', 'words from s and t are associated with pretrained vectors v ( w i )  #AUTHOR_TAG.', 'these vectors are averaged across each problem, resulting in vectors s and t. the cosine similarity is then computed as cos ( s, t ) = s · t | | st | |.', 'vector averaging has previously been used as a strong baseline for a variety of sentence similarity tasks  #AUTHOR_TAG']",5
"['', 'following  #TAUTHOR_TAG we evaluate a seq2']","['equation template on encodings of the word problem text.', 'following  #TAUTHOR_TAG we evaluate a seq2seq with lstms as the encoder and decoder.', 'we also evaluate']","['', 'we generate equation templates with seq2seq models  #AUTHOR_TAG with attention mechanisms  #AUTHOR_TAG.', 'these models condition the token - by - token generation of the equation template on encodings of the word problem text.', 'following  #TAUTHOR_TAG we evaluate a seq2seq with lstms as the encoder and decoder.', 'we also evaluate the use of convolutional neural networks ( cnns ) in the encoder and decoder']","['methods treat equation templates as strings of formal symbols.', 'the production of a template is considered a sequence prediction problem conditioned on the word problem text.', 'by treating templates as sequences rather than monolithic structures, generation methods have the potential to learn finer - grained relationships between the input text and output template.', 'they also are the only methods studied here which can induce templates during inference which were not seen at training.', 'we generate equation templates with seq2seq models  #AUTHOR_TAG with attention mechanisms  #AUTHOR_TAG.', 'these models condition the token - by - token generation of the equation template on encodings of the word problem text.', 'following  #TAUTHOR_TAG we evaluate a seq2seq with lstms as the encoder and decoder.', 'we also evaluate the use of convolutional neural networks ( cnns ) in the encoder and decoder']",5
"['dataset  #TAUTHOR_TAG, and']","['chinese language math23k dataset  #TAUTHOR_TAG, and']","['for comparison, we report solution accuracy on the chinese language math23k dataset  #TAUTHOR_TAG, and the english language draw  #AUTHOR_TAG']","['for comparison, we report solution accuracy on the chinese language math23k dataset  #TAUTHOR_TAG, and the english language draw  #AUTHOR_TAG and mawps ( koncel -  #AUTHOR_TAG datasets.', 'math23k and mawps consist of single equation problems, and draw contains both single and simultaneous equation problems.', 'details on the datasets are shown in table 1.', 'the math23k dataset contains problems with possibly irrelevant quantities.', 'to prune these quantities, we implement a significant number identifier ( sni ) as discussed in  #TAUTHOR_TAG.', 'our best accuracy for sni is 97 %, slightly weaker than previous results.', 'each dataset.', ""we also explore two modifications of the bilstm's embedding matrix w e, either by using pretrained glove embeddings  #AUTHOR_TAG or using the elmo technique of  #AUTHOR_TAG as implemented in the allennlp toolkit ( gardner et al. ) with pretrained character embeddings."", 'for seq2seq modeling, we use opennmt  #AUTHOR_TAG with 500 dimensional hidden states and embeddings and a dropout rate of 0. 3.', 'the cnn uses a kernel width of 3.', 'optimization is done using sgd with a learning rate of 1, decayed by half if the validation perplexity does not decrease after an epoch.', 'table 2 reports the accuracies of the data - driven models for solving algebra word problems.', 'the classification models perform better than retrieval or generation models, despite their limited modeling power.', 'the self - attention classification model performs well across all datasets.', 'for the largest dataset ( math23k ), a simple, well - tuned classifier can outperform the more sophisticated sequenceto - sequence and self - attention models.', 'table 3 shows results of augmenting the clas']",5
"['dataset  #TAUTHOR_TAG, and']","['chinese language math23k dataset  #TAUTHOR_TAG, and']","['for comparison, we report solution accuracy on the chinese language math23k dataset  #TAUTHOR_TAG, and the english language draw  #AUTHOR_TAG']","['for comparison, we report solution accuracy on the chinese language math23k dataset  #TAUTHOR_TAG, and the english language draw  #AUTHOR_TAG and mawps ( koncel -  #AUTHOR_TAG datasets.', 'math23k and mawps consist of single equation problems, and draw contains both single and simultaneous equation problems.', 'details on the datasets are shown in table 1.', 'the math23k dataset contains problems with possibly irrelevant quantities.', 'to prune these quantities, we implement a significant number identifier ( sni ) as discussed in  #TAUTHOR_TAG.', 'our best accuracy for sni is 97 %, slightly weaker than previous results.', 'each dataset.', ""we also explore two modifications of the bilstm's embedding matrix w e, either by using pretrained glove embeddings  #AUTHOR_TAG or using the elmo technique of  #AUTHOR_TAG as implemented in the allennlp toolkit ( gardner et al. ) with pretrained character embeddings."", 'for seq2seq modeling, we use opennmt  #AUTHOR_TAG with 500 dimensional hidden states and embeddings and a dropout rate of 0. 3.', 'the cnn uses a kernel width of 3.', 'optimization is done using sgd with a learning rate of 1, decayed by half if the validation perplexity does not decrease after an epoch.', 'table 2 reports the accuracies of the data - driven models for solving algebra word problems.', 'the classification models perform better than retrieval or generation models, despite their limited modeling power.', 'the self - attention classification model performs well across all datasets.', 'for the largest dataset ( math23k ), a simple, well - tuned classifier can outperform the more sophisticated sequenceto - sequence and self - attention models.', 'table 3 shows results of augmenting the clas']",5
"['the search space significantly.', 'more recently,  #TAUTHOR_TAG provide a large']","['the search space significantly.', 'more recently,  #TAUTHOR_TAG provide a large']","['subsequently align numbers and unknowns from the text.', ' #AUTHOR_TAG only assign numbers to the predicted template, reducing the search space significantly.', 'more recently,  #TAUTHOR_TAG provide a large dataset of chinese algebra word problems and learn a hybrid model consisting of both retrieval and seq2seq components.', 'the current work extends these approaches by exploring advanced techniques in data - driven solving']","['solvers provide some scaffolding for the grounding of word problem texts to equations.', ' #AUTHOR_TAG solve simple word problems by categorizing their operations as partwhole, change, or comparison.', ' #AUTHOR_TAG learn a semantic parser by semi - automatically inducing 9600 grammar rules over a dataset of number word problems.', 'works such has  #AUTHOR_TAG and koncel -  #AUTHOR_TAG treat arithmetic word problem templates as equation trees and perform efficient tree - search by learning how to combine quantities using textual information.', ' #AUTHOR_TAG advance this approach by considering unit consistency in the tree - search procedure.', ' #AUTHOR_TAG advance this line of work even further by modeling the search using deep q - learning.', 'still, these semantic approaches are limited by their inability to model systems of equations as well as use of hand - engineered features.', 'data - driven math word problem solvers include, who learn to predict equation templates and subsequently align numbers and unknowns from the text.', ' #AUTHOR_TAG only assign numbers to the predicted template, reducing the search space significantly.', 'more recently,  #TAUTHOR_TAG provide a large dataset of chinese algebra word problems and learn a hybrid model consisting of both retrieval and seq2seq components.', 'the current work extends these approaches by exploring advanced techniques in data - driven solving']",6
"['not.', ' #TAUTHOR_TAG performed empirical studies to validate both types of properties using english source texts and other texts translated into english.', 'obviously, corpora of this sort, which focus on a single language, are not adequate']","['not.', ' #TAUTHOR_TAG performed empirical studies to validate both types of properties using english source texts and other texts translated into english.', 'obviously, corpora of this sort, which focus on a single language, are not adequate']","['not.', ' #TAUTHOR_TAG performed empirical studies to validate both types of properties using english source texts and other texts translated into english.', 'obviously, corpora of this sort, which focus on a single language, are not adequate']","[', translation scholars have made some general claims about translation properties.', 'some of these are source language independent while others are not.', ' #TAUTHOR_TAG performed empirical studies to validate both types of properties using english source texts and other texts translated into english.', 'obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties.', 'in this paper, we are validating both types of translation properties using original and translated texts from six european languages']",1
"['of spanish.', ' #TAUTHOR_TAG.', '']","['of spanish.', ' #TAUTHOR_TAG.', '']","['have been translated to other languages from english ( for instance, word order )  #AUTHOR_TAG.', ' #AUTHOR_TAG and  #AUTHOR_TAG ; have provided empirical evidence of simplification translation properties using a comparable corpus of spanish.', ' #TAUTHOR_TAG.', 'they used a comparable corpus of original english and english translated']","['', 'they focus on the general effects of the translation process.', ' #AUTHOR_TAG has a different theory from these.', 'he stated that some interference effects will be observable in the translated text.', 'that is, a translated text will carry some fingerprints of its source language.', 'specific properties of the english language are visible in user manuals that have been translated to other languages from english ( for instance, word order )  #AUTHOR_TAG.', ' #AUTHOR_TAG and  #AUTHOR_TAG ; have provided empirical evidence of simplification translation properties using a comparable corpus of spanish.', ' #TAUTHOR_TAG.', 'they used a comparable corpus of original english and english translated from five other european languages.', 'in addition, original english and english translated from greek and korean was also used in their experiment.', 'they have found that a translated text contains both source language dependent and independent features.', 'obviously, corpora of this sort, which focus on a single language ( e. g., english ), are not adequate for claiming the universal validity of translation properties.', 'different languages ( and language families ) have different linguistic properties.', 'a corpus that contains original and translated texts from different source languages will be ideal for this kind of study.', 'in this paper, we are validating both types of translation properties using original and translated texts from six european languages.', 'as features, we used frequencies of the 100 most frequent words of each target language.', 'the paper is organized as follows : section 2 discusses related work']",1
"['. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a']","['by  #AUTHOR_TAG.', 'in total, 21 quantitative features ( e. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a']","['. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a classifier that can identify']","['- based translation studies is a recent field of research with a growing interest within the field of computational linguistics.', ' #AUTHOR_TAG started corpus - based translation studies empirically, where they work on a corpus of geo - political journal articles.', 'a support vector machine ( svm ) was used to distinguish original and translated italian text using n - gram based features.', 'according to their results, word bigrams play an important role in the classification task.', ' #AUTHOR_TAG uses the europarl corpus for the first time to identify the source language of text for which the source language marker was missing.', 'support vector regression was the best performing method.', ' #AUTHOR_TAG and  #AUTHOR_TAG ; perform classification of spanish original and translated text.', 'the focus of their works is to investigate the simplification relation that was proposed by  #AUTHOR_TAG.', 'in total, 21 quantitative features ( e. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a classifier that can identify the correct source of the translated text ( given different possible source languages ).', 'they have built another classifier which can identify source text and translated text.', 'furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages.', 'they have gained impressive results for both of the tasks.', 'however, the limitation of this study is that they only used a corpus of english original text and english text translated from various european languages.', 'a list of 300 function words  #AUTHOR_TAG was used as feature vector for these classifications.', ' #AUTHOR_TAG uses string kernels  #AUTHOR_TAG to study translation properties.', 'a classifier was built to classify english original texts and english translated texts from french and german books that were written in the nineteenth century.', 'the p - spectrum normalized kernel was used for the experiment.', 'the system works on a character level rather than on a word level.', 'the system performs poorly when the source language of the training corpus is different from the one of the test corpus.', 'we can not compare our findings directly with  #TAUTHOR_TAG even though we use text from the same corpus and similar techniques.', 'the english language is not considered for this study due to unavailability of english translations for some languages included in this work.', 'furthermore, instead of the list of 300 function words used by  #TAUTHOR_TAG, we used the 100 most frequent words for each candidate language']",1
"['. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a']","['by  #AUTHOR_TAG.', 'in total, 21 quantitative features ( e. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a']","['. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a classifier that can identify']","['- based translation studies is a recent field of research with a growing interest within the field of computational linguistics.', ' #AUTHOR_TAG started corpus - based translation studies empirically, where they work on a corpus of geo - political journal articles.', 'a support vector machine ( svm ) was used to distinguish original and translated italian text using n - gram based features.', 'according to their results, word bigrams play an important role in the classification task.', ' #AUTHOR_TAG uses the europarl corpus for the first time to identify the source language of text for which the source language marker was missing.', 'support vector regression was the best performing method.', ' #AUTHOR_TAG and  #AUTHOR_TAG ; perform classification of spanish original and translated text.', 'the focus of their works is to investigate the simplification relation that was proposed by  #AUTHOR_TAG.', 'in total, 21 quantitative features ( e. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a classifier that can identify the correct source of the translated text ( given different possible source languages ).', 'they have built another classifier which can identify source text and translated text.', 'furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages.', 'they have gained impressive results for both of the tasks.', 'however, the limitation of this study is that they only used a corpus of english original text and english text translated from various european languages.', 'a list of 300 function words  #AUTHOR_TAG was used as feature vector for these classifications.', ' #AUTHOR_TAG uses string kernels  #AUTHOR_TAG to study translation properties.', 'a classifier was built to classify english original texts and english translated texts from french and german books that were written in the nineteenth century.', 'the p - spectrum normalized kernel was used for the experiment.', 'the system works on a character level rather than on a word level.', 'the system performs poorly when the source language of the training corpus is different from the one of the test corpus.', 'we can not compare our findings directly with  #TAUTHOR_TAG even though we use text from the same corpus and similar techniques.', 'the english language is not considered for this study due to unavailability of english translations for some languages included in this work.', 'furthermore, instead of the list of 300 function words used by  #TAUTHOR_TAG, we used the 100 most frequent words for each candidate language']",3
"[' #TAUTHOR_TAG.', 'our target is to']","[' #TAUTHOR_TAG.', 'our target is to']","[' #TAUTHOR_TAG.', 'our target is to']","['field of translation studies lacks a multilingual corpus that can be used to validate translation properties proposed by translation scholars.', 'there are many multilingual corpora available used for different nlp applications.', 'a customized version of the europarl corpus  #AUTHOR_TAG is freely available for corpus - based translation studies.', 'however, this corpus is not suitable for the experiment we are performing here.', 'we extract a suitable corpus from the europarl corpus in a way similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'our target is to extract texts that are translated from and to the languages considered here.', 'we trust the source language marker that has been put by the respective translator, as did  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'to experiment with stylistic differences in translated text, a list of function words and']",3
"['source language.', 'the experimental result of  #TAUTHOR_TAG shows that text translated into english holds this property.', 'if']","['source language.', 'the experimental result of  #TAUTHOR_TAG shows that text translated into english holds this property.', 'if']","['this experiment, our goal is to validate the translation properties postulated by  #AUTHOR_TAG.', 'he stated that a translated text inherits some fingerprints from the source language.', 'the experimental result of  #TAUTHOR_TAG shows that text translated into english holds this property.', 'if this characteristic']","['this experiment, our goal is to validate the translation properties postulated by  #AUTHOR_TAG.', 'he stated that a translated text inherits some fingerprints from the source language.', 'the experimental result of  #TAUTHOR_TAG shows that text translated into english holds this property.', 'if this characteristic also holds for text translated into other languages, then it will corroborate the claim by  #AUTHOR_TAG.', 'if it does not hold for a single language then it might be claimed that this translation property is not universal.', 'in order to train a classifier, we use texts translated into the same language from different source languages.', 'table 1 shows the statistics of the corpus used for source language identification experiments.', 'later, each corpus is divided into a number of chunks ( see table 2 ).', 'each chunk contains at least seven sentences.', ""our hypothesis is again similar to  #TAUTHOR_TAG, that is, if the classifier's accuracy is close to 20 %, then we cannot say that there is an interference effect in translated text."", ""if the classifier's accuracy is close to 100 % then our conclusion will be that interference effects exist in translated text."", 'table 3 and table 4 show the evaluation results.', 'table 4 : source language identification evaluation ( accuracy )', 'ancestor.', 'in the vast majority of cases, members of the same language family share a considerable number of words and grammatical structures.', '']",3
"['language is sufficient to obtain interpretable results.', 'we find our results to be compatible with  #TAUTHOR_TAG who used 300 function words.', 'a list of the 100 most frequent words is easily obtain']","['language is sufficient to obtain interpretable results.', 'we find our results to be compatible with  #TAUTHOR_TAG who used 300 function words.', 'a list of the 100 most frequent words is easily obtainable for a vast']","['results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.', 'we find our results to be compatible with  #TAUTHOR_TAG who used 300 function words.', 'a list of the 100 most frequent words is easily obtain']","['results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.', 'we find our results to be compatible with  #TAUTHOR_TAG who used 300 function words.', 'a list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.', 'while the 100 most frequent words of a language are sufficient to train a classifier for germanic or romance languages, it fails to perform equally well for slavic languages.', ' #TAUTHOR_TAG claim that  #AUTHOR_TAG findings of interference of a translation hold true ; we find the assumption to be too simplistic, since for slavic text either as a source or target language this statement cannot supported.', 'although function words do exist in all the languages we examined, the language families differ in the degree to which it is necessary to use them.', 'for instance, french lacks a case system  #AUTHOR_TAG, and makes instead use of prepositions.', 'on the other hand, polish and czech most extensively use ( inflectional ) affixes  #AUTHOR_TAG.', '']",3
"['. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a']","['by  #AUTHOR_TAG.', 'in total, 21 quantitative features ( e. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a']","['. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a classifier that can identify']","['- based translation studies is a recent field of research with a growing interest within the field of computational linguistics.', ' #AUTHOR_TAG started corpus - based translation studies empirically, where they work on a corpus of geo - political journal articles.', 'a support vector machine ( svm ) was used to distinguish original and translated italian text using n - gram based features.', 'according to their results, word bigrams play an important role in the classification task.', ' #AUTHOR_TAG uses the europarl corpus for the first time to identify the source language of text for which the source language marker was missing.', 'support vector regression was the best performing method.', ' #AUTHOR_TAG and  #AUTHOR_TAG ; perform classification of spanish original and translated text.', 'the focus of their works is to investigate the simplification relation that was proposed by  #AUTHOR_TAG.', 'in total, 21 quantitative features ( e. g. a number of different pos, average sentence length, the parse - tree depth etc. ) were used where, nine ( 9 ) of them are able to grasp the simplification translation property.', ' #TAUTHOR_TAG have built a classifier that can identify the correct source of the translated text ( given different possible source languages ).', 'they have built another classifier which can identify source text and translated text.', 'furthermore, they have shown that the degree of difference between two translated texts, translated from two different languages into the same target language reflects, the degree of difference of the source languages.', 'they have gained impressive results for both of the tasks.', 'however, the limitation of this study is that they only used a corpus of english original text and english text translated from various european languages.', 'a list of 300 function words  #AUTHOR_TAG was used as feature vector for these classifications.', ' #AUTHOR_TAG uses string kernels  #AUTHOR_TAG to study translation properties.', 'a classifier was built to classify english original texts and english translated texts from french and german books that were written in the nineteenth century.', 'the p - spectrum normalized kernel was used for the experiment.', 'the system works on a character level rather than on a word level.', 'the system performs poorly when the source language of the training corpus is different from the one of the test corpus.', 'we can not compare our findings directly with  #TAUTHOR_TAG even though we use text from the same corpus and similar techniques.', 'the english language is not considered for this study due to unavailability of english translations for some languages included in this work.', 'furthermore, instead of the list of 300 function words used by  #TAUTHOR_TAG, we used the 100 most frequent words for each candidate language']",4
"['these translation properties  #TAUTHOR_TAG.', '']","['these translation properties  #TAUTHOR_TAG.', '']","['these translation properties  #TAUTHOR_TAG.', '']","['texts have distinctive features that make them different from original or non translated text.', 'according to  #AUTHOR_TAG ; 1996 ),  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG there are some general properties of translations that are responsible for the difference between these two text types.', 'some of these properties are source and target language independent.', 'according to their findings, a translated text will be similar to another translated text but will be different from a source text.', 'in the past, researchers have used comparable corpora to validate these translation properties  #TAUTHOR_TAG.', 'most of them used comparable corpora for two - class classification, distinguishing translated texts from the original texts.', 'only  #TAUTHOR_TAG used english texts translated from multiple source languages.', 'we perform similar experiments only for six european languages as shown in table 1.', 'in this experiment, the translated text in our training and test set will be a combination of all languages other than the target language.', 'for example : when the original class contains original texts ( source ) in german, then the translation class contains texts that are translated german texts, translated from french, dutch, spanish, polish, and czech texts.', 'each class contains 200 chunks of texts, where as the translated class has 40 chunks from each of the source languages.', 'the source language texts are extracted for the corresponding languages in a similar way from the europarl corpus.', ' #TAUTHOR_TAG received the highest accuracy ( 96. 7 % ) among all works noted above.', 'the training and test data are generated in similar ways as in our previous experiment.', 'that is, 80 % of the data is randomly extracted for training and the rest of the data is used for testing.', 'expected f - scores are calculated from 100 samples.', 'table 5 shows the evaluation results.', 'even though the classifier for german achieves around 99 % accuracy, we cannot compare the result with  #TAUTHOR_TAG as the amount of chunks for the classes are different.', 'the classifiers for other languages also display very high accuracy.', 'the result of table 5 shows that general translation properties exist for all languages used in this experiment']",4
"[' #TAUTHOR_TAG.', 'our target is to']","[' #TAUTHOR_TAG.', 'our target is to']","[' #TAUTHOR_TAG.', 'our target is to']","['field of translation studies lacks a multilingual corpus that can be used to validate translation properties proposed by translation scholars.', 'there are many multilingual corpora available used for different nlp applications.', 'a customized version of the europarl corpus  #AUTHOR_TAG is freely available for corpus - based translation studies.', 'however, this corpus is not suitable for the experiment we are performing here.', 'we extract a suitable corpus from the europarl corpus in a way similar to  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'our target is to extract texts that are translated from and to the languages considered here.', 'we trust the source language marker that has been put by the respective translator, as did  #AUTHOR_TAG and  #TAUTHOR_TAG.', 'to experiment with stylistic differences in translated text, a list of function words and']",5
"['language is sufficient to obtain interpretable results.', 'we find our results to be compatible with  #TAUTHOR_TAG who used 300 function words.', 'a list of the 100 most frequent words is easily obtain']","['language is sufficient to obtain interpretable results.', 'we find our results to be compatible with  #TAUTHOR_TAG who used 300 function words.', 'a list of the 100 most frequent words is easily obtainable for a vast']","['results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.', 'we find our results to be compatible with  #TAUTHOR_TAG who used 300 function words.', 'a list of the 100 most frequent words is easily obtain']","['results show that training a classifier based on the 100 most frequent words of a language is sufficient to obtain interpretable results.', 'we find our results to be compatible with  #TAUTHOR_TAG who used 300 function words.', 'a list of the 100 most frequent words is easily obtainable for a vast number of languages, while lists consisting strictly of function words are rare and cannot be produced without considerable additional effort.', 'while the 100 most frequent words of a language are sufficient to train a classifier for germanic or romance languages, it fails to perform equally well for slavic languages.', ' #TAUTHOR_TAG claim that  #AUTHOR_TAG findings of interference of a translation hold true ; we find the assumption to be too simplistic, since for slavic text either as a source or target language this statement cannot supported.', 'although function words do exist in all the languages we examined, the language families differ in the degree to which it is necessary to use them.', 'for instance, french lacks a case system  #AUTHOR_TAG, and makes instead use of prepositions.', 'on the other hand, polish and czech most extensively use ( inflectional ) affixes  #AUTHOR_TAG.', '']",7
"[')  #TAUTHOR_TAG.', 'if you have two']","['dimensions )  #TAUTHOR_TAG.', 'if you have two three - layered, recurrent']",[' #TAUTHOR_TAG'],"['- task learning ( mtl ) in deep neural networks is typically a result of parameter sharing between two networks ( of usually the same dimensions )  #TAUTHOR_TAG.', 'if you have two three - layered, recurrent neural networks, both with an embedding inner layer and each recurrent layer feeding the task - specific classifier function through a feed - forward neural network, we have 19 pairs of layers that could share parameters.', 'with the option of having private spaces, this gives us 5 19 = 19, 073, 486, 328, 125 possible mtl architectures.', 'if we additionally consider soft sharing of parameters, the number of possible architectures grows infinite.', 'it is obviously not feasible to search this space.', 'neural architecture search ( nas ) ( zoph and le 2017 ) typically requires learning from a large pool of experiments with different architectures.', 'searching for multi - task architectures via reinforcement learning ( wong and gesmundo 2018 ) or evolutionary approaches ( liang, meyerson, and miikkulainen 2018 ) can therefore be quite expensive.', 'in this paper, we jointly learn a latent multi - task architecture and task - specific models, paying a minimal computational cost over single task learning and standard multi - task learning ( 5 - 7 % training time ).', '']",0
[';  #TAUTHOR_TAG should allow'],[';  #TAUTHOR_TAG should allow'],"['2×d. subspaces ( virtanen, klami, and kaski 2011 ;  #TAUTHOR_TAG should allow the model to focus on task - specific and shared features in different parts of']","['', '∈ r 2×d. subspaces ( virtanen, klami, and kaski 2011 ;  #TAUTHOR_TAG should allow the model to focus on task - specific and shared features in different parts of its parameter space. extending the α', '- layers to include subspaces, for 2 tasks and 2 subspaces, we obtain', 'an α matrix ∈ r 4×4 that not only controls the interaction between the layers of both tasks, but also between their subspaces : where h a1, k is the output of the first subspace of the k - th layer of task a and h a1, k is the linear combination', '']",0
"[';  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model']","[';  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model also consistently outperforms single - task learning']","['plank', '2017 ;  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model']","['', '2017 ;  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model also consistently outperforms single - task learning. overall, this demonstrates that our meta - architecture for learning which parts of multi - task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods.', '']",0
"['ablations.', 'task properties and performance  #TAUTHOR_TAG correlate meta - characteristics of task pairs and gains compared']","['ablations.', 'task properties and performance  #TAUTHOR_TAG correlate meta - characteristics of task pairs and gains compared']","['ablations.', 'task properties and performance  #TAUTHOR_TAG correlate meta - characteristics of task pairs and gains compared']","['better understand the properties and behavior of our metaarchitecture, we conduct a series of analyses and ablations.', 'task properties and performance  #TAUTHOR_TAG correlate meta - characteristics of task pairs and gains compared to hard parameter sharing across a large set of nlp task pairs.', 'similarly, we correlate various metacharacteristics with error reductions and α, β values.', '']",0
"['parameter sharing  #TAUTHOR_TAG is easy to implement, reduces overfitting, but is only guaranteed to']","['parameter sharing  #TAUTHOR_TAG is easy to implement, reduces overfitting, but is only guaranteed to']","['parameter sharing  #TAUTHOR_TAG is easy to implement, reduces overfitting, but is only guaranteed to']","['parameter sharing  #TAUTHOR_TAG is easy to implement, reduces overfitting, but is only guaranteed to work for ( certain types of ) closely related tasks ( baxter 2000 ; maurer 2007 ).', ' #AUTHOR_TAG apply a variation of hard parameter sharing to multi - domain multi - task sequence tagging with a shared crf layer and domain - specific projection layers.', ' #AUTHOR_TAG use hard parameter sharing to jointly learn different sequence - tagging tasks across languages.', 'martinez  #AUTHOR_TAG explore a similar set - up, but sharing is limited to the initial layer.', 'in all three papers, the amount of sharing between the networks is fixed in advance.', 'in soft parameter sharing ( duong et al. 2015 ), each task has separate parameters and separate hidden layers, as in our architecture, but the loss at the outer layer is regularized by the current distance between the models.', 'kumar and daume iii ( 2012 ) and  #AUTHOR_TAG enable selective sharing by allowing task predictors to select from sparse parameter bases for homogeneous tasks.', 'show that low - level tasks, i. e. syntactic tasks typically used for preprocessing such as pos tagging and ner, should be supervised at lower layers when used as auxiliary tasks.', 'another line of work looks into separating the learned space into a private ( i. e. task - specific ) and shared space ( salzmann et al. 2010 ; virtanen, klami, and kaski 2011 ) to more explicitly capture the difference between task - specific and cross - task features.', 'constraints are enforced to prevent the models from duplicating information.', ' #AUTHOR_TAG use shared and private encoders regularized with orthogonality and similarity constraints for domain adaptation for computer vision.', ' #AUTHOR_TAG use a similar technique for sentiment analysis.', 'in contrast, we do not limit ourselves to a predefined way of sharing, but let the model learn which parts of the network to share using latent variables, the weights of which are learned in an end - to - end fashion.', ' #AUTHOR_TAG, focusing on applications in computer vision, consider a small subset of the sharing architectures that are learnable in sluice networks, i. e., split architectures, in which two n - layer networks share the innermost k layers with 0 ≤ k ≤ n, but learn k with a mechanism very similar to α - values.', 'our method is also related to the classic mixture - of - experts layer ( jacobs et al. 1991 ).', 'in contrast to this approach, our method is designed for multi - task learning and thus encourages a ) the sharing of parameters between different task "" experts "" if this is beneficial as well as b )']",0
"['', 'we explicitly add inductive bias to the model via the regularizer ω below, but our model also implicitly learns regularization through multi - task learning  #TAUTHOR_TAG mediated by the α parameters, while the β parameters are used to learn the mixture functions']","['importance of the different tasks during training.', 'we explicitly add inductive bias to the model via the regularizer ω below, but our model also implicitly learns regularization through multi - task learning  #TAUTHOR_TAG mediated by the α parameters, while the β parameters are used to learn the mixture functions']","['', 'we explicitly add inductive bias to the model via the regularizer ω below, but our model also implicitly learns regularization through multi - task learning  #TAUTHOR_TAG mediated by the α parameters, while the β parameters are used to learn the mixture functions']","['', 'for simplicity, we assume that all the deep networks have the same hyperparameters at the outset.', 'let w ∈ r m ×d be a matrix in which each row i corresponds to a model θ i with d parameters.', 'the loss that sluice networks minimize, with a penalty term ω, is then as follows :', 'the loss functions l i are crossentropy functions of the form − y p ( y ) log q ( y ) where y i are the labels of task i. note that sluice networks are not restricted to tasks with the same loss functions, but could also be applied to jointly learn regression and classification tasks.', 'the weights λ i determine the importance of the different tasks during training.', 'we explicitly add inductive bias to the model via the regularizer ω below, but our model also implicitly learns regularization through multi - task learning  #TAUTHOR_TAG mediated by the α parameters, while the β parameters are used to learn the mixture functions f ( · ), as detailed in the following']",5
[';  #TAUTHOR_TAG should allow'],[';  #TAUTHOR_TAG should allow'],"['2×d. subspaces ( virtanen, klami, and kaski 2011 ;  #TAUTHOR_TAG should allow the model to focus on task - specific and shared features in different parts of']","['', '∈ r 2×d. subspaces ( virtanen, klami, and kaski 2011 ;  #TAUTHOR_TAG should allow the model to focus on task - specific and shared features in different parts of its parameter space. extending the α', '- layers to include subspaces, for 2 tasks and 2 subspaces, we obtain', 'an α matrix ∈ r 4×4 that not only controls the interaction between the layers of both tasks, but also between their subspaces : where h a1, k is the output of the first subspace of the k - th layer of task a and h a1, k is the linear combination', '']",5
"[';  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model']","[';  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model also consistently outperforms single - task learning']","['plank', '2017 ;  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model']","['', '2017 ;  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model also consistently outperforms single - task learning. overall, this demonstrates that our meta - architecture for learning which parts of multi - task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods.', '']",5
[';  #TAUTHOR_TAG should allow'],[';  #TAUTHOR_TAG should allow'],"['2×d. subspaces ( virtanen, klami, and kaski 2011 ;  #TAUTHOR_TAG should allow the model to focus on task - specific and shared features in different parts of']","['', '∈ r 2×d. subspaces ( virtanen, klami, and kaski 2011 ;  #TAUTHOR_TAG should allow the model to focus on task - specific and shared features in different parts of its parameter space. extending the α', '- layers to include subspaces, for 2 tasks and 2 subspaces, we obtain', 'an α matrix ∈ r 4×4 that not only controls the interaction between the layers of both tasks, but also between their subspaces : where h a1, k is the output of the first subspace of the k - th layer of task a and h a1, k is the linear combination', '']",3
"['ablations.', 'task properties and performance  #TAUTHOR_TAG correlate meta - characteristics of task pairs and gains compared']","['ablations.', 'task properties and performance  #TAUTHOR_TAG correlate meta - characteristics of task pairs and gains compared']","['ablations.', 'task properties and performance  #TAUTHOR_TAG correlate meta - characteristics of task pairs and gains compared']","['better understand the properties and behavior of our metaarchitecture, we conduct a series of analyses and ablations.', 'task properties and performance  #TAUTHOR_TAG correlate meta - characteristics of task pairs and gains compared to hard parameter sharing across a large set of nlp task pairs.', 'similarly, we correlate various metacharacteristics with error reductions and α, β values.', '']",3
"[';  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model']","[';  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model also consistently outperforms single - task learning']","['plank', '2017 ;  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model']","['', '2017 ;  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model also consistently outperforms single - task learning. overall, this demonstrates that our meta - architecture for learning which parts of multi - task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods.', '']",4
"[';  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model']","[';  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model also consistently outperforms single - task learning']","['plank', '2017 ;  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model']","['', '2017 ;  #TAUTHOR_TAG ; augenstein, ruder, and søgaard 2018 ), our model also consistently outperforms single - task learning. overall, this demonstrates that our meta - architecture for learning which parts of multi - task models to share, with a small set of additional parameters to learn, can achieve significant and consistent improvements over strong baseline methods.', '']",4
"['', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstract']","['on extractive techniques.', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries']","['on extractive techniques.', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries are preferred to extractive']","['', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries are preferred to extractive ones by human judges.', 'the possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries.', '']",1
"['', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstract']","['on extractive techniques.', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries']","['on extractive techniques.', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries are preferred to extractive']","['', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries are preferred to extractive ones by human judges.', 'the possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries.', '']",0
"['', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstract']","['on extractive techniques.', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries']","['on extractive techniques.', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries are preferred to extractive']","['', 'however, as pointed out in  #AUTHOR_TAG and  #TAUTHOR_TAG, abstractive summaries are preferred to extractive ones by human judges.', 'the possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differences between spoken conversations and humanauthored summaries.', '']",5
"['of  #TAUTHOR_TAG and, starting from human - author']","['of  #TAUTHOR_TAG and, starting from human - authored summaries,']","['of  #TAUTHOR_TAG and, starting from human - authored summaries,']","['generation follows the approach of  #TAUTHOR_TAG and, starting from human - authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps.', 'the information required for the template generation are part - of - speech ( pos ) tags, noun and verb phrase chunks, and root verbs from dependency parsing.', 'for english, we use illinois chunker  #AUTHOR_TAG to identify noun phrases and extract part - of - speech tags ; and the the tool of  #AUTHOR_TAG for generating dependency parses.', 'for italian, on the other hand, we use textpro 2. 0  #AUTHOR_TAG to perform all the natural language processing tasks.', 'in the slot labeling step, noun phrases from human - authored summaries are replaced by wordnet  #AUTHOR_TAG synset ids of the head nouns ( right most for english ).', 'for a word, synset id of the most frequent sense is selected with respect to the pos - tag.', 'to get hypernyms for italian we use multiwordnet  #AUTHOR_TAG.', 'the clustering of the abstract templates generated in the previous step is performed using the wordnet hierarchy of the root verb of a sentence.', 'the similarity between verbs is computed with respect to the shortest path that connects the senses in the hypernym taxonomy of wordnet.', 'the template graphs, created using this similarity, are then clustered using the normalized cuts method  #AUTHOR_TAG.', 'the clustered templates are further generalized using a word graph algorithm extended to templates in  #TAUTHOR_TAG.', 'the paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster']",5
"['( pm ) ).', 'following  #TAUTHOR_TAG, we removed 20 dialogs used']","['( pm ) ).', 'following  #TAUTHOR_TAG, we removed 20 dialogs used']","['. g. project manager ( pm ) ).', 'following  #TAUTHOR_TAG, we removed 20 dialogs used']","['two corpora used for the evaluation of the heuristics are ami and luna.', ""the ami meeting corpus  #AUTHOR_TAG is a collection of 139 meeting records where groups of people are engaged in a'roleplay'as a team and each speaker assumes a certain role in a team ( e. g. project manager ( pm ) )."", 'following  #TAUTHOR_TAG, we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross - validation.', 'the luna human - human corpus  #AUTHOR_TAG consists of 572 call - center dialogs where a client and an agent are engaged in a problem solving task over the phone.', 'the 200 italian luna dialogs have been annotated with summaries by 5 native speakers ( 5 summaries per dialog ).', 'for the call centre conversation summarization ( cccs ) shared task a set of 100 dialogs was manually translated to english.', 'the conversations are equally split into training and testing sets as 100 / 100 for italian, and 50 / 50 for english']",5
"['- measure between a set of reference and hypothesis summaries.', 'for ami corpus, following  #TAUTHOR_TAG, we report ro']","['f - measure between a set of reference and hypothesis summaries.', 'for ami corpus, following  #TAUTHOR_TAG, we report rouge - 2 f - measures on 3 - fold cross - validation.', 'for luna corpus,']","['- measure between a set of reference and hypothesis summaries.', 'for ami corpus, following  #TAUTHOR_TAG, we report ro']","['##uge - 2 metric  #AUTHOR_TAG is used for the evaluation.', 'the metric considers bigram - level precision, recall and f - measure between a set of reference and hypothesis summaries.', 'for ami corpus, following  #TAUTHOR_TAG, we report rouge - 2 f - measures on 3 - fold cross - validation.', 'for luna corpus, on the other hand, we have used the modified version of rouge 1. 5. 5 toolkit from the cccs shared task, which was adapted to deal with a conversation - dependent length limit of 7 %.', 'unlike the ami corpus, the official reported results for the cccs shared task were recall ; thus, for luna corpus the reported values are rouge - 2 recall.', 'for statistical significance testing, we use a paired bootstrap resampling method proposed in  #AUTHOR_TAG.', 'we create new virtual test sets of 15 conversations with random re - sampling 100 times.', 'for each set, we compute the rouge - 2 score and compare the system performances using paired t - test with p = 0. 05']",5
"['systems reported in  #TAUTHOR_TAG.', 'the']","['abstractive systems reported in  #TAUTHOR_TAG.', 'the']","['to the abstractive systems reported in  #TAUTHOR_TAG.', 'the performances of the heuristics on ami corpus are given in table 1.', 'in the table we also']","['this section we report on the results of the abstractive summarization system using the community creation heuristics described in section 2.', 'following the call - center conversation summarization shared task at multiling 2015, for luna corpus  #AUTHOR_TAG we compare performances to three extractive baselines : ( 1 ) the longest turn in the conversation up to the length limit ( 7 % of a conversation ) ( baseline - l ), ( 2 ) the longest turn in the first 25 % of the conversation up to the length limit ( baseline - lb )  #AUTHOR_TAG, and ( 3 ) maximal marginal relevance ( mmr )  #AUTHOR_TAG with λ = 0. 7.', 'for ami corpus, on the other hand, we compare performances to the abstractive systems reported in  #TAUTHOR_TAG.', 'the performances of the heuristics on ami corpus are given in table 1.', 'in the table we also report the performances of the previously published summarization systems that make use of the manual communities -  #TAUTHOR_TAG.', '']",5
"['systems reported in  #TAUTHOR_TAG.', 'the']","['abstractive systems reported in  #TAUTHOR_TAG.', 'the']","['to the abstractive systems reported in  #TAUTHOR_TAG.', 'the performances of the heuristics on ami corpus are given in table 1.', 'in the table we also']","['this section we report on the results of the abstractive summarization system using the community creation heuristics described in section 2.', 'following the call - center conversation summarization shared task at multiling 2015, for luna corpus  #AUTHOR_TAG we compare performances to three extractive baselines : ( 1 ) the longest turn in the conversation up to the length limit ( 7 % of a conversation ) ( baseline - l ), ( 2 ) the longest turn in the first 25 % of the conversation up to the length limit ( baseline - lb )  #AUTHOR_TAG, and ( 3 ) maximal marginal relevance ( mmr )  #AUTHOR_TAG with λ = 0. 7.', 'for ami corpus, on the other hand, we compare performances to the abstractive systems reported in  #TAUTHOR_TAG.', 'the performances of the heuristics on ami corpus are given in table 1.', 'in the table we also report the performances of the previously published summarization systems that make use of the manual communities -  #TAUTHOR_TAG.', '']",5
"['of  #TAUTHOR_TAG and, starting from human - author']","['of  #TAUTHOR_TAG and, starting from human - authored summaries,']","['of  #TAUTHOR_TAG and, starting from human - authored summaries,']","['generation follows the approach of  #TAUTHOR_TAG and, starting from human - authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps.', 'the information required for the template generation are part - of - speech ( pos ) tags, noun and verb phrase chunks, and root verbs from dependency parsing.', 'for english, we use illinois chunker  #AUTHOR_TAG to identify noun phrases and extract part - of - speech tags ; and the the tool of  #AUTHOR_TAG for generating dependency parses.', 'for italian, on the other hand, we use textpro 2. 0  #AUTHOR_TAG to perform all the natural language processing tasks.', 'in the slot labeling step, noun phrases from human - authored summaries are replaced by wordnet  #AUTHOR_TAG synset ids of the head nouns ( right most for english ).', 'for a word, synset id of the most frequent sense is selected with respect to the pos - tag.', 'to get hypernyms for italian we use multiwordnet  #AUTHOR_TAG.', 'the clustering of the abstract templates generated in the previous step is performed using the wordnet hierarchy of the root verb of a sentence.', 'the similarity between verbs is computed with respect to the shortest path that connects the senses in the hypernym taxonomy of wordnet.', 'the template graphs, created using this similarity, are then clustered using the normalized cuts method  #AUTHOR_TAG.', 'the clustered templates are further generalized using a word graph algorithm extended to templates in  #TAUTHOR_TAG.', 'the paths in the word graph are ranked using language models trained on the abstract templates and the top 10 are selected as a template for the cluster']",6
"['ranking using the token and part - of - speech tag 3 - gram language models.', 'in this paper, different from  #TAUTHOR_TAG, the sentence ranking is based solely on the n - gram language models trained on the tokens and part - ofspeech tags from the human - authored summaries']","['ranking using the token and part - of - speech tag 3 - gram language models.', 'in this paper, different from  #TAUTHOR_TAG, the sentence ranking is based solely on the n - gram language models trained on the tokens and part - ofspeech tags from the human - authored summaries']","['to the ranking using the token and part - of - speech tag 3 - gram language models.', 'in this paper, different from  #TAUTHOR_TAG, the sentence ranking is based solely on the n - gram language models trained on the tokens and part - ofspeech tags from the human - authored summaries']","['the system produces many sentences that might repeat the same information, the final set of automatic sentences is selected from these filled templates with respect to the ranking using the token and part - of - speech tag 3 - gram language models.', 'in this paper, different from  #TAUTHOR_TAG, the sentence ranking is based solely on the n - gram language models trained on the tokens and part - ofspeech tags from the human - authored summaries']",4
[' #TAUTHOR_TAG which makes efficient use of global statistics of text words'],['embeddings is glove  #TAUTHOR_TAG which makes efficient use of global statistics of text words'],['generating word embeddings is glove  #TAUTHOR_TAG which makes efficient use of global statistics of text words'],"['vector space models of language were developed in the 90s to predict joint probabilities of words that appear together in a sequence.', 'a particular upturn was proposed by bengio et al. in [ 1 ], replacing sparse n - gram models with word embeddings which are more compact representations obtained using feed - forward or more advanced neural networks.', 'recently, high quality and easy to train skip - gram shallow architectures were presented in [ 10 ] and considerably improved in [ 11 ] with the introduction of negative sampling and subsampling of frequent words.', 'the "" magical "" ability of word embeddings to capture syntactic and semantic regularities on text words is applicable in various applications like machine translations, error correcting systems, sentiment analyzers etc.', 'this ability has been tested in [ 12 ] and other studies with analogy question tests of the form "" a is to b as c is to "" or male / female relations.', 'a recent improved method for generating word embeddings is glove  #TAUTHOR_TAG which makes efficient use of global statistics of text words and preserves the linear substructure of skip - gram word2vec, the other popular method.', 'authors report that glove outperforms other methods such as skip - gram in several tasks like word similarity, word analogy etc.', 'in this paper we examine the quality of word embeddings on 2 sentiment analysis tasks : lyrics mood recognition and movie review polarity analysis.', 'we compare various models pretrained with glove and skip - gram, together with corpora we train ourself.', 'our goal is to report the best performing models as well as to observe the impact that certain factors like training method, corpus size and thematic relevance of texts might have on model quality.', 'according to the results, common crawl, twitter tweets and google news are the best performing models.', 'corpus size and thematic relevance have a significant role on the performance of the generated word vectors.', 'we noticed that models trained with glove slightly outperform those trained with skip - gram in most of experiments']",0
['of  #TAUTHOR_TAG to evaluate glove performance'],"['of  #TAUTHOR_TAG to evaluate glove performance.', 'wikipedia dependency corpus is a collection of 1']","['of  #TAUTHOR_TAG to evaluate glove performance.', 'wikipedia dependency corpus is a collection of 1 billion tokens from wikipedia.', 'the method used']","['this section we present the different word embedding models that we compare.', 'most of them are pretrained and publicly available.', 'two of them ( text8corpus and moody - corpus ) were trained by us.', 'the full list with some basic characteristics is presented in table 1.', 'wikipedia gigaword is a combination of wikipedia 2014 dump and gigaword 5 with about 6 billion tokens in total.', 'it was created by authors of  #TAUTHOR_TAG to evaluate glove performance.', 'wikipedia dependency corpus is a collection of 1 billion tokens from wikipedia.', 'the method used for training it is a modified version of skip - gram word2vec described in [ 7 ].', 'google news is one of the biggest and richest text sets with 100 billion tokens and a vocabulary of 3 million words and phrases [ 10 ].', 'it was trained using skipgram word2vec with negative sampling, windows size 5 and 300 dimensions.', 'even bigger is common crawl 840, a huge corpus of 840 billion tokens and 2. 2 million word vectors also used at  #TAUTHOR_TAG.', '']",0
['domain  #TAUTHOR_TAG is'],['domain  #TAUTHOR_TAG is'],"[' #TAUTHOR_TAG is displayed in table 1.', 'nevertheless, we argue that this domain - mismatch problem can be alleviated by using english text in the target - domain as a pivot language  #AUTHOR_TAG.', 'to this end, we explore this idea on the task of neural data - to - text generation which has been']","['', 'an example of the english / pidgin text in the restaurant domain  #TAUTHOR_TAG is displayed in table 1.', 'nevertheless, we argue that this domain - mismatch problem can be alleviated by using english text in the target - domain as a pivot language  #AUTHOR_TAG.', 'to this end, we explore this idea on the task of neural data - to - text generation which has been the subject of much recent research.', 'neural data - to - pidgin generation is essential in the african continent especially given the fact that many english there is a pub blue spice located in the centre of the city that provides chinese food.', 'pidgin', 'wan pub blue spice dey for centre of city wey dey give chinese food.', 'existing data - to - text systems are english - based e. g. weather reporting systems  #AUTHOR_TAG.', 'this work aims at bridging the gap between many of these english - based systems and pidgin by training an in - domain english - to - pidgin mt system in an unsupervised way.', 'by this means, english - based nlg systems can be locally adapted by translating the output english text into pidgin english.', 'we employ the publicly available parallel data - to - text corpus e2e  #TAUTHOR_TAG consisting of tabulated data and english descriptions in the restaur']",5
['domain  #TAUTHOR_TAG is'],['domain  #TAUTHOR_TAG is'],"[' #TAUTHOR_TAG is displayed in table 1.', 'nevertheless, we argue that this domain - mismatch problem can be alleviated by using english text in the target - domain as a pivot language  #AUTHOR_TAG.', 'to this end, we explore this idea on the task of neural data - to - text generation which has been']","['', 'an example of the english / pidgin text in the restaurant domain  #TAUTHOR_TAG is displayed in table 1.', 'nevertheless, we argue that this domain - mismatch problem can be alleviated by using english text in the target - domain as a pivot language  #AUTHOR_TAG.', 'to this end, we explore this idea on the task of neural data - to - text generation which has been the subject of much recent research.', 'neural data - to - pidgin generation is essential in the african continent especially given the fact that many english there is a pub blue spice located in the centre of the city that provides chinese food.', 'pidgin', 'wan pub blue spice dey for centre of city wey dey give chinese food.', 'existing data - to - text systems are english - based e. g. weather reporting systems  #AUTHOR_TAG.', 'this work aims at bridging the gap between many of these english - based systems and pidgin by training an in - domain english - to - pidgin mt system in an unsupervised way.', 'by this means, english - based nlg systems can be locally adapted by translating the output english text into pidgin english.', 'we employ the publicly available parallel data - to - text corpus e2e  #TAUTHOR_TAG consisting of tabulated data and english descriptions in the restaur']",5
['conduct experiments on the e2e corpus  #TAUTHOR_TAG which amounts to roughly 42'],"['conduct experiments on the e2e corpus  #TAUTHOR_TAG which amounts to roughly 42k samples in the training set.', 'the monolingual pidgin corpus contains']","['conduct experiments on the e2e corpus  #TAUTHOR_TAG which amounts to roughly 42k samples in the training set.', 'the monolingual pidgin corpus contains']","['conduct experiments on the e2e corpus  #TAUTHOR_TAG which amounts to roughly 42k samples in the training set.', 'the monolingual pidgin corpus contains 56, 695 sentences and 32, 925 unique words.', 'the human evaluation was performed on the test set ( 630 data instances for e2e ) by averaging over scores by 2 native pidgin speakers on both relevance ( 0 or 1 to indicate relevant or not ) and fluency ( 0, 1, or 2 to indicate readability ).', 'table 2 shows that model self outperforms direct translation ( pidginunmt ) and unsupervisedly - trained model model unsup on relevance and performing on par with pidginunmt on fluency.', 'we also display relevant sample outputs in table 3 at all levels of fluency.', 'pidgin text fluency every money of money on food and at least 1 of 1 points.', '0 and na one na di best food for di world.', '1 people dey feel the good food but all of us no dey available.', '']",5
"['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the']","['', 'at different levels of granularity. the same labels were used in the study of  #AUTHOR_TAG.', ' #AUTHOR_TAG han et al. (, 2014, who released twitter', '- world, use the information provided by the geoname dataset 1 in order to identify a set of cities around the', 'world with at least 100k inhabitants. then they refer their geo - tagged texts to those cities, creating', 'easily interpretable geographic places.  #AUTHOR_TAG proposed a voting - based grid selection scheme, with the classification referred to regions / states', 'in us. most works use deep learning techniques for classification  #AUTHOR_TAG. often, they include multi - view models, considering different sources  #AUTHOR_TAG a ). in particular,  #AUTHOR_TAG implemented a multi - channel', 'convolutional network, structurally similar to our model.  #AUTHOR_TAG proposes a graph - convolutional neural', 'network, though the text features are represented by a bag - of - words, while we rely on word embeddings. the ability of the labels to reflect real anthropological areas, however,', 'affects primarily the models which rely on linguistic data. this is the case of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the so - called location - indicative words ( liw ). recently, neural models have been built with the same purpose  #AUTHOR_TAG']",0
"['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the']","['', 'at different levels of granularity. the same labels were used in the study of  #AUTHOR_TAG.', ' #AUTHOR_TAG han et al. (, 2014, who released twitter', '- world, use the information provided by the geoname dataset 1 in order to identify a set of cities around the', 'world with at least 100k inhabitants. then they refer their geo - tagged texts to those cities, creating', 'easily interpretable geographic places.  #AUTHOR_TAG proposed a voting - based grid selection scheme, with the classification referred to regions / states', 'in us. most works use deep learning techniques for classification  #AUTHOR_TAG. often, they include multi - view models, considering different sources  #AUTHOR_TAG a ). in particular,  #AUTHOR_TAG implemented a multi - channel', 'convolutional network, structurally similar to our model.  #AUTHOR_TAG proposes a graph - convolutional neural', 'network, though the text features are represented by a bag - of - words, while we rely on word embeddings. the ability of the labels to reflect real anthropological areas, however,', 'affects primarily the models which rely on linguistic data. this is the case of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the so - called location - indicative words ( liw ). recently, neural models have been built with the same purpose  #AUTHOR_TAG']",0
"['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the']","['', 'at different levels of granularity. the same labels were used in the study of  #AUTHOR_TAG.', ' #AUTHOR_TAG han et al. (, 2014, who released twitter', '- world, use the information provided by the geoname dataset 1 in order to identify a set of cities around the', 'world with at least 100k inhabitants. then they refer their geo - tagged texts to those cities, creating', 'easily interpretable geographic places.  #AUTHOR_TAG proposed a voting - based grid selection scheme, with the classification referred to regions / states', 'in us. most works use deep learning techniques for classification  #AUTHOR_TAG. often, they include multi - view models, considering different sources  #AUTHOR_TAG a ). in particular,  #AUTHOR_TAG implemented a multi - channel', 'convolutional network, structurally similar to our model.  #AUTHOR_TAG proposes a graph - convolutional neural', 'network, though the text features are represented by a bag - of - words, while we rely on word embeddings. the ability of the labels to reflect real anthropological areas, however,', 'affects primarily the models which rely on linguistic data. this is the case of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the so - called location - indicative words ( liw ). recently, neural models have been built with the same purpose  #AUTHOR_TAG']",1
"['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the']","['', 'at different levels of granularity. the same labels were used in the study of  #AUTHOR_TAG.', ' #AUTHOR_TAG han et al. (, 2014, who released twitter', '- world, use the information provided by the geoname dataset 1 in order to identify a set of cities around the', 'world with at least 100k inhabitants. then they refer their geo - tagged texts to those cities, creating', 'easily interpretable geographic places.  #AUTHOR_TAG proposed a voting - based grid selection scheme, with the classification referred to regions / states', 'in us. most works use deep learning techniques for classification  #AUTHOR_TAG. often, they include multi - view models, considering different sources  #AUTHOR_TAG a ). in particular,  #AUTHOR_TAG implemented a multi - channel', 'convolutional network, structurally similar to our model.  #AUTHOR_TAG proposes a graph - convolutional neural', 'network, though the text features are represented by a bag - of - words, while we rely on word embeddings. the ability of the labels to reflect real anthropological areas, however,', 'affects primarily the models which rely on linguistic data. this is the case of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the so - called location - indicative words ( liw ). recently, neural models have been built with the same purpose  #AUTHOR_TAG']",1
"['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based']","['of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the']","['', 'at different levels of granularity. the same labels were used in the study of  #AUTHOR_TAG.', ' #AUTHOR_TAG han et al. (, 2014, who released twitter', '- world, use the information provided by the geoname dataset 1 in order to identify a set of cities around the', 'world with at least 100k inhabitants. then they refer their geo - tagged texts to those cities, creating', 'easily interpretable geographic places.  #AUTHOR_TAG proposed a voting - based grid selection scheme, with the classification referred to regions / states', 'in us. most works use deep learning techniques for classification  #AUTHOR_TAG. often, they include multi - view models, considering different sources  #AUTHOR_TAG a ). in particular,  #AUTHOR_TAG implemented a multi - channel', 'convolutional network, structurally similar to our model.  #AUTHOR_TAG proposes a graph - convolutional neural', 'network, though the text features are represented by a bag - of - words, while we rely on word embeddings. the ability of the labels to reflect real anthropological areas, however,', 'affects primarily the models which rely on linguistic data. this is the case of', 'the studies of  #TAUTHOR_TAG and  #AUTHOR_TAG who based their predictions on the so - called location - indicative words ( liw ). recently, neural models have been built with the same purpose  #AUTHOR_TAG']",1
"['of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information']","['of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information']","['selective removal of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information gain ratio ( ig']","['feature selection the two corpora have very different vocabulary sizes. despite fewer instances, twitter - us contains a much richer vocabulary than twitter -', 'world : 14 vs. 6. 5 millions words. this size is computationally infeasible. in order', 'to maximize discrimination, we filter the vocabulary with several steps. in order not to waste the possible', 'geographic information carried by the huge amount of low frequency terms, we use replacement', 'tokens as follows : we again take only the training data', 'into account. first, we discard the hapax leg', '##omena, that is the words with frequency 1, as there is no evidence that these words could be found elsewhere. then, we discard words with frequency', 'greater than 1, if they appear in more than one place', '. we replace low frequency terms which appear uniquely in on place with a replacement token specific for that place, i. e., label. finally, we substitute these words with their replacement token in the whole', 'corpus, including development and test set. since the word distribution follows the zipf curve  #AUTHOR_TAG we are able to exploit', 'the geographic information of millions of words using only a small number of replacement tokens. the use of this information is', 'fair, as it relies on the information present in the training set only. in terms of performance, however, the effect of the replacement tokens is theoretically not different from that resulting from the direct inclusion of the single words in the vocabulary. the benefit is in terms of noise reduction, for the selective removal of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information gain ratio ( igr ), selecting the terms with', 'the highest values until we reach a computationally feasible vocabulary size : here, 750k and 460k for twitter - us and twitter - world. attention - based cnn for classification,', 'we use an attention - based convolutional neural model. we first train our own word embeddings for each corpus, and feed the texts into two convolutional channels ( with window size 4 and 8 ) and max - pooling, followed by an overall attention mechanism, and finally a fully - connected layer with softmax activation for prediction. for evaluation, as discussed', 'in section 3, we use the common metrics considered in literature : acc @ 161, that is the accuracy within 161 km ( 100 mi ) from the target point, and mean and median distance between the predicted and the target points. we are also interested in the exact accuracy. this', 'metric is often not shown in literature, but is important for the geolocation in real case scenarios. we evaluate significance via bootstrap sampling,', 'following']",5
"['of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information']","['of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information']","['selective removal of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information gain ratio ( ig']","['feature selection the two corpora have very different vocabulary sizes. despite fewer instances, twitter - us contains a much richer vocabulary than twitter -', 'world : 14 vs. 6. 5 millions words. this size is computationally infeasible. in order', 'to maximize discrimination, we filter the vocabulary with several steps. in order not to waste the possible', 'geographic information carried by the huge amount of low frequency terms, we use replacement', 'tokens as follows : we again take only the training data', 'into account. first, we discard the hapax leg', '##omena, that is the words with frequency 1, as there is no evidence that these words could be found elsewhere. then, we discard words with frequency', 'greater than 1, if they appear in more than one place', '. we replace low frequency terms which appear uniquely in on place with a replacement token specific for that place, i. e., label. finally, we substitute these words with their replacement token in the whole', 'corpus, including development and test set. since the word distribution follows the zipf curve  #AUTHOR_TAG we are able to exploit', 'the geographic information of millions of words using only a small number of replacement tokens. the use of this information is', 'fair, as it relies on the information present in the training set only. in terms of performance, however, the effect of the replacement tokens is theoretically not different from that resulting from the direct inclusion of the single words in the vocabulary. the benefit is in terms of noise reduction, for the selective removal of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information gain ratio ( igr ), selecting the terms with', 'the highest values until we reach a computationally feasible vocabulary size : here, 750k and 460k for twitter - us and twitter - world. attention - based cnn for classification,', 'we use an attention - based convolutional neural model. we first train our own word embeddings for each corpus, and feed the texts into two convolutional channels ( with window size 4 and 8 ) and max - pooling, followed by an overall attention mechanism, and finally a fully - connected layer with softmax activation for prediction. for evaluation, as discussed', 'in section 3, we use the common metrics considered in literature : acc @ 161, that is the accuracy within 161 km ( 100 mi ) from the target point, and mean and median distance between the predicted and the target points. we are also interested in the exact accuracy. this', 'metric is often not shown in literature, but is important for the geolocation in real case scenarios. we evaluate significance via bootstrap sampling,', 'following']",5
"['of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information']","['of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information']","['selective removal of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information gain ratio ( ig']","['feature selection the two corpora have very different vocabulary sizes. despite fewer instances, twitter - us contains a much richer vocabulary than twitter -', 'world : 14 vs. 6. 5 millions words. this size is computationally infeasible. in order', 'to maximize discrimination, we filter the vocabulary with several steps. in order not to waste the possible', 'geographic information carried by the huge amount of low frequency terms, we use replacement', 'tokens as follows : we again take only the training data', 'into account. first, we discard the hapax leg', '##omena, that is the words with frequency 1, as there is no evidence that these words could be found elsewhere. then, we discard words with frequency', 'greater than 1, if they appear in more than one place', '. we replace low frequency terms which appear uniquely in on place with a replacement token specific for that place, i. e., label. finally, we substitute these words with their replacement token in the whole', 'corpus, including development and test set. since the word distribution follows the zipf curve  #AUTHOR_TAG we are able to exploit', 'the geographic information of millions of words using only a small number of replacement tokens. the use of this information is', 'fair, as it relies on the information present in the training set only. in terms of performance, however, the effect of the replacement tokens is theoretically not different from that resulting from the direct inclusion of the single words in the vocabulary. the benefit is in terms of noise reduction, for the selective removal of geographically ambiguous words, and computational', 'affordability. following  #TAUTHOR_TAG, we further filter the vocabulary via information gain ratio ( igr ), selecting the terms with', 'the highest values until we reach a computationally feasible vocabulary size : here, 750k and 460k for twitter - us and twitter - world. attention - based cnn for classification,', 'we use an attention - based convolutional neural model. we first train our own word embeddings for each corpus, and feed the texts into two convolutional channels ( with window size 4 and 8 ) and max - pooling, followed by an overall attention mechanism, and finally a fully - connected layer with softmax activation for prediction. for evaluation, as discussed', 'in section 3, we use the common metrics considered in literature : acc @ 161, that is the accuracy within 161 km ( 100 mi ) from the target point, and mean and median distance between the predicted and the target points. we are also interested in the exact accuracy. this', 'metric is often not shown in literature, but is important for the geolocation in real case scenarios. we evaluate significance via bootstrap sampling,', 'following']",5
"[',, 2016, dialect identification  #AUTHOR_TAG b ;  #TAUTHOR_TAG ; and automatic essay scoring  #AUTHOR_TAG']","[', 2016, dialect identification  #AUTHOR_TAG b ;  #TAUTHOR_TAG ; and automatic essay scoring  #AUTHOR_TAG']","[',, 2016, dialect identification  #AUTHOR_TAG b ;  #TAUTHOR_TAG ; and automatic essay scoring  #AUTHOR_TAG.', 'as long']","['recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification  #AUTHOR_TAG and sentiment analysis ( gimenez - perez et al., 2017 ; to native language identification  #AUTHOR_TAG ionescu et al.,, 2016, dialect identification  #AUTHOR_TAG b ;  #TAUTHOR_TAG ; and automatic essay scoring  #AUTHOR_TAG.', 'as long as a labeled training set is available, string kernels can reach state - of - the - art results in various languages including english  #AUTHOR_TAG gimenez - perez et al., 2017 ;  #AUTHOR_TAG, arabic  #TAUTHOR_TAG ;, chinese and norwegian  #AUTHOR_TAG.', 'different from all these recent approaches, we use unlabeled data from the test set to significantly increase the performance of string kernels.', 'more precisely, we propose two transductive learning approaches combined into a unified framework.', 'we show that the proposed framework improves the results of string kernels in two different tasks ( cross - domain sentiment classification and arabic dialect identification ) and two different languages ( english and arabic ).', 'to the best of our knowledge, transductive learning frameworks based on string kernels have not been studied in previous works']",0
"[',, 2016, dialect identification  #AUTHOR_TAG b ;  #TAUTHOR_TAG ; and automatic essay scoring  #AUTHOR_TAG']","[', 2016, dialect identification  #AUTHOR_TAG b ;  #TAUTHOR_TAG ; and automatic essay scoring  #AUTHOR_TAG']","[',, 2016, dialect identification  #AUTHOR_TAG b ;  #TAUTHOR_TAG ; and automatic essay scoring  #AUTHOR_TAG.', 'as long']","['recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification  #AUTHOR_TAG and sentiment analysis ( gimenez - perez et al., 2017 ; to native language identification  #AUTHOR_TAG ionescu et al.,, 2016, dialect identification  #AUTHOR_TAG b ;  #TAUTHOR_TAG ; and automatic essay scoring  #AUTHOR_TAG.', 'as long as a labeled training set is available, string kernels can reach state - of - the - art results in various languages including english  #AUTHOR_TAG gimenez - perez et al., 2017 ;  #AUTHOR_TAG, arabic  #TAUTHOR_TAG ;, chinese and norwegian  #AUTHOR_TAG.', 'different from all these recent approaches, we use unlabeled data from the test set to significantly increase the performance of string kernels.', 'more precisely, we propose two transductive learning approaches combined into a unified framework.', 'we show that the proposed framework improves the results of string kernels in two different tasks ( cross - domain sentiment classification and arabic dialect identification ) and two different languages ( english and arabic ).', 'to the best of our knowledge, transductive learning frameworks based on string kernels have not been studied in previous works']",0
"['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['set.', 'the arabic dialect identification ( adi ) data set  #AUTHOR_TAG contains audio recordings and automatic speech recognition ( asr ) transcripts of arabic speech collected from the broadcast news domain.', 'the classification task is to discriminate between modern standard arabic and four arabic dialects, namely egyptian, gulf, levantine, and maghrebi.', 'the training set contains 14000 samples, the development set contains 1524 samples, and the test contains another 1492 samples.', 'the data set was used in the adi shared task of the 2017 vardial evaluation campaign.', 'baseline.', 'we choose as baseline the approach of ionescu and  #TAUTHOR_TAG, which is based on string kernels and multiple kernel learning.', 'the approach that we consider as baseline is the winner of the 2017 adi shared task.', 'in addition, we also compare with the second - best approach ( meta - classifier ).', 'evaluation procedure and parameters.', 'ionescu and  #TAUTHOR_TAG combined four kernels into a sum, and used kernel ridge regression for training.', 'three of the kernels are based on character ngrams extracted from asr transcripts.', 'these are the presence bits string kernel ( k 0 / 1 ), the intersection string kernel ( k ∩ ), and a kernel based on local rank distance ( k lrd ).', 'the fourth kernel is an rbf kernel ( k ivec ) based on the i - vectors provided with the adi data set  #AUTHOR_TAG.', 'in our experiments, we employ the exact same kernels as ionescu and  #TAUTHOR_TAG and the first runner up.', 'the best accuracy rates are highlighted in bold.', ""the marker * indicates that the performance is significantly better than ( ionescu and  #TAUTHOR_TAG according to a paired mcnemar's test performed at a significance level of 0. 01."", 'proach.', 'as in the polarity classification experiments, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training the transductive classifier, and we use kernel ridge regression with a regularization of 10 −5 in all our adi experiments.', 'results.', 'the results for the cross - domain arabic dialect identification experiments on both the development and the test sets are presented in table 3.', 'the domain - adapted sum of kernels obtains improvements above 0. 8 % over the stateof - the - art sum of kernels ( ionescu and  #TAUTHOR_TAG.', 'the improvement on the development set ( from 64. 17 % to 65. 42 % ) is statistically significant.', 'nevertheless, we obtain higher and significant improvements when we employ the transductive classifier.', 'our best accuracy is 66.']",0
"[', 2017 ;  #TAUTHOR_TAG. we also report better results than other domain adaptation methods  #AUTHOR_TAG franc']","['., 2017 ;  #TAUTHOR_TAG. we also report better results than other domain adaptation methods  #AUTHOR_TAG franco -  #AUTHOR_TAG']",['2017 ;  #TAUTHOR_TAG. we also report better results than other domain adaptation methods  #AUTHOR_TAG franc'],"['', 'even though a small percent ( less than 8 % in all experiments ) of the predicted labels corresponding to the newly included samples are wrong, the classifier has the chance to learn some useful patterns ( from the correctly predicted labels ) only visible in the test data. the transductive kernel classifier ( tkc ) is based on the intuition that the added test samples bring more useful information than', 'noise, since the majority of added test samples have correct labels. finally, we would like to stress out that the groundtruth test labels are never used in our transductive algorithm. the proposed transductive learning approaches are used together in a unified framework. as any other transductive learning method, the main disadvantage of the proposed framework is that the unlabeled test samples from the target domain need to be used in', 'the training stage. nevertheless, we present empirical results indicating that our approach can obtain significantly better accuracy rates', 'in cross - domain polarity classification and arabic dialect identification compared to state - of - the - art', 'methods based on string kernels ( gimen', '##ez - perez et al., 2017 ;  #TAUTHOR_TAG. we also report better results than other domain adaptation methods  #AUTHOR_TAG franco -  #AUTHOR_TAG']",4
"['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['set.', 'the arabic dialect identification ( adi ) data set  #AUTHOR_TAG contains audio recordings and automatic speech recognition ( asr ) transcripts of arabic speech collected from the broadcast news domain.', 'the classification task is to discriminate between modern standard arabic and four arabic dialects, namely egyptian, gulf, levantine, and maghrebi.', 'the training set contains 14000 samples, the development set contains 1524 samples, and the test contains another 1492 samples.', 'the data set was used in the adi shared task of the 2017 vardial evaluation campaign.', 'baseline.', 'we choose as baseline the approach of ionescu and  #TAUTHOR_TAG, which is based on string kernels and multiple kernel learning.', 'the approach that we consider as baseline is the winner of the 2017 adi shared task.', 'in addition, we also compare with the second - best approach ( meta - classifier ).', 'evaluation procedure and parameters.', 'ionescu and  #TAUTHOR_TAG combined four kernels into a sum, and used kernel ridge regression for training.', 'three of the kernels are based on character ngrams extracted from asr transcripts.', 'these are the presence bits string kernel ( k 0 / 1 ), the intersection string kernel ( k ∩ ), and a kernel based on local rank distance ( k lrd ).', 'the fourth kernel is an rbf kernel ( k ivec ) based on the i - vectors provided with the adi data set  #AUTHOR_TAG.', 'in our experiments, we employ the exact same kernels as ionescu and  #TAUTHOR_TAG and the first runner up.', 'the best accuracy rates are highlighted in bold.', ""the marker * indicates that the performance is significantly better than ( ionescu and  #TAUTHOR_TAG according to a paired mcnemar's test performed at a significance level of 0. 01."", 'proach.', 'as in the polarity classification experiments, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training the transductive classifier, and we use kernel ridge regression with a regularization of 10 −5 in all our adi experiments.', 'results.', 'the results for the cross - domain arabic dialect identification experiments on both the development and the test sets are presented in table 3.', 'the domain - adapted sum of kernels obtains improvements above 0. 8 % over the stateof - the - art sum of kernels ( ionescu and  #TAUTHOR_TAG.', 'the improvement on the development set ( from 64. 17 % to 65. 42 % ) is statistically significant.', 'nevertheless, we obtain higher and significant improvements when we employ the transductive classifier.', 'our best accuracy is 66.']",4
"['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['set.', 'the arabic dialect identification ( adi ) data set  #AUTHOR_TAG contains audio recordings and automatic speech recognition ( asr ) transcripts of arabic speech collected from the broadcast news domain.', 'the classification task is to discriminate between modern standard arabic and four arabic dialects, namely egyptian, gulf, levantine, and maghrebi.', 'the training set contains 14000 samples, the development set contains 1524 samples, and the test contains another 1492 samples.', 'the data set was used in the adi shared task of the 2017 vardial evaluation campaign.', 'baseline.', 'we choose as baseline the approach of ionescu and  #TAUTHOR_TAG, which is based on string kernels and multiple kernel learning.', 'the approach that we consider as baseline is the winner of the 2017 adi shared task.', 'in addition, we also compare with the second - best approach ( meta - classifier ).', 'evaluation procedure and parameters.', 'ionescu and  #TAUTHOR_TAG combined four kernels into a sum, and used kernel ridge regression for training.', 'three of the kernels are based on character ngrams extracted from asr transcripts.', 'these are the presence bits string kernel ( k 0 / 1 ), the intersection string kernel ( k ∩ ), and a kernel based on local rank distance ( k lrd ).', 'the fourth kernel is an rbf kernel ( k ivec ) based on the i - vectors provided with the adi data set  #AUTHOR_TAG.', 'in our experiments, we employ the exact same kernels as ionescu and  #TAUTHOR_TAG and the first runner up.', 'the best accuracy rates are highlighted in bold.', ""the marker * indicates that the performance is significantly better than ( ionescu and  #TAUTHOR_TAG according to a paired mcnemar's test performed at a significance level of 0. 01."", 'proach.', 'as in the polarity classification experiments, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training the transductive classifier, and we use kernel ridge regression with a regularization of 10 −5 in all our adi experiments.', 'results.', 'the results for the cross - domain arabic dialect identification experiments on both the development and the test sets are presented in table 3.', 'the domain - adapted sum of kernels obtains improvements above 0. 8 % over the stateof - the - art sum of kernels ( ionescu and  #TAUTHOR_TAG.', 'the improvement on the development set ( from 64. 17 % to 65. 42 % ) is statistically significant.', 'nevertheless, we obtain higher and significant improvements when we employ the transductive classifier.', 'our best accuracy is 66.']",4
"['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['set.', 'the arabic dialect identification ( adi ) data set  #AUTHOR_TAG contains audio recordings and automatic speech recognition ( asr ) transcripts of arabic speech collected from the broadcast news domain.', 'the classification task is to discriminate between modern standard arabic and four arabic dialects, namely egyptian, gulf, levantine, and maghrebi.', 'the training set contains 14000 samples, the development set contains 1524 samples, and the test contains another 1492 samples.', 'the data set was used in the adi shared task of the 2017 vardial evaluation campaign.', 'baseline.', 'we choose as baseline the approach of ionescu and  #TAUTHOR_TAG, which is based on string kernels and multiple kernel learning.', 'the approach that we consider as baseline is the winner of the 2017 adi shared task.', 'in addition, we also compare with the second - best approach ( meta - classifier ).', 'evaluation procedure and parameters.', 'ionescu and  #TAUTHOR_TAG combined four kernels into a sum, and used kernel ridge regression for training.', 'three of the kernels are based on character ngrams extracted from asr transcripts.', 'these are the presence bits string kernel ( k 0 / 1 ), the intersection string kernel ( k ∩ ), and a kernel based on local rank distance ( k lrd ).', 'the fourth kernel is an rbf kernel ( k ivec ) based on the i - vectors provided with the adi data set  #AUTHOR_TAG.', 'in our experiments, we employ the exact same kernels as ionescu and  #TAUTHOR_TAG and the first runner up.', 'the best accuracy rates are highlighted in bold.', ""the marker * indicates that the performance is significantly better than ( ionescu and  #TAUTHOR_TAG according to a paired mcnemar's test performed at a significance level of 0. 01."", 'proach.', 'as in the polarity classification experiments, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training the transductive classifier, and we use kernel ridge regression with a regularization of 10 −5 in all our adi experiments.', 'results.', 'the results for the cross - domain arabic dialect identification experiments on both the development and the test sets are presented in table 3.', 'the domain - adapted sum of kernels obtains improvements above 0. 8 % over the stateof - the - art sum of kernels ( ionescu and  #TAUTHOR_TAG.', 'the improvement on the development set ( from 64. 17 % to 65. 42 % ) is statistically significant.', 'nevertheless, we obtain higher and significant improvements when we employ the transductive classifier.', 'our best accuracy is 66.']",5
"['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['set.', 'the arabic dialect identification ( adi ) data set  #AUTHOR_TAG contains audio recordings and automatic speech recognition ( asr ) transcripts of arabic speech collected from the broadcast news domain.', 'the classification task is to discriminate between modern standard arabic and four arabic dialects, namely egyptian, gulf, levantine, and maghrebi.', 'the training set contains 14000 samples, the development set contains 1524 samples, and the test contains another 1492 samples.', 'the data set was used in the adi shared task of the 2017 vardial evaluation campaign.', 'baseline.', 'we choose as baseline the approach of ionescu and  #TAUTHOR_TAG, which is based on string kernels and multiple kernel learning.', 'the approach that we consider as baseline is the winner of the 2017 adi shared task.', 'in addition, we also compare with the second - best approach ( meta - classifier ).', 'evaluation procedure and parameters.', 'ionescu and  #TAUTHOR_TAG combined four kernels into a sum, and used kernel ridge regression for training.', 'three of the kernels are based on character ngrams extracted from asr transcripts.', 'these are the presence bits string kernel ( k 0 / 1 ), the intersection string kernel ( k ∩ ), and a kernel based on local rank distance ( k lrd ).', 'the fourth kernel is an rbf kernel ( k ivec ) based on the i - vectors provided with the adi data set  #AUTHOR_TAG.', 'in our experiments, we employ the exact same kernels as ionescu and  #TAUTHOR_TAG and the first runner up.', 'the best accuracy rates are highlighted in bold.', ""the marker * indicates that the performance is significantly better than ( ionescu and  #TAUTHOR_TAG according to a paired mcnemar's test performed at a significance level of 0. 01."", 'proach.', 'as in the polarity classification experiments, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training the transductive classifier, and we use kernel ridge regression with a regularization of 10 −5 in all our adi experiments.', 'results.', 'the results for the cross - domain arabic dialect identification experiments on both the development and the test sets are presented in table 3.', 'the domain - adapted sum of kernels obtains improvements above 0. 8 % over the stateof - the - art sum of kernels ( ionescu and  #TAUTHOR_TAG.', 'the improvement on the development set ( from 64. 17 % to 65. 42 % ) is statistically significant.', 'nevertheless, we obtain higher and significant improvements when we employ the transductive classifier.', 'our best accuracy is 66.']",5
"['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['ionescu and  #TAUTHOR_TAG,']","['set.', 'the arabic dialect identification ( adi ) data set  #AUTHOR_TAG contains audio recordings and automatic speech recognition ( asr ) transcripts of arabic speech collected from the broadcast news domain.', 'the classification task is to discriminate between modern standard arabic and four arabic dialects, namely egyptian, gulf, levantine, and maghrebi.', 'the training set contains 14000 samples, the development set contains 1524 samples, and the test contains another 1492 samples.', 'the data set was used in the adi shared task of the 2017 vardial evaluation campaign.', 'baseline.', 'we choose as baseline the approach of ionescu and  #TAUTHOR_TAG, which is based on string kernels and multiple kernel learning.', 'the approach that we consider as baseline is the winner of the 2017 adi shared task.', 'in addition, we also compare with the second - best approach ( meta - classifier ).', 'evaluation procedure and parameters.', 'ionescu and  #TAUTHOR_TAG combined four kernels into a sum, and used kernel ridge regression for training.', 'three of the kernels are based on character ngrams extracted from asr transcripts.', 'these are the presence bits string kernel ( k 0 / 1 ), the intersection string kernel ( k ∩ ), and a kernel based on local rank distance ( k lrd ).', 'the fourth kernel is an rbf kernel ( k ivec ) based on the i - vectors provided with the adi data set  #AUTHOR_TAG.', 'in our experiments, we employ the exact same kernels as ionescu and  #TAUTHOR_TAG and the first runner up.', 'the best accuracy rates are highlighted in bold.', ""the marker * indicates that the performance is significantly better than ( ionescu and  #TAUTHOR_TAG according to a paired mcnemar's test performed at a significance level of 0. 01."", 'proach.', 'as in the polarity classification experiments, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training the transductive classifier, and we use kernel ridge regression with a regularization of 10 −5 in all our adi experiments.', 'results.', 'the results for the cross - domain arabic dialect identification experiments on both the development and the test sets are presented in table 3.', 'the domain - adapted sum of kernels obtains improvements above 0. 8 % over the stateof - the - art sum of kernels ( ionescu and  #TAUTHOR_TAG.', 'the improvement on the development set ( from 64. 17 % to 65. 42 % ) is statistically significant.', 'nevertheless, we obtain higher and significant improvements when we employ the transductive classifier.', 'our best accuracy is 66.']",3
"['', 'bert  #TAUTHOR_TAG,']","['to train on extremely large datasets.', 'bert  #TAUTHOR_TAG,']","['to train on extremely large datasets.', 'bert  #TAUTHOR_TAG,']","[', wide - scale pre - training and deep contextual representations have taken the world by storm.', ' #AUTHOR_TAG underscored the importance of bidirectional contextual representations by using traditional neural networks trained on a large corpus of text.', ' #AUTHOR_TAG used transformers  #AUTHOR_TAG and word masking to pre - train on another large corpus of data, reporting human - level performance on one commonsense dataset  #AUTHOR_TAG.', 'achieves state - of - the - art on race  #AUTHOR_TAG with a transformer - xl based model.', 'the key to success in the performance of many of these models is their ability to train on extremely large datasets.', 'bert  #TAUTHOR_TAG, for example, trains on the bookscorpus  #AUTHOR_TAG and english wikipedia, for a combined 3, 200m words.', 'other iterations increased the amount of knowledge used during pre - training, such as roberta  #AUTHOR_TAG.', 'training large - scale models on these massive datasets has drawbacks, such as significantly increased carbon pollution and harm to the environment  #AUTHOR_TAG.', 'we present a methodology of combining queries from commonsense knowledge bases with contextual embeddings, big mood - bert infused graphs : matching over other embeddings, and abbreviated for its relationship to human knowledge awareness.', 'our methodology achieves a increase without significant additional fine - tuning or pre - training.', 'instead, it learns a separate representation from commonsense graphical knowledge bases, and augments the bert representation with this learned explicit representation.', 'we introduce several methods of combining and querying knowledge base embeddings to introduce them to the bert embedding layers']",0
"['passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with']","['passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with wordpiece embeddings  #AUTHOR_TAG.', 'then,']","['prepossess each passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with']","['prepossess each passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with wordpiece embeddings  #AUTHOR_TAG.', 'then, we append special [CLS] and [ sep ] to each datum.', 'we append [CLS] to the beginning of each datum, and [ sep ] to separate the question with the answer, as such :', 'then, we randomly mask 15 % of all wordpiece embeddings.', 'unlike  #TAUTHOR_TAG, we run the randomization script once per each training epoch.', 'otherwise, we follow the procedure in  #TAUTHOR_TAG.', '80 % of the time, we replace the word with the [ m ask ] prediction, to be replaced through cloze task prediction.', '10 % of the time, we replace the word with a random word.', '10 % of the time, we keep the word unchanged.', 'combined with the above cloze task, we process the data for next sentence prediction.', 'we do this process after the cloze task masking, similar to  #TAUTHOR_TAG.', 'for each datum, we randomly pick either a sentence labeled correctly as the next sentence 50 % of the time, or a random sentence 50 % of the time.', 'we ensure that the random sentence is not the next sentence']",0
"['language modeling, we create training data similar to those in bert  #TAUTHOR_TAG.', '']","['language modeling, we create training data similar to those in bert  #TAUTHOR_TAG.', '']","['language modeling, we create training data similar to those in bert  #TAUTHOR_TAG.', '']","['model usage, we preprocess the data in two ways to make it easier for the model to un - derstand.', 'for language modeling, we create training data similar to those in bert  #TAUTHOR_TAG.', 'for knowledge graph use, we preprocess language to create commonsense object and relationship vocabulary and to match as many related commonsense objects as possible']",3
"['passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with']","['passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with wordpiece embeddings  #AUTHOR_TAG.', 'then,']","['prepossess each passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with']","['prepossess each passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with wordpiece embeddings  #AUTHOR_TAG.', 'then, we append special [CLS] and [ sep ] to each datum.', 'we append [CLS] to the beginning of each datum, and [ sep ] to separate the question with the answer, as such :', 'then, we randomly mask 15 % of all wordpiece embeddings.', 'unlike  #TAUTHOR_TAG, we run the randomization script once per each training epoch.', 'otherwise, we follow the procedure in  #TAUTHOR_TAG.', '80 % of the time, we replace the word with the [ m ask ] prediction, to be replaced through cloze task prediction.', '10 % of the time, we replace the word with a random word.', '10 % of the time, we keep the word unchanged.', 'combined with the above cloze task, we process the data for next sentence prediction.', 'we do this process after the cloze task masking, similar to  #TAUTHOR_TAG.', 'for each datum, we randomly pick either a sentence labeled correctly as the next sentence 50 % of the time, or a random sentence 50 % of the time.', 'we ensure that the random sentence is not the next sentence']",3
"['passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with']","['passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with wordpiece embeddings  #AUTHOR_TAG.', 'then,']","['prepossess each passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with']","['prepossess each passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with wordpiece embeddings  #AUTHOR_TAG.', 'then, we append special [CLS] and [ sep ] to each datum.', 'we append [CLS] to the beginning of each datum, and [ sep ] to separate the question with the answer, as such :', 'then, we randomly mask 15 % of all wordpiece embeddings.', 'unlike  #TAUTHOR_TAG, we run the randomization script once per each training epoch.', 'otherwise, we follow the procedure in  #TAUTHOR_TAG.', '80 % of the time, we replace the word with the [ m ask ] prediction, to be replaced through cloze task prediction.', '10 % of the time, we replace the word with a random word.', '10 % of the time, we keep the word unchanged.', 'combined with the above cloze task, we process the data for next sentence prediction.', 'we do this process after the cloze task masking, similar to  #TAUTHOR_TAG.', 'for each datum, we randomly pick either a sentence labeled correctly as the next sentence 50 % of the time, or a random sentence 50 % of the time.', 'we ensure that the random sentence is not the next sentence']",4
"['to  #TAUTHOR_TAG, we']","['to  #TAUTHOR_TAG, we']","['to  #TAUTHOR_TAG, we do language model fine - tuning in']","['to  #TAUTHOR_TAG, we do language model fine - tuning in addition to classification finetuning.', 'we find that this generally provides better results, and allows for more stable accuracy since the shared task involves a small dataset.', 'for each prompt, we use the previous preprocessed data to create tasks for our model to predict.', 'we do this before token realignment, so this happens before any extra knowledge graph embeddings are added to the model architecture.', 'for masked tokens, we predict that token through bidirectional context, the same as  #TAUTHOR_TAG.', 'for next sentence prediction, we use the unbiased method previously introduced as well as in  #TAUTHOR_TAG']",4
"['passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with']","['passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with wordpiece embeddings  #AUTHOR_TAG.', 'then,']","['prepossess each passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with']","['prepossess each passage for training.', 'we use this process for each training epoch, since it allows for the most dense pretraining framework.', 'commonly known as a cloze task,  #TAUTHOR_TAG introduced a framework that pretrained transformers  #AUTHOR_TAG based on masked token prediction.', 'first, we preprocess the tokens with wordpiece embeddings  #AUTHOR_TAG.', 'then, we append special [CLS] and [ sep ] to each datum.', 'we append [CLS] to the beginning of each datum, and [ sep ] to separate the question with the answer, as such :', 'then, we randomly mask 15 % of all wordpiece embeddings.', 'unlike  #TAUTHOR_TAG, we run the randomization script once per each training epoch.', 'otherwise, we follow the procedure in  #TAUTHOR_TAG.', '80 % of the time, we replace the word with the [ m ask ] prediction, to be replaced through cloze task prediction.', '10 % of the time, we replace the word with a random word.', '10 % of the time, we keep the word unchanged.', 'combined with the above cloze task, we process the data for next sentence prediction.', 'we do this process after the cloze task masking, similar to  #TAUTHOR_TAG.', 'for each datum, we randomly pick either a sentence labeled correctly as the next sentence 50 % of the time, or a random sentence 50 % of the time.', 'we ensure that the random sentence is not the next sentence']",5
"['to  #TAUTHOR_TAG, we']","['to  #TAUTHOR_TAG, we']","['to  #TAUTHOR_TAG, we do language model fine - tuning in']","['to  #TAUTHOR_TAG, we do language model fine - tuning in addition to classification finetuning.', 'we find that this generally provides better results, and allows for more stable accuracy since the shared task involves a small dataset.', 'for each prompt, we use the previous preprocessed data to create tasks for our model to predict.', 'we do this before token realignment, so this happens before any extra knowledge graph embeddings are added to the model architecture.', 'for masked tokens, we predict that token through bidirectional context, the same as  #TAUTHOR_TAG.', 'for next sentence prediction, we use the unbiased method previously introduced as well as in  #TAUTHOR_TAG']",5
"['to  #TAUTHOR_TAG, we']","['to  #TAUTHOR_TAG, we']","['to  #TAUTHOR_TAG, we do language model fine - tuning in']","['to  #TAUTHOR_TAG, we do language model fine - tuning in addition to classification finetuning.', 'we find that this generally provides better results, and allows for more stable accuracy since the shared task involves a small dataset.', 'for each prompt, we use the previous preprocessed data to create tasks for our model to predict.', 'we do this before token realignment, so this happens before any extra knowledge graph embeddings are added to the model architecture.', 'for masked tokens, we predict that token through bidirectional context, the same as  #TAUTHOR_TAG.', 'for next sentence prediction, we use the unbiased method previously introduced as well as in  #TAUTHOR_TAG']",5
"['. 1.', 'we also ablate our use of this extra attention layer, showing that it is important to learn comparisons between knowledge embeddings.', 'for bert baselines, we use the process in  #TAUTHOR_TAG, and use the [CLS] token, without attention, for classification']","['if seq 1 be fine - tuned during this process.', 'we also allow the knowledge embeddings to be modified through this back - propagation.', 'hyperparameters are noted in section 4. 1.', 'we also ablate our use of this extra attention layer, showing that it is important to learn comparisons between knowledge embeddings.', 'for bert baselines, we use the process in  #TAUTHOR_TAG, and use the [CLS] token, without attention, for classification']","['if seq 1 be fine - tuned during this process.', 'we also allow the knowledge embeddings to be modified through this back - propagation.', 'hyperparameters are noted in section 4. 1.', 'we also ablate our use of this extra attention layer, showing that it is important to learn comparisons between knowledge embeddings.', 'for bert baselines, we use the process in  #TAUTHOR_TAG, and use the [CLS] token, without attention, for classification']","['', 'we add a simple linear layer over the sequence embedding for classification, and softmax over the given choices.', 'note that we do not freeze any weights along the process, allowing the transformer and perceptron to algorithm 2 : psuedocode for the token realignment algorithm, a method of finding token alignments between two different sequences.', 'token realignment ( seq 1, seq 2 ) : alignment dict = dict seq 1 i = 0 seq 2 i = 0 while seq 1 i < len ( seq 1 ) & seq 2 i < len ( seq 2 ) do if seq 1 be fine - tuned during this process.', 'we also allow the knowledge embeddings to be modified through this back - propagation.', 'hyperparameters are noted in section 4. 1.', 'we also ablate our use of this extra attention layer, showing that it is important to learn comparisons between knowledge embeddings.', 'for bert baselines, we use the process in  #TAUTHOR_TAG, and use the [CLS] token, without attention, for classification']",5
"['embedding pieces of natural language text as fixed - length, real - valued vectors.', 'extending the word2vec framework  #TAUTHOR_TAG, paragraph vectors']","['embedding pieces of natural language text as fixed - length, real - valued vectors.', 'extending the word2vec framework  #TAUTHOR_TAG, paragraph vectors']","['embedding pieces of natural language text as fixed - length, real - valued vectors.', 'extending the word2vec framework  #TAUTHOR_TAG, paragraph vectors']","['vectors  #AUTHOR_TAG are a recent method for embedding pieces of natural language text as fixed - length, real - valued vectors.', 'extending the word2vec framework  #TAUTHOR_TAG, paragraph vectors are typically presented as neural language models, and compute a single vector representation for each paragraph.', 'unlike word embeddings, paragraph vectors are not shared across the entire corpus, but are instead local to each paragraph.', 'when interpreted as a latent variable, we expect them to have higher uncertainty when the paragraphs are short.', ' #AUTHOR_TAG proposed a probabilistic view of word2vec that has motivated research on combining word2vec with other priors  #AUTHOR_TAG.', 'inspired by this progress, we extend paragraph vectors to a probabilistic model.', 'our model may be specified via modern inference tools like edward  #AUTHOR_TAG, which makes it easy to experiment with different inference algorithms.', 'the experiments in sec. 4 confirm the intuition that paragraph vectors have higher posterior uncertainty when paragraphs are short, and we show that explicitly modeling this uncertainty improves performance in supervised prediction tasks']",1
"['bayesian skip - gram model  #AUTHOR_TAG is a probabilistic interpretation of word2vec  #TAUTHOR_TAG.', 'the left part of']","['bayesian skip - gram model  #AUTHOR_TAG is a probabilistic interpretation of word2vec  #TAUTHOR_TAG.', 'the left part of']","['bayesian skip - gram model  #AUTHOR_TAG is a probabilistic interpretation of word2vec  #TAUTHOR_TAG.', 'the left part of figure 1 shows the generative process.', 'for each word i in the vocabulary, the model draws a latent word embedding']","['bayesian skip - gram model  #AUTHOR_TAG is a probabilistic interpretation of word2vec  #TAUTHOR_TAG.', 'the left part of figure 1 shows the generative process.', 'for each word i in the vocabulary, the model draws a latent word embedding vector u i ∈ r e and a latent context embedding vector v i ∈ r e from a gaussian prior n ( 0, λ 2 i ).', 'here, e is the embedding dimension and λ is a hyperparameter.', 'the model then constructs n pairs labeled pairs of words following a twostep process.', 'first, a proposal pair of words ( i, j ) is drawn from a uniform distribution over the vocabulary.', 'then, the model assigns to the proposal pair a binary label', 'is the sigmoid function.', 'the pairs with label z ij = 1 form the so - called positive examples, and are assumed to correspond to occurrences of the word i in the context of word j somewhere in the corpus.', 'the so - called negative examples with label z ij = 0 do not correspond to any observation in the corpus.', 'when training the model, we resort to the heuristics proposed in  #TAUTHOR_TAG to create artificial evidence for the negative examples ( see section 3. 2 below )']",5
"['not observed in the corpus.', 'following  #TAUTHOR_TAG, we construct artificial evidence']","['not observed in the corpus.', 'following  #TAUTHOR_TAG, we construct artificial evidence']","['of the t th token, t runs over all tokens in document n, δ runs from −c to c where c is a small context window size, and we exclude δ = 0.', 'negative examples are not observed in the corpus.', 'following  #TAUTHOR_TAG, we construct artificial evidence x − n']","['paragraph vectors ( bpv ) are a direct extension of the bayesian skip - gram model.', 'the right part of figure 1 shows the generative process.', 'in addition to global word and context embeddings u and v, the model draws a paragraph vector d n ∼ n ( 0, φ 2 i ) for each of the n docs documents in the corpus.', ' #AUTHOR_TAG, we add d n to the context vector v j when we classify a given pair of words ( i, j ) as a positive or a negative example.', 'thus, the likelihood of a word pair ( i, j ) in document n to have label z n, ij ∈ { 0, 1 } is', 'we collect evidence for the positive examples x + n in each document n by forming pairs of words ( w n, t, w n, t + δ ).', 'here, w n, t is the word class of the t th token, t runs over all tokens in document n, δ runs from −c to c where c is a small context window size, and we exclude δ = 0.', 'negative examples are not observed in the corpus.', 'following  #TAUTHOR_TAG, we construct artificial evidence x − n for negative pairs by sampling from the noise distribution', ', where f is the empirical unigram frequency across the training corpus.', 'the log - likelihood of the entire data is thus', 'in the limit d n → 0, eq. ( 2 ) reduces to the negative loss function of word2vec.', 'bpv can be easily specified in edward, a python library for probabilistic modeling and inference  #AUTHOR_TAG :', 'from edward. models import bernoulli, normal u = normal ( loc = tf. zeros ( ( w, e ), dtype = tf. float32 ), scale = lam ) v = normal ( loc = tf. zeros ( ( w, e ), dtype = tf. float32 ), scale = lam ) d _ n = normal ( loc = tf. zeros ( e, dtype = tf. float32 ), scale = phi ) u _ n = tf. nn. embedding _ lookup ( u, indices _ n _ i ) v _ n = tf. nn. embedding _ lookup ( v, indices _ n _ j ) z _ n = bernoulli ( logits = tf. reduce _ sum ( u _ n * ( v _ n + d _ n ), axis = 1 )']",5
"['##ner tool  #TAUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in']","['and also opensubtitles  #AUTHOR_TAG.', 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG engine customization the data was cleaned using the bicleaner tool  #TAUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in']","['and also opensubtitles  #AUTHOR_TAG.', 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG engine customization the data was cleaned using the bicleaner tool  #TAUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in']","['iadaatpa 1 project coded as n • 2016 - eu - ia - 0132 that ended in february 2019 is made for building of customized, domain - specific engines for public administrations from eu member states.', 'the consortium of the project decided to use neural machine translation at the beginning of the project.', 'this represented a challenge for all involved, and the positive aspect is that all public administrations engaged in the iadaatpa project were able to try, test and use state - of - the - art neural technology with a high level of satisfaction.', 'one of the main challenges faced by all partners was data availability.', 'although all public administrations had some data available, it was clearly insufficient for high - level customization.', 'in some cases, we had merely a few hundred words or several tens of thousand words.', 'each domain ( field ) has its own unique word distribution and neural machine translation systems are known to suffer a decrease in performance when data is out - of - domain.', 'pangeanic is a language service provider ( lsp ) specialised in natural language processing and machine translation.', 'it provides solutions to cognitive companies, institutions, translation professionals, and corporations.', 'the problem faced by the iadaatpa project at pangeanic was twofold : data acquisition for translation from spanish to russian there was no available in - domain data.', 'therefore, 2 translators were contracted as part of the project to create 30, 000 segments of in - domain data, translating public administrations websites.', 'they also cleaned united nations material and post - edited general - domain data that was previously filtered as indomain following the "" invitation model ""  #AUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', ""the main part of the training corpora ( approximately 75 % ) was part of pangeanic's own repository harvested through web crawling and also opensubtitles  #AUTHOR_TAG."", 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG engine customization the data was cleaned using the bicleaner tool  #TAUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in order to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #AUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #AUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the source as the extra']",5
"['##ner tool  #TAUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in']","['was cleaned using the bicleaner tool  #TAUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in']","['##zig  #AUTHOR_TAG.', 'engine customization the data was cleaned using the bicleaner tool  #TAUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in']","['of the main challenges faced by all partners was data availability.', 'although all public administrations had some data available, it was clearly insufficient for high - level customization.', 'in some cases, we had merely a few hundred words or several tens of thousand words.', 'each domain ( field ) has its own unique word distribution and neural machine translation systems are known to suffer a decrease in performance when data is out - of - domain.', 'pangeanic is a language service provider ( lsp ) specialised in natural language processing and machine translation.', 'it provides solutions to cognitive companies, institutions, translation professionals, and corporations.', 'the problem faced by the iadaatpa project at pangeanic was twofold : data acquisition for translation from spanish to russian there was no available in - domain data.', 'therefore, 2 translators were contracted as part of the project to create 30, 000 segments of in - domain data, translating public administrations websites.', 'they also cleaned united nations material and post - edited general - domain data that was previously filtered as indomain following the "" invitation model ""  #AUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', ""the main part of the training corpora ( approximately 75 % ) was part of pangeanic's own repository harvested through web crawling and also opensubtitles  #AUTHOR_TAG."", 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG.', 'engine customization the data was cleaned using the bicleaner tool  #TAUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in order to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #AUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #AUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the source as the extra token was added at target - side and there was no need for apriori domain information.', 'this approach allowed the model to improve the quality for each domain']",5
"['original formulation by  #TAUTHOR_TAG, which leads to drastic']","['original formulation by  #TAUTHOR_TAG, which leads to drastic']","['paper investigates the problem of image - caption retrieval using joint visualsemantic embeddings.', 'we introduce a very simple change to the loss function used in the original formulation by  #TAUTHOR_TAG, which leads to drastic improvements in']","['paper investigates the problem of image - caption retrieval using joint visualsemantic embeddings.', 'we introduce a very simple change to the loss function used in the original formulation by  #TAUTHOR_TAG, which leads to drastic improvements in the retrieval performance.', 'in particular, the original paper uses the rank loss which computes the sum of violations across the negative training examples.', 'instead, we penalize the model according to the hardest negative examples.', 'we then make several additional modifications according to the current best practices in image - caption retrieval.', 'we showcase our model on the ms - coco and flickr30k datasets through comparisons and ablation studies.', 'on ms - coco, we improve caption retrieval by 21 % in r @ 1 with respect to the original formulation.', '']",6
"[' #TAUTHOR_TAG, karpathy']","[' #TAUTHOR_TAG, karpathy']","[' #TAUTHOR_TAG, karpathy']","['task of image - caption retrieval is considered as a benchmark for image and language understanding (  #AUTHOR_TAG ).', 'the most common approach to the retrieval task is to use joint embedding spaces.', 'in such an approach, we learn two mappings, one for images and another for captions, that embed the two modalities in a joint space.', 'given a similarity measure in this space, the task of image retrieval can be formulated as a nearest neighbor search problem.', 'works such as  #TAUTHOR_TAG, karpathy & fei -  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG use a rank loss to learn the joint visual - semantic embedding.', ' #AUTHOR_TAG and  #AUTHOR_TAG use canonical correlation analysis ( cca ) to compute a linear projection for two views to a common space where the correlation of the transformed views is maximized.', 'older work (  #AUTHOR_TAG a ) ) performed matching between words and objects based on classification scores.', 'recent methods that learn visual - semantic embeddings propose new model architectures for computing the embedding vectors or computing the similarity score between the embedding vectors.', ' #AUTHOR_TAG propose an embedding network to fully replace the similarity measure used for the rank loss.', 'an attention mechanism on both image and caption is used by  #AUTHOR_TAG, where the authors sequentially and selectively focus on a subset of words and image regions to compute the similarity.', ' #AUTHOR_TAG, the authors use a multi - modal context - modulated attention mechanism to compute the matching score between an image and a caption.', 'our work builds upon the work by  #TAUTHOR_TAG, in which the authors use a rank loss to optimize the embedding.', 'works such as karpathy & fei -  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG use a similar loss to optimize more sophisticated models.', 'our modifications are orthogonal to most of their approaches and can potentially lead to improvements of these models as well']",6
"['on visual - semantic embeddings  #TAUTHOR_TAG.', 'in']","['on visual - semantic embeddings  #TAUTHOR_TAG.', 'in']","['on visual - semantic embeddings  #TAUTHOR_TAG.', 'in']","['work builds on visual - semantic embeddings  #TAUTHOR_TAG.', 'in what follows, we first define the task of image - caption retrieval and summarize the original model and its loss function.', 'then we introduce our new loss']",6
"['original formulation by  #TAUTHOR_TAG, which leads to drastic']","['original formulation by  #TAUTHOR_TAG, which leads to drastic']","['paper investigates the problem of image - caption retrieval using joint visualsemantic embeddings.', 'we introduce a very simple change to the loss function used in the original formulation by  #TAUTHOR_TAG, which leads to drastic improvements in']","['paper investigates the problem of image - caption retrieval using joint visualsemantic embeddings.', 'we introduce a very simple change to the loss function used in the original formulation by  #TAUTHOR_TAG, which leads to drastic improvements in the retrieval performance.', 'in particular, the original paper uses the rank loss which computes the sum of violations across the negative training examples.', 'instead, we penalize the model according to the hardest negative examples.', 'we then make several additional modifications according to the current best practices in image - caption retrieval.', 'we showcase our model on the ms - coco and flickr30k datasets through comparisons and ablation studies.', 'on ms - coco, we improve caption retrieval by 21 % in r @ 1 with respect to the original formulation.', '']",4
['original formulation of  #TAUTHOR_TAG ('],['original formulation of  #TAUTHOR_TAG ( referred'],['the original formulation of  #TAUTHOR_TAG ('],"['perform experiments with our vse + + and compare it to the original formulation of  #TAUTHOR_TAG ( referred to as vse ), as well as state - of - the - art approaches.', ""we re - implemented vse with the help of the authors'open - source code 2."", '']",4
"[' #TAUTHOR_TAG ;  #AUTHOR_TAG ).', 'we performed experiments on the ms - coc']","[' #TAUTHOR_TAG ;  #AUTHOR_TAG ).', 'we performed experiments on the ms - coco']","[' #TAUTHOR_TAG ;  #AUTHOR_TAG ).', 'we performed experiments on the ms - coco']","['this paper, we focused on the task of image - caption retrieval and investigated visual - semantic embeddings.', 'we have shown that a new loss function that uses only violation incurred by hard negatives drastically improves performance over the typical loss that sums the violations across the negatives, typically used in previous work  #TAUTHOR_TAG ;  #AUTHOR_TAG ).', 'we performed experiments on the ms - coco and flickr30k datasets.', 'we observed that the improved loss can better guide a more powerful image encoder, resnet152, and also guide better when fine - tuning an image encoder.', 'with all modifications, our vse + + model achieves state - of - the - art performance on the ms - coco dataset, and is slightly below the best recent model on the flickr30k dataset.', 'our proposed improvement can be used to train more sophisticated models that have been using a similar rank loss for training']",4
[' #TAUTHOR_TAG ; karpathy'],[' #TAUTHOR_TAG ; karpathy'],['generation  #TAUTHOR_TAG ; karpathy'],"['problem of image - caption retrieval has received significant attention over the past few years, quickly gearing towards more semantic search engines.', 'several different approaches have been proposed, most sharing the core idea of embedding images and language in a common space.', 'this allows us to easily search for semantically meaningful neighbors in either modality.', 'learning such embeddings using powerful neural networks has led to significant advancements in image - caption retrieval and generation  #TAUTHOR_TAG ; karpathy & fei -  #AUTHOR_TAG, video - to - text alignment  #AUTHOR_TAG, and question - answering  #AUTHOR_TAG.', 'this paper investigates the visual - semantic embeddings ( vse ) of  #TAUTHOR_TAG for imagecaption retrieval.', 'we propose a set of simple modifications to the original formulation that prove to be extremely effective.', 'in particular, we change the rank loss used in the original formulation to penalize the model according to the hardest negative training exemplars instead of averaging the individual violations across the negatives.', 'this is a sensible modification because it is the hardest negative that affects nearest neighbor recall.', 'we refer to this model as vse + +.', 'we achieve further improvements by fine - tuning a more powerful network, exploiting more data, and employing a multi - crop trick from  #AUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG, karpathy']","[' #TAUTHOR_TAG, karpathy']","[' #TAUTHOR_TAG, karpathy']","['task of image - caption retrieval is considered as a benchmark for image and language understanding (  #AUTHOR_TAG ).', 'the most common approach to the retrieval task is to use joint embedding spaces.', 'in such an approach, we learn two mappings, one for images and another for captions, that embed the two modalities in a joint space.', 'given a similarity measure in this space, the task of image retrieval can be formulated as a nearest neighbor search problem.', 'works such as  #TAUTHOR_TAG, karpathy & fei -  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG use a rank loss to learn the joint visual - semantic embedding.', ' #AUTHOR_TAG and  #AUTHOR_TAG use canonical correlation analysis ( cca ) to compute a linear projection for two views to a common space where the correlation of the transformed views is maximized.', 'older work (  #AUTHOR_TAG a ) ) performed matching between words and objects based on classification scores.', 'recent methods that learn visual - semantic embeddings propose new model architectures for computing the embedding vectors or computing the similarity score between the embedding vectors.', ' #AUTHOR_TAG propose an embedding network to fully replace the similarity measure used for the rank loss.', 'an attention mechanism on both image and caption is used by  #AUTHOR_TAG, where the authors sequentially and selectively focus on a subset of words and image regions to compute the similarity.', ' #AUTHOR_TAG, the authors use a multi - modal context - modulated attention mechanism to compute the matching score between an image and a caption.', 'our work builds upon the work by  #TAUTHOR_TAG, in which the authors use a rank loss to optimize the embedding.', 'works such as karpathy & fei -  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG use a similar loss to optimize more sophisticated models.', 'our modifications are orthogonal to most of their approaches and can potentially lead to improvements of these models as well']",0
"['following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈']","['following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈']","['define the similarity measure s ( i, c ) in the joint embedding space following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈ r d φ be the representation of the image ( e. g. the representation before logits in']","['define the similarity measure s ( i, c ) in the joint embedding space following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈ r d φ be the representation of the image ( e. g. the representation before logits in vgg19  #AUTHOR_TAG or resnet152  #AUTHOR_TAG ).', 'similarly, let ψ ( c ; θ ψ ) ∈ r d ψ be the embedding of a caption c in a caption embedding space ( e. g. a gru - based text encoder ).', 'here θ φ, θ ψ denote the model parameters of the image and caption representations.', 'the mappings of each modality into the joint embedding space are defined as follows :', 'here w f ∈ r d φ ×d, w g ∈ r d ψ ×d represent the two mappings.', 'we further normalize φ ( i ), f ( i ; w f, θ φ ), and g ( c ; w g, θ ψ ), so that they lie on a unit hypersphere.', 'finally, the similarity measure is defined as s ( i, c ) = f ( i ; w f, θ φ ) · g ( c ; w g, θ ψ ).', 'let θ = { w f, w g, θ ψ } be the model parameters.', 'we will include θ φ in θ when the aim is also to fine - tune the image encoder.', 'we define e ( θ, s ) = 1 n n n = 1 ( i n, c n ) to be the empirical loss of the model, parametrized by θ, over the training samples s = { ( i n, c n ) } n n = 1, where ( i n, c n ) is a loss of a single example.', 'the rank loss used in  #TAUTHOR_TAG,  #AUTHOR_TAG, and karpathy & fei -  #AUTHOR_TAG is defined as follows :', 'where α represents the margin, c denotes a negative caption for the query image i, andi a negative image for the query caption c. here, we used a shorthand notation [ x ] + ≡ max ( x, 0 ).', 'this loss is composed of two symmetric terms, by considering i and c as individual queries.', 'in each term, the loss is defined by the sum of the violations for each negative sample']",0
['original formulation of  #TAUTHOR_TAG ('],['original formulation of  #TAUTHOR_TAG ( referred'],['the original formulation of  #TAUTHOR_TAG ('],"['perform experiments with our vse + + and compare it to the original formulation of  #TAUTHOR_TAG ( referred to as vse ), as well as state - of - the - art approaches.', ""we re - implemented vse with the help of the authors'open - source code 2."", '']",0
[' #TAUTHOR_TAG ; karpathy'],[' #TAUTHOR_TAG ; karpathy'],['generation  #TAUTHOR_TAG ; karpathy'],"['problem of image - caption retrieval has received significant attention over the past few years, quickly gearing towards more semantic search engines.', 'several different approaches have been proposed, most sharing the core idea of embedding images and language in a common space.', 'this allows us to easily search for semantically meaningful neighbors in either modality.', 'learning such embeddings using powerful neural networks has led to significant advancements in image - caption retrieval and generation  #TAUTHOR_TAG ; karpathy & fei -  #AUTHOR_TAG, video - to - text alignment  #AUTHOR_TAG, and question - answering  #AUTHOR_TAG.', 'this paper investigates the visual - semantic embeddings ( vse ) of  #TAUTHOR_TAG for imagecaption retrieval.', 'we propose a set of simple modifications to the original formulation that prove to be extremely effective.', 'in particular, we change the rank loss used in the original formulation to penalize the model according to the hardest negative training exemplars instead of averaging the individual violations across the negatives.', 'this is a sensible modification because it is the hardest negative that affects nearest neighbor recall.', 'we refer to this model as vse + +.', 'we achieve further improvements by fine - tuning a more powerful network, exploiting more data, and employing a multi - crop trick from  #AUTHOR_TAG.', '']",5
"['following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈']","['following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈']","['define the similarity measure s ( i, c ) in the joint embedding space following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈ r d φ be the representation of the image ( e. g. the representation before logits in']","['define the similarity measure s ( i, c ) in the joint embedding space following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈ r d φ be the representation of the image ( e. g. the representation before logits in vgg19  #AUTHOR_TAG or resnet152  #AUTHOR_TAG ).', 'similarly, let ψ ( c ; θ ψ ) ∈ r d ψ be the embedding of a caption c in a caption embedding space ( e. g. a gru - based text encoder ).', 'here θ φ, θ ψ denote the model parameters of the image and caption representations.', 'the mappings of each modality into the joint embedding space are defined as follows :', 'here w f ∈ r d φ ×d, w g ∈ r d ψ ×d represent the two mappings.', 'we further normalize φ ( i ), f ( i ; w f, θ φ ), and g ( c ; w g, θ ψ ), so that they lie on a unit hypersphere.', 'finally, the similarity measure is defined as s ( i, c ) = f ( i ; w f, θ φ ) · g ( c ; w g, θ ψ ).', 'let θ = { w f, w g, θ ψ } be the model parameters.', 'we will include θ φ in θ when the aim is also to fine - tune the image encoder.', 'we define e ( θ, s ) = 1 n n n = 1 ( i n, c n ) to be the empirical loss of the model, parametrized by θ, over the training samples s = { ( i n, c n ) } n n = 1, where ( i n, c n ) is a loss of a single example.', 'the rank loss used in  #TAUTHOR_TAG,  #AUTHOR_TAG, and karpathy & fei -  #AUTHOR_TAG is defined as follows :', 'where α represents the margin, c denotes a negative caption for the query image i, andi a negative image for the query caption c. here, we used a shorthand notation [ x ] + ≡ max ( x, 0 ).', 'this loss is composed of two symmetric terms, by considering i and c as individual queries.', 'in each term, the loss is defined by the sum of the violations for each negative sample']",5
"['following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈']","['following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈']","['define the similarity measure s ( i, c ) in the joint embedding space following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈ r d φ be the representation of the image ( e. g. the representation before logits in']","['define the similarity measure s ( i, c ) in the joint embedding space following  #TAUTHOR_TAG.', 'let φ ( i ; θ φ ) ∈ r d φ be the representation of the image ( e. g. the representation before logits in vgg19  #AUTHOR_TAG or resnet152  #AUTHOR_TAG ).', 'similarly, let ψ ( c ; θ ψ ) ∈ r d ψ be the embedding of a caption c in a caption embedding space ( e. g. a gru - based text encoder ).', 'here θ φ, θ ψ denote the model parameters of the image and caption representations.', 'the mappings of each modality into the joint embedding space are defined as follows :', 'here w f ∈ r d φ ×d, w g ∈ r d ψ ×d represent the two mappings.', 'we further normalize φ ( i ), f ( i ; w f, θ φ ), and g ( c ; w g, θ ψ ), so that they lie on a unit hypersphere.', 'finally, the similarity measure is defined as s ( i, c ) = f ( i ; w f, θ φ ) · g ( c ; w g, θ ψ ).', 'let θ = { w f, w g, θ ψ } be the model parameters.', 'we will include θ φ in θ when the aim is also to fine - tune the image encoder.', 'we define e ( θ, s ) = 1 n n n = 1 ( i n, c n ) to be the empirical loss of the model, parametrized by θ, over the training samples s = { ( i n, c n ) } n n = 1, where ( i n, c n ) is a loss of a single example.', 'the rank loss used in  #TAUTHOR_TAG,  #AUTHOR_TAG, and karpathy & fei -  #AUTHOR_TAG is defined as follows :', 'where α represents the margin, c denotes a negative caption for the query image i, andi a negative image for the query caption c. here, we used a shorthand notation [ x ] + ≡ max ( x, 0 ).', 'this loss is composed of two symmetric terms, by considering i and c as individual queries.', 'in each term, the loss is defined by the sum of the violations for each negative sample']",5
['original formulation of  #TAUTHOR_TAG ('],['original formulation of  #TAUTHOR_TAG ( referred'],['the original formulation of  #TAUTHOR_TAG ('],"['perform experiments with our vse + + and compare it to the original formulation of  #TAUTHOR_TAG ( referred to as vse ), as well as state - of - the - art approaches.', ""we re - implemented vse with the help of the authors'open - source code 2."", '']",5
"['to  #TAUTHOR_TAG, while empty circles are negative']","['to  #TAUTHOR_TAG, while empty circles are negative']","['to  #TAUTHOR_TAG, while empty circles are negative samples']",[' #TAUTHOR_TAG'],3
['original formulation of  #TAUTHOR_TAG ('],['original formulation of  #TAUTHOR_TAG ( referred'],['the original formulation of  #TAUTHOR_TAG ('],"['perform experiments with our vse + + and compare it to the original formulation of  #TAUTHOR_TAG ( referred to as vse ), as well as state - of - the - art approaches.', ""we re - implemented vse with the help of the authors'open - source code 2."", '']",3
,,,,5
,,,,5
,,,,5
['##15  #TAUTHOR_TAG as well as the statistics of the par'],['constrained ( c ) systems in wmt15  #TAUTHOR_TAG as well as the statistics of the'],"['##15  #TAUTHOR_TAG as well as the statistics of the parfda selected training and lm data.', 'par']","['machine translation performance is influenced by the data : if you already have the translations for the source being translated in your training set or even portions of it, then the translation task becomes easier.', 'if some token does not appear in your language model ( lm ), then it becomes harder for the smt engine to find its correct position in the translation.', 'the importance of parfda increases with the proliferation of training material available for building smt systems.', 'table 1 presents the statistics of the available training and lm corpora for the constrained ( c ) systems in wmt15  #TAUTHOR_TAG as well as the statistics of the parfda selected training and lm data.', 'parfda ( bicici, 2013 ; bicici et al., 2014 ) runs separate fda5 ( bicici and  #AUTHOR_TAG models on randomized subsets of the training data and combines the selections afterwards.', 'fda5 is available at http : / / github. com / bicici / fda.', 'we run parfda smt experiments using moses  #AUTHOR_TAG in all language pairs in wmt15  #TAUTHOR_TAG and obtain smt performance close to the top constrained moses systems.', 'parfda allows rapid prototyping of smt systems for a given target domain or task.', 'we use parfda for selecting parallel training data and lm data for building smt systems.', 'we select the lm training data with parfda based on the following observation ( bicici, 2013 ) :', 'no word not appearing in the training set can appear in the translation.', 'thus we are only interested in correctly ordering the words appearing in the training corpus and collecting the sentences that contain them for building the lm.', 'at the same time, a compact and more relevant lm corpus is also useful for modeling longer range dependencies with higher order ngram models.', 'we use 3 - grams for selecting training data and 2 - grams for lm corpus selection']",5
['##15  #TAUTHOR_TAG as well as the statistics of the par'],['constrained ( c ) systems in wmt15  #TAUTHOR_TAG as well as the statistics of the'],"['##15  #TAUTHOR_TAG as well as the statistics of the parfda selected training and lm data.', 'par']","['machine translation performance is influenced by the data : if you already have the translations for the source being translated in your training set or even portions of it, then the translation task becomes easier.', 'if some token does not appear in your language model ( lm ), then it becomes harder for the smt engine to find its correct position in the translation.', 'the importance of parfda increases with the proliferation of training material available for building smt systems.', 'table 1 presents the statistics of the available training and lm corpora for the constrained ( c ) systems in wmt15  #TAUTHOR_TAG as well as the statistics of the parfda selected training and lm data.', 'parfda ( bicici, 2013 ; bicici et al., 2014 ) runs separate fda5 ( bicici and  #AUTHOR_TAG models on randomized subsets of the training data and combines the selections afterwards.', 'fda5 is available at http : / / github. com / bicici / fda.', 'we run parfda smt experiments using moses  #AUTHOR_TAG in all language pairs in wmt15  #TAUTHOR_TAG and obtain smt performance close to the top constrained moses systems.', 'parfda allows rapid prototyping of smt systems for a given target domain or task.', 'we use parfda for selecting parallel training data and lm data for building smt systems.', 'we select the lm training data with parfda based on the following observation ( bicici, 2013 ) :', 'no word not appearing in the training set can appear in the translation.', 'thus we are only interested in correctly ordering the words appearing in the training corpus and collecting the sentences that contain them for building the lm.', 'at the same time, a compact and more relevant lm corpus is also useful for modeling longer range dependencies with higher order ngram models.', 'we use 3 - grams for selecting training data and 2 - grams for lm corpus selection']",5
"['##15 translation task  #TAUTHOR_TAG, which include english - czech (']","['language pairs in both directions in the wmt15 translation task  #TAUTHOR_TAG, which include english - czech ( en - cs ), english - german']","['##a smt experiments for all language pairs in both directions in the wmt15 translation task  #TAUTHOR_TAG, which include english - czech (']","['run parfda smt experiments for all language pairs in both directions in the wmt15 translation task  #TAUTHOR_TAG, which include english - czech ( en - cs ), english - german ( en - de ), english - finnish ( en - fi ), english - french ( en - fr ), and english - russian ( en - ru ).', 'we truecase all of the corpora, set the maximum sentence length to 126, use 150 - best lists during tuning, set the lm order to a value in [ 7, 10 ] for all language pairs, and train the lm using srilm  #AUTHOR_TAG with - unk option.', 'for giza + +  #AUTHOR_TAG, max - fertility is set to 10, with the number of iterations set to 7, 3, 5, 5, 7 for ibm models 1, 2, 3, 4, and the hmm model, and 70 word table 1 : data statistics for the available training and lm corpora in the constrained ( c ) setting compared with the parfda selected training and lm data.', '# words is in millions ( m ) and # sents in thousands ( k ).', 'classes are learned over 3 iterations with the mkcls tool during training.', ""the development set contains up to 5000 sentences randomly sampled from previous years'development sets ( 2010 - 2014 ) and remaining come from the development set for wmt15""]",5
,,,,3
"['the other hand,', ' #TAUTHOR_TAG attempted to']","['the other hand,', ' #TAUTHOR_TAG attempted to']","['the other hand,', ' #TAUTHOR_TAG attempted']","['task of describing a phrase in a given context. however, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase,', 'which makes the task of generating an accurate and coherent definition difficult ( perhaps as difficult as a human comprehending the phrase itself ). on the other hand,', ' #TAUTHOR_TAG attempted to generate a definition of a word from its word embedding induced from massive text, followed by  #AUTHOR_TAG that refers to a local context to define a', 'polysemous word with a local context by choosing relevant dimensions of their embeddings. although these research', 'efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. in this study, we tackle a task of describing ( defining ) a phrase when given its local context', 'as  #AUTHOR_TAG, while allowing access to other usage examples via word embeddings trained from massive text ( global contexts )  #AUTHOR_TAG. we present log - cad, a neural', 'network - based description generator ( figure 1 ) to directly solve this task. given a word with its context, our generator takes advantage', ""of the target word's embedding, pre - trained from massive text ( global contexts ), while also encoding the given local context, combining both to generate a natural language"", '']",0
['construct character - level cnns ( eq. ( 5 ) ) following  #TAUTHOR_TAG. note that the input to'],"[', we construct character - level cnns ( eq. ( 5 ) ) following  #TAUTHOR_TAG. note that the input to the cnns is']","['construct character - level cnns ( eq. ( 5 ) ) following  #TAUTHOR_TAG. note that the input to the cnns is a sequence of words in x trg, which are concatenated', 'with special character "", "" such as']","['of the decoder lstm, and y t−1 is a jointly - trained word embedding of', 'the previous output word y t−1. considering the fact that the local context can be relatively long ( e. g. around 20 words on average in the wikipedia dataset that will be introduced in the next section ) it', 'is hard for a decoder to focus on important words in local contexts. in order to deal with this problem, attention ( · ) function in eq. ( 4 ) decides which words in the local context', 'x to focus on at each time step. d t can be computed with an attention mechanism  #AUTHOR_TAG as where u h and u s are matrices that map the encoder and decoder hidden states into a common space, respectively. in order to capture prefixes and suffixes in x trg, we construct character - level cnns ( eq. ( 5 ) ) following  #TAUTHOR_TAG. note that the input to the cnns is a sequence of words in x trg, which are concatenated', 'with special character "", "" such as "" sonic boom. ""', 'following  #TAUTHOR_TAG, we set the kernels of length 2 - 6 and size 10, 30, 40, 40, 40', '']",0
"['.', 'recently,  #TAUTHOR_TAG introduced a task of generating']","['word.', 'recently,  #TAUTHOR_TAG introduced a task of generating']","['', 'recently,  #TAUTHOR_TAG introduced a task of']","['', 'paraphrasing  #AUTHOR_TAG ( or text simplification  #AUTHOR_TAG ) can be used to rephrase words with unknown senses.', 'however, the target of paraphrase acquisition are words ( or phrases ) with no specified context.', 'although several studies  #AUTHOR_TAG consider sub - sentential ( context - sensitive ) paraphrases, they do not intend to obtain a definition - like description as a paraphrase of a word.', 'recently,  #TAUTHOR_TAG introduced a task of generating a definition sentence of a word from its pre - trained embedding.', '']",0
"['.', 'recently,  #TAUTHOR_TAG introduced a task of generating']","['word.', 'recently,  #TAUTHOR_TAG introduced a task of generating']","['', 'recently,  #TAUTHOR_TAG introduced a task of']","['', 'paraphrasing  #AUTHOR_TAG ( or text simplification  #AUTHOR_TAG ) can be used to rephrase words with unknown senses.', 'however, the target of paraphrase acquisition are words ( or phrases ) with no specified context.', 'although several studies  #AUTHOR_TAG consider sub - sentential ( context - sensitive ) paraphrases, they do not intend to obtain a definition - like description as a paraphrase of a word.', 'recently,  #TAUTHOR_TAG introduced a task of generating a definition sentence of a word from its pre - trained embedding.', '']",0
"['the other hand,', ' #TAUTHOR_TAG attempted to']","['the other hand,', ' #TAUTHOR_TAG attempted to']","['the other hand,', ' #TAUTHOR_TAG attempted']","['task of describing a phrase in a given context. however, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase,', 'which makes the task of generating an accurate and coherent definition difficult ( perhaps as difficult as a human comprehending the phrase itself ). on the other hand,', ' #TAUTHOR_TAG attempted to generate a definition of a word from its word embedding induced from massive text, followed by  #AUTHOR_TAG that refers to a local context to define a', 'polysemous word with a local context by choosing relevant dimensions of their embeddings. although these research', 'efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. in this study, we tackle a task of describing ( defining ) a phrase when given its local context', 'as  #AUTHOR_TAG, while allowing access to other usage examples via word embeddings trained from massive text ( global contexts )  #AUTHOR_TAG. we present log - cad, a neural', 'network - based description generator ( figure 1 ) to directly solve this task. given a word with its context, our generator takes advantage', ""of the target word's embedding, pre - trained from massive text ( global contexts ), while also encoding the given local context, combining both to generate a natural language"", '']",1
"['the other hand,', ' #TAUTHOR_TAG attempted to']","['the other hand,', ' #TAUTHOR_TAG attempted to']","['the other hand,', ' #TAUTHOR_TAG attempted']","['task of describing a phrase in a given context. however, they follow the strict assumption that the target phrase is unknown and there is only a single local context available for the phrase,', 'which makes the task of generating an accurate and coherent definition difficult ( perhaps as difficult as a human comprehending the phrase itself ). on the other hand,', ' #TAUTHOR_TAG attempted to generate a definition of a word from its word embedding induced from massive text, followed by  #AUTHOR_TAG that refers to a local context to define a', 'polysemous word with a local context by choosing relevant dimensions of their embeddings. although these research', 'efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. in this study, we tackle a task of describing ( defining ) a phrase when given its local context', 'as  #AUTHOR_TAG, while allowing access to other usage examples via word embeddings trained from massive text ( global contexts )  #AUTHOR_TAG. we present log - cad, a neural', 'network - based description generator ( figure 1 ) to directly solve this task. given a word with its context, our generator takes advantage', ""of the target word's embedding, pre - trained from massive text ( global contexts ), while also encoding the given local context, combining both to generate a natural language"", '']",4
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['link prediction  #TAUTHOR_TAG,']","['graphs have become a very useful framework to organize and store knowledge.', 'their interconnected nature is not just a natural way to represent facts, but it has potential that the separate storage of facts does not have, such as : ( i ) we can use it as a relational model of meaning, and derive jointly representations for nodes ( entities ) and edges ( relations ) ; ( ii ) the structure can be explored to discover systematic patterns that reveal interesting and exploitable regularities, such as paths connecting nodes in direct relations, ( iii ) discovering and inducing new connections.', 'link prediction methods in knowledge graphs ( see  #AUTHOR_TAG for an overview ) predict additional edges in the graph, based on induced node and edge representations that encode the structure of the graph and thus capture regularities ( such as homophily ).', ' #AUTHOR_TAG introduced a new method that predicts direct links based on paths that connect the source and target nodes.', 'such paths are not only useful for link prediction  #TAUTHOR_TAG, but also for finding explanations for direct links and help with targeted information extraction to fill in incomplete knowledge repositories  #AUTHOR_TAG.', 'these approaches rely on the structure of the knowledge graph, which is inherently incomplete.', '']",0
['incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG'],['incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG'],"['of link prediction  #AUTHOR_TAG, and extended to incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG']","['embedding models while rescal  #AUTHOR_TAG, distmult  #AUTHOR_TAG, transe  #AUTHOR_TAG, complex  #AUTHOR_TAG are examples of latent factor models.  #AUTHOR_TAG introduced a novel way to exploit information in knowledge graphs : using weighted extracted paths as features in four different recommendation tasks, which can be modeled as typed proximity queries. the idea of using paths in the graph has then', 'been applied to the task of link prediction  #AUTHOR_TAG, and extended to incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG obtain paths for given node pairs using random walks over the', '']",0
['incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG'],['incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG'],"['of link prediction  #AUTHOR_TAG, and extended to incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG']","['embedding models while rescal  #AUTHOR_TAG, distmult  #AUTHOR_TAG, transe  #AUTHOR_TAG, complex  #AUTHOR_TAG are examples of latent factor models.  #AUTHOR_TAG introduced a novel way to exploit information in knowledge graphs : using weighted extracted paths as features in four different recommendation tasks, which can be modeled as typed proximity queries. the idea of using paths in the graph has then', 'been applied to the task of link prediction  #AUTHOR_TAG, and extended to incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG obtain paths for given node pairs using random walks over the', '']",0
"['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['path ranking algorithm formalism originally proposed by  #AUTHOR_TAG performs two main steps to represent of a pair of nodes in a graph : ( i ) feature selection - adding paths that connect the node pair ; ( ii ) feature computation - table 1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and their abstract versions associating a value for each added path.', 'obtaining paths from a large graph is a computationally intensive problem, particularly in graphs that have numerous nodes with high degrees.', '']",0
"['g.  #TAUTHOR_TAG, ( ii ) to fill in larger portions of']","['( i ) as features in a link prediction system ( e. g.  #TAUTHOR_TAG, ( ii ) to fill in larger portions of']","['g.  #TAUTHOR_TAG, ( ii ) to fill in larger portions of the graph by producing, rather than']","['abstract paths are hypothetical paths that could connect the source s and target t of a < s, r, t > tuple.', 'they can be used in different ways, e. g. ( i ) as features in a link prediction system ( e. g.  #TAUTHOR_TAG, ( ii ) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances.', 'in the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data.', 'after finding the set of abstract paths { π i, r } associated with a relation r, for a given instance of the relation r - < s, r, t > - we can ( try to ) ground the paths as follows : ( i ) we first eliminate set relations from the abstract paths : at this point set relations between relation types domain and ranges are not useful ( they were necessary only for the connectivity and search process in the abstract graph ).', '']",0
['described in  #TAUTHOR_TAG'],['described in  #TAUTHOR_TAG'],"['described in  #TAUTHOR_TAG.', 'we then use the extracted paths']","['build abstract graphs and paths from the freebase and nell data described in  #TAUTHOR_TAG.', 'we then use the extracted paths for link prediction.', 'the graphs built by  #TAUTHOR_TAG triples extracted from dependency parses of clueweb documents.', 'table 1 shows the statistics for each original and abstract graph.', '']",0
['described in  #TAUTHOR_TAG'],['described in  #TAUTHOR_TAG'],"['described in  #TAUTHOR_TAG.', 'we then use the extracted paths']","['build abstract graphs and paths from the freebase and nell data described in  #TAUTHOR_TAG.', 'we then use the extracted paths for link prediction.', 'the graphs built by  #TAUTHOR_TAG triples extracted from dependency parses of clueweb documents.', 'table 1 shows the statistics for each original and abstract graph.', '']",0
,,,,0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['link prediction  #TAUTHOR_TAG,']","['graphs have become a very useful framework to organize and store knowledge.', 'their interconnected nature is not just a natural way to represent facts, but it has potential that the separate storage of facts does not have, such as : ( i ) we can use it as a relational model of meaning, and derive jointly representations for nodes ( entities ) and edges ( relations ) ; ( ii ) the structure can be explored to discover systematic patterns that reveal interesting and exploitable regularities, such as paths connecting nodes in direct relations, ( iii ) discovering and inducing new connections.', 'link prediction methods in knowledge graphs ( see  #AUTHOR_TAG for an overview ) predict additional edges in the graph, based on induced node and edge representations that encode the structure of the graph and thus capture regularities ( such as homophily ).', ' #AUTHOR_TAG introduced a new method that predicts direct links based on paths that connect the source and target nodes.', 'such paths are not only useful for link prediction  #TAUTHOR_TAG, but also for finding explanations for direct links and help with targeted information extraction to fill in incomplete knowledge repositories  #AUTHOR_TAG.', 'these approaches rely on the structure of the knowledge graph, which is inherently incomplete.', '']",5
"['( the version used by  #TAUTHOR_TAG and in this paper ).', '']","['and nodes frequencies for freebase and nell ( the version used by  #TAUTHOR_TAG and in this paper ).', '']","['( the version used by  #TAUTHOR_TAG and in this paper ).', 'every data point is the degree of a node ( top plots ),']","['', ""where the weight of a set relation between kg a's nodes quantifies the overlap between the two sets :"", 'figure 1 : knowledge graphs statistics on a logarithmic scale : relation and nodes frequencies for freebase and nell ( the version used by  #TAUTHOR_TAG and in this paper ).', 'every data point is the degree of a node ( top plots ), or frequency of a relation ( bottom plots ).', 'the data points are ordered monotonically, the x axis is just an index.', ""building such a graph makes sense only for knowledge repositories that have strongly typed relations - like freebase and nell - but we do not require knowledge of the types of the relations'domains and ranges."", 'such information is not finegrained enough : for example, the relation capital has a type city as a domain, but capital cities are a very small subset of the set of all cities.', 'using an "" atomic "" node to represent the domain / range of a relation would not allow us to make finer grained connections and distinctions between the domains and ranges of the existing relations.', 'figure 2 shows a subset of the abstract graph built from the freebase dataset.', '']",5
"['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['path ranking algorithm formalism originally proposed by  #AUTHOR_TAG performs two main steps to represent of a pair of nodes in a graph : ( i ) feature selection - adding paths that connect the node pair ; ( ii ) feature computation - table 1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and their abstract versions associating a value for each added path.', 'obtaining paths from a large graph is a computationally intensive problem, particularly in graphs that have numerous nodes with high degrees.', '']",5
"['g.  #TAUTHOR_TAG, ( ii ) to fill in larger portions of']","['( i ) as features in a link prediction system ( e. g.  #TAUTHOR_TAG, ( ii ) to fill in larger portions of']","['g.  #TAUTHOR_TAG, ( ii ) to fill in larger portions of the graph by producing, rather than']","['abstract paths are hypothetical paths that could connect the source s and target t of a < s, r, t > tuple.', 'they can be used in different ways, e. g. ( i ) as features in a link prediction system ( e. g.  #TAUTHOR_TAG, ( ii ) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances.', 'in the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data.', 'after finding the set of abstract paths { π i, r } associated with a relation r, for a given instance of the relation r - < s, r, t > - we can ( try to ) ground the paths as follows : ( i ) we first eliminate set relations from the abstract paths : at this point set relations between relation types domain and ranges are not useful ( they were necessary only for the connectivity and search process in the abstract graph ).', '']",5
"['using pra, we use the experimental set - up of  #TAUTHOR_TAG, where we replace the feature selection']","['using pra, we use the experimental set - up of  #TAUTHOR_TAG, where we replace the feature selection']","['we want to compare the abstract paths found using the abstract graph with paths found using pra, we use the experimental set - up of  #TAUTHOR_TAG, where we replace the feature selection']","['we want to compare the abstract paths found using the abstract graph with paths found using pra, we use the experimental set - up of  #TAUTHOR_TAG, where we replace the feature selection and feature computation steps with the approach presented here.', 'a big difference will be caused by the negative sampling, which also makes the results not directly comparable.', 'the issues are explained in the negative sampling paragraph below.', 'the data thus obtained is used for training a linear regression model ( similarly to  #TAUTHOR_TAG, and tested on the provided test sets and evaluated using mean average precision ( map )']",5
['described in  #TAUTHOR_TAG'],['described in  #TAUTHOR_TAG'],"['described in  #TAUTHOR_TAG.', 'we then use the extracted paths']","['build abstract graphs and paths from the freebase and nell data described in  #TAUTHOR_TAG.', 'we then use the extracted paths for link prediction.', 'the graphs built by  #TAUTHOR_TAG triples extracted from dependency parses of clueweb documents.', 'table 1 shows the statistics for each original and abstract graph.', '']",5
['described in  #TAUTHOR_TAG'],['described in  #TAUTHOR_TAG'],"['described in  #TAUTHOR_TAG.', 'we then use the extracted paths']","['build abstract graphs and paths from the freebase and nell data described in  #TAUTHOR_TAG.', 'we then use the extracted paths for link prediction.', 'the graphs built by  #TAUTHOR_TAG triples extracted from dependency parses of clueweb documents.', 'table 1 shows the statistics for each original and abstract graph.', '']",5
,,,,5
,,,,5
,,,,5
,,,,5
['incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG'],['incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG'],"['of link prediction  #AUTHOR_TAG, and extended to incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG']","['embedding models while rescal  #AUTHOR_TAG, distmult  #AUTHOR_TAG, transe  #AUTHOR_TAG, complex  #AUTHOR_TAG are examples of latent factor models.  #AUTHOR_TAG introduced a novel way to exploit information in knowledge graphs : using weighted extracted paths as features in four different recommendation tasks, which can be modeled as typed proximity queries. the idea of using paths in the graph has then', 'been applied to the task of link prediction  #AUTHOR_TAG, and extended to incorporate textual information  #TAUTHOR_TAG.  #AUTHOR_TAG obtain paths for given node pairs using random walks over the', '']",3
"['using pra, we use the experimental set - up of  #TAUTHOR_TAG, where we replace the feature selection']","['using pra, we use the experimental set - up of  #TAUTHOR_TAG, where we replace the feature selection']","['we want to compare the abstract paths found using the abstract graph with paths found using pra, we use the experimental set - up of  #TAUTHOR_TAG, where we replace the feature selection']","['we want to compare the abstract paths found using the abstract graph with paths found using pra, we use the experimental set - up of  #TAUTHOR_TAG, where we replace the feature selection and feature computation steps with the approach presented here.', 'a big difference will be caused by the negative sampling, which also makes the results not directly comparable.', 'the issues are explained in the negative sampling paragraph below.', 'the data thus obtained is used for training a linear regression model ( similarly to  #TAUTHOR_TAG, and tested on the provided test sets and evaluated using mean average precision ( map )']",3
"['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and']","['path ranking algorithm formalism originally proposed by  #AUTHOR_TAG performs two main steps to represent of a pair of nodes in a graph : ( i ) feature selection - adding paths that connect the node pair ; ( ii ) feature computation - table 1 : graph statistics on the datasets used by  #TAUTHOR_TAG, and their abstract versions associating a value for each added path.', 'obtaining paths from a large graph is a computationally intensive problem, particularly in graphs that have numerous nodes with high degrees.', '']",4
,,,,4
"[' #TAUTHOR_TAG 1, a recently released large - scale non - factoid qa']","['selection datasets : ( 1 ) insuranceqa  #TAUTHOR_TAG 1, a recently released large - scale non - factoid qa']","['generation according to the question context.', 'we report experimental results for two answer selection datasets : ( 1 ) insuranceqa  #TAUTHOR_TAG 1, a recently released large - scale non - factoid qa dataset from']","['', 'in order to obtain better embeddings for the questions and answers, we build a convolutional neural network ( cnn ) structure on top of bilstm.', 'secondly, in order to better distinguish candidate answers according to the question, we introduce a simple but efficient attention model to this framework for the answer embedding generation according to the question context.', 'we report experimental results for two answer selection datasets : ( 1 ) insuranceqa  #TAUTHOR_TAG 1, a recently released large - scale non - factoid qa dataset from the insurance domain.', 'the rest of the paper is organized as follows : section 2 describes the related work for answer selection ; section 3 provides the details of the proposed models ; experimental settings and results of insuranceqa and trec - qa datasets are discussed in section 4 and 5 respectively ; finally, we draw conclusions in section 6']",5
['similarity metrics  #TAUTHOR_TAG dos  #AUTHOR_TAG'],['similarity metrics  #TAUTHOR_TAG dos  #AUTHOR_TAG'],"['by certain similarity metrics  #TAUTHOR_TAG dos  #AUTHOR_TAG.', 'secondly,']","['', 'some work tried to fulfill the matching using minimal edit sequences between dependency parse trees  #AUTHOR_TAG.', 'recently, discriminative tree - edit features extraction and engineering over parsing trees were automated in  #AUTHOR_TAG.', 'while these methods show effectiveness, they might suffer from the availability of additional resources, the effort of feature engineering and the systematic complexity by introducing linguistic tools, such as parse trees and dependency trees.', 'there were prior methods using deep learning technologies for the answer selection task.', 'the approaches for non - factoid question answering generally pursue the solution on the following directions : firstly, the question and answer representations are learned and matched by certain similarity metrics  #TAUTHOR_TAG dos  #AUTHOR_TAG.', '']",5
"['same ranking loss in  #TAUTHOR_TAG, we define']","['same ranking loss in  #TAUTHOR_TAG, we define']","['basic model in this work is shown in figure 1.', 'bilstm generates distributed representations for both the question and answer independently, and then utilize cosine similarity to measure their distance.', 'following the same ranking loss in  #TAUTHOR_TAG, we define the training objective as a hinge loss.', 'where a + is']","['basic model in this work is shown in figure 1.', 'bilstm generates distributed representations for both the question and answer independently, and then utilize cosine similarity to measure their distance.', 'following the same ranking loss in  #TAUTHOR_TAG, we define the training objective as a hinge loss.', '']",5
"['same ranking loss in  #TAUTHOR_TAG, we define']","['same ranking loss in  #TAUTHOR_TAG, we define']","['basic model in this work is shown in figure 1.', 'bilstm generates distributed representations for both the question and answer independently, and then utilize cosine similarity to measure their distance.', 'following the same ranking loss in  #TAUTHOR_TAG, we define the training objective as a hinge loss.', 'where a + is']","['basic model in this work is shown in figure 1.', 'bilstm generates distributed representations for both the question and answer independently, and then utilize cosine similarity to measure their distance.', 'following the same ranking loss in  #TAUTHOR_TAG, we define the training objective as a hinge loss.', '']",5
"[', provided by  #TAUTHOR_TAG.', 'the insurance']","['dataset, insuranceqa, provided by  #TAUTHOR_TAG.', 'the insuranceqa']","['described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, insuranceqa, provided by  #TAUTHOR_TAG.', 'the insuranceqa dataset provides a training set,']","['described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, insuranceqa, provided by  #TAUTHOR_TAG.', 'the insuranceqa dataset provides a training set, a validation set, and two test sets.', ""we do not see obvious categorical differentiation between two tests'questions."", 'one can see the details of insuranceqa data in  #TAUTHOR_TAG.', 'we list the numbers of questions and answers of the dataset in table 1.', 'a question may correspond to multiple answers.', 'the questions are much shorter than answers.', 'the average length of questions is 7, and the average length of answers is 94.', 'the long answers comparing to the questions post challenges for answer selection task.', 'this corpus contains 24981 unique answers in total.', 'for the development and test sets, the dataset also includes an answer pool of 500 candidate answers for each question.', 'these answer pools were constructed by including the correct answer ( s ) and randomly selecting candidate from the complete set of unique answers.', 'the top - 1 accuracy of the answer pool is reported']",5
"[', provided by  #TAUTHOR_TAG.', 'the insurance']","['dataset, insuranceqa, provided by  #TAUTHOR_TAG.', 'the insuranceqa']","['described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, insuranceqa, provided by  #TAUTHOR_TAG.', 'the insuranceqa dataset provides a training set,']","['described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, insuranceqa, provided by  #TAUTHOR_TAG.', 'the insuranceqa dataset provides a training set, a validation set, and two test sets.', ""we do not see obvious categorical differentiation between two tests'questions."", 'one can see the details of insuranceqa data in  #TAUTHOR_TAG.', 'we list the numbers of questions and answers of the dataset in table 1.', 'a question may correspond to multiple answers.', 'the questions are much shorter than answers.', 'the average length of questions is 7, and the average length of answers is 94.', 'the long answers comparing to the questions post challenges for answer selection task.', 'this corpus contains 24981 unique answers in total.', 'for the development and test sets, the dataset also includes an answer pool of 500 candidate answers for each question.', 'these answer pools were constructed by including the correct answer ( s ) and randomly selecting candidate from the complete set of unique answers.', 'the top - 1 accuracy of the answer pool is reported']",5
"['be referred to  #AUTHOR_TAG.', 'we implemented the architecture ii in  #TAUTHOR_TAG from scratch.', ' #AUTHOR_TAG and  #TAUTHOR_TAG are the best baselines on map']","['be referred to  #AUTHOR_TAG.', 'we implemented the architecture ii in  #TAUTHOR_TAG from scratch.', ' #AUTHOR_TAG and  #TAUTHOR_TAG are the best baselines on map']","['are calculated using the official evaluation scripts.', 'in table 4, we list the performance of some prior work on this dataset, which can be referred to  #AUTHOR_TAG.', 'we implemented the architecture ii in  #TAUTHOR_TAG from scratch.', ' #AUTHOR_TAG and  #TAUTHOR_TAG are the best baselines on map']","['this paper, we adopt trec - qa, created by  #AUTHOR_TAG following previous work on this task, we use mean average precision ( map ) and mean reciprocal rank ( mrr ) as evaluation metrics, which are calculated using the official evaluation scripts.', 'in table 4, we list the performance of some prior work on this dataset, which can be referred to  #AUTHOR_TAG.', 'we implemented the architecture ii in  #TAUTHOR_TAG from scratch.', ' #AUTHOR_TAG and  #TAUTHOR_TAG are the best baselines on map and mrr respectively']",5
"['same ranking loss in  #TAUTHOR_TAG, we define']","['same ranking loss in  #TAUTHOR_TAG, we define']","['basic model in this work is shown in figure 1.', 'bilstm generates distributed representations for both the question and answer independently, and then utilize cosine similarity to measure their distance.', 'following the same ranking loss in  #TAUTHOR_TAG, we define the training objective as a hinge loss.', 'where a + is']","['basic model in this work is shown in figure 1.', 'bilstm generates distributed representations for both the question and answer independently, and then utilize cosine similarity to measure their distance.', 'following the same ranking loss in  #TAUTHOR_TAG, we define the training objective as a hinge loss.', '']",3
"['ii in  #TAUTHOR_TAG, except that']","['ii in  #TAUTHOR_TAG, except that']","[') in table 2 ).', 'row f shared a highly analogous cnn structure with architecture ii in  #TAUTHOR_TAG, except that the later used a shallow hidden layer']","['', 'row f shared a highly analogous cnn structure with architecture ii in  #TAUTHOR_TAG, except that the later used a shallow hidden layer to transform the word embeddings into the input of cnn structure, while row f take the output of bilstm as cnn input.', 'row ( g ) and ( h ) corresponds to qa - lstm with the attention model.', '']",3
"['.', 'architecture - ii in  #TAUTHOR_TAG : instead of using lstm, a cnn model is employed']","['answer.', 'architecture - ii in  #TAUTHOR_TAG : instead of using lstm, a cnn model is employed']","['', 'architecture - ii in  #TAUTHOR_TAG : instead of using lstm, a cnn model is employed']","['idf - weighted sum of word vectors for the question and for all of its answer candidates is used as a feature vector.', 'similar to this work, the candidates are re - ranked according the cosine similarity to a question.', 'metzler - bendersky ir model : a state - of - the - art weighted dependency ( wd ) model, which employs a weighted combination of term - based and term proximity - based ranking features to score each candidate answer.', 'architecture - ii in  #TAUTHOR_TAG : instead of using lstm, a cnn model is employed to learn a distributed vector representation of a given question and its answer candidates, and the answers are scored by cosine similarity with the question.', 'no attention model is used in this baseline']",0
"['##d is used to measure the distance between the question and answers.', 'this is the model which achieved the best performance in  #TAUTHOR_TAG']","['##d is used to measure the distance between the question and answers.', 'this is the model which achieved the best performance in  #TAUTHOR_TAG']","['##d is used to measure the distance between the question and answers.', 'this is the model which achieved the best performance in  #TAUTHOR_TAG']","['##d is used to measure the distance between the question and answers.', 'this is the model which achieved the best performance in  #TAUTHOR_TAG']",0
"['be referred to  #AUTHOR_TAG.', 'we implemented the architecture ii in  #TAUTHOR_TAG from scratch.', ' #AUTHOR_TAG and  #TAUTHOR_TAG are the best baselines on map']","['be referred to  #AUTHOR_TAG.', 'we implemented the architecture ii in  #TAUTHOR_TAG from scratch.', ' #AUTHOR_TAG and  #TAUTHOR_TAG are the best baselines on map']","['are calculated using the official evaluation scripts.', 'in table 4, we list the performance of some prior work on this dataset, which can be referred to  #AUTHOR_TAG.', 'we implemented the architecture ii in  #TAUTHOR_TAG from scratch.', ' #AUTHOR_TAG and  #TAUTHOR_TAG are the best baselines on map']","['this paper, we adopt trec - qa, created by  #AUTHOR_TAG following previous work on this task, we use mean average precision ( map ) and mean reciprocal rank ( mrr ) as evaluation metrics, which are calculated using the official evaluation scripts.', 'in table 4, we list the performance of some prior work on this dataset, which can be referred to  #AUTHOR_TAG.', 'we implemented the architecture ii in  #TAUTHOR_TAG from scratch.', ' #AUTHOR_TAG and  #TAUTHOR_TAG are the best baselines on map and mrr respectively']",0
['.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of'],['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the'],['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the'],"['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the knowledge base entities. the attributes of these entities', 'are used during coreference by incorporating them in the mention features. since alignment of mentions to the external entities is itself a difficult task, these systems favor', 'high - precision linking. unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer ( such as the non - transcript documents in  #TAUTHOR_TAG.  #AUTHOR_TAG link each mention to multiple entities', '']",0
['.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of'],['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the'],['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the'],"['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the knowledge base entities. the attributes of these entities', 'are used during coreference by incorporating them in the mention features. since alignment of mentions to the external entities is itself a difficult task, these systems favor', 'high - precision linking. unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer ( such as the non - transcript documents in  #TAUTHOR_TAG.  #AUTHOR_TAG link each mention to multiple entities', '']",0
"['', 'linking is often noisy, so only selecting the high - precision links as in  #TAUTHOR_TAG']","['lists empty.', 'linking is often noisy, so only selecting the high - precision links as in  #TAUTHOR_TAG']","['', 'linking is often noisy, so only selecting the high - precision links as in  #TAUTHOR_TAG results in too few matches, while picking an aggregation of all links results in more noise']","['create the initial entity candidate lists for proper noun mentions, we query a knowledge base searcher  #AUTHOR_TAG with the text of these mentions.', 'these queries return scored, ranked lists of entity candidates ( wikipedia articles ), which we associate with each proper noun mention, leaving the rest of the candidate lists empty.', 'linking is often noisy, so only selecting the high - precision links as in  #TAUTHOR_TAG results in too few matches, while picking an aggregation of all links results in more noise due to lower precision  #AUTHOR_TAG.', 'additionally, since linking is often performed in pre - processing, two mentions that are determined coreferent during inference could still be linked to different kb entities.', 'to avoid these problems, we keep a list of candidate links for each mention, merging the lists when two mentions are determined coreferent, and rerank this list during inference']",0
"[' #TAUTHOR_TAG, however']","[' #TAUTHOR_TAG, however']","[' #TAUTHOR_TAG, however']","['linking to wikipedia, we have a list of candidate kb entities for each mention.', 'each entity has access to external information keyed on the wikipedia article, but this information could more generally come from any knowledge base.', 'given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine - grained wikipedia categories as used by  #TAUTHOR_TAG, however most of these features may not be relevant to the task of within - document coreference.', 'instead, an important resource for linking non - proper mentions of an entity is to identify the possible name variations of the entity.', 'for example, it would be useful to know that massachusetts is also referred to as "" the 6th state "", however this information is not readily available from wikipedia.', '1 we instead use the corpus described in  #AUTHOR_TAG that consists of anchor texts of links to wikipedia that appear on web pages.', 'this collection of anchor texts is sufficiently extensive to cover many common misspellings of entity names, as well as many name variations missing from wikipedia.', 'for example, for the entity "" massachusetts "", our anchor texts include misspellings like "" massachussetts "" and "" messuchusetts "", and the ( debatably ) affectionate nickname of "" taxachusetts "" - none of which are found in wikipedia.', 'using these anchor texts, each entity candidate provides a rich set of name variations that we use for disambiguation, as described in the next section']",0
"['', ' #TAUTHOR_TAG extend the multi - sie']","['time.', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying']","['at any time.', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying']","['', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying at most a single candidate for each mention, and incorporating high - precision attributes extracted from wikipedia.', 'the high - precision mention - candidate pairings are precomputed and fixed ; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied.', 'with these restrictions, they show improvements over the state - ofthe - art on a subset of ace mentions that are more easily aligned to wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts.', 'there are a number of approaches that provide an alignment from mentions in a document to wikipedia.', 'wikifier  #AUTHOR_TAG analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in  #TAUTHOR_TAG.', ' #AUTHOR_TAG introduce an approximation to the above approach, but incorporate retrieval - based supervised reranking that provides multiple candidates and scores ; this approach performed competitively on previous tac - kbp entity linking benchmarks  #AUTHOR_TAG.', 'alignment to an external knowledge - base has improved performance for a number of nlp and information extraction tasks, such']",0
"['', ' #TAUTHOR_TAG extend the multi - sie']","['time.', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying']","['at any time.', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying']","['', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying at most a single candidate for each mention, and incorporating high - precision attributes extracted from wikipedia.', 'the high - precision mention - candidate pairings are precomputed and fixed ; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied.', 'with these restrictions, they show improvements over the state - ofthe - art on a subset of ace mentions that are more easily aligned to wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts.', 'there are a number of approaches that provide an alignment from mentions in a document to wikipedia.', 'wikifier  #AUTHOR_TAG analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in  #TAUTHOR_TAG.', ' #AUTHOR_TAG introduce an approximation to the above approach, but incorporate retrieval - based supervised reranking that provides multiple candidates and scores ; this approach performed competitively on previous tac - kbp entity linking benchmarks  #AUTHOR_TAG.', 'alignment to an external knowledge - base has improved performance for a number of nlp and information extraction tasks, such']",0
['.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of'],['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the'],['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the'],"['research.  #AUTHOR_TAG and  #TAUTHOR_TAG precompute a fixed alignment of the mentions to the knowledge base entities. the attributes of these entities', 'are used during coreference by incorporating them in the mention features. since alignment of mentions to the external entities is itself a difficult task, these systems favor', 'high - precision linking. unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer ( such as the non - transcript documents in  #TAUTHOR_TAG.  #AUTHOR_TAG link each mention to multiple entities', '']",5
"[',  #TAUTHOR_TAG only use the']","['transcripts provide an additional challenge for alignment and coreference,  #TAUTHOR_TAG only use the']","[',  #TAUTHOR_TAG only use the set']","['quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document.', 'in particular, table 2 : evaluation on the ace test data, with the system trained on the train and development sets.', 'ace contains a large number of broadcast news documents, many of which consist of transcribed data containing noise in the form of incomplete sentences and disfluencies.', 'since these transcripts provide an additional challenge for alignment and coreference,  #TAUTHOR_TAG only use the set of non - transcripts for their evaluation.', 'using dynamic linking and a large set of surface string variations, our approach may be able to provide an improvement even on the transcripts.', 'to identify the transcripts in the test set, we use the approximation from  #TAUTHOR_TAG that considers a document to be non - transcribed if it contains proper noun mentions and at least a third of those start with a capital letter.', 'the performance is shown in table 3, while the improvement over our baseline is shown in figure 3.', 'our static linking matches the performance of  #TAUTHOR_TAG on the non - transcripts.', 'further, the improvement of static linking on the transcripts over the baseline is lower than that on the non - transcript data, suggesting that noisy mentions and text result in poor quality alignment.', 'dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than on non - transcripts.', 'this indicates that dynamic linking approach is robust to noise, and its wider variety of surface strings and flexible alignments are especially useful for transcripts']",5
"['', 'linking is often noisy, so only selecting the high - precision links as in  #TAUTHOR_TAG']","['lists empty.', 'linking is often noisy, so only selecting the high - precision links as in  #TAUTHOR_TAG']","['', 'linking is often noisy, so only selecting the high - precision links as in  #TAUTHOR_TAG results in too few matches, while picking an aggregation of all links results in more noise']","['create the initial entity candidate lists for proper noun mentions, we query a knowledge base searcher  #AUTHOR_TAG with the text of these mentions.', 'these queries return scored, ranked lists of entity candidates ( wikipedia articles ), which we associate with each proper noun mention, leaving the rest of the candidate lists empty.', 'linking is often noisy, so only selecting the high - precision links as in  #TAUTHOR_TAG results in too few matches, while picking an aggregation of all links results in more noise due to lower precision  #AUTHOR_TAG.', 'additionally, since linking is often performed in pre - processing, two mentions that are determined coreferent during inference could still be linked to different kb entities.', 'to avoid these problems, we keep a list of candidate links for each mention, merging the lists when two mentions are determined coreferent, and rerank this list during inference']",4
"[' #TAUTHOR_TAG, however']","[' #TAUTHOR_TAG, however']","[' #TAUTHOR_TAG, however']","['linking to wikipedia, we have a list of candidate kb entities for each mention.', 'each entity has access to external information keyed on the wikipedia article, but this information could more generally come from any knowledge base.', 'given these entities, there are many possible features that may be used for disambiguation of the mentions, such as gender and fine - grained wikipedia categories as used by  #TAUTHOR_TAG, however most of these features may not be relevant to the task of within - document coreference.', 'instead, an important resource for linking non - proper mentions of an entity is to identify the possible name variations of the entity.', 'for example, it would be useful to know that massachusetts is also referred to as "" the 6th state "", however this information is not readily available from wikipedia.', '1 we instead use the corpus described in  #AUTHOR_TAG that consists of anchor texts of links to wikipedia that appear on web pages.', 'this collection of anchor texts is sufficiently extensive to cover many common misspellings of entity names, as well as many name variations missing from wikipedia.', 'for example, for the entity "" massachusetts "", our anchor texts include misspellings like "" massachussetts "" and "" messuchusetts "", and the ( debatably ) affectionate nickname of "" taxachusetts "" - none of which are found in wikipedia.', 'using these anchor texts, each entity candidate provides a rich set of name variations that we use for disambiguation, as described in the next section']",4
"[',  #TAUTHOR_TAG only use the']","['transcripts provide an additional challenge for alignment and coreference,  #TAUTHOR_TAG only use the']","[',  #TAUTHOR_TAG only use the set']","['quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document.', 'in particular, table 2 : evaluation on the ace test data, with the system trained on the train and development sets.', 'ace contains a large number of broadcast news documents, many of which consist of transcribed data containing noise in the form of incomplete sentences and disfluencies.', 'since these transcripts provide an additional challenge for alignment and coreference,  #TAUTHOR_TAG only use the set of non - transcripts for their evaluation.', 'using dynamic linking and a large set of surface string variations, our approach may be able to provide an improvement even on the transcripts.', 'to identify the transcripts in the test set, we use the approximation from  #TAUTHOR_TAG that considers a document to be non - transcribed if it contains proper noun mentions and at least a third of those start with a capital letter.', 'the performance is shown in table 3, while the improvement over our baseline is shown in figure 3.', 'our static linking matches the performance of  #TAUTHOR_TAG on the non - transcripts.', 'further, the improvement of static linking on the transcripts over the baseline is lower than that on the non - transcript data, suggesting that noisy mentions and text result in poor quality alignment.', 'dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than on non - transcripts.', 'this indicates that dynamic linking approach is robust to noise, and its wider variety of surface strings and flexible alignments are especially useful for transcripts']",4
"['', ' #TAUTHOR_TAG extend the multi - sie']","['time.', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying']","['at any time.', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying']","['', 'further, they do not provide a comparison on a standard dataset.', ' #TAUTHOR_TAG extend the multi - sieve coreference model  #AUTHOR_TAG by identifying at most a single candidate for each mention, and incorporating high - precision attributes extracted from wikipedia.', 'the high - precision mention - candidate pairings are precomputed and fixed ; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied.', 'with these restrictions, they show improvements over the state - ofthe - art on a subset of ace mentions that are more easily aligned to wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts.', 'there are a number of approaches that provide an alignment from mentions in a document to wikipedia.', 'wikifier  #AUTHOR_TAG analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in  #TAUTHOR_TAG.', ' #AUTHOR_TAG introduce an approximation to the above approach, but incorporate retrieval - based supervised reranking that provides multiple candidates and scores ; this approach performed competitively on previous tac - kbp entity linking benchmarks  #AUTHOR_TAG.', 'alignment to an external knowledge - base has improved performance for a number of nlp and information extraction tasks, such']",4
['of  #AUTHOR_TAG and  #TAUTHOR_TAG'],['of  #AUTHOR_TAG and  #TAUTHOR_TAG'],['of  #AUTHOR_TAG and  #TAUTHOR_TAG'],"['', 'wikipedia linking : as a simple baseline, we directly evaluate the quality of the alignment for coreference by merging all pairs of proper noun mentions that share at least one common candidate, as per kb bridge.', 'further, the non - pronoun mentions are linked to these proper nouns if the mention string matches any of the entity titles or anchor texts.', ' #AUTHOR_TAG : a pairwise coreference model containing a rich set of features, as described and evaluated in  #AUTHOR_TAG.', 'baseline : our implementation of a pairwise model that is similar to the approach in  #AUTHOR_TAG with the differences described in section 2.', 'this is our baseline system that performs coreference without the use of external knowledge.', 'incidentally, it outperforms  #AUTHOR_TAG.', 'dynamic linking : this is our complete system as described in section 3, in which the list of candidates associated with each mention is reranked and modified during inference.', 'static linking : identical to dynamic linking except that entity candidate lists are not merged during inference ( i. e., algorithm 1 without line 17 ).', 'this approach is comparable to the fixed alignment model, as in the approaches of  #AUTHOR_TAG and  #TAUTHOR_TAG']",3
"[',  #TAUTHOR_TAG only use the']","['transcripts provide an additional challenge for alignment and coreference,  #TAUTHOR_TAG only use the']","[',  #TAUTHOR_TAG only use the set']","['quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document.', 'in particular, table 2 : evaluation on the ace test data, with the system trained on the train and development sets.', 'ace contains a large number of broadcast news documents, many of which consist of transcribed data containing noise in the form of incomplete sentences and disfluencies.', 'since these transcripts provide an additional challenge for alignment and coreference,  #TAUTHOR_TAG only use the set of non - transcripts for their evaluation.', 'using dynamic linking and a large set of surface string variations, our approach may be able to provide an improvement even on the transcripts.', 'to identify the transcripts in the test set, we use the approximation from  #TAUTHOR_TAG that considers a document to be non - transcribed if it contains proper noun mentions and at least a third of those start with a capital letter.', 'the performance is shown in table 3, while the improvement over our baseline is shown in figure 3.', 'our static linking matches the performance of  #TAUTHOR_TAG on the non - transcripts.', 'further, the improvement of static linking on the transcripts over the baseline is lower than that on the non - transcript data, suggesting that noisy mentions and text result in poor quality alignment.', 'dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than on non - transcripts.', 'this indicates that dynamic linking approach is robust to noise, and its wider variety of surface strings and flexible alignments are especially useful for transcripts']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG tsarfaty'],[' #TAUTHOR_TAG tsarfaty'],"['morphologically rich languages ( mrls ) is difficult due to the complex relationship of syntax to morphology.', 'but the success of neural networks offer an appealing solution to this problem by computing word representation from characters.', 'character - level models  #AUTHOR_TAG learn relationship between similar word forms and have shown to be effective for parsing mrls  #AUTHOR_TAG bjorkelund et al., 2017 ).', 'does that mean that we can do away with explicit modeling of morphology altogether? consider two challenges in parsing mrls raised by  #TAUTHOR_TAG tsarfaty et al. (, 2013 :', '• can we represent words abstractly so as to reflect shared morphological aspects between them? • which types of morphological information should we include in the parsing model?', '']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG tsarfaty'],[' #TAUTHOR_TAG tsarfaty'],"['morphologically rich languages ( mrls ) is difficult due to the complex relationship of syntax to morphology.', 'but the success of neural networks offer an appealing solution to this problem by computing word representation from characters.', 'character - level models  #AUTHOR_TAG learn relationship between similar word forms and have shown to be effective for parsing mrls  #AUTHOR_TAG bjorkelund et al., 2017 ).', 'does that mean that we can do away with explicit modeling of morphology altogether? consider two challenges in parsing mrls raised by  #TAUTHOR_TAG tsarfaty et al. (, 2013 :', '• can we represent words abstractly so as to reflect shared morphological aspects between them? • which types of morphological information should we include in the parsing model?', '']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG tsarfaty'],[' #TAUTHOR_TAG tsarfaty'],"['morphologically rich languages ( mrls ) is difficult due to the complex relationship of syntax to morphology.', 'but the success of neural networks offer an appealing solution to this problem by computing word representation from characters.', 'character - level models  #AUTHOR_TAG learn relationship between similar word forms and have shown to be effective for parsing mrls  #AUTHOR_TAG bjorkelund et al., 2017 ).', 'does that mean that we can do away with explicit modeling of morphology altogether? consider two challenges in parsing mrls raised by  #TAUTHOR_TAG tsarfaty et al. (, 2013 :', '• can we represent words abstractly so as to reflect shared morphological aspects between them? • which types of morphological information should we include in the parsing model?', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['', 'we train a morphological tagger to predict case information.', ""the tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer."", 'we found that predicted case improves accuracy, although the effect is different across languages.', 'these results are interesting, since in vintage parsers, predicted case usually harmed accuracy  #TAUTHOR_TAG.', 'however, we note that our taggers use gold pos, which might help.', 'pipeline model vs. multi - task learning in general, mtl models achieve similar or slightly better performance than the character - only models, suggesting that supplying case in this way is beneficial.', 'however, we found that using predicted case in a pipeline model gives more improvements than mtl.', 'we also observe an interesting pattern in which mtl achieves better tagging accuracy than the pipeline model but lower performance in parsing ( table 2 ).', ""this is surprising since it suggests that the mtl model must learn to effectively encode case in the model's representation, but must not effectively use it for parsing""]",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['', 'we train a morphological tagger to predict case information.', ""the tagger has the same structure as the parser's encoder, with an additional feedforward neural network with one hidden layer followed by a softmax layer."", 'we found that predicted case improves accuracy, although the effect is different across languages.', 'these results are interesting, since in vintage parsers, predicted case usually harmed accuracy  #TAUTHOR_TAG.', 'however, we note that our taggers use gold pos, which might help.', 'pipeline model vs. multi - task learning in general, mtl models achieve similar or slightly better performance than the character - only models, suggesting that supplying case in this way is beneficial.', 'however, we found that using predicted case in a pipeline model gives more improvements than mtl.', 'we also observe an interesting pattern in which mtl achieves better tagging accuracy than the pipeline model but lower performance in parsing ( table 2 ).', ""this is surprising since it suggests that the mtl model must learn to effectively encode case in the model's representation, but must not effectively use it for parsing""]",4
,,,,0
,,,,0
['.  #TAUTHOR_TAG formulated identifying'],['precision.  #TAUTHOR_TAG formulated identifying opinion relations'],[' #TAUTHOR_TAG formulated identifying opinion relations between words as an alignment process'],[' #TAUTHOR_TAG'],0
,,,,1
['.  #TAUTHOR_TAG formulated identifying'],['precision.  #TAUTHOR_TAG formulated identifying opinion relations'],[' #TAUTHOR_TAG formulated identifying opinion relations between words as an alignment process'],[' #TAUTHOR_TAG'],1
"[' #TAUTHOR_TAG, which is a graph -']","[' #TAUTHOR_TAG, which is a graph - based extraction']","[' #TAUTHOR_TAG, which is a graph - based extraction framework']",[' #TAUTHOR_TAG'],5
"['alignment model.', 'similar to  #TAUTHOR_TAG,']","['alignment model.', 'similar to  #TAUTHOR_TAG,']","['unsupervised word alignment model.', 'similar to  #TAUTHOR_TAG, every sentence in reviews is replicated to']","['', 'in this subsection, we present our method for capturing opinion relations using unsupervised word alignment model.', 'similar to  #TAUTHOR_TAG, every sentence in reviews is replicated to generate a parallel sentence pair, and the word alignment algorithm is applied to the monolingual scenario to align a noun / noun phase with its modifiers.', 'we select ibm - 3 model  #AUTHOR_TAG as the alignment model.', 'formally, given a sentence s = { w 1, w 2,..., w n }, we have', 'where t ( w j | w a j ) models the co - occurrence information of two words in dataset.', 'd ( j | a j, n ) models word position information, which describes the probability of a word in position a j aligned with a word in position j. and n ( φ i | w i ) describes the ability of a word for modifying ( being modified by ) several words.', 'φ i denotes the number of words that are aligned with w i.', 'in our experiments, we set φ i = 2.', 'since we only have interests on capturing opinion relations between words, we only pay attentions on the alignments between opinion target candidates ( nouns / noun phrases ) and potential opinion words ( adjectives / verbs']",5
"['t ).', 'then, similar to  #TAUTHOR_TAG,']","['t ).', 'then, similar to  #TAUTHOR_TAG,']","['t ).', 'then, similar to  #TAUTHOR_TAG, the association between']",[' #TAUTHOR_TAG'],5
['adopt a graph - based algorithm used in  #TAUTHOR_TAG'],['adopt a graph - based algorithm used in  #TAUTHOR_TAG'],"['the second component, we adopt a graph - based algorithm used in  #TAUTHOR_TAG']","['the second component, we adopt a graph - based algorithm used in  #TAUTHOR_TAG to compute the confidence of each opinion target candidate, and the candidates with higher confidence than the threshold will be extracted as the opinion targets.', 'here, opinion words are regarded as the important indicators.', '']",5
['adopt a graph - based algorithm used in  #TAUTHOR_TAG'],['adopt a graph - based algorithm used in  #TAUTHOR_TAG'],"['the second component, we adopt a graph - based algorithm used in  #TAUTHOR_TAG']","['the second component, we adopt a graph - based algorithm used in  #TAUTHOR_TAG to compute the confidence of each opinion target candidate, and the candidates with higher confidence than the threshold will be extracted as the opinion targets.', 'here, opinion words are regarded as the important indicators.', '']",5
"['through syntactic patterns, and liu  #TAUTHOR_TAG, which fulfilled this']","['through syntactic patterns, and liu  #TAUTHOR_TAG, which fulfilled this']","['through syntactic patterns, and liu  #TAUTHOR_TAG, which fulfilled this task by using unsupervised wam.', 'the parameter settings in these baselines are']","['', 'to further prove the effectiveness of our combination, we compare pswam with some state - of - the - art methods, including hu  #AUTHOR_TAG a ), which extracted frequent opinion target words based on association mining rules, dp  #AUTHOR_TAG, which extracted opinion targets through syntactic patterns, and liu  #TAUTHOR_TAG, which fulfilled this task by using unsupervised wam.', 'the parameter settings in these baselines are the same as the settings in the original papers.', 'because of the space limitation, we only show the results on restaurant and hotel, as shown in figure 6 and 7.', 'from the experimental results, we can obtain the following observations.', '']",5
"[' #TAUTHOR_TAG, which is a graph -']","[' #TAUTHOR_TAG, which is a graph - based extraction']","[' #TAUTHOR_TAG, which is a graph - based extraction framework']",[' #TAUTHOR_TAG'],3
"['as large, which includes reviews from three different domains and different languages.', 'this collection was also used in  #TAUTHOR_TAG.', 'in the experiments, reviews are first segmented into sentences according to punctuation.', 'the detailed statistical information of the used collection is shown in']","['as large, which includes reviews from three different domains and different languages.', 'this collection was also used in  #TAUTHOR_TAG.', 'in the experiments, reviews are first segmented into sentences according to punctuation.', 'the detailed statistical information of the used collection is shown in']","['as large, which includes reviews from three different domains and different languages.', 'this collection was also used in  #TAUTHOR_TAG.', 'in the experiments, reviews are first segmented into sentences according to punctuation.', 'the detailed statistical information of the used collection is shown in table 2, where']","['this section, to answer the questions mentioned in the first section, we collect a large collection named as large, which includes reviews from three different domains and different languages.', 'this collection was also used in  #TAUTHOR_TAG.', 'in the experiments, reviews are first segmented into sentences according to punctuation.', 'the detailed statistical information of the used collection is shown in table 2, where restaurant is crawled from the chinese web site : www. dianping. com.', 'the hotel and mp3 are used in  #AUTHOR_TAG, which are respectively crawled from www. tripadvisor. com and www. amazon. com.', 'for each dataset, we perform random sampling to generate testing set with different sizes, where we use sampled subsets with # sentences = 5 × 10 2, 10 3, 5 × 10 3, 10 4, 5 × 10 4, 10 5 and 10 6 sentences respectively.', 'each sentence is tokenized, part - of - speech tagged by using stanford nlp tool 3, and parsed by using minipar toolkit. and the method of  #AUTHOR_TAG is used to identify noun phrases.', 'we select precision and recall as the metrics.', '']",3
"['-  #AUTHOR_TAG and  #TAUTHOR_TAG, capture syntactic']","['indonesian word embeddings introduced in, e. g., ( al -  #AUTHOR_TAG and  #TAUTHOR_TAG, capture syntactic']","['-  #AUTHOR_TAG and  #TAUTHOR_TAG, capture syntactic']","['the existence of various indonesian pretrained word embeddings, there are no publicly available indonesian analogy task datasets on which to evaluate these embeddings.', 'consequently, it is unknown if indonesian word embeddings introduced in, e. g., ( al -  #AUTHOR_TAG and  #TAUTHOR_TAG, capture syntactic or semantic information as measured by analogy tasks.', 'also, such embeddings are usually trained on indonesian wikipedia ( al -  #AUTHOR_TAG whose size is relatively small, approximately 60m tokens.', 'therefore, in this work, we introduce kawat ( kata word analogy task ), an indonesian word analogy task dataset, and new indonesian word embeddings pretrained on 160m tokens of online news corpus.', 'we evaluated these embeddings on kawat, and also tested them on pos tagging and text summarization as representatives of syntactic and semantic downstream task respectively']",0
"['embeddings introduced in  #AUTHOR_TAG and  #TAUTHOR_TAG,']","['used fasttext pretrained embeddings introduced in  #AUTHOR_TAG and  #TAUTHOR_TAG,']","['', 'we used fasttext pretrained embeddings introduced in  #AUTHOR_TAG and  #TAUTHOR_TAG,']","['', 'kawat is available online.', '1 one of the goals of this work is to evaluate and compare existing indonesian pretrained word embeddings.', 'we used fasttext pretrained embeddings introduced in  #AUTHOR_TAG and  #TAUTHOR_TAG, which have been trained on indonesian wikipedia and indonesian wikipedia plus common crawl data respectively.', '']",5
"[', while  #TAUTHOR_TAG distinguish hat']","['otherwise offensive language, while  #TAUTHOR_TAG distinguish hate speech from generally']",['while  #TAUTHOR_TAG distinguish hate speech from'],[' #TAUTHOR_TAG'],0
"[', while  #TAUTHOR_TAG distinguish hat']","['otherwise offensive language, while  #TAUTHOR_TAG distinguish hate speech from generally']",['while  #TAUTHOR_TAG distinguish hate speech from'],[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","['a pronoun. 2 cyberbullying and trolling are instances of directed abuse, aimed at individuals and online communities respectively. the second row shows cases with abusive expressions towards generalized groups such as racial categories and sexual orientations. previous work has identified instances of hate speech that are both directed and generalized  #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other dimension is the extent to which abusive language is explicit or implicit. this is roughly analogous to the distinction in', 'linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by  #AUTHOR_TAG. explicit abusive lan - guage is that which is unambiguous in its potential to be ab', '##usive, for example language that contains racial or homophobic slurs. previous research has indicated a great deal of variation within such language  #TAUTHOR_TAG, with abusive terms being used in a colloquial manner or by people who are victims of abuse. implicit abusive language is that which does not immediately imply or denote abuse. here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches', ' #AUTHOR_TAG. social scientists and activists have recently been paying more attention to implicit, and even unconscious, instances of abuse that have been termed "" microaggressions ""  #AUTHOR_TAG. as the examples show, such language may nonetheless have extremely abusive connotations. the first column of table 1 shows instances of explicit abuse, where it should be apparent to the reader that the content is abusive.', 'the messages in the second column are implicit and it is harder to determine whether they are abusive without knowing the context. for example, the word "" them "" in the first two examples in', 'the generalized and implicit cell refers to an ethnic group, and the words "" skypes "" and "" google "" are used as euphemisms for slurs about jews and african - americans respectively. abuse using sarcasm can be even more elusive for detection systems, for instance the seemingly harmless', ""comment praising someone's intelligence was a sarcastic response to a beauty pageant contestants unsatisfactory answer to a question ( dinakar et al., in the following section we outline"", 'the implications of this typology, highlighting where the existing literatures indicate how we can understand, measure, and', 'model each subtype of abuse']",0
"[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","['a pronoun. 2 cyberbullying and trolling are instances of directed abuse, aimed at individuals and online communities respectively. the second row shows cases with abusive expressions towards generalized groups such as racial categories and sexual orientations. previous work has identified instances of hate speech that are both directed and generalized  #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other dimension is the extent to which abusive language is explicit or implicit. this is roughly analogous to the distinction in', 'linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by  #AUTHOR_TAG. explicit abusive lan - guage is that which is unambiguous in its potential to be ab', '##usive, for example language that contains racial or homophobic slurs. previous research has indicated a great deal of variation within such language  #TAUTHOR_TAG, with abusive terms being used in a colloquial manner or by people who are victims of abuse. implicit abusive language is that which does not immediately imply or denote abuse. here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches', ' #AUTHOR_TAG. social scientists and activists have recently been paying more attention to implicit, and even unconscious, instances of abuse that have been termed "" microaggressions ""  #AUTHOR_TAG. as the examples show, such language may nonetheless have extremely abusive connotations. the first column of table 1 shows instances of explicit abuse, where it should be apparent to the reader that the content is abusive.', 'the messages in the second column are implicit and it is harder to determine whether they are abusive without knowing the context. for example, the word "" them "" in the first two examples in', 'the generalized and implicit cell refers to an ethnic group, and the words "" skypes "" and "" google "" are used as euphemisms for slurs about jews and african - americans respectively. abuse using sarcasm can be even more elusive for detection systems, for instance the seemingly harmless', ""comment praising someone's intelligence was a sarcastic response to a beauty pageant contestants unsatisfactory answer to a question ( dinakar et al., in the following section we outline"", 'the implications of this typology, highlighting where the existing literatures indicate how we can understand, measure, and', 'model each subtype of abuse']",0
"[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","[' #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other']","['a pronoun. 2 cyberbullying and trolling are instances of directed abuse, aimed at individuals and online communities respectively. the second row shows cases with abusive expressions towards generalized groups such as racial categories and sexual orientations. previous work has identified instances of hate speech that are both directed and generalized  #TAUTHOR_TAG, although  #AUTHOR_TAG come closest to making a distinction between directed and generalized hate. the other dimension is the extent to which abusive language is explicit or implicit. this is roughly analogous to the distinction in', 'linguistics and semiotics between denotation, the literal meaning of a term or symbol, and connotation, its sociocultural associations, famously articulated by  #AUTHOR_TAG. explicit abusive lan - guage is that which is unambiguous in its potential to be ab', '##usive, for example language that contains racial or homophobic slurs. previous research has indicated a great deal of variation within such language  #TAUTHOR_TAG, with abusive terms being used in a colloquial manner or by people who are victims of abuse. implicit abusive language is that which does not immediately imply or denote abuse. here, the true nature is often obscured by the use of ambiguous terms, sarcasm, lack of profanity or hateful terms, and other means, generally making it more difficult to detect by both annotators and machine learning approaches', ' #AUTHOR_TAG. social scientists and activists have recently been paying more attention to implicit, and even unconscious, instances of abuse that have been termed "" microaggressions ""  #AUTHOR_TAG. as the examples show, such language may nonetheless have extremely abusive connotations. the first column of table 1 shows instances of explicit abuse, where it should be apparent to the reader that the content is abusive.', 'the messages in the second column are implicit and it is harder to determine whether they are abusive without knowing the context. for example, the word "" them "" in the first two examples in', 'the generalized and implicit cell refers to an ethnic group, and the words "" skypes "" and "" google "" are used as euphemisms for slurs about jews and african - americans respectively. abuse using sarcasm can be even more elusive for detection systems, for instance the seemingly harmless', ""comment praising someone's intelligence was a sarcastic response to a beauty pageant contestants unsatisfactory answer to a question ( dinakar et al., in the following section we outline"", 'the implications of this typology, highlighting where the existing literatures indicate how we can understand, measure, and', 'model each subtype of abuse']",0
"['more subtle distinctions  #TAUTHOR_TAG, others find that they']","['more subtle distinctions  #TAUTHOR_TAG, others find that they']","['more subtle distinctions  #TAUTHOR_TAG, others find that they do not']","['', 'future work in these subtasks should aim to have annotators distinguish between targeted and generalized abuse so that each subtype can be modeled more effectively.', 'annotation ( via crowd - sourcing and other methods ) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon  #AUTHOR_TAG b ), but is considerably more difficult when implicit abuse is considered  #AUTHOR_TAG.', 'the connotations of language can be difficult to classify without domainspecific knowledge.', 'furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions  #TAUTHOR_TAG, others find that they do not improve the reliability of non - expert classifications  #AUTHOR_TAG.', 'in such cases, expert annotators with domain specific knowledge are preferred as they tend to produce more accurate classifications  #AUTHOR_TAG a ).', 'ultimately, the nature of abusive language can be extremely subjective, and researchers must endeavor to take this into account when using human annotators.', ' #TAUTHOR_TAG, for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism.', 'as such, it is important that researchers consider the social biases that']",0
"['more subtle distinctions  #TAUTHOR_TAG, others find that they']","['more subtle distinctions  #TAUTHOR_TAG, others find that they']","['more subtle distinctions  #TAUTHOR_TAG, others find that they do not']","['', 'future work in these subtasks should aim to have annotators distinguish between targeted and generalized abuse so that each subtype can be modeled more effectively.', 'annotation ( via crowd - sourcing and other methods ) tends to be more straightforward when explicit instances of abusive language can be identified and agreed upon  #AUTHOR_TAG b ), but is considerably more difficult when implicit abuse is considered  #AUTHOR_TAG.', 'the connotations of language can be difficult to classify without domainspecific knowledge.', 'furthermore, while some argue that detailed guidelines can help annotators to make more subtle distinctions  #TAUTHOR_TAG, others find that they do not improve the reliability of non - expert classifications  #AUTHOR_TAG.', 'in such cases, expert annotators with domain specific knowledge are preferred as they tend to produce more accurate classifications  #AUTHOR_TAG a ).', 'ultimately, the nature of abusive language can be extremely subjective, and researchers must endeavor to take this into account when using human annotators.', ' #TAUTHOR_TAG, for instance, show that annotators tend to code racism as hate speech at a higher rate than sexism.', 'as such, it is important that researchers consider the social biases that']",0
"['expression of hatred  #TAUTHOR_TAG.', 'typed dependencies offer a more sophisticated way to capture the']","['expression of hatred  #TAUTHOR_TAG.', 'typed dependencies offer a more sophisticated way to capture the']","['- speech sequences to model the expression of hatred  #TAUTHOR_TAG.', 'typed dependencies offer a more sophisticated way to capture the relationship between terms  #AUTHOR_TAG.', 'overall, there are many tools that researchers can use']","['', 'syntactical features have also proven to be successful in identifying abusive language.', 'a number of studies on hate speech use part - of - speech sequences to model the expression of hatred  #TAUTHOR_TAG.', 'typed dependencies offer a more sophisticated way to capture the relationship between terms  #AUTHOR_TAG.', 'overall, there are many tools that researchers can use to model the relationship between abusive language and targets, although many of these require high - quality annotations to use as training data.', 'generalized abuse.', 'generalized abuse online tends to target people belonging to a small set of categories, primarily racial, religious, and sexual minorities  #AUTHOR_TAG.', 'researchers should consider identifying forms of abuse unique to each target group addressed, as vocabularies may depend on the groups targeted.', 'for example, the language used to abuse trans - people and that used against latin american people are likely to differ, both in the nouns used to denote the target group and the other terms associated with them.', 'in some cases a lexical method may therefore be an appropriate strategy.', 'further research is necessary to determine if there are underlying syntactic structures associated with generalized abusive language.', 'explicit abuse explicit abuse, whether directed or generalized, is often indicated by specific keywords.', 'hence, dictionary - based approaches may be well suited to identify this type of abuse  #AUTHOR_TAG, although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways  #TAUTHOR_TAG.', 'negative polarity and sentiment of the text are also']",0
"['expression of hatred  #TAUTHOR_TAG.', 'typed dependencies offer a more sophisticated way to capture the']","['expression of hatred  #TAUTHOR_TAG.', 'typed dependencies offer a more sophisticated way to capture the']","['- speech sequences to model the expression of hatred  #TAUTHOR_TAG.', 'typed dependencies offer a more sophisticated way to capture the relationship between terms  #AUTHOR_TAG.', 'overall, there are many tools that researchers can use']","['', 'syntactical features have also proven to be successful in identifying abusive language.', 'a number of studies on hate speech use part - of - speech sequences to model the expression of hatred  #TAUTHOR_TAG.', 'typed dependencies offer a more sophisticated way to capture the relationship between terms  #AUTHOR_TAG.', 'overall, there are many tools that researchers can use to model the relationship between abusive language and targets, although many of these require high - quality annotations to use as training data.', 'generalized abuse.', 'generalized abuse online tends to target people belonging to a small set of categories, primarily racial, religious, and sexual minorities  #AUTHOR_TAG.', 'researchers should consider identifying forms of abuse unique to each target group addressed, as vocabularies may depend on the groups targeted.', 'for example, the language used to abuse trans - people and that used against latin american people are likely to differ, both in the nouns used to denote the target group and the other terms associated with them.', 'in some cases a lexical method may therefore be an appropriate strategy.', 'further research is necessary to determine if there are underlying syntactic structures associated with generalized abusive language.', 'explicit abuse explicit abuse, whether directed or generalized, is often indicated by specific keywords.', 'hence, dictionary - based approaches may be well suited to identify this type of abuse  #AUTHOR_TAG, although the presence of particular words should not be the only criteria, even terms that denote abuse may be used in a variety of different ways  #TAUTHOR_TAG.', 'negative polarity and sentiment of the text are also']",2
"[') parsing  #TAUTHOR_TAG, 2004 ;  #AUTHOR_TAG,']","['parsing  #TAUTHOR_TAG, 2004 ;  #AUTHOR_TAG,']","[') parsing  #TAUTHOR_TAG, 2004 ;  #AUTHOR_TAG,']","['tutorial discusses a framework of online global discriminative learning and beam - search decoding for syntactic processing  #AUTHOR_TAG b ), which has recently been applied to a wide variety of natural language processing ( nlp ) tasks, including word segmentation  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG b ;  #AUTHOR_TAG, context free grammar ( cfg ) parsing  #TAUTHOR_TAG, 2004 ;  #AUTHOR_TAG, combinational categorial grammar ( ccg ) parsing  #AUTHOR_TAG a ;  #AUTHOR_TAG and machine translation  #AUTHOR_TAG, achieving stateof - the - art accuracies and efficiencies.', '']",5
['- update strategy  #TAUTHOR_TAG'],['early - update strategy  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of nlp problems, giving theoretical discussions and demonstrating a software implementation.', 'we start with a detailed introduction of the framework, describing the averaged perceptron algorithm  #AUTHOR_TAG and its efficient implementation issues  #AUTHOR_TAG, as well as beam - search and the early - update strategy  #TAUTHOR_TAG.', 'we then illustrate how the framework can be applied to nlp tasks, including word segmentation, joint segmentation & pos - tagging, labeled and unlabeled dependency parsing, joint pos - tagging and dependency parsing, cfg parsing, ccg parsing, and joint segmentation, pos - tagging and parsing.', 'in each case, we illustrate how the task is turned into an incremental left - to - right output - building process, and how rich features are defined to give competitive accuracies.', 'these examples can serve as guidance in applying the framework to other structural prediction tasks.', 'in the second part of the tutorial, we give some analysis on why the framework is effective.', 'we discuss several alternative learning algorithms, 13 and compare beam - search with greedy search on dependency parsing.', 'we show that accuracy benefits from interaction between learning and search.', 'finally, the tutorial concludes with an introduction to zpar, an open source toolkit that provides optimized c + + implementations of of all the above tasks.', 'ting liu is a professor at hit - scir.', 'his research interest includes social computing, information retrieval and natural language processing']",5
"['type - level approximation of full supervision  #TAUTHOR_TAG, - combination of projection and type constraints  #AUTHOR_TAG tackstrom et']","['type - level approximation of full supervision  #TAUTHOR_TAG, - combination of projection and type constraints  #AUTHOR_TAG tackstrom et al., 2013 ), - rapid annotation of seed training data.', 'however,']","['type - level approximation of full supervision  #TAUTHOR_TAG, - combination of projection and type constraints  #AUTHOR_TAG tackstrom et']","['- resource languages lack manually annotated data to learn even the most basic models such as part - of - speech ( pos ) taggers.', 'to compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models : - aligned parallel corpora to project pos annotations to target languages  #AUTHOR_TAG agic et al., 2015 ;  #AUTHOR_TAG, - noisy tag dictionaries for type - level approximation of full supervision  #TAUTHOR_TAG, - combination of projection and type constraints  #AUTHOR_TAG tackstrom et al., 2013 ), - rapid annotation of seed training data.', 'however, only one or two compatible sources of distant supervision are typically employed.', 'in reality severely under - resourced languages may require a more pragmatic "" take what you can get "" viewpoint.', 'our results suggest that combining supervision sources is the way to go about creating viable low - resource taggers.', 'we propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals.', 'our system is a uniform neural model for pos tagging that learns from disparate sources of distant supervision ( dsds ).', 'we use it to combine : i ) multi - source annotation projection, ii ) instance selection, iii ) noisy tag dictionaries, and iv ) distributed word and sub - word representations.', 'we examine how far we can get by exploiting only the wide - coverage resources that are currently readily available for more than 300 languages, which is the breadth of the parallel corpus we employ.', 'dsds yields a new state of the art by jointly leveraging disparate sources of distant supervision in an experiment with 25 languages.', 'we demonstrate : i ) substantial gains in carefully selecting high - quality instances in annotation projection, ii ) the usefulness of lexicon features for neural tagging, and iii ) the importance of word embeddings initialization for faster convergence']",0
"['for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such']","['for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information']","['training instances. dictionaries. dictionaries are a useful source for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information :']","['sentences by percentage of words covered by', 'word alignment from 21 sources of agic et al. ( 2016 ), and select the top k', 'covered instances for training. in specific, we employ the mean coverage', 'ranking of target sentences, whereby each target sentence is coupled with the arithmetic mean of the 21 individual word alignment coverages for each of the 21 source - language sentences. we show that this simple approach to instance selection offers substantial improvements : across all languages, we learn better taggers with significantly fewer training instances. dictionaries. dictionaries are a useful source for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information : i ) as type constraints during encoding ( tackstrom et al., 2013 ), ii ) to guide unsupervised learning  #AUTHOR_TAG, or iii ) as additional signal at training. we focus', 'on the latter', 'and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two : a ) by representing lexicon properties as n - hot vector', '( e. g., if a word has two properties according to lexicon src, it results in a 2 -', 'hot vector, if the word is not present in src, a zero vector ), with m the number of lexicon properties ; b ) by embedding the lexical features, i. e., e src', '']",0
"['the rest. this is in slight contrast to 50 iterations that  #TAUTHOR_TAG recommend, although selecting', '50']","['the rest. this is in slight contrast to 50 iterations that  #TAUTHOR_TAG recommend, although selecting', '50']","['all the rest. this is in slight contrast to 50 iterations that  #TAUTHOR_TAG recommend, although selecting', '50 does']","['information', 'always helps, even in cases where only 1k entries are available, and embedding it is usually the most beneficial way. for closely - related languages such as', 'serbian and croatian, using resources for one aids tagging the other, and modern resources are a better fit. for example, using the croatian wtc projections to train a model for serbian is preferable over', 'in - language serbian bible data where the oov rate is much higher. how much gold data?', 'we assume not having access to any gold annotated data. it is thus interesting to ask how much gold data is', 'needed to reach', 'our performance. this is a tricky question, as training within', 'the same corpus naturally favors the same corpus data. we test both in - corpus ( ud ) and out', '- of - corpus data ( our test sets ) and notice an important gap', ': while in - corpus only 50 sentences are sufficient, outside the corpus one would need over 200 sentences. this experiment was done for a subset of 18', 'languages with both in - and out - ofcorpus test data. further comparison. in table 1 we directly report the accuracies from the original contributions by das', ', li, garrette, and agic over the same test data. we additionally attempted to reach the scores of li by running their tagger over the table 1 data setup. the', 'results are depicted in figure', '4 as mean accuracies over em iterations until convergence. we', 'show : i ) li peaks at 10 iterations for their test languages, and at 35', 'iterations for all the rest. this is in slight contrast to 50 iterations that  #TAUTHOR_TAG recommend, although selecting', '50 does not dramatically hurt the scores ; ii ) our replication falls ∼5 points short of their 84. 9 accuracy. there is a large 33 - point accuracy gap between the scores of  #TAUTHOR_TAG, where the dictionaries are large,', 'and the other languages in figure 4, with smaller dictionaries. compared to das, our tagger clearly benefits from', 'pre - trained', 'word embeddings, while theirs relies on label propagation through europarl, a much cleaner corpus that lacks the coverage of the noisier wt', '##c. similar applies to tackstrom et al. ( 2013 ), as they use 1 - 5m', 'near - perfect parallel sentences. even if we use much smaller and noisier', 'data sources, dsds is almost on par : 86. 2 vs. 87. 3 for the 8 languages from  #AUTHOR_TAG', ', and we even outperform theirs on four languages : czech, french, italian, and spanish']",0
"['on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and']","['on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and']","['successful work on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and all the remaining languages in table 1.  #AUTHOR_TAG.', 'our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.', 'most prior work on neural sequence prediction follows']","['successful work on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and all the remaining languages in table 1.  #AUTHOR_TAG.', 'our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.', 'most prior work on neural sequence prediction follows the commonly perceived wisdom that hand - crafted features are unnecessary for deep learning methods.', 'they rely on end - to - end training without resorting to additional linguistic resources.', 'our study shows that this is not the case.', 'only few prior studies investigate such sources, e. g., for mt  #AUTHOR_TAG and sagot and martinez  #AUTHOR_TAG for pos tagging use lexicons, but only as n - hot features and without examining the cross - lingual aspect']",0
"['type - level approximation of full supervision  #TAUTHOR_TAG, - combination of projection and type constraints  #AUTHOR_TAG tackstrom et']","['type - level approximation of full supervision  #TAUTHOR_TAG, - combination of projection and type constraints  #AUTHOR_TAG tackstrom et al., 2013 ), - rapid annotation of seed training data.', 'however,']","['type - level approximation of full supervision  #TAUTHOR_TAG, - combination of projection and type constraints  #AUTHOR_TAG tackstrom et']","['- resource languages lack manually annotated data to learn even the most basic models such as part - of - speech ( pos ) taggers.', 'to compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models : - aligned parallel corpora to project pos annotations to target languages  #AUTHOR_TAG agic et al., 2015 ;  #AUTHOR_TAG, - noisy tag dictionaries for type - level approximation of full supervision  #TAUTHOR_TAG, - combination of projection and type constraints  #AUTHOR_TAG tackstrom et al., 2013 ), - rapid annotation of seed training data.', 'however, only one or two compatible sources of distant supervision are typically employed.', 'in reality severely under - resourced languages may require a more pragmatic "" take what you can get "" viewpoint.', 'our results suggest that combining supervision sources is the way to go about creating viable low - resource taggers.', 'we propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals.', 'our system is a uniform neural model for pos tagging that learns from disparate sources of distant supervision ( dsds ).', 'we use it to combine : i ) multi - source annotation projection, ii ) instance selection, iii ) noisy tag dictionaries, and iv ) distributed word and sub - word representations.', 'we examine how far we can get by exploiting only the wide - coverage resources that are currently readily available for more than 300 languages, which is the breadth of the parallel corpus we employ.', 'dsds yields a new state of the art by jointly leveraging disparate sources of distant supervision in an experiment with 25 languages.', 'we demonstrate : i ) substantial gains in carefully selecting high - quality instances in annotation projection, ii ) the usefulness of lexicon features for neural tagging, and iii ) the importance of word embeddings initialization for faster convergence']",1
"['for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such']","['for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information']","['training instances. dictionaries. dictionaries are a useful source for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information :']","['sentences by percentage of words covered by', 'word alignment from 21 sources of agic et al. ( 2016 ), and select the top k', 'covered instances for training. in specific, we employ the mean coverage', 'ranking of target sentences, whereby each target sentence is coupled with the arithmetic mean of the 21 individual word alignment coverages for each of the 21 source - language sentences. we show that this simple approach to instance selection offers substantial improvements : across all languages, we learn better taggers with significantly fewer training instances. dictionaries. dictionaries are a useful source for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information : i ) as type constraints during encoding ( tackstrom et al., 2013 ), ii ) to guide unsupervised learning  #AUTHOR_TAG, or iii ) as additional signal at training. we focus', 'on the latter', 'and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two : a ) by representing lexicon properties as n - hot vector', '( e. g., if a word has two properties according to lexicon src, it results in a 2 -', 'hot vector, if the word is not present in src, a zero vector ), with m the number of lexicon properties ; b ) by embedding the lexical features, i. e., e src', '']",5
"['for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such']","['for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information']","['training instances. dictionaries. dictionaries are a useful source for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information :']","['sentences by percentage of words covered by', 'word alignment from 21 sources of agic et al. ( 2016 ), and select the top k', 'covered instances for training. in specific, we employ the mean coverage', 'ranking of target sentences, whereby each target sentence is coupled with the arithmetic mean of the 21 individual word alignment coverages for each of the 21 source - language sentences. we show that this simple approach to instance selection offers substantial improvements : across all languages, we learn better taggers with significantly fewer training instances. dictionaries. dictionaries are a useful source for distant supervision  #TAUTHOR_TAG ; tackstrom et al., 2013 ). there are several ways to exploit such information : i ) as type constraints during encoding ( tackstrom et al., 2013 ), ii ) to guide unsupervised learning  #AUTHOR_TAG, or iii ) as additional signal at training. we focus', 'on the latter', 'and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two : a ) by representing lexicon properties as n - hot vector', '( e. g., if a word has two properties according to lexicon src, it results in a 2 -', 'hot vector, if the word is not present in src, a zero vector ), with m the number of lexicon properties ; b ) by embedding the lexical features, i. e., e src', '']",5
"[': the approach by that works with projections, dictionaries, and unlabeled target text.', '- li : wiktionary supervision  #TAUTHOR_TAG.', 'data.', 'our set of 25 languages is motivated by accessibility to embeddings and dictionaries.', 'in']","['data.', '- garrette : the approach by that works with projections, dictionaries, and unlabeled target text.', '- li : wiktionary supervision  #TAUTHOR_TAG.', 'data.', 'our set of 25 languages is motivated by accessibility to embeddings and dictionaries.', 'in']","[': the approach by that works with projections, dictionaries, and unlabeled target text.', '- li : wiktionary supervision  #TAUTHOR_TAG.', 'data.', 'our set of 25 languages is motivated by accessibility to embeddings and dictionaries.', 'in all experiments we work with the 12 universal pos tags  #AUTHOR_TAG.', '']","['##s.', 'we compare to the following weaklysupervised pos taggers : - agic : multi - source annotation projection with bible parallel data by agic et al. ( 2015 ).', '- das : the label propagation approach by  #AUTHOR_TAG over europarl data.', '- garrette : the approach by that works with projections, dictionaries, and unlabeled target text.', '- li : wiktionary supervision  #TAUTHOR_TAG.', 'data.', 'our set of 25 languages is motivated by accessibility to embeddings and dictionaries.', 'in all experiments we work with the 12 universal pos tags  #AUTHOR_TAG.', 'for development, we use 21 dev sets of the universal dependencies 2. 1  #AUTHOR_TAG.', 'we employ ud test sets on additional languages as well as the test sets of agic et al. ( 2015 ) to facilitate comparisons.', 'their test sets are a mixture of conll  #AUTHOR_TAG and hamledt test data  #AUTHOR_TAG, and are more distant from the training and development data.', 'model and parameters.', 'we extend an off - theshelf state - of - the - art bi - lstm tagger with lexicon information.', 'the code is available at : https : / / github. com / bplank / bilstm - aux.', 'the parameter l = 40 was set on dev data across all languages.', 'besides using 10 epochs, word dropout rate ( p =. 25 ) and 40 - dimensional lexicon embeddings, we use the parameters from  #AUTHOR_TAG.', 'for all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.', 'for the learning curve, we average over 5 random samples with 3 runs each.', 'table 1 shows the tagging accuracy for individual languages, while the means over all languages are given in figure 2.', 'there are several take - aways']",5
"['over 8 test languages intersecting  #TAUTHOR_TAG and agic et al. ( 2016 ).', 'it reaches 86']","['over 8 test languages intersecting  #TAUTHOR_TAG and agic et al. ( 2016 ).', 'it reaches 86. 2 over the more commonly used 8 languages of  #AUTHOR_TAG, compared to their 83. 4.', 'this shows that our novel "" soft "" inclusion of noisy dictionaries is superior to a hard decoding restriction, and including lexicons in neural taggers helps.', 'we did not assume any gold data to further enrich the lexicons, nor fix possible tagset divergences']","['over 8 test languages intersecting  #TAUTHOR_TAG and agic et al. ( 2016 ).', 'it reaches 86']","['', 'only on 4 out of 21 languages are type constraints better.', 'this is the case for only one language for n - hot encoding ( french ).', 'the best approach is to embed both wiktionary and unimorph, boosting performance further to 84. 0, and resulting in our final model.', 'it helps the most on morphological rich languages such as uralic.', 'on the test sets ( table 4, right ) dsds reaches 87. 2 over 8 test languages intersecting  #TAUTHOR_TAG and agic et al. ( 2016 ).', 'it reaches 86. 2 over the more commonly used 8 languages of  #AUTHOR_TAG, compared to their 83. 4.', 'this shows that our novel "" soft "" inclusion of noisy dictionaries is superior to a hard decoding restriction, and including lexicons in neural taggers helps.', 'we did not assume any gold data to further enrich the lexicons, nor fix possible tagset divergences']",3
"['the rest. this is in slight contrast to 50 iterations that  #TAUTHOR_TAG recommend, although selecting', '50']","['the rest. this is in slight contrast to 50 iterations that  #TAUTHOR_TAG recommend, although selecting', '50']","['all the rest. this is in slight contrast to 50 iterations that  #TAUTHOR_TAG recommend, although selecting', '50 does']","['information', 'always helps, even in cases where only 1k entries are available, and embedding it is usually the most beneficial way. for closely - related languages such as', 'serbian and croatian, using resources for one aids tagging the other, and modern resources are a better fit. for example, using the croatian wtc projections to train a model for serbian is preferable over', 'in - language serbian bible data where the oov rate is much higher. how much gold data?', 'we assume not having access to any gold annotated data. it is thus interesting to ask how much gold data is', 'needed to reach', 'our performance. this is a tricky question, as training within', 'the same corpus naturally favors the same corpus data. we test both in - corpus ( ud ) and out', '- of - corpus data ( our test sets ) and notice an important gap', ': while in - corpus only 50 sentences are sufficient, outside the corpus one would need over 200 sentences. this experiment was done for a subset of 18', 'languages with both in - and out - ofcorpus test data. further comparison. in table 1 we directly report the accuracies from the original contributions by das', ', li, garrette, and agic over the same test data. we additionally attempted to reach the scores of li by running their tagger over the table 1 data setup. the', 'results are depicted in figure', '4 as mean accuracies over em iterations until convergence. we', 'show : i ) li peaks at 10 iterations for their test languages, and at 35', 'iterations for all the rest. this is in slight contrast to 50 iterations that  #TAUTHOR_TAG recommend, although selecting', '50 does not dramatically hurt the scores ; ii ) our replication falls ∼5 points short of their 84. 9 accuracy. there is a large 33 - point accuracy gap between the scores of  #TAUTHOR_TAG, where the dictionaries are large,', 'and the other languages in figure 4, with smaller dictionaries. compared to das, our tagger clearly benefits from', 'pre - trained', 'word embeddings, while theirs relies on label propagation through europarl, a much cleaner corpus that lacks the coverage of the noisier wt', '##c. similar applies to tackstrom et al. ( 2013 ), as they use 1 - 5m', 'near - perfect parallel sentences. even if we use much smaller and noisier', 'data sources, dsds is almost on par : 86. 2 vs. 87. 3 for the 8 languages from  #AUTHOR_TAG', ', and we even outperform theirs on four languages : czech, french, italian, and spanish']",4
"['on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and']","['on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and']","['successful work on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and all the remaining languages in table 1.  #AUTHOR_TAG.', 'our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.', 'most prior work on neural sequence prediction follows']","['successful work on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and all the remaining languages in table 1.  #AUTHOR_TAG.', 'our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.', 'most prior work on neural sequence prediction follows the commonly perceived wisdom that hand - crafted features are unnecessary for deep learning methods.', 'they rely on end - to - end training without resorting to additional linguistic resources.', 'our study shows that this is not the case.', 'only few prior studies investigate such sources, e. g., for mt  #AUTHOR_TAG and sagot and martinez  #AUTHOR_TAG for pos tagging use lexicons, but only as n - hot features and without examining the cross - lingual aspect']",4
"['on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and']","['on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and']","['successful work on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and all the remaining languages in table 1.  #AUTHOR_TAG.', 'our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.', 'most prior work on neural sequence prediction follows']","['successful work on low - resource pos tagging is based on projection  #AUTHOR_TAG, tag dictionaries  #TAUTHOR_TAG and all the remaining languages in table 1.  #AUTHOR_TAG.', 'our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.', 'most prior work on neural sequence prediction follows the commonly perceived wisdom that hand - crafted features are unnecessary for deep learning methods.', 'they rely on end - to - end training without resorting to additional linguistic resources.', 'our study shows that this is not the case.', 'only few prior studies investigate such sources, e. g., for mt  #AUTHOR_TAG and sagot and martinez  #AUTHOR_TAG for pos tagging use lexicons, but only as n - hot features and without examining the cross - lingual aspect']",6
"['', 'our shared task submission adopts the bidirectional lstm - cnn model of  #TAUTHOR_TAG,']","['in twitter.', 'our shared task submission adopts the bidirectional lstm - cnn model of  #TAUTHOR_TAG,']","['', 'our shared task submission adopts the bidirectional lstm - cnn model of  #TAUTHOR_TAG,']","['this paper, we describe the deepnnner entry to the 2nd workshop on noisy user - generated text ( wnut ) shared task # 2 : named entity recognition in twitter.', 'our shared task submission adopts the bidirectional lstm - cnn model of  #TAUTHOR_TAG, as it has been shown to perform well on both newswire and web texts.', 'it uses word embeddings trained on large - scale web text collections together with text normalization to cope with the diversity in web texts, and lexicons for target named entity classes constructed from publicly - available sources.', ""extended evaluation comparing the effectiveness of various word embeddings, text normalization, and lexicon settings shows that our system achieves a maximum f1 - score of 47. 24, performance surpassing that of the shared task's second - ranked system""]",5
['with little feature engineering  #TAUTHOR_TAG'],['with little feature engineering  #TAUTHOR_TAG'],"['can be achieved with little feature engineering  #TAUTHOR_TAG.', 'however, despite']","['entity recognition ( ner ) is an important part of natural language processing.', 'it is a challenging task that requires robust recognition to detect common entities over a large variety of expressions and vocabularies.', 'these problems are intensified when targeting web texts because of challenges such as differences in spelling and punctuation conventions, neologisms, and web markup  #AUTHOR_TAG.', 'traditional approaches to ner on newswire texts has been dominated by machine learning methods that rely heavily on manual feature engineering and external knowledge sources  #AUTHOR_TAG.', 'recently, neural network models - especially those that use recursive models - have shown that state of the art performance can be achieved with little feature engineering  #TAUTHOR_TAG.', 'however, despite their popularity for ner on newswire texts, neural networks have not been widely adopted for ner on web texts, with the exception of the feed - forward neural network ( ffnn ) model of  #AUTHOR_TAG.', 'in this paper, we present the deepnnner entry to the wnut 2016 shared task # 2 : named entity recognition in twitter.', 'our shared task submission is based on the model of  #TAUTHOR_TAG, a hybrid model of bidirectional long short - term memory ( blstm ) networks and convolutional neural networks ( cnn ) that automatically learns both character - and word - level features, and which holds the current state - of - the - art on both newswire texts ( conll 2003 ) and diverse corpora including web texts ( ontonotes 5. 0 ).', 'in contrast to crfs, ffnns, and other windowed models, the blstm gives our model effectively infinite context on both sides of a word during sequential labeling.', 'the character - level cnn allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace.', '']",5
['with little feature engineering  #TAUTHOR_TAG'],['with little feature engineering  #TAUTHOR_TAG'],"['can be achieved with little feature engineering  #TAUTHOR_TAG.', 'however, despite']","['entity recognition ( ner ) is an important part of natural language processing.', 'it is a challenging task that requires robust recognition to detect common entities over a large variety of expressions and vocabularies.', 'these problems are intensified when targeting web texts because of challenges such as differences in spelling and punctuation conventions, neologisms, and web markup  #AUTHOR_TAG.', 'traditional approaches to ner on newswire texts has been dominated by machine learning methods that rely heavily on manual feature engineering and external knowledge sources  #AUTHOR_TAG.', 'recently, neural network models - especially those that use recursive models - have shown that state of the art performance can be achieved with little feature engineering  #TAUTHOR_TAG.', 'however, despite their popularity for ner on newswire texts, neural networks have not been widely adopted for ner on web texts, with the exception of the feed - forward neural network ( ffnn ) model of  #AUTHOR_TAG.', 'in this paper, we present the deepnnner entry to the wnut 2016 shared task # 2 : named entity recognition in twitter.', 'our shared task submission is based on the model of  #TAUTHOR_TAG, a hybrid model of bidirectional long short - term memory ( blstm ) networks and convolutional neural networks ( cnn ) that automatically learns both character - and word - level features, and which holds the current state - of - the - art on both newswire texts ( conll 2003 ) and diverse corpora including web texts ( ontonotes 5. 0 ).', 'in contrast to crfs, ffnns, and other windowed models, the blstm gives our model effectively infinite context on both sides of a word during sequential labeling.', 'the character - level cnn allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace.', '']",5
"['is given figure 1.', 'our system is based on the blstm - cnn model of  #TAUTHOR_TAG, and, unless otherwise noted, follows their training and tagging methodology, which the']","['is given figure 1.', 'our system is based on the blstm - cnn model of  #TAUTHOR_TAG, and, unless otherwise noted, follows their training and tagging methodology, which the']","['this section, we describe the architecture of our shared task submission.', 'an overview is given figure 1.', 'our system is based on the blstm - cnn model of  #TAUTHOR_TAG, and, unless otherwise noted, follows their training and tagging methodology, which the reader is referred to for more details']","['this section, we describe the architecture of our shared task submission.', 'an overview is given figure 1.', 'our system is based on the blstm - cnn model of  #TAUTHOR_TAG, and, unless otherwise noted, follows their training and tagging methodology, which the reader is referred to for more details']",5
[' #TAUTHOR_TAG reported them to be the highest'],[' #TAUTHOR_TAG reported them to be the highest'],[' #TAUTHOR_TAG reported them to be the highest performing on both conll - 2003'],"['embeddings are critical for high - performance neural networks in nlp tasks  #AUTHOR_TAG.', 'in this paper, we compare six publicly available pre - trained word embeddings.', 'the embeddings are described in detail in table 3.', 'the neural embeddings of  #AUTHOR_TAG were chosen because  #TAUTHOR_TAG reported them to be the highest performing on both conll - 2003 and ontonotes 5. 0 datasets.', 'to evaluate embeddings trained on data closer to the wnut dataset, we also selected the glove embeddings of  #AUTHOR_TAG, trained on both web text and tweets, and word2vec embeddings trained on google news data  #AUTHOR_TAG and on tweets  #AUTHOR_TAG.', ""preliminary evaluation on the dev1 data showed that glove 27b outperformed collobert's embeddings ( see table 5 ) and word2vec 3b, so they were used in our submission."", ' #AUTHOR_TAG, we use lookup tables to extract embeddings and every word is lower cased before lookup']",5
"[' #TAUTHOR_TAG, we use a cnn to extract features from 25 dim.', 'character embeddings randomly - initialized from a uniform distribution']","[' #TAUTHOR_TAG, we use a cnn to extract features from 25 dim.', 'character embeddings randomly - initialized from a uniform distribution']","[' #TAUTHOR_TAG, we use a cnn to extract features from 25 dim.', 'character embeddings randomly - initialized from a uniform distribution']","[' #TAUTHOR_TAG, we use a cnn to extract features from 25 dim.', 'character embeddings randomly - initialized from a uniform distribution between - 0. 5 and 0. 5.', 'to accommodate text normalization, we added embeddings for the normalization symbols described in section 2. 2, namely < url >, < user >, < smile >, < lolface >, < sadface >, < neutralface >, < heart >, < number > and < hashtag >. all experiments were conducted with the same character embeddings']",5
"[' #TAUTHOR_TAG to the input text, as shown in']","[' #TAUTHOR_TAG to the input text, as shown in']","[' #TAUTHOR_TAG to the input text, as shown in figure 2.', 'each lexicon']","['', 'second, in order to deal with inconsistencies between person and musicartist classes as discussed in section 3, the music lexicon is a combination of the subtypes band and musicalartist 1.', 'finally, in order to maximize coverage, the product lexicon is a combination of the subtype device from the dbpedia ontology and the lexicon product distributed with wnut dataset.', 'every other category is as described in table 1.', 'to generate lexicon features, we apply the partial matching algorithm of  #TAUTHOR_TAG to the input text, as shown in figure 2.', 'each lexicon and match type ( bioes ) is associated with a randomly - initialized 5 dim.', 'embedding.', 'the embeddings for all lexicons are concatenated together to produce the lexicon feature for each word in the input.', 'to facilitate matching, all entries were stripped of parentheses and tokenized with the penn treebank tokenization script']",5
"[' #TAUTHOR_TAG, we used different symbols for word - level capitalization feature each assigned a randomly initialized embedding : allcaps, upperinitial, lowercase, mixedcaps and noinfo.', 'similar symbols were used']","[' #TAUTHOR_TAG, we used different symbols for word - level capitalization feature each assigned a randomly initialized embedding : allcaps, upperinitial, lowercase, mixedcaps and noinfo.', 'similar symbols were used']","[' #TAUTHOR_TAG, we used different symbols for word - level capitalization feature each assigned a randomly initialized embedding : allcaps, upperinitial, lowercase, mixedcaps and noinfo.', 'similar symbols were used']","[' #TAUTHOR_TAG, we used different symbols for word - level capitalization feature each assigned a randomly initialized embedding : allcaps, upperinitial, lowercase, mixedcaps and noinfo.', 'similar symbols were used for character - level ( upper case, lower case, punctuation, other )']",5
"['follow the training and inference methodology of  #TAUTHOR_TAG, training our neural']","['follow the training and inference methodology of  #TAUTHOR_TAG, training our neural']","['follow the training and inference methodology of  #TAUTHOR_TAG, training our neural network to maximize the sentence - level log - likelihood from  #AUTHOR_TAG.', 'training is done by mini - batch sgd with a fixed learning rate, and']","['follow the training and inference methodology of  #TAUTHOR_TAG, training our neural network to maximize the sentence - level log - likelihood from  #AUTHOR_TAG.', 'training is done by mini - batch sgd with a fixed learning rate, and we apply dropout  #AUTHOR_TAG to the output nodes.', 'all feature representations are "" unfrozen "" and allowed to be updated by the training algorithm.', 'we used the iob tag scheme to annotate named entities.', 'we also explored the bioes tag scheme 4, as it was reported to outperform iob  #AUTHOR_TAG, however, iob outperformed bioes in preliminary experiments.', 'we suspect that data sparsity prevented the model from learning meaningful representations for the extra tags.', ""our shared task submission's model trained in approximately 90 minutes and tags the test set in approximately 20 seconds, with memory usage peaking at 350mb 5""]",5
"['of  #TAUTHOR_TAG, which combined blstms']","['of  #TAUTHOR_TAG, which combined blstms']","['architecture of  #TAUTHOR_TAG, which combined blstms to maximize context over the tagged']","['', ', classification with random forests  #AUTHOR_TAG. our system adopts the architecture of  #TAUTHOR_TAG, which combined blstms to maximize context over the tagged word sequence and word - level cnns to automatically generate characterlevel features with a partial - matching lexicon to', 'achieve the state - of - the - art for ner on both conll 2003 and ontonotes datasets. our system can be viewed', 'as an investigation into how well state - of - theart neural approaches adapt to the challenges of ner on noisy web data']",5
"['in twitter, which adopted the blstm - cnn model of  #TAUTHOR_TAG']","['in twitter, which adopted the blstm - cnn model of  #TAUTHOR_TAG']","['in twitter, which adopted the blstm - cnn model of  #TAUTHOR_TAG.', 'extensive evaluation showed that high word type coverage for word embeddings']","['this paper, we described the deepnnner entry to the wnut 2016 shared task # 2 : named entity recognition in twitter, which adopted the blstm - cnn model of  #TAUTHOR_TAG.', 'extensive evaluation showed that high word type coverage for word embeddings is crucial to ner performance, likely due to rare words in entities, and that both text normalization and partial matching on lexicons constructed from dbpedia  #AUTHOR_TAG contribute significantly to performance.', ""our best - performing system uses text normalization, lexicon partial matching, and the glove word embeddings of  #AUTHOR_TAG trained on 42b words of common crawl data, and it achieves a maximum f1 - score of 47. 24, performance surpassing that of the shared task's second - ranked system""]",5
['with little feature engineering  #TAUTHOR_TAG'],['with little feature engineering  #TAUTHOR_TAG'],"['can be achieved with little feature engineering  #TAUTHOR_TAG.', 'however, despite']","['entity recognition ( ner ) is an important part of natural language processing.', 'it is a challenging task that requires robust recognition to detect common entities over a large variety of expressions and vocabularies.', 'these problems are intensified when targeting web texts because of challenges such as differences in spelling and punctuation conventions, neologisms, and web markup  #AUTHOR_TAG.', 'traditional approaches to ner on newswire texts has been dominated by machine learning methods that rely heavily on manual feature engineering and external knowledge sources  #AUTHOR_TAG.', 'recently, neural network models - especially those that use recursive models - have shown that state of the art performance can be achieved with little feature engineering  #TAUTHOR_TAG.', 'however, despite their popularity for ner on newswire texts, neural networks have not been widely adopted for ner on web texts, with the exception of the feed - forward neural network ( ffnn ) model of  #AUTHOR_TAG.', 'in this paper, we present the deepnnner entry to the wnut 2016 shared task # 2 : named entity recognition in twitter.', 'our shared task submission is based on the model of  #TAUTHOR_TAG, a hybrid model of bidirectional long short - term memory ( blstm ) networks and convolutional neural networks ( cnn ) that automatically learns both character - and word - level features, and which holds the current state - of - the - art on both newswire texts ( conll 2003 ) and diverse corpora including web texts ( ontonotes 5. 0 ).', 'in contrast to crfs, ffnns, and other windowed models, the blstm gives our model effectively infinite context on both sides of a word during sequential labeling.', 'the character - level cnn allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace.', '']",0
"['of  #TAUTHOR_TAG, which combined blstms']","['of  #TAUTHOR_TAG, which combined blstms']","['architecture of  #TAUTHOR_TAG, which combined blstms to maximize context over the tagged']","['', ', classification with random forests  #AUTHOR_TAG. our system adopts the architecture of  #TAUTHOR_TAG, which combined blstms to maximize context over the tagged word sequence and word - level cnns to automatically generate characterlevel features with a partial - matching lexicon to', 'achieve the state - of - the - art for ner on both conll 2003 and ontonotes datasets. our system can be viewed', 'as an investigation into how well state - of - theart neural approaches adapt to the challenges of ner on noisy web data']",0
['with little feature engineering  #TAUTHOR_TAG'],['with little feature engineering  #TAUTHOR_TAG'],"['can be achieved with little feature engineering  #TAUTHOR_TAG.', 'however, despite']","['entity recognition ( ner ) is an important part of natural language processing.', 'it is a challenging task that requires robust recognition to detect common entities over a large variety of expressions and vocabularies.', 'these problems are intensified when targeting web texts because of challenges such as differences in spelling and punctuation conventions, neologisms, and web markup  #AUTHOR_TAG.', 'traditional approaches to ner on newswire texts has been dominated by machine learning methods that rely heavily on manual feature engineering and external knowledge sources  #AUTHOR_TAG.', 'recently, neural network models - especially those that use recursive models - have shown that state of the art performance can be achieved with little feature engineering  #TAUTHOR_TAG.', 'however, despite their popularity for ner on newswire texts, neural networks have not been widely adopted for ner on web texts, with the exception of the feed - forward neural network ( ffnn ) model of  #AUTHOR_TAG.', 'in this paper, we present the deepnnner entry to the wnut 2016 shared task # 2 : named entity recognition in twitter.', 'our shared task submission is based on the model of  #TAUTHOR_TAG, a hybrid model of bidirectional long short - term memory ( blstm ) networks and convolutional neural networks ( cnn ) that automatically learns both character - and word - level features, and which holds the current state - of - the - art on both newswire texts ( conll 2003 ) and diverse corpora including web texts ( ontonotes 5. 0 ).', 'in contrast to crfs, ffnns, and other windowed models, the blstm gives our model effectively infinite context on both sides of a word during sequential labeling.', 'the character - level cnn allows our model to learn relevant features from the orthography of words, which is important in task where unseen words are commonplace.', '']",6
"['11, 4 ], images  #TAUTHOR_TAG or summaries']","['represented by a list of t terms with the highest probability.', 'in recent works, short phrases [ 11, 4 ], images  #TAUTHOR_TAG or summaries [ 19 ] have been']","['11, 4 ], images  #TAUTHOR_TAG or summaries']","['models [ 5 ] are a popular method for organizing and interpreting large document collections by grouping documents into various thematic subjects ( e. g. sports, politics or lifestyle ) called topics.', 'topics are multinomial distributions over a predefined vocabulary whereas documents are represented as probability distributions over topics.', 'topic models have proven to be an elegant way to build exploratory interfaces ( i. e. topic browsers ) for visualizing document collections by presenting to the users lists of topics [ 6, 15, 14 ] where they select documents of a particular topic of interest.', 'a topic is traditionally represented by a list of t terms with the highest probability.', 'in recent works, short phrases [ 11, 4 ], images  #TAUTHOR_TAG or summaries [ 19 ] have been used as alternatives.', 'particularly, images offer a language independent representation of the topic which can also be complementary to textual labels.', 'the visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [ 1, 2 ].', '']",0
"['11, 4 ], images  #TAUTHOR_TAG or summaries']","['represented by a list of t terms with the highest probability.', 'in recent works, short phrases [ 11, 4 ], images  #TAUTHOR_TAG or summaries [ 19 ] have been']","['11, 4 ], images  #TAUTHOR_TAG or summaries']","['models [ 5 ] are a popular method for organizing and interpreting large document collections by grouping documents into various thematic subjects ( e. g. sports, politics or lifestyle ) called topics.', 'topics are multinomial distributions over a predefined vocabulary whereas documents are represented as probability distributions over topics.', 'topic models have proven to be an elegant way to build exploratory interfaces ( i. e. topic browsers ) for visualizing document collections by presenting to the users lists of topics [ 6, 15, 14 ] where they select documents of a particular topic of interest.', 'a topic is traditionally represented by a list of t terms with the highest probability.', 'in recent works, short phrases [ 11, 4 ], images  #TAUTHOR_TAG or summaries [ 19 ] have been used as alternatives.', 'particularly, images offer a language independent representation of the topic which can also be complementary to textual labels.', 'the visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [ 1, 2 ].', '']",0
"['11, 4 ], images  #TAUTHOR_TAG or summaries']","['represented by a list of t terms with the highest probability.', 'in recent works, short phrases [ 11, 4 ], images  #TAUTHOR_TAG or summaries [ 19 ] have been']","['11, 4 ], images  #TAUTHOR_TAG or summaries']","['models [ 5 ] are a popular method for organizing and interpreting large document collections by grouping documents into various thematic subjects ( e. g. sports, politics or lifestyle ) called topics.', 'topics are multinomial distributions over a predefined vocabulary whereas documents are represented as probability distributions over topics.', 'topic models have proven to be an elegant way to build exploratory interfaces ( i. e. topic browsers ) for visualizing document collections by presenting to the users lists of topics [ 6, 15, 14 ] where they select documents of a particular topic of interest.', 'a topic is traditionally represented by a list of t terms with the highest probability.', 'in recent works, short phrases [ 11, 4 ], images  #TAUTHOR_TAG or summaries [ 19 ] have been used as alternatives.', 'particularly, images offer a language independent representation of the topic which can also be complementary to textual labels.', 'the visual representation of a topic has been shown to be as effective as the textual labels on retrieving information using a topic browser while it can be understood quickly by the users [ 1, 2 ].', '']",0
['1  #TAUTHOR_TAG'],['of the publicly available 16 - layer vgg - net [ 1  #TAUTHOR_TAG'],['of the publicly available 16 - layer vgg - net [ 1  #TAUTHOR_TAG'],"['', 'we use pre - computed dependency - based word embeddings [ 12 ] whose d is 300.', 'the resulting representations of t and c are the mean vectors of their constituent words, x t and x c respectively.', 'the visual information from the image v is converted into a dense vectorized representation, x v.', 'that is the output of the publicly available 16 - layer vgg - net [ 1  #TAUTHOR_TAG trained over the imagenet dataset [ 9 ].', 'vgg - net provides a 1000 dimensional vector which is the soft - max classification output of imagenet classes.', '']",5
"[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles']","['evaluate our model on the publicly available data set provided by  #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles taken from the new york times.', 'each topic is represented by ten terms with the highest probability.', 'they are also associated with 20 candidate image labels and their human ratings between 0 ( lowest ) and 3 ( highest ) denoting the appropriateness of these images for the topic.', 'that results into a total of 6k images and their associated textual metadata which are considered as captions.', 'the task is to choose the image with the highest rating from the set of the 20 candidates for a given topic.', 'the 20 candidate image labels per topic are collected by  #TAUTHOR_TAG using an information retrieval engine ( google ).', '']",5
"[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles']","['evaluate our model on the publicly available data set provided by  #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles taken from the new york times.', 'each topic is represented by ten terms with the highest probability.', 'they are also associated with 20 candidate image labels and their human ratings between 0 ( lowest ) and 3 ( highest ) denoting the appropriateness of these images for the topic.', 'that results into a total of 6k images and their associated textual metadata which are considered as captions.', 'the task is to choose the image with the highest rating from the set of the 20 candidates for a given topic.', 'the 20 candidate image labels per topic are collected by  #TAUTHOR_TAG using an information retrieval engine ( google ).', '']",5
"[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it']","[' #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles']","['evaluate our model on the publicly available data set provided by  #TAUTHOR_TAG.', 'it consists of 300 topics generated using wikipedia articles and news articles taken from the new york times.', 'each topic is represented by ten terms with the highest probability.', 'they are also associated with 20 candidate image labels and their human ratings between 0 ( lowest ) and 3 ( highest ) denoting the appropriateness of these images for the topic.', 'that results into a total of 6k images and their associated textual metadata which are considered as captions.', 'the task is to choose the image with the highest rating from the set of the 20 candidates for a given topic.', 'the 20 candidate image labels per topic are collected by  #TAUTHOR_TAG using an information retrieval engine ( google ).', '']",5
['uses personalized pagerank  #TAUTHOR_TAG'],['uses personalized pagerank  #TAUTHOR_TAG'],['uses personalized pagerank  #TAUTHOR_TAG'],"['compare our approach to the state - of - the - art method that uses personalized pagerank  #TAUTHOR_TAG to re - rank image candidates ( local ppr ) and an adapted version that computes the pagerank scores of all the available images in the test set ( global ppr ).', 'we also test other baselines methods : ( 1 ) a relevant approach originally proposed for image annotation that learns a joint model of text and image features ( wsabie ) [ 20 ], ( 2 ) linear regression and svm models that use the concatenation of the topic, the caption and the image vectors as input, lr ( topic + caption + vgg ) and svm ( topic + caption + vgg ) respectively.', 'finally, we test two versions of our own dnn using only either the caption ( dnn ( topic + caption ) ) or the visual information of the image ( dnn ( topic + vgg ) ).', '']",5
"['3 ], and more recently deep', 'bi - directional language models ( bilm )  #TAUTHOR_TAG have']","['to [ 3 ], and more recently deep', 'bi - directional language models ( bilm )  #TAUTHOR_TAG have']","['to [ 3 ], and more recently deep', 'bi - directional language models ( bilm )  #TAUTHOR_TAG']","['improvement by leveraging the transformer architecture [ 37 ], which is solely based on attention mechanisms, although without providing an evaluation with other', 'baselines and previous works such as infersent [ 10 ]. neural language models can be tracked back to [ 3 ], and more recently deep', 'bi - directional language models ( bilm )  #TAUTHOR_TAG have successfully been applied to word embeddings in order to incorporate contextual information. very recently, [ 34 ] used unsupervised generative pre - training of language models followed by discriminative fine - tunning to achieve state - of - the - art results in several nlp downstream tasks', '( improving 9 out of 12 tasks ). the authors concluded that using language model as objective to fine - tuning helped both in model generalization and convergence. a similar approach', 'of transfer learning using pre - trained language model was previously presented in ulmfit paper [ 18 ], with surprisingly good results even when fine -', 'tuning using small datasets. despite the fast development of a variety of recent methods for sentence embedding, there', 'are no extensive evaluations covering recent techniques on common grounds. the developmental pace of new methods has surpassed the pace of inter - methodology evaluation. senteval [ 9 ] was recently proposed to reduce', '']",0
"['3 ], and more recently deep', 'bi - directional language models ( bilm )  #TAUTHOR_TAG have']","['to [ 3 ], and more recently deep', 'bi - directional language models ( bilm )  #TAUTHOR_TAG have']","['to [ 3 ], and more recently deep', 'bi - directional language models ( bilm )  #TAUTHOR_TAG']","['improvement by leveraging the transformer architecture [ 37 ], which is solely based on attention mechanisms, although without providing an evaluation with other', 'baselines and previous works such as infersent [ 10 ]. neural language models can be tracked back to [ 3 ], and more recently deep', 'bi - directional language models ( bilm )  #TAUTHOR_TAG have successfully been applied to word embeddings in order to incorporate contextual information. very recently, [ 34 ] used unsupervised generative pre - training of language models followed by discriminative fine - tunning to achieve state - of - the - art results in several nlp downstream tasks', '( improving 9 out of 12 tasks ). the authors concluded that using language model as objective to fine - tuning helped both in model generalization and convergence. a similar approach', 'of transfer learning using pre - trained language model was previously presented in ulmfit paper [ 18 ], with surprisingly good results even when fine -', 'tuning using small datasets. despite the fast development of a variety of recent methods for sentence embedding, there', 'are no extensive evaluations covering recent techniques on common grounds. the developmental pace of new methods has surpassed the pace of inter - methodology evaluation. senteval [ 9 ] was recently proposed to reduce', '']",0
"[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[' #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']",[' #TAUTHOR_TAG'],0
"[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[' #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']",[' #TAUTHOR_TAG'],0
"[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[' #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']",[' #TAUTHOR_TAG'],0
"[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[' #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']",[' #TAUTHOR_TAG'],0
"[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[' #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']",[' #TAUTHOR_TAG'],0
"[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[' #TAUTHOR_TAG was recently introduced.', 'it uses representations']","[')  #TAUTHOR_TAG was recently introduced.', 'it uses representations']",[' #TAUTHOR_TAG'],0
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",0
"['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in']","['table 8 we report the results for the linguistic probing tasks and in figure 3 we show a graphical comparison as well.', 'as we can see in table 8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in the bshift ( bi - gram shift ) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, elmo  #TAUTHOR_TAG achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as "" this is my eve christmas "", a sample from the bshift dataset.', 'in [ 11 ], they found that the binned sentence length task ( sentlen ) was negatively correlated with the performance in downstream tasks.', 'this hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [ 11 ].', 'however, the  #TAUTHOR_TAG bag - of - words not only achieved the best result in the sentlent task but also in many downstream tasks.', 'our hypothesis is that this is due to the fact that elmo  #TAUTHOR_TAG is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging somo task.', '']",0
"['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in']","['table 8 we report the results for the linguistic probing tasks and in figure 3 we show a graphical comparison as well.', 'as we can see in table 8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in the bshift ( bi - gram shift ) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, elmo  #TAUTHOR_TAG achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as "" this is my eve christmas "", a sample from the bshift dataset.', 'in [ 11 ], they found that the binned sentence length task ( sentlen ) was negatively correlated with the performance in downstream tasks.', 'this hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [ 11 ].', 'however, the  #TAUTHOR_TAG bag - of - words not only achieved the best result in the sentlent task but also in many downstream tasks.', 'our hypothesis is that this is due to the fact that elmo  #TAUTHOR_TAG is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging somo task.', '']",0
[' #TAUTHOR_TAG is capturing'],"['elmo  #TAUTHOR_TAG is capturing.', 'we also showed that']",['elmo  #TAUTHOR_TAG is capturing'],[' #TAUTHOR_TAG'],0
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",5
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",5
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",5
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",5
"['tasks, elmo  #TAUTHOR_TAG achieved best']","['tasks, elmo  #TAUTHOR_TAG achieved best']","['6, although no method had a consistent performance among all tasks, elmo  #TAUTHOR_TAG achieved best results in']","['table 6 we show the tabular results for the downstream classification tasks, and in figure 1 we show a graphical comparison between the different methods.', 'as seen in table 6, although no method had a consistent performance among all tasks, elmo  #TAUTHOR_TAG achieved best results in 5 out of 9 tasks.', 'even though elmo  #TAUTHOR_TAG was trained on a language model objective, it is important to note that in this experiment a bag - of - words approach was employed.', 'therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating elmo  #TAUTHOR_TAG into infersent [ 10 ].', '']",5
"['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in']","['table 8 we report the results for the linguistic probing tasks and in figure 3 we show a graphical comparison as well.', 'as we can see in table 8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in the bshift ( bi - gram shift ) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, elmo  #TAUTHOR_TAG achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as "" this is my eve christmas "", a sample from the bshift dataset.', 'in [ 11 ], they found that the binned sentence length task ( sentlen ) was negatively correlated with the performance in downstream tasks.', 'this hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [ 11 ].', 'however, the  #TAUTHOR_TAG bag - of - words not only achieved the best result in the sentlent task but also in many downstream tasks.', 'our hypothesis is that this is due to the fact that elmo  #TAUTHOR_TAG is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging somo task.', '']",5
"['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in']","['table 8 we report the results for the linguistic probing tasks and in figure 3 we show a graphical comparison as well.', 'as we can see in table 8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in the bshift ( bi - gram shift ) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, elmo  #TAUTHOR_TAG achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as "" this is my eve christmas "", a sample from the bshift dataset.', 'in [ 11 ], they found that the binned sentence length task ( sentlen ) was negatively correlated with the performance in downstream tasks.', 'this hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [ 11 ].', 'however, the  #TAUTHOR_TAG bag - of - words not only achieved the best result in the sentlent task but also in many downstream tasks.', 'our hypothesis is that this is due to the fact that elmo  #TAUTHOR_TAG is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging somo task.', '']",5
"['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in']","['table 8 we report the results for the linguistic probing tasks and in figure 3 we show a graphical comparison as well.', 'as we can see in table 8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in the bshift ( bi - gram shift ) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, elmo  #TAUTHOR_TAG achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as "" this is my eve christmas "", a sample from the bshift dataset.', 'in [ 11 ], they found that the binned sentence length task ( sentlen ) was negatively correlated with the performance in downstream tasks.', 'this hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [ 11 ].', 'however, the  #TAUTHOR_TAG bag - of - words not only achieved the best result in the sentlent task but also in many downstream tasks.', 'our hypothesis is that this is due to the fact that elmo  #TAUTHOR_TAG is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging somo task.', '']",5
"['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in']","['table 8 we report the results for the linguistic probing tasks and in figure 3 we show a graphical comparison as well.', 'as we can see in table 8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in the bshift ( bi - gram shift ) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, elmo  #TAUTHOR_TAG achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as "" this is my eve christmas "", a sample from the bshift dataset.', 'in [ 11 ], they found that the binned sentence length task ( sentlen ) was negatively correlated with the performance in downstream tasks.', 'this hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [ 11 ].', 'however, the  #TAUTHOR_TAG bag - of - words not only achieved the best result in the sentlent task but also in many downstream tasks.', 'our hypothesis is that this is due to the fact that elmo  #TAUTHOR_TAG is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging somo task.', '']",5
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",4
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",4
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",4
"['tasks, elmo  #TAUTHOR_TAG achieved best']","['tasks, elmo  #TAUTHOR_TAG achieved best']","['6, although no method had a consistent performance among all tasks, elmo  #TAUTHOR_TAG achieved best results in']","['table 6 we show the tabular results for the downstream classification tasks, and in figure 1 we show a graphical comparison between the different methods.', 'as seen in table 6, although no method had a consistent performance among all tasks, elmo  #TAUTHOR_TAG achieved best results in 5 out of 9 tasks.', 'even though elmo  #TAUTHOR_TAG was trained on a language model objective, it is important to note that in this experiment a bag - of - words approach was employed.', 'therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating elmo  #TAUTHOR_TAG into infersent [ 10 ].', '']",4
[' #TAUTHOR_TAG is capturing'],"['elmo  #TAUTHOR_TAG is capturing.', 'we also showed that']",['elmo  #TAUTHOR_TAG is capturing'],[' #TAUTHOR_TAG'],4
['. 5b )  #TAUTHOR_TAG : this model was'],"['layers, 5. 5b )  #TAUTHOR_TAG : this model was']",['. 5b )  #TAUTHOR_TAG : this model was obtained'],"['this section, we describe where the pre - trained models were obtained as well as the procedures employed to evaluate each method.', ""elmo ( bow, all layers, 5. 5b )  #TAUTHOR_TAG : this model was obtained from the authors'website at https : / / allennlp. org / elmo."", 'according to the authors, the model was trained on a dataset with 5. 5b tokens consisting of wikipedia ( 1. 9b ) and all of the monolingual news crawl data from wmt 2008 - 2012 ( 3. 6b ).', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, all layers, original )  #TAUTHOR_TAG : this model was obtained from the authors website at https : / / allennlp. org / elmo.', 'according to the authors, the model was trained on the 1 billion word benchmark, approximately 800m tokens of news crawl data from wmt 2011.', 'to evaluate this model, we used the allennlp framework [ 13 ].', 'an averaging bag - of - words was employed to produce the sentence embeddings, using features from all three layers of the elmo  #TAUTHOR_TAG model and averaging along the word dimension.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', 'elmo ( bow, top layer, original )  #TAUTHOR_TAG model.', 'as shown in  #TAUTHOR_TAG, the higher - level lstm representations capture context - dependent aspects of meaning, while the lower level representations capture aspects of syntax.', 'therefore, we split the evaluation of the top layer from the evaluation using all layers described in the previous experiment.', 'we did not employ the trainable task - specific weighting scheme described in  #TAUTHOR_TAG.', '[ 7 ] to measure the semantic similarity between two sentences from 0 ( not similar', 'knowledge semantic relatedness ( sick - r ) [ 27 ] to measure the degree of semantic relatedness between sentences from 0 ( not related ) to 5 ( related )', 'a man is singing a song and playing the guitar', 'a man is opening a package that contains head']",6
"['tasks, elmo  #TAUTHOR_TAG achieved best']","['tasks, elmo  #TAUTHOR_TAG achieved best']","['6, although no method had a consistent performance among all tasks, elmo  #TAUTHOR_TAG achieved best results in']","['table 6 we show the tabular results for the downstream classification tasks, and in figure 1 we show a graphical comparison between the different methods.', 'as seen in table 6, although no method had a consistent performance among all tasks, elmo  #TAUTHOR_TAG achieved best results in 5 out of 9 tasks.', 'even though elmo  #TAUTHOR_TAG was trained on a language model objective, it is important to note that in this experiment a bag - of - words approach was employed.', 'therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating elmo  #TAUTHOR_TAG into infersent [ 10 ].', '']",2
"['tasks, elmo  #TAUTHOR_TAG achieved best']","['tasks, elmo  #TAUTHOR_TAG achieved best']","['6, although no method had a consistent performance among all tasks, elmo  #TAUTHOR_TAG achieved best results in']","['table 6 we show the tabular results for the downstream classification tasks, and in figure 1 we show a graphical comparison between the different methods.', 'as seen in table 6, although no method had a consistent performance among all tasks, elmo  #TAUTHOR_TAG achieved best results in 5 out of 9 tasks.', 'even though elmo  #TAUTHOR_TAG was trained on a language model objective, it is important to note that in this experiment a bag - of - words approach was employed.', 'therefore, these results are quite impressive, which lead us to believe that excellent results can be obtained by integrating elmo  #TAUTHOR_TAG into infersent [ 10 ].', '']",2
"['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high']","['8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in']","['table 8 we report the results for the linguistic probing tasks and in figure 3 we show a graphical comparison as well.', 'as we can see in table 8, elmo  #TAUTHOR_TAG was one of the methods that were able to achieve high performance on a broad set of different tasks.', 'interestingly, in the bshift ( bi - gram shift ) task, where the goal is to identify whether if two consecutive tokens within the sentence have been inverted or not, elmo  #TAUTHOR_TAG achieved a result that was better by a large margin when compared to all other methods, clearly a benefit of the language model objective, where it makes it easy to spot token inversion in sentences such as "" this is my eve christmas "", a sample from the bshift dataset.', 'in [ 11 ], they found that the binned sentence length task ( sentlen ) was negatively correlated with the performance in downstream tasks.', 'this hypothesis was also supported by the model learning dynamics, since it seems that as model starts to capture deeper linguistic properties, it will tend to forget about this superficial feature [ 11 ].', 'however, the  #TAUTHOR_TAG bag - of - words not only achieved the best result in the sentlent task but also in many downstream tasks.', 'our hypothesis is that this is due to the fact that elmo  #TAUTHOR_TAG is a deep representation composed by different levels that can capture superficial features such as sentence length as well as deep linguistic properties as seen in the challenging somo task.', '']",2
[' #TAUTHOR_TAG is capturing'],"['elmo  #TAUTHOR_TAG is capturing.', 'we also showed that']",['elmo  #TAUTHOR_TAG is capturing'],[' #TAUTHOR_TAG'],2
[' #TAUTHOR_TAG is capturing'],"['elmo  #TAUTHOR_TAG is capturing.', 'we also showed that']",['elmo  #TAUTHOR_TAG is capturing'],[' #TAUTHOR_TAG'],2
['of reading level prediction  #TAUTHOR_TAG ; such works are described in further detail in the next section 2'],"['of reading level prediction  #TAUTHOR_TAG ; such works are described in further detail in the next section 2.', 'in recent']","['of reading level prediction  #TAUTHOR_TAG ; such works are described in further detail in the next section 2.', 'in recent work  #AUTHOR_TAG, we approached']","[""the reading level of a children's book is an important task in the educational setting."", 'teachers want to have leveling for books in the school library ; parents are trying to select appropriate books for their children ; writers need guidance while writing for different literacy needs ( e. g. text simplification ) - reading level assessment is required in a variety of contexts.', 'the history of assessing readability using simple arithmetic metrics dates back to the 1920s when  #AUTHOR_TAG has measured difficulty of texts by tabulating words according to the frequency of their use in general literature.', 'most of the traditional readability formulas were also based on countable features of text, such as syllable counts  #AUTHOR_TAG.', 'more advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction  #TAUTHOR_TAG ; such works are described in further detail in the next section 2.', 'in recent work  #AUTHOR_TAG, we approached the problem of fine - grained leveling of books, demonstrating that a ranking approach to predicting reading level outperforms both classification and regression approaches in that domain.', ""a further finding was that visually - oriented features that consider the visual layout of the page ( e. g. number of text lines per annotated text region, text region area compared to the whole page area and font size etc. ) play an important role in predicting the reading levels of children's books in which pictures and textual layout dominate the book content over text."", 'however, the data preparation process in our previous study involves human intervention - we ask human annotators to draw rectangle markups around text region over pages.', 'moreover, we only use a very shallow surface level text - based feature set to compare with the visually - oriented features.', 'hence in this paper, we assess the effect of using completely automated annotation processing within the same framework.', 'we are interested in exploring how much performance will change by completely eliminating manual intervention.', 'at the same time, we have also extended our previous feature set by introducing a richer set of automatically derived textbased features, proposed by  #TAUTHOR_TAG, which capture deeper syntactic complexities of the text.', 'unlike our previous work, the major goal of this paper is not trying to compare different machine learning techniques used in readability assessment task, but rather to compare the performance differences between with and without human labor involved within our previous proposed system framework.', 'we begin the paper with the description of related work in section 2, followed by detailed explanation regarding data preparation and automatic annotations in section 3.', 'the extended features will be covered in section 4, followed by experimental analysis in section 5, in which we will compare the results between']",0
"['by  #TAUTHOR_TAG.', '']","['by  #TAUTHOR_TAG.', '']","['automatic readability assessment has been done by  #TAUTHOR_TAG.', '']","['1920, approximately 200 readability formulas have been reported in the literature ( du  #AUTHOR_TAG ; statistical language processing techniques have recently entered into the fray for readability assessment.', ' #AUTHOR_TAG and collins -  #AUTHOR_TAG have demonstrated the use of language models is more robust for web documents and passages.', ' #AUTHOR_TAG studied the impact of grammar - based features combined with language modeling approach for readability assessment of first and second language texts.', 'they argued that grammar - based features are more pertinent for second language learners than for the first language readers.', ' #AUTHOR_TAG and  #AUTHOR_TAG both used a support vector machine to classify texts based on the reading level.', 'they combined traditional methods of readability assessment and the features from language models and parsers.', "" #AUTHOR_TAG have developed a tool for text simplification for the authoring process which addresses lexical and syntactic phenomena to make text readable but their assessment takes place at more coarse levels of literacy instead of finer - grained levels used for children's books."", 'a detailed analysis of various features for automatic readability assessment has been done by  #TAUTHOR_TAG.', '']",0
['of reading level prediction  #TAUTHOR_TAG ; such works are described in further detail in the next section 2'],"['of reading level prediction  #TAUTHOR_TAG ; such works are described in further detail in the next section 2.', 'in recent']","['of reading level prediction  #TAUTHOR_TAG ; such works are described in further detail in the next section 2.', 'in recent work  #AUTHOR_TAG, we approached']","[""the reading level of a children's book is an important task in the educational setting."", 'teachers want to have leveling for books in the school library ; parents are trying to select appropriate books for their children ; writers need guidance while writing for different literacy needs ( e. g. text simplification ) - reading level assessment is required in a variety of contexts.', 'the history of assessing readability using simple arithmetic metrics dates back to the 1920s when  #AUTHOR_TAG has measured difficulty of texts by tabulating words according to the frequency of their use in general literature.', 'most of the traditional readability formulas were also based on countable features of text, such as syllable counts  #AUTHOR_TAG.', 'more advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction  #TAUTHOR_TAG ; such works are described in further detail in the next section 2.', 'in recent work  #AUTHOR_TAG, we approached the problem of fine - grained leveling of books, demonstrating that a ranking approach to predicting reading level outperforms both classification and regression approaches in that domain.', ""a further finding was that visually - oriented features that consider the visual layout of the page ( e. g. number of text lines per annotated text region, text region area compared to the whole page area and font size etc. ) play an important role in predicting the reading levels of children's books in which pictures and textual layout dominate the book content over text."", 'however, the data preparation process in our previous study involves human intervention - we ask human annotators to draw rectangle markups around text region over pages.', 'moreover, we only use a very shallow surface level text - based feature set to compare with the visually - oriented features.', 'hence in this paper, we assess the effect of using completely automated annotation processing within the same framework.', 'we are interested in exploring how much performance will change by completely eliminating manual intervention.', 'at the same time, we have also extended our previous feature set by introducing a richer set of automatically derived textbased features, proposed by  #TAUTHOR_TAG, which capture deeper syntactic complexities of the text.', 'unlike our previous work, the major goal of this paper is not trying to compare different machine learning techniques used in readability assessment task, but rather to compare the performance differences between with and without human labor involved within our previous proposed system framework.', 'we begin the paper with the description of related work in section 2, followed by detailed explanation regarding data preparation and automatic annotations in section 3.', 'the extended features will be covered in section 4, followed by experimental analysis in section 5, in which we will compare the results between']",5
['based on those used in  #TAUTHOR_TAG'],['based on those used in  #TAUTHOR_TAG'],['the following features from the xml files based on those used in  #TAUTHOR_TAG'],"['our previous work only uses surface level of text features, we are interested in investigating the contribution of high - level structural features to the current system.', ' #AUTHOR_TAG found several parsing - based features and part - of - speech based features to be useful.', 'we utilize the stanford parser  #AUTHOR_TAG to extract the following features from the xml files based on those used in  #TAUTHOR_TAG']",5
['those used in  #TAUTHOR_TAG ( described in section'],"['those used in  #TAUTHOR_TAG ( described in section 4. 2 ), with the features in our previous study.', 'annotation under the ±1 accuracy metric,']","['those used in  #TAUTHOR_TAG ( described in section 4. 2 ), with the features in our previous study.', 'annotation under the ±1 accuracy metric, the visual features and the structural features have']","['previous study demonstrated that combining surface features with visual features produces promising results.', 'as mentioned above, the second aim of this study is to see how much benefit we can get from incorporating high - level structural features, such as those used in  #TAUTHOR_TAG ( described in section 4. 2 ), with the features in our previous study.', 'annotation under the ±1 accuracy metric, the visual features and the structural features have the same performance, whose accuracy are both slightly lower than that of surface level features.', '']",5
"['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has']","['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has']","['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which']","['machine translation has achieved great success in the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has outperformed previous rnn / cnn based translation models  #AUTHOR_TAG, is based on multi - layer self - attention networks and can be trained very efficiently.', 'the multi - layer structure allows the transformer to model complicated functions.', 'increasing the depth of models can increase their capacity but may also cause optimization difficulties  #AUTHOR_TAG.', 'in order to ease optimization, the transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks  #AUTHOR_TAG.', 'however, even with residual connections and layer normalization, deep transformers are still hard to train : the original transformer  #TAUTHOR_TAG only contains 6 encoder / decoder layers.', 'show that transformer models with more than 12 encoder layers fail to converge, and propose the transparent attention ( ta ) mechanism which weighted combines outputs of all encoder layers as encoded representation.', ""however, the ta mechanism has to value outputs of shallow encoder layers to feedback sufficient gradients during back - propagation to ensure their convergence, which implies that weights of deep layers are likely to be hampered and against the motivation when go very deep, and as a result cannot get further improvements with more than 16 layers. reveal that deep transformers with proper use of layer normalization is able to converge and propose to aggregate previous layers'outputs for each layer instead of at the end of encoding."", ' #AUTHOR_TAG research on incremental increasing the depth of the transformer big by freezing pre - trained shallow layers.', 'in concurrent work,  #AUTHOR_TAG also point out the same issue as in this work, but there are differences between.', 'in contrast to all previous works, we empirically show that with proper parameter initialization, deep transformers with the original computation order can converge.', ""the contributions of our work are as follows : we empirically demonstrate that a simple modification made in the transformer's official implementation which changes the computation order of residual connection and layer normalization can effectively ease its optimization ;"", 'we deeply analyze how the subtle difference of computation order affects the convergence deep transformer models, and propose to initialize deep transformer models under lipschitz restriction ;', 'our simple approach effectively ensures the convergence of deep transformers with up to 24 layers, and bring + 1. 50 and + 0. 92 bleu improvements in the wmt 14 english to german task and the wmt 15 czech to english task ;', 'we study the influence of the deep decoder in addition to the deep encoder studied by previous works, and show that deep decoders can also benefit the performance of the transformer']",1
"['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has']","['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has']","['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which']","['machine translation has achieved great success in the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has outperformed previous rnn / cnn based translation models  #AUTHOR_TAG, is based on multi - layer self - attention networks and can be trained very efficiently.', 'the multi - layer structure allows the transformer to model complicated functions.', 'increasing the depth of models can increase their capacity but may also cause optimization difficulties  #AUTHOR_TAG.', 'in order to ease optimization, the transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks  #AUTHOR_TAG.', 'however, even with residual connections and layer normalization, deep transformers are still hard to train : the original transformer  #TAUTHOR_TAG only contains 6 encoder / decoder layers.', 'show that transformer models with more than 12 encoder layers fail to converge, and propose the transparent attention ( ta ) mechanism which weighted combines outputs of all encoder layers as encoded representation.', ""however, the ta mechanism has to value outputs of shallow encoder layers to feedback sufficient gradients during back - propagation to ensure their convergence, which implies that weights of deep layers are likely to be hampered and against the motivation when go very deep, and as a result cannot get further improvements with more than 16 layers. reveal that deep transformers with proper use of layer normalization is able to converge and propose to aggregate previous layers'outputs for each layer instead of at the end of encoding."", ' #AUTHOR_TAG research on incremental increasing the depth of the transformer big by freezing pre - trained shallow layers.', 'in concurrent work,  #AUTHOR_TAG also point out the same issue as in this work, but there are differences between.', 'in contrast to all previous works, we empirically show that with proper parameter initialization, deep transformers with the original computation order can converge.', ""the contributions of our work are as follows : we empirically demonstrate that a simple modification made in the transformer's official implementation which changes the computation order of residual connection and layer normalization can effectively ease its optimization ;"", 'we deeply analyze how the subtle difference of computation order affects the convergence deep transformer models, and propose to initialize deep transformer models under lipschitz restriction ;', 'our simple approach effectively ensures the convergence of deep transformers with up to 24 layers, and bring + 1. 50 and + 0. 92 bleu improvements in the wmt 14 english to german task and the wmt 15 czech to english task ;', 'we study the influence of the deep decoder in addition to the deep encoder studied by previous works, and show that deep decoders can also benefit the performance of the transformer']",1
"['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has']","['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has']","['the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which']","['machine translation has achieved great success in the last few years  #TAUTHOR_TAG.', 'the transformer  #TAUTHOR_TAG, which has outperformed previous rnn / cnn based translation models  #AUTHOR_TAG, is based on multi - layer self - attention networks and can be trained very efficiently.', 'the multi - layer structure allows the transformer to model complicated functions.', 'increasing the depth of models can increase their capacity but may also cause optimization difficulties  #AUTHOR_TAG.', 'in order to ease optimization, the transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks  #AUTHOR_TAG.', 'however, even with residual connections and layer normalization, deep transformers are still hard to train : the original transformer  #TAUTHOR_TAG only contains 6 encoder / decoder layers.', 'show that transformer models with more than 12 encoder layers fail to converge, and propose the transparent attention ( ta ) mechanism which weighted combines outputs of all encoder layers as encoded representation.', ""however, the ta mechanism has to value outputs of shallow encoder layers to feedback sufficient gradients during back - propagation to ensure their convergence, which implies that weights of deep layers are likely to be hampered and against the motivation when go very deep, and as a result cannot get further improvements with more than 16 layers. reveal that deep transformers with proper use of layer normalization is able to converge and propose to aggregate previous layers'outputs for each layer instead of at the end of encoding."", ' #AUTHOR_TAG research on incremental increasing the depth of the transformer big by freezing pre - trained shallow layers.', 'in concurrent work,  #AUTHOR_TAG also point out the same issue as in this work, but there are differences between.', 'in contrast to all previous works, we empirically show that with proper parameter initialization, deep transformers with the original computation order can converge.', ""the contributions of our work are as follows : we empirically demonstrate that a simple modification made in the transformer's official implementation which changes the computation order of residual connection and layer normalization can effectively ease its optimization ;"", 'we deeply analyze how the subtle difference of computation order affects the convergence deep transformer models, and propose to initialize deep transformer models under lipschitz restriction ;', 'our simple approach effectively ensures the convergence of deep transformers with up to 24 layers, and bring + 1. 50 and + 0. 92 bleu improvements in the wmt 14 english to german task and the wmt 15 czech to english task ;', 'we study the influence of the deep decoder in addition to the deep encoder studied by previous works, and show that deep decoders can also benefit the performance of the transformer']",0
['published version  #TAUTHOR_TAG ('],"['published version  #TAUTHOR_TAG ( figure 1 a ),']","['published version  #TAUTHOR_TAG ( figure 1 a ),']","['our research we focus on training problems of deep transformers which prevent them from convergence ( as opposed to other important issues such as over - fitting on the training set ).', 'to alleviate the training problem for the standard transformer model, layer normalization  #AUTHOR_TAG and residual connection  #AUTHOR_TAG are adopted.', 'the official implementation of the transformer uses a different computation sequence ( figure 1 b ) compared to the published version  #TAUTHOR_TAG ( figure 1 a ), since it seems better for harder - to - learn models 1.', 'though several papers  #AUTHOR_TAG mentioned this change, how this modification impacts on the performance of the transformer, especially for deep transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back - propagation, and  #AUTHOR_TAG point out the same effects of normalization in concurrent work.', 'in order to compare with, we used the datasets from the wmt 14 english to german task and the wmt 15 czech to english task for experiments.', 'we applied joint byte - pair encoding ( bpe )  #AUTHOR_TAG with 32k merge operations.', 'we used the same setting as the transformer base  #TAUTHOR_TAG except the number of warm - up steps was set to 8k.', 'we conducted our experiments based on the neutron implementation  #AUTHOR_TAG of the transformer.', 'parameters were initialized with glorot initialization 2  #AUTHOR_TAG like in many other transformer implementation  #AUTHOR_TAG.', 'our experiments run on 2 gtx 1080 ti gpus, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches.', 'we used a beam size of 4 for decoding, and evaluated tokenized case - sensitive bleu with the averaged model of the last 5 checkpoints saved with an interval of 1, 500 training steps  #TAUTHOR_TAG.', 'results of two different computation order are shown in table 1.', 'v1 and v2 stand for the computation order of the proposed transformer  #TAUTHOR_TAG and that of the official implementation respectively. "" ¬ "" means fail to converge, "" none "" means not reported in original works, "" * "" indicates our implementation of their approach.', '† and ‡ mean p < 0. 01 and p < 0. 05 while comparing between v1 and v2 of the same number of layers in significance test']",0
['published version  #TAUTHOR_TAG ('],"['published version  #TAUTHOR_TAG ( figure 1 a ),']","['published version  #TAUTHOR_TAG ( figure 1 a ),']","['our research we focus on training problems of deep transformers which prevent them from convergence ( as opposed to other important issues such as over - fitting on the training set ).', 'to alleviate the training problem for the standard transformer model, layer normalization  #AUTHOR_TAG and residual connection  #AUTHOR_TAG are adopted.', 'the official implementation of the transformer uses a different computation sequence ( figure 1 b ) compared to the published version  #TAUTHOR_TAG ( figure 1 a ), since it seems better for harder - to - learn models 1.', 'though several papers  #AUTHOR_TAG mentioned this change, how this modification impacts on the performance of the transformer, especially for deep transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back - propagation, and  #AUTHOR_TAG point out the same effects of normalization in concurrent work.', 'in order to compare with, we used the datasets from the wmt 14 english to german task and the wmt 15 czech to english task for experiments.', 'we applied joint byte - pair encoding ( bpe )  #AUTHOR_TAG with 32k merge operations.', 'we used the same setting as the transformer base  #TAUTHOR_TAG except the number of warm - up steps was set to 8k.', 'we conducted our experiments based on the neutron implementation  #AUTHOR_TAG of the transformer.', 'parameters were initialized with glorot initialization 2  #AUTHOR_TAG like in many other transformer implementation  #AUTHOR_TAG.', 'our experiments run on 2 gtx 1080 ti gpus, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches.', 'we used a beam size of 4 for decoding, and evaluated tokenized case - sensitive bleu with the averaged model of the last 5 checkpoints saved with an interval of 1, 500 training steps  #TAUTHOR_TAG.', 'results of two different computation order are shown in table 1.', 'v1 and v2 stand for the computation order of the proposed transformer  #TAUTHOR_TAG and that of the official implementation respectively. "" ¬ "" means fail to converge, "" none "" means not reported in original works, "" * "" indicates our implementation of their approach.', '† and ‡ mean p < 0. 01 and p < 0. 05 while comparing between v1 and v2 of the same number of layers in significance test']",0
['computation order as in  #TAUTHOR_TAG have difficulty in convergence'],"['computation order as in  #TAUTHOR_TAG have difficulty in convergence.', 'we empirically show that deep transformers with']",[' #AUTHOR_TAG which show that deep transformers with the computation order as in  #TAUTHOR_TAG have difficulty in convergence'],"['contrast to all previous works  #AUTHOR_TAG which show that deep transformers with the computation order as in  #TAUTHOR_TAG have difficulty in convergence.', 'we empirically show that deep transformers with the original computation order can converge as long as with proper parameter initialization.', 'in this paper, we first investigate convergence differences between the published transformer  #TAUTHOR_TAG and the official implementation of the transformer, and compare the differences of computation orders between them.', 'then we conjecture the training problem of deep transformers is because layer normalization sometimes shrinks residual connections, and propose this can be tackled simply with lipschitz restricted parameter initialization.', 'our experiments demonstrate the effectiveness of our simple approach on the convergence of deep transformers, and brings significant improvements on the wmt 14 english to german and the wmt 15 czech to english news translation tasks.', 'we also study the effects of deep decoders in addition to deep encoders concerned in previous works']",0
['published version  #TAUTHOR_TAG ('],"['published version  #TAUTHOR_TAG ( figure 1 a ),']","['published version  #TAUTHOR_TAG ( figure 1 a ),']","['our research we focus on training problems of deep transformers which prevent them from convergence ( as opposed to other important issues such as over - fitting on the training set ).', 'to alleviate the training problem for the standard transformer model, layer normalization  #AUTHOR_TAG and residual connection  #AUTHOR_TAG are adopted.', 'the official implementation of the transformer uses a different computation sequence ( figure 1 b ) compared to the published version  #TAUTHOR_TAG ( figure 1 a ), since it seems better for harder - to - learn models 1.', 'though several papers  #AUTHOR_TAG mentioned this change, how this modification impacts on the performance of the transformer, especially for deep transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back - propagation, and  #AUTHOR_TAG point out the same effects of normalization in concurrent work.', 'in order to compare with, we used the datasets from the wmt 14 english to german task and the wmt 15 czech to english task for experiments.', 'we applied joint byte - pair encoding ( bpe )  #AUTHOR_TAG with 32k merge operations.', 'we used the same setting as the transformer base  #TAUTHOR_TAG except the number of warm - up steps was set to 8k.', 'we conducted our experiments based on the neutron implementation  #AUTHOR_TAG of the transformer.', 'parameters were initialized with glorot initialization 2  #AUTHOR_TAG like in many other transformer implementation  #AUTHOR_TAG.', 'our experiments run on 2 gtx 1080 ti gpus, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches.', 'we used a beam size of 4 for decoding, and evaluated tokenized case - sensitive bleu with the averaged model of the last 5 checkpoints saved with an interval of 1, 500 training steps  #TAUTHOR_TAG.', 'results of two different computation order are shown in table 1.', 'v1 and v2 stand for the computation order of the proposed transformer  #TAUTHOR_TAG and that of the official implementation respectively. "" ¬ "" means fail to converge, "" none "" means not reported in original works, "" * "" indicates our implementation of their approach.', '† and ‡ mean p < 0. 01 and p < 0. 05 while comparing between v1 and v2 of the same number of layers in significance test']",4
['computation order as in  #TAUTHOR_TAG have difficulty in convergence'],"['computation order as in  #TAUTHOR_TAG have difficulty in convergence.', 'we empirically show that deep transformers with']",[' #AUTHOR_TAG which show that deep transformers with the computation order as in  #TAUTHOR_TAG have difficulty in convergence'],"['contrast to all previous works  #AUTHOR_TAG which show that deep transformers with the computation order as in  #TAUTHOR_TAG have difficulty in convergence.', 'we empirically show that deep transformers with the original computation order can converge as long as with proper parameter initialization.', 'in this paper, we first investigate convergence differences between the published transformer  #TAUTHOR_TAG and the official implementation of the transformer, and compare the differences of computation orders between them.', 'then we conjecture the training problem of deep transformers is because layer normalization sometimes shrinks residual connections, and propose this can be tackled simply with lipschitz restricted parameter initialization.', 'our experiments demonstrate the effectiveness of our simple approach on the convergence of deep transformers, and brings significant improvements on the wmt 14 english to german and the wmt 15 czech to english news translation tasks.', 'we also study the effects of deep decoders in addition to deep encoders concerned in previous works']",4
['published version  #TAUTHOR_TAG ('],"['published version  #TAUTHOR_TAG ( figure 1 a ),']","['published version  #TAUTHOR_TAG ( figure 1 a ),']","['our research we focus on training problems of deep transformers which prevent them from convergence ( as opposed to other important issues such as over - fitting on the training set ).', 'to alleviate the training problem for the standard transformer model, layer normalization  #AUTHOR_TAG and residual connection  #AUTHOR_TAG are adopted.', 'the official implementation of the transformer uses a different computation sequence ( figure 1 b ) compared to the published version  #TAUTHOR_TAG ( figure 1 a ), since it seems better for harder - to - learn models 1.', 'though several papers  #AUTHOR_TAG mentioned this change, how this modification impacts on the performance of the transformer, especially for deep transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back - propagation, and  #AUTHOR_TAG point out the same effects of normalization in concurrent work.', 'in order to compare with, we used the datasets from the wmt 14 english to german task and the wmt 15 czech to english task for experiments.', 'we applied joint byte - pair encoding ( bpe )  #AUTHOR_TAG with 32k merge operations.', 'we used the same setting as the transformer base  #TAUTHOR_TAG except the number of warm - up steps was set to 8k.', 'we conducted our experiments based on the neutron implementation  #AUTHOR_TAG of the transformer.', 'parameters were initialized with glorot initialization 2  #AUTHOR_TAG like in many other transformer implementation  #AUTHOR_TAG.', 'our experiments run on 2 gtx 1080 ti gpus, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches.', 'we used a beam size of 4 for decoding, and evaluated tokenized case - sensitive bleu with the averaged model of the last 5 checkpoints saved with an interval of 1, 500 training steps  #TAUTHOR_TAG.', 'results of two different computation order are shown in table 1.', 'v1 and v2 stand for the computation order of the proposed transformer  #TAUTHOR_TAG and that of the official implementation respectively. "" ¬ "" means fail to converge, "" none "" means not reported in original works, "" * "" indicates our implementation of their approach.', '† and ‡ mean p < 0. 01 and p < 0. 05 while comparing between v1 and v2 of the same number of layers in significance test']",6
['published version  #TAUTHOR_TAG ('],"['published version  #TAUTHOR_TAG ( figure 1 a ),']","['published version  #TAUTHOR_TAG ( figure 1 a ),']","['our research we focus on training problems of deep transformers which prevent them from convergence ( as opposed to other important issues such as over - fitting on the training set ).', 'to alleviate the training problem for the standard transformer model, layer normalization  #AUTHOR_TAG and residual connection  #AUTHOR_TAG are adopted.', 'the official implementation of the transformer uses a different computation sequence ( figure 1 b ) compared to the published version  #TAUTHOR_TAG ( figure 1 a ), since it seems better for harder - to - learn models 1.', 'though several papers  #AUTHOR_TAG mentioned this change, how this modification impacts on the performance of the transformer, especially for deep transformers, has never been deeply studied before with empirical results to the best of our knowledge, except analyzed the difference between two computation orders during back - propagation, and  #AUTHOR_TAG point out the same effects of normalization in concurrent work.', 'in order to compare with, we used the datasets from the wmt 14 english to german task and the wmt 15 czech to english task for experiments.', 'we applied joint byte - pair encoding ( bpe )  #AUTHOR_TAG with 32k merge operations.', 'we used the same setting as the transformer base  #TAUTHOR_TAG except the number of warm - up steps was set to 8k.', 'we conducted our experiments based on the neutron implementation  #AUTHOR_TAG of the transformer.', 'parameters were initialized with glorot initialization 2  #AUTHOR_TAG like in many other transformer implementation  #AUTHOR_TAG.', 'our experiments run on 2 gtx 1080 ti gpus, and a batch size of at least 25k target tokens is achieved through gradient accumulation of small batches.', 'we used a beam size of 4 for decoding, and evaluated tokenized case - sensitive bleu with the averaged model of the last 5 checkpoints saved with an interval of 1, 500 training steps  #TAUTHOR_TAG.', 'results of two different computation order are shown in table 1.', 'v1 and v2 stand for the computation order of the proposed transformer  #TAUTHOR_TAG and that of the official implementation respectively. "" ¬ "" means fail to converge, "" none "" means not reported in original works, "" * "" indicates our implementation of their approach.', '† and ‡ mean p < 0. 01 and p < 0. 05 while comparing between v1 and v2 of the same number of layers in significance test']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'despite']","['', 'more recently, another contextual word representation model based on transformer, called bert, was released, and has become widely used for many nlp tasks  #AUTHOR_TAG.', 'instead of conducting directional modeling of context of a word, transformers like bert model relations between all pairs of tokens using a self - supervised strategy.', 'advances in these contextual representa - tion based model have created new state - of - the - art performance in many nlp benchmark tasks.', 'recent studies have shown that training contextual representations in texts from specific domains improves power of the model by capturing domain specific linguistic characteristics.', 'texts from biomedical publications and electronic medical record have been used to pre - train bert models for nlp task in this domain and showed considerable improvement in many downstream tasks  #TAUTHOR_TAG.', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'despite']","['', 'more recently, another contextual word representation model based on transformer, called bert, was released, and has become widely used for many nlp tasks  #AUTHOR_TAG.', 'instead of conducting directional modeling of context of a word, transformers like bert model relations between all pairs of tokens using a self - supervised strategy.', 'advances in these contextual representa - tion based model have created new state - of - the - art performance in many nlp benchmark tasks.', 'recent studies have shown that training contextual representations in texts from specific domains improves power of the model by capturing domain specific linguistic characteristics.', 'texts from biomedical publications and electronic medical record have been used to pre - train bert models for nlp task in this domain and showed considerable improvement in many downstream tasks  #TAUTHOR_TAG.', '']",0
"['− 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['− 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['2e − 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['all silos. at each silo, a model was trained using only data from that site. only model parameters of the models were then sent back to the analyzer for aggregation. an updated model is generated by averaging the parameters of models distributively trained', ', weighted by sample size ( konecny et al., 2016 ; mc  #AUTHOR_TAG. in this study, sample size is defined as the number of patients in the pretraining stage and number of notes in fine -', 'tuning stage. after model aggregation, the updated model was sent out to all sites again to repeat', 'the global training cycle. formally, the weight update is specified by : where q t ag is the', 'parameter of aggregated model at global cycle t, k is the number of data silos. n k is the number of samples at the', 'k th site, n is the total number of samples across all sites', ', and q k is the parameters learned from the k th data site alone. t is the global', 'cycle number in the range of [ 1, t ]. in the pre -', 'training stage, in each global cycle the bert model was trained for one epoch through', 'all clinical notes data at each of the silos using the default settings from the original bert publication  #AUTHOR_TAG. a total of 15 global cycles were run. the downstream task', 'performance plateaued out around cycle 10. therefore, bert model federately trained for 10 global cycles was used for down stream tasks. centralized', 'fine - tuning of the i2b2 ner tasks plateaued after 4 epochs, with the learning rate', 'set at 2e − 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine tuning using the same settings as centralized fine tuning, one epoch of training', 'was conducted in each global cycle and a total of 6 global cycles were conducted when the performance plateaued. each pre - training global cycle took around 4 hours on a tesla k80 gpu which', 'has a single precision gflops of 55918736. a single fed', '##erated figure 1 : federated bert model trained can be conducted at both pre - training and fine tuning stages. in federated pre - training stage, unlabelled clinical texts from different silos, such as', 'hospitals, are used for self - supervised training', 'to learn domain - specific linguistic characterises in a fed', '##erated manner with moving data outside their silos. in task - specific fine tuning stage, pre', '- trained bert model were further trained using labelled texts from different silos in a federated manner. fine tuning global took around 20 mins on the same device']",0
"['- beginning ( iob ) format  #AUTHOR_TAG using i2b2 2010  #AUTHOR_TAG and 2012  #AUTHOR_TAG data  #TAUTHOR_TAG. original training / development / test splits in the challenges were used.', '']","['( iob ) format  #AUTHOR_TAG using i2b2 2010  #AUTHOR_TAG and 2012  #AUTHOR_TAG data  #TAUTHOR_TAG. original training / development / test splits in the challenges were used.', '']","['- beginning ( iob ) format  #AUTHOR_TAG using i2b2 2010  #AUTHOR_TAG and 2012  #AUTHOR_TAG data  #TAUTHOR_TAG. original training / development / test splits in the challenges were used.', '']","['bert pre - trained on mimic corpus has been reported to have superior performance on ner tasks in inside - outside - beginning ( iob ) format  #AUTHOR_TAG using i2b2 2010  #AUTHOR_TAG and 2012  #AUTHOR_TAG data  #TAUTHOR_TAG. original training / development / test splits in the challenges were used.', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'despite']","['', 'more recently, another contextual word representation model based on transformer, called bert, was released, and has become widely used for many nlp tasks  #AUTHOR_TAG.', 'instead of conducting directional modeling of context of a word, transformers like bert model relations between all pairs of tokens using a self - supervised strategy.', 'advances in these contextual representa - tion based model have created new state - of - the - art performance in many nlp benchmark tasks.', 'recent studies have shown that training contextual representations in texts from specific domains improves power of the model by capturing domain specific linguistic characteristics.', 'texts from biomedical publications and electronic medical record have been used to pre - train bert models for nlp task in this domain and showed considerable improvement in many downstream tasks  #TAUTHOR_TAG.', '']",1
['in this corpus is only marginally worse than model trained on all notes types  #TAUTHOR_TAG'],['in this corpus is only marginally worse than model trained on all notes types  #TAUTHOR_TAG'],['in this corpus is only marginally worse than model trained on all notes types  #TAUTHOR_TAG'],"['this study, the publicly available mimic - iii corpus  #AUTHOR_TAG was used for contextual representation learning.', 'this corpus contains information for more than 58, 000 admissions for more than 45, 000 patients admitted to beth israel deaconess medical center in boston between 2001 and 2012.', 'different type of clinical notes are available in this corpus including discharge summaries, nursing notes and so on.', 'we included only discharge summaries in our study as previous studies have shown that performance of a model trained on only discharge summaries in this corpus is only marginally worse than model trained on all notes types  #TAUTHOR_TAG']",3
"['− 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['− 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['2e − 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['all silos. at each silo, a model was trained using only data from that site. only model parameters of the models were then sent back to the analyzer for aggregation. an updated model is generated by averaging the parameters of models distributively trained', ', weighted by sample size ( konecny et al., 2016 ; mc  #AUTHOR_TAG. in this study, sample size is defined as the number of patients in the pretraining stage and number of notes in fine -', 'tuning stage. after model aggregation, the updated model was sent out to all sites again to repeat', 'the global training cycle. formally, the weight update is specified by : where q t ag is the', 'parameter of aggregated model at global cycle t, k is the number of data silos. n k is the number of samples at the', 'k th site, n is the total number of samples across all sites', ', and q k is the parameters learned from the k th data site alone. t is the global', 'cycle number in the range of [ 1, t ]. in the pre -', 'training stage, in each global cycle the bert model was trained for one epoch through', 'all clinical notes data at each of the silos using the default settings from the original bert publication  #AUTHOR_TAG. a total of 15 global cycles were run. the downstream task', 'performance plateaued out around cycle 10. therefore, bert model federately trained for 10 global cycles was used for down stream tasks. centralized', 'fine - tuning of the i2b2 ner tasks plateaued after 4 epochs, with the learning rate', 'set at 2e − 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine tuning using the same settings as centralized fine tuning, one epoch of training', 'was conducted in each global cycle and a total of 6 global cycles were conducted when the performance plateaued. each pre - training global cycle took around 4 hours on a tesla k80 gpu which', 'has a single precision gflops of 55918736. a single fed', '##erated figure 1 : federated bert model trained can be conducted at both pre - training and fine tuning stages. in federated pre - training stage, unlabelled clinical texts from different silos, such as', 'hospitals, are used for self - supervised training', 'to learn domain - specific linguistic characterises in a fed', '##erated manner with moving data outside their silos. in task - specific fine tuning stage, pre', '- trained bert model were further trained using labelled texts from different silos in a federated manner. fine tuning global took around 20 mins on the same device']",3
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['bertbase model was pre - trained by mimic3 discharge summaries in a centralized manner  #TAUTHOR_TAG.', 'lastly, we look']","['order to understand whether large size contextual language representation model like bert can be pre - trained and fine tuned in a federated manner using data from different silos, we designed and conducted the following 6 experiments.', 'first of all, we looked at the scenarios no domain specific bert model pre - training was con - ducted.', 'in those cases, the parameters ( checkpoint ) from original bert base model trained on books corpus and english wikipedia  #AUTHOR_TAG.', 'we also looked at the scenarios where bertbase model was pre - trained by mimic3 discharge summaries in a centralized manner  #TAUTHOR_TAG.', 'lastly, we look at the scenarios where bertbase model was pre - trained by mimic3 discharge summaries in a federated manner.', 'for each of these conditions, we fine tuned bert models for downstream tasks using centralized vs. federated learning.', 'to summarize, six experiments were conducted']",3
"['− 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['− 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['2e − 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine']","['all silos. at each silo, a model was trained using only data from that site. only model parameters of the models were then sent back to the analyzer for aggregation. an updated model is generated by averaging the parameters of models distributively trained', ', weighted by sample size ( konecny et al., 2016 ; mc  #AUTHOR_TAG. in this study, sample size is defined as the number of patients in the pretraining stage and number of notes in fine -', 'tuning stage. after model aggregation, the updated model was sent out to all sites again to repeat', 'the global training cycle. formally, the weight update is specified by : where q t ag is the', 'parameter of aggregated model at global cycle t, k is the number of data silos. n k is the number of samples at the', 'k th site, n is the total number of samples across all sites', ', and q k is the parameters learned from the k th data site alone. t is the global', 'cycle number in the range of [ 1, t ]. in the pre -', 'training stage, in each global cycle the bert model was trained for one epoch through', 'all clinical notes data at each of the silos using the default settings from the original bert publication  #AUTHOR_TAG. a total of 15 global cycles were run. the downstream task', 'performance plateaued out around cycle 10. therefore, bert model federately trained for 10 global cycles was used for down stream tasks. centralized', 'fine - tuning of the i2b2 ner tasks plateaued after 4 epochs, with the learning rate', 'set at 2e − 5 and a batch', 'size of 32  #TAUTHOR_TAG. when conducting federated fine tuning using the same settings as centralized fine tuning, one epoch of training', 'was conducted in each global cycle and a total of 6 global cycles were conducted when the performance plateaued. each pre - training global cycle took around 4 hours on a tesla k80 gpu which', 'has a single precision gflops of 55918736. a single fed', '##erated figure 1 : federated bert model trained can be conducted at both pre - training and fine tuning stages. in federated pre - training stage, unlabelled clinical texts from different silos, such as', 'hospitals, are used for self - supervised training', 'to learn domain - specific linguistic characterises in a fed', '##erated manner with moving data outside their silos. in task - specific fine tuning stage, pre', '- trained bert model were further trained using labelled texts from different silos in a federated manner. fine tuning global took around 20 mins on the same device']",5
"['sequence models  #TAUTHOR_TAG, there are two things changing : from linear architecture to non -']","['random fields ( crf ) with newly proposed "" deep architecture "" sequence models  #TAUTHOR_TAG, there are two things changing : from linear architecture to non - linear, and']","['deep architecture "" sequence models  #TAUTHOR_TAG, there are two things changing : from linear architecture to non -']","['we compare the widely used conditional random fields ( crf ) with newly proposed "" deep architecture "" sequence models  #TAUTHOR_TAG, there are two things changing : from linear architecture to non - linear, and from discrete feature representation to distributional.', 'it is unclear, however, what utility nonlinearity offers in conventional featurebased models.', 'in this study, we show the close connection between crf and "" sequence model "" neural nets, and present an empirical investigation to compare their performance on two sequence labeling tasks - named entity recognition and syntactic chunking.', 'our results suggest that non - linear models are highly effective in low - dimensional distributional spaces.', 'somewhat surprisingly, we find that a nonlinear architecture offers no benefits in a high - dimensional discrete feature space']",4
['in  #TAUTHOR_TAG. we can add a hidden linear layer to this'],"['in  #TAUTHOR_TAG. we can add a hidden linear layer to this architecture to formulate a two - layer linear neural network ( lnn', '), as shown in the middle diagram of']",['in  #TAUTHOR_TAG. we can add a hidden linear layer to this'],"['##n ). 1 normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the ionn, which was commonly done in neural networks, such as in  #TAUTHOR_TAG. we can add a hidden linear layer to this architecture to formulate a two - layer linear neural network ( lnn', '']",4
"['grouped into unknown.', 'we attempt to replicate the model described in  #TAUTHOR_TAG mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly']","['grouped into unknown.', 'we attempt to replicate the model described in  #TAUTHOR_TAG mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly']","['with token pad, and unknown words are grouped into unknown.', 'we attempt to replicate the model described in  #TAUTHOR_TAG mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly']","['sequences of numbers are replaced with num ( e. g., "" ps1 "" would become "" psnum "" ), sentence boundaries are padded with token pad, and unknown words are grouped into unknown.', 'we attempt to replicate the model described in  #TAUTHOR_TAG mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly']",4
"['models ).', 'recently,  #TAUTHOR_TAG proposed']","['the probability distributions ( e. g., "" log - linear "" models ).', 'recently,  #TAUTHOR_TAG proposed "" deep architecture "" models for sequence labeling ( named']","['log - linear "" models ).', 'recently,  #TAUTHOR_TAG proposed']","['labeling encompasses an important class of nlp problems that aim at annotating natural language texts with various syntactic and semantic information, such as part - of - speech tags and named - entity labels.', 'output from such systems can facilitate downstream applications such as question answering and relation extraction.', 'most methods developed so far for sequence labeling employ generalized linear statistical models, meaning methods that describe the data as a combination of linear basis functions, either directly in the input variables space ( e. g., svm ) or through some transformation of the probability distributions ( e. g., "" log - linear "" models ).', 'recently,  #TAUTHOR_TAG proposed "" deep architecture "" models for sequence labeling ( named sentence - level likelihood neural nets, abbreviated as slnn henceforth ), and showed promising results on a range of tasks ( pos tagging, ner, chunking, and srl ).', 'two new changes were suggested : extending the model from a linear to non - linear architecture ; and replacing discrete feature representations with distributional feature representations in a continuous space.', 'it has generally been argued that nonlinearity between layers is vital to the power of neural models  #AUTHOR_TAG.', 'the relative contribution of these changes, however, is unclear, as is the question of whether gains can be made by introducing non - linearity to conventional featurebased models.', 'in this paper, we illustrate the close relationship between crf and slnn models, and conduct an empirical investigation of the effect of nonlinearity with different feature representations.', 'experiments on named entity recognition ( ner ) and syntactic chunking tasks suggest that non - linear models are highly effective in low - dimensional distributed feature space, but offer no benefits in high - dimensional discrete space.', 'furthermore, both linear and non - linear models improve when we combine the discrete and continuous feature spaces, but a linear model still outperforms the non - linear one']",1
['in  #TAUTHOR_TAG. we can add a hidden linear layer to this'],"['in  #TAUTHOR_TAG. we can add a hidden linear layer to this architecture to formulate a two - layer linear neural network ( lnn', '), as shown in the middle diagram of']",['in  #TAUTHOR_TAG. we can add a hidden linear layer to this'],"['##n ). 1 normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the ionn, which was commonly done in neural networks, such as in  #TAUTHOR_TAG. we can add a hidden linear layer to this architecture to formulate a two - layer linear neural network ( lnn', '']",3
['in  #TAUTHOR_TAG. we can add a hidden linear layer to this'],"['in  #TAUTHOR_TAG. we can add a hidden linear layer to this architecture to formulate a two - layer linear neural network ( lnn', '), as shown in the middle diagram of']",['in  #TAUTHOR_TAG. we can add a hidden linear layer to this'],"['##n ). 1 normalizing locally in a logistic regression is equivalent to adding a softmax layer to the output layer of the ionn, which was commonly done in neural networks, such as in  #TAUTHOR_TAG. we can add a hidden linear layer to this architecture to formulate a two - layer linear neural network ( lnn', '']",3
"['earlier literature  #TAUTHOR_TAG.', 'for models']","['earlier literature  #TAUTHOR_TAG.', 'for models']","['earlier literature  #TAUTHOR_TAG.', 'for models']","['all experiments, we used the development portion of the conll - 2003 data to tune the 2 - regularization parameter σ ( variance in gaussian prior ), and found 20 to be a stable value.', 'overall tuning σ does not affect the qualitative results in our experiments.', 'we terminate l - bfgs training when the average improvement is less than 1e - 3.', 'all model parameters are initialized to a random value in [ −0. 1, 0. 1 ] in order to break symmetry.', 'we did not explicitly tune the features used in crf to optimize for performance, since feature engineering is not the focus of this study.', 'however, overall we found that the feature set we used is competitive with crf results from earlier literature  #TAUTHOR_TAG.', 'for models that embed hidden layers, we set the number of hidden nodes to 300.', '2 results are reported on the standard evaluation metrics of entity / chunk precision, recall and f1 measure.', 'for experiments with continuous space feature representations ( a. k. a., word embeddings ), we took the word embeddings ( 130k words, 50 dimensions ) used in  #TAUTHOR_TAG, which were trained for 2 months over wikipedia text']",3
"['earlier literature  #TAUTHOR_TAG.', 'for models']","['earlier literature  #TAUTHOR_TAG.', 'for models']","['earlier literature  #TAUTHOR_TAG.', 'for models']","['all experiments, we used the development portion of the conll - 2003 data to tune the 2 - regularization parameter σ ( variance in gaussian prior ), and found 20 to be a stable value.', 'overall tuning σ does not affect the qualitative results in our experiments.', 'we terminate l - bfgs training when the average improvement is less than 1e - 3.', 'all model parameters are initialized to a random value in [ −0. 1, 0. 1 ] in order to break symmetry.', 'we did not explicitly tune the features used in crf to optimize for performance, since feature engineering is not the focus of this study.', 'however, overall we found that the feature set we used is competitive with crf results from earlier literature  #TAUTHOR_TAG.', 'for models that embed hidden layers, we set the number of hidden nodes to 300.', '2 results are reported on the standard evaluation metrics of entity / chunk precision, recall and f1 measure.', 'for experiments with continuous space feature representations ( a. k. a., word embeddings ), we took the word embeddings ( 130k words, 50 dimensions ) used in  #TAUTHOR_TAG, which were trained for 2 months over wikipedia text']",3
"['capture capitalization patterns, as described in  #TAUTHOR_TAG.', 'results of the crf and sl']","['capture capitalization patterns, as described in  #TAUTHOR_TAG.', 'results of the crf and slnn under this setting for the ner task is show in']","['capture capitalization patterns, as described in  #TAUTHOR_TAG.', 'results of the crf and sl']","['the next experiment, we replace the discrete input features with a continuous space representation by looking up the embedding of each word, and concatenate the embeddings of a five word window centered around the current position.', 'four binary features are also appended to each word embedding to capture capitalization patterns, as described in  #TAUTHOR_TAG.', 'results of the crf and slnn under this setting for the ner task is show in table 3.', 'with a continuous space representation, the slnn model works significantly better than a crf, by as much as 7 % on the conll development set, and 3. 7 % on ace dataset.', 'this suggests that there exist statistical dependencies within this low - dimensional ( 300 ) data that cannot be effectively captured by linear transformations, but can be modeled in the non - linear neural nets.', 'this perhaps coincides with the large performance im - provements observed from neural nets in handwritten digit recognition datasets as well  #AUTHOR_TAG, where dimensionality is also relatively low']",3
['the nonlinear neural networks used in  #TAUTHOR_TAG and'],['the nonlinear neural networks used in  #TAUTHOR_TAG and'],['the nonlinear neural networks used in  #TAUTHOR_TAG and'],"['carefully compared and analyzed the nonlinear neural networks used in  #TAUTHOR_TAG and the widely adopted crf, and revealed their close relationship.', 'through extensive experiments on ner and syntactic chunking, we have shown that non - linear architectures are effective in low dimensional continuous input spaces, but that they are not better suited for conventional highdimensional discrete input spaces.', 'furthermore, both linear and non - linear models benefit greatly from the combination of continuous and discrete features, especially for out - of - domain datasets.', 'this finding confirms earlier results that distributional representations can be used to achieve better generalization']",3
"['earlier literature  #TAUTHOR_TAG.', 'for models']","['earlier literature  #TAUTHOR_TAG.', 'for models']","['earlier literature  #TAUTHOR_TAG.', 'for models']","['all experiments, we used the development portion of the conll - 2003 data to tune the 2 - regularization parameter σ ( variance in gaussian prior ), and found 20 to be a stable value.', 'overall tuning σ does not affect the qualitative results in our experiments.', 'we terminate l - bfgs training when the average improvement is less than 1e - 3.', 'all model parameters are initialized to a random value in [ −0. 1, 0. 1 ] in order to break symmetry.', 'we did not explicitly tune the features used in crf to optimize for performance, since feature engineering is not the focus of this study.', 'however, overall we found that the feature set we used is competitive with crf results from earlier literature  #TAUTHOR_TAG.', 'for models that embed hidden layers, we set the number of hidden nodes to 300.', '2 results are reported on the standard evaluation metrics of entity / chunk precision, recall and f1 measure.', 'for experiments with continuous space feature representations ( a. k. a., word embeddings ), we took the word embeddings ( 130k words, 50 dimensions ) used in  #TAUTHOR_TAG, which were trained for 2 months over wikipedia text']",5
"['capture capitalization patterns, as described in  #TAUTHOR_TAG.', 'results of the crf and sl']","['capture capitalization patterns, as described in  #TAUTHOR_TAG.', 'results of the crf and slnn under this setting for the ner task is show in']","['capture capitalization patterns, as described in  #TAUTHOR_TAG.', 'results of the crf and sl']","['the next experiment, we replace the discrete input features with a continuous space representation by looking up the embedding of each word, and concatenate the embeddings of a five word window centered around the current position.', 'four binary features are also appended to each word embedding to capture capitalization patterns, as described in  #TAUTHOR_TAG.', 'results of the crf and slnn under this setting for the ner task is show in table 3.', 'with a continuous space representation, the slnn model works significantly better than a crf, by as much as 7 % on the conll development set, and 3. 7 % on ace dataset.', 'this suggests that there exist statistical dependencies within this low - dimensional ( 300 ) data that cannot be effectively captured by linear transformations, but can be modeled in the non - linear neural nets.', 'this perhaps coincides with the large performance im - provements observed from neural nets in handwritten digit recognition datasets as well  #AUTHOR_TAG, where dimensionality is also relatively low']",5
['the nonlinear neural networks used in  #TAUTHOR_TAG and'],['the nonlinear neural networks used in  #TAUTHOR_TAG and'],['the nonlinear neural networks used in  #TAUTHOR_TAG and'],"['carefully compared and analyzed the nonlinear neural networks used in  #TAUTHOR_TAG and the widely adopted crf, and revealed their close relationship.', 'through extensive experiments on ner and syntactic chunking, we have shown that non - linear architectures are effective in low dimensional continuous input spaces, but that they are not better suited for conventional highdimensional discrete input spaces.', 'furthermore, both linear and non - linear models benefit greatly from the combination of continuous and discrete features, especially for out - of - domain datasets.', 'this finding confirms earlier results that distributional representations can be used to achieve better generalization']",5
"['grouped into unknown.', 'we attempt to replicate the model described in  #TAUTHOR_TAG mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly']","['grouped into unknown.', 'we attempt to replicate the model described in  #TAUTHOR_TAG mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly']","['with token pad, and unknown words are grouped into unknown.', 'we attempt to replicate the model described in  #TAUTHOR_TAG mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly']","['sequences of numbers are replaced with num ( e. g., "" ps1 "" would become "" psnum "" ), sentence boundaries are padded with token pad, and unknown words are grouped into unknown.', 'we attempt to replicate the model described in  #TAUTHOR_TAG mentioned 5 binary features that look at the capitalization pattern of words to append to the embedding as additional dimensions, but only 4 were described in the paper, which we implemented accordingly']",6
"['system  #TAUTHOR_TAG to the problem, without further modification.', 'in']","['a state - of - the - art, discriminative l2p system  #TAUTHOR_TAG to the problem, without further modification.', 'in']","['a state - of - the - art, discriminative l2p system  #TAUTHOR_TAG to the problem, without further modification.', 'in']","['interpret the problem of transliterating english named entities into hindi or japanese katakana as a variant of the letter - to - phoneme ( l2p ) subtask of textto - speech processing.', 'therefore, we apply a re - implementation of a state - of - the - art, discriminative l2p system  #TAUTHOR_TAG to the problem, without further modification.', 'in doing so, we hope to provide a baseline for the news 2009 machine transliteration shared task  #AUTHOR_TAG, indicating how much can be achieved without transliteration - specific technology.', '']",5
"['described by  #TAUTHOR_TAG as faithfully as possible, and apply']","['described by  #TAUTHOR_TAG as faithfully as possible, and apply']","[""target lexicons  #AUTHOR_TAG, distributional similarity  #AUTHOR_TAG, or the dates of an entity's mentions in the news  #AUTHOR_TAG."", ""however, this task's focus on generation has isolated the character - level component, which makes l2p technology a nearideal match."", 'for our submission, we re - implement the l2p approach described by  #TAUTHOR_TAG as faithfully as possible, and apply']","['##iteration occurs when a word is borrowed into a language with a different character set from its language of origin.', 'the word is transcribed into the new character set in a manner that maintains phonetic correspondence.', 'when attempting to automate machine transliteration, modeling the channel that transforms source language characters into transliterated target language characters is a key component to good performance.', 'since the primary signal followed by human transliterators is phonetic correspondence, it makes sense that a letter - to - phoneme ( l2p ) transcription engine would perform well at this task.', ""of course, transliteration is often framed within the larger problems of translation and bilingual named entity co - reference, making available a number of other interesting features, such as target lexicons  #AUTHOR_TAG, distributional similarity  #AUTHOR_TAG, or the dates of an entity's mentions in the news  #AUTHOR_TAG."", ""however, this task's focus on generation has isolated the character - level component, which makes l2p technology a nearideal match."", 'for our submission, we re - implement the l2p approach described by  #TAUTHOR_TAG as faithfully as possible, and apply it unmodified to the transliteration shared task for the english - to - hindi  #AUTHOR_TAG and english - to - japanese katakana 1 tests']",5
"['feature templates described by  #TAUTHOR_TAG.', 'context features are centered around a transduction operation.', 'these features include an indicator']","['feature templates described by  #TAUTHOR_TAG.', 'context features are centered around a transduction operation.', 'these features include an indicator']","['follow the first two feature templates described by  #TAUTHOR_TAG.', 'context features are centered around a transduction operation.', 'these features include an indicator']","['core of the l2p transduction engine is the dynamic programming algorithm for monotone phrasal decoding  #AUTHOR_TAG.', 'the main feature of this algorithm is its capability to transduce many consecutive characters with a single operation.', 'this algorithm is used to conduct a search for a max - weight derivation according to a linear model with indicator features.', 'a sample derivation is shown in figure 1.', 'there are two main categories of features : context and transition features, which follow the first two feature templates described by  #TAUTHOR_TAG.', 'context features are centered around a transduction operation.', 'these features include an indicator for the operation itself, which is then conjoined with indicators for all n - grams of source context within a fixed window of the operation.', 'transition features are markov or n - gram features.', 'they ensure that the produced target string makes sense as a character sequence, and are represented as indicators on the presence of target n - grams.', 'the feature templates have two main parameters, the size s of the character window from which source context features are drawn, and the maximum length t of target n - gram indicators.', ""we fit these parameters using grid search over 1 - best the engine's features are trained using the structured perceptron  #AUTHOR_TAG."", ' #AUTHOR_TAG show strong improvements in the l2p domain using mira in place of the perceptron update ; unfortunately, we did not implement a k - best mira update due to time constraints.', 'in our implementation, no special consideration was given to the availability of multiple correct answers in the training data ; we always pick the first reference transliteration and treat it as the only correct answer.', 'investigating the use of all correct answers would be an obvious next step to improve the system']",5
"['by  #TAUTHOR_TAG, mostly']","['by  #TAUTHOR_TAG, mostly']","['by  #TAUTHOR_TAG, mostly']","['system made two alternate design decisions ( we do not claim improvements ) over those made by  #TAUTHOR_TAG, mostly based on the availability of software.', 'first, we employed a beam of 40 candidates in our decoder, to enable efficient use of large language model contexts.', 'this is put to good use in the hindi task, where we found n - gram indicators of length up to n = 6 provided optimal development performance.', 'second, we employed an alternate character aligner to create our training derivations.', 'this aligner is similar to recent non - compositional phrasal word - alignment models  #AUTHOR_TAG, limited so it can only produce monotone character alignments.', 'the aligner creates substring alignments, without insertion or deletion operators.', 'as such, an aligned transliteration pair also serves as a transliteration derivation.', 'we employed a maximum substring length of 3.', 'the training data was heuristically cleaned after alignment.', 'any derivation found by the aligner that uses an operation occurring fewer than 3 times throughout the entire training set was eliminated.', 'this reduced training set sizes to 8, 511 pairs for english - hindi and 20, 306 pairs for englishkatakana']",4
[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],['recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],"['field of neural machine translation ( nmt ), which seeks to use end - to - end neural networks to translate natural language text, has existed for only three years.', 'in that time, researchers have explored architectures ranging from convolutional neural networks  #AUTHOR_TAG to recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved better performance than traditional statistical or syntax - based mt techniques on many language pairs.', 'nmt models first achieved state - of - the - art performance on the wmt english→german news - domain task in 2015  #TAUTHOR_TAG and subsequent improvements have been reported since then  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the problem of machine translation is fundamentally a sequence - to - sequence transduction task, and most approaches have been based on an encoder - decoder architecture  #AUTHOR_TAG.', '']",0
[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],['recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],"['field of neural machine translation ( nmt ), which seeks to use end - to - end neural networks to translate natural language text, has existed for only three years.', 'in that time, researchers have explored architectures ranging from convolutional neural networks  #AUTHOR_TAG to recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved better performance than traditional statistical or syntax - based mt techniques on many language pairs.', 'nmt models first achieved state - of - the - art performance on the wmt english→german news - domain task in 2015  #TAUTHOR_TAG and subsequent improvements have been reported since then  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the problem of machine translation is fundamentally a sequence - to - sequence transduction task, and most approaches have been based on an encoder - decoder architecture  #AUTHOR_TAG.', '']",0
[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],['recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],"['field of neural machine translation ( nmt ), which seeks to use end - to - end neural networks to translate natural language text, has existed for only three years.', 'in that time, researchers have explored architectures ranging from convolutional neural networks  #AUTHOR_TAG to recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved better performance than traditional statistical or syntax - based mt techniques on many language pairs.', 'nmt models first achieved state - of - the - art performance on the wmt english→german news - domain task in 2015  #TAUTHOR_TAG and subsequent improvements have been reported since then  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the problem of machine translation is fundamentally a sequence - to - sequence transduction task, and most approaches have been based on an encoder - decoder architecture  #AUTHOR_TAG.', '']",0
[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],['recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],"['field of neural machine translation ( nmt ), which seeks to use end - to - end neural networks to translate natural language text, has existed for only three years.', 'in that time, researchers have explored architectures ranging from convolutional neural networks  #AUTHOR_TAG to recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved better performance than traditional statistical or syntax - based mt techniques on many language pairs.', 'nmt models first achieved state - of - the - art performance on the wmt english→german news - domain task in 2015  #TAUTHOR_TAG and subsequent improvements have been reported since then  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the problem of machine translation is fundamentally a sequence - to - sequence transduction task, and most approaches have been based on an encoder - decoder architecture  #AUTHOR_TAG.', '']",0
[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],['recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],"['field of neural machine translation ( nmt ), which seeks to use end - to - end neural networks to translate natural language text, has existed for only three years.', 'in that time, researchers have explored architectures ranging from convolutional neural networks  #AUTHOR_TAG to recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved better performance than traditional statistical or syntax - based mt techniques on many language pairs.', 'nmt models first achieved state - of - the - art performance on the wmt english→german news - domain task in 2015  #TAUTHOR_TAG and subsequent improvements have been reported since then  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the problem of machine translation is fundamentally a sequence - to - sequence transduction task, and most approaches have been based on an encoder - decoder architecture  #AUTHOR_TAG.', '']",3
"['described in  #TAUTHOR_TAG, using']","['described in  #TAUTHOR_TAG, using']","['described in  #TAUTHOR_TAG, using the attention mechanism']","['model identified as metamind - single is based on the attention - based encoder - decoder framework described in  #TAUTHOR_TAG, using the attention mechanism referred to as "" global attention ( dot ).', '']",3
[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],[' #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],['recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved'],"['field of neural machine translation ( nmt ), which seeks to use end - to - end neural networks to translate natural language text, has existed for only three years.', 'in that time, researchers have explored architectures ranging from convolutional neural networks  #AUTHOR_TAG to recurrent neural networks  #AUTHOR_TAG to attentional models  #TAUTHOR_TAG and achieved better performance than traditional statistical or syntax - based mt techniques on many language pairs.', 'nmt models first achieved state - of - the - art performance on the wmt english→german news - domain task in 2015  #TAUTHOR_TAG and subsequent improvements have been reported since then  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'the problem of machine translation is fundamentally a sequence - to - sequence transduction task, and most approaches have been based on an encoder - decoder architecture  #AUTHOR_TAG.', '']",5
"['described in  #TAUTHOR_TAG, using']","['described in  #TAUTHOR_TAG, using']","['described in  #TAUTHOR_TAG, using the attention mechanism']","['model identified as metamind - single is based on the attention - based encoder - decoder framework described in  #TAUTHOR_TAG, using the attention mechanism referred to as "" global attention ( dot ).', '']",5
"['described in  #TAUTHOR_TAG, using']","['described in  #TAUTHOR_TAG, using']","['described in  #TAUTHOR_TAG, using the attention mechanism']","['model identified as metamind - single is based on the attention - based encoder - decoder framework described in  #TAUTHOR_TAG, using the attention mechanism referred to as "" global attention ( dot ).', '']",4
"['on  #TAUTHOR_TAG, but provided a small additional boost to the ensemble']","['on  #TAUTHOR_TAG, but provided a small additional boost to the ensemble.', 'the primary contribution of this model is to demonstrate that purely attentional nmt is possible : the only']","['on  #TAUTHOR_TAG, but provided a small additional boost to the ensemble.', 'the primary contribution of this model is to demonstrate that purely attentional nmt is possible : the only inputs to the decoder are through the attention mechanism.', 'this may be helpful']","['for all three runs described above are presented in table 1.', ""only the ensemble was submitted to the human evaluation process, with a final ranking of second place ( behind u. edinburgh's ensemble of four independently initialized models )."", 'our best single model matches the performance of the best model from u. edinburgh, which applies a similar attentional framework, subword splitting, and back - translated augmentation.', 'the y - lstm model underperformed relative to the model based on  #TAUTHOR_TAG, but provided a small additional boost to the ensemble.', 'the primary contribution of this model is to demonstrate that purely attentional nmt is possible : the only inputs to the decoder are through the attention mechanism.', 'this may be helpful for using translation to build general attentional sentence encoding models, since the representation of the input sentence is entirely in the attentional encoding, not split between an attentional encoding vector and a vector representing the last timestep of the multilayer encoder hidden state']",4
"['that subjectivity detection may be easier to achieve  #TAUTHOR_TAG.', 'these additional']","['have been looking at the ability of characters to carry sentiment information  #AUTHOR_TAG.', 'in romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve  #TAUTHOR_TAG.', 'these additional']","['chinese, researchers have been looking at the ability of characters to carry sentiment information  #AUTHOR_TAG.', 'in romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve  #TAUTHOR_TAG.', 'these additional sources of information']","['much of the research work in this area has been applied to english, research on other languages is growing, including japanese, chinese, german, spanish, romanian.', 'while most of the researchers in the field are familiar with the methods applied on english, few of them have closely looked at the original research carried out in other languages.', 'for example, in languages such as chinese, researchers have been looking at the ability of characters to carry sentiment information  #AUTHOR_TAG.', 'in romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve  #TAUTHOR_TAG.', 'these additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in english as well.', 'the development and interest in these methods is also highly motivated by the fact that only 27 % of internet users speak english ( www. internetworldstats. com / stats. htm, oct 11, 2011 ), and that number diminishes further every year, as more people across the globe gain internet access.', 'the aim of this tutorial is to familiarize the attendees with the subjectivity and sentiment research carried out on languages other than english in order to enable and promote crossfertilization.', 'specifically, we will review work along three main directions.', 'first, we will present methods where the resources and tools have been specifically developed for a given target language.', 'in this category, we will also briefly overview the main methods that have been proposed for english, but which can be easily ported to other languages.', 'second, we will describe cross - lingual approaches, including several methods that have been proposed to leverage on the resources and tools available in english by using cross - lingual projections.', 'finally, third, we will show how the expression of opinions and polarity pervades language boundaries, and thus methods that holistically explore multiple languages at the same time can be effectively considered']",0
"['that subjectivity detection may be easier to achieve  #TAUTHOR_TAG.', 'these additional']","['have been looking at the ability of characters to carry sentiment information  #AUTHOR_TAG.', 'in romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve  #TAUTHOR_TAG.', 'these additional']","['chinese, researchers have been looking at the ability of characters to carry sentiment information  #AUTHOR_TAG.', 'in romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve  #TAUTHOR_TAG.', 'these additional sources of information']","['much of the research work in this area has been applied to english, research on other languages is growing, including japanese, chinese, german, spanish, romanian.', 'while most of the researchers in the field are familiar with the methods applied on english, few of them have closely looked at the original research carried out in other languages.', 'for example, in languages such as chinese, researchers have been looking at the ability of characters to carry sentiment information  #AUTHOR_TAG.', 'in romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve  #TAUTHOR_TAG.', 'these additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in english as well.', 'the development and interest in these methods is also highly motivated by the fact that only 27 % of internet users speak english ( www. internetworldstats. com / stats. htm, oct 11, 2011 ), and that number diminishes further every year, as more people across the globe gain internet access.', 'the aim of this tutorial is to familiarize the attendees with the subjectivity and sentiment research carried out on languages other than english in order to enable and promote crossfertilization.', 'specifically, we will review work along three main directions.', 'first, we will present methods where the resources and tools have been specifically developed for a given target language.', 'in this category, we will also briefly overview the main methods that have been proposed for english, but which can be easily ported to other languages.', 'second, we will describe cross - lingual approaches, including several methods that have been proposed to leverage on the resources and tools available in english by using cross - lingual projections.', 'finally, third, we will show how the expression of opinions and polarity pervades language boundaries, and thus methods that holistically explore multiple languages at the same time can be effectively considered']",1
"['systems has improved significantly  #TAUTHOR_TAG.', 'nonetheless, one cannot expect any single system, trained on a particular dataset, to']","['state - of - the - art systems has improved significantly  #TAUTHOR_TAG.', 'nonetheless, one cannot expect any single system, trained on a particular dataset, to']","['has improved significantly  #TAUTHOR_TAG.', 'nonetheless, one cannot expect any single system, trained on a particular dataset, to']","['', 'it is well known that sentences with difficult vocabulary, passive voice or complex structures, such as relative and subordinated clauses, can be challenging to understand.', 'text simplification has been found to be beneficial for language learners  #AUTHOR_TAG, children  #AUTHOR_TAG, and adults with low literacy skills ( arnaldo candido jr. and erick maziero and caroline gasperin and thiago a. s. pardo and lucia specia and sandra m.  #AUTHOR_TAG or language disabilities  #AUTHOR_TAG luz rello and ricardo baeza -  #AUTHOR_TAG.', 'to cater to these target reader populations, language teachers, linguists and other editors are often called upon to manually adapt a text.', 'to automate this time - consuming task, there has been much effort in developing systems for lexical simplification  #AUTHOR_TAG and syntactic simplification  #AUTHOR_TAG.', 'the performance of the state - of - the - art systems has improved significantly  #TAUTHOR_TAG.', 'nonetheless, one cannot expect any single system, trained on a particular dataset, to simplify arbitrary texts in a way that would suit all readers - for example, the kinds of english words and structures suitable for a native speaker in grade 6 are unlikely to be suitable for a non - native speaker in grade 4.', '']",0
"['', 'following  #TAUTHOR_TAG, our']","['', 'following  #TAUTHOR_TAG, our']","['', 'following  #TAUTHOR_TAG, our']","['', 'following  #TAUTHOR_TAG, our system simplifies neither proper nouns, as identified by the natural language toolkit  #AUTHOR_TAG, nor words in our stoplist, which are already simple.', 'in terms of the three - step framework described above, we use the word2vec model 1 to retrieve candidates for substitution in the first step.', 'we trained the model with all sentences from wikipedia.', 'for each target word, the model returns a list of the most similar words ; we extract the top 20 in this list that are included in the user - supplied vocabulary list.', 'in the next step, substitution selection, we re - rank these 20 words with a language model.', 'we trained a trigram model with the kenlm  #AUTHOR_TAG, again using all sentences from wikipedia.', 'we then place the 10 words with the highest probabilities in a drop - down list in our editor 2 ; for example, figure 1 shows the ten candidates offered for the word "" municipal "".', 'if none of the candidates are appropriate, the user can easily revert to the original word, which is also included in the drop - down list ; alternatively, the user can click on the text to directly edit it']",5
"['set  #TAUTHOR_TAG.', 'this dataset contains 500 manually annotated sentences ; the target word in']","['set  #TAUTHOR_TAG.', 'this dataset contains 500 manually annotated sentences ; the target word in']","['evaluated the performance of our algorithm on the mechanical turk lexical simplification data set  #TAUTHOR_TAG.', 'this dataset contains 500 manually annotated sentences ; the target word in each sentence was annotated by 50 independent annotators.', 'to simulate a teacher adapting an english text for hong kong pupils, we used the vocabulary list']","['evaluated the performance of our algorithm on the mechanical turk lexical simplification data set  #TAUTHOR_TAG.', 'this dataset contains 500 manually annotated sentences ; the target word in each sentence was annotated by 50 independent annotators.', 'to simulate a teacher adapting an english text for hong kong pupils, we used the vocabulary list from the hong kong education bureau ( edb, 2012 ).', 'to enable automatic evaluation, we considered only the 249 sentences in the dataset whose target word is not in our vocabulary list, but whose human annotations contain at least one word in the list.', 'precision is at 31 % for the top candidate ; it is at 57 % for the top ten candidates.', 'in other words, for 57 % of the target words, a valid substitution can be found in the drop - down list in the editor.', 'the input sentence is "" city of faizabad, the headquarters of faizabad district, is a municipal board in the state of uttar pradesh, india, and situated on the banks of river ghaghra. "" for syntactic simplification ( section 3 ), the system first splits its coordinated clauses into two sentences, s 1 = "" city of faizabad... state of uttar pradesh, india. "" ; and s 2 = "" city of faizabad is situated on the banks of river ghaghra "".', '']",5
"['evaluated the quality of syntactic simplification on the first 300 sentences in the mechanical turk lexical simplification data set  #TAUTHOR_TAG.', 'for']","['evaluated the quality of syntactic simplification on the first 300 sentences in the mechanical turk lexical simplification data set  #TAUTHOR_TAG.', 'for']","['evaluated the quality of syntactic simplification on the first 300 sentences in the mechanical turk lexical simplification data set  #TAUTHOR_TAG.', 'for each sentence, we asked a professor of linguistics to mark the types of syntactic simplification']","['evaluated the quality of syntactic simplification on the first 300 sentences in the mechanical turk lexical simplification data set  #TAUTHOR_TAG.', 'for each sentence, we asked a professor of linguistics to mark the types of syntactic simplification']",5
"['by  #TAUTHOR_TAG.', 'the corpus of articles we use is a complete xml dum']","['by  #TAUTHOR_TAG.', 'the corpus of articles we use is a complete xml dump of']","['by  #TAUTHOR_TAG.', 'the corpus of articles we use is a complete xml dump of the english wikipedia date']","['experimented with four datasets widely used in literature : bless  #AUTHOR_TAG, evalution  #AUTHOR_TAG, lenci / benotto  #AUTHOR_TAG, and weeds  #AUTHOR_TAG taken from the repository provided by  #TAUTHOR_TAG.', 'the corpus of articles we use is a complete xml dump of the english wikipedia dated 3 nov 2017']",3
"['.', 'similar to slqs, our depth measure is motivated by distributional informativeness hypothesis  #TAUTHOR_TAG.', 'however, without using the extensive computation of context vectors and entropy, we are able to demonstrate good']","['of slqs is 87 %.', 'similar to slqs, our depth measure is motivated by distributional informativeness hypothesis  #TAUTHOR_TAG.', 'however, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance.', 'as can be seen by physically examining wikipedia articles, many of them']","['.', 'similar to slqs, our depth measure is motivated by distributional informativeness hypothesis  #TAUTHOR_TAG.', 'however, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance.', 'as can be seen by physically examining wikipedia articles, many of them tend to have a star topology.', 'this is also indicative that the topology used plays a major role in this feature.', 'more sophisticated techniques will be needed to identify the topology of individual articles']","['', 'however, here too, it is 91. 8 %.', 'for bless, as seen in  #AUTHOR_TAG, the performance of slqs is 87 %.', 'similar to slqs, our depth measure is motivated by distributional informativeness hypothesis  #TAUTHOR_TAG.', 'however, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance.', 'as can be seen by physically examining wikipedia articles, many of them tend to have a star topology.', 'this is also indicative that the topology used plays a major role in this feature.', 'more sophisticated techniques will be needed to identify the topology of individual articles']",3
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",3
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",3
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",3
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",3
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",3
"['by  #TAUTHOR_TAG.', 'the corpus of articles we use is a complete xml dum']","['by  #TAUTHOR_TAG.', 'the corpus of articles we use is a complete xml dump of']","['by  #TAUTHOR_TAG.', 'the corpus of articles we use is a complete xml dump of the english wikipedia date']","['experimented with four datasets widely used in literature : bless  #AUTHOR_TAG, evalution  #AUTHOR_TAG, lenci / benotto  #AUTHOR_TAG, and weeds  #AUTHOR_TAG taken from the repository provided by  #TAUTHOR_TAG.', 'the corpus of articles we use is a complete xml dump of the english wikipedia dated 3 nov 2017']",5
"['.', 'similar to slqs, our depth measure is motivated by distributional informativeness hypothesis  #TAUTHOR_TAG.', 'however, without using the extensive computation of context vectors and entropy, we are able to demonstrate good']","['of slqs is 87 %.', 'similar to slqs, our depth measure is motivated by distributional informativeness hypothesis  #TAUTHOR_TAG.', 'however, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance.', 'as can be seen by physically examining wikipedia articles, many of them']","['.', 'similar to slqs, our depth measure is motivated by distributional informativeness hypothesis  #TAUTHOR_TAG.', 'however, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance.', 'as can be seen by physically examining wikipedia articles, many of them tend to have a star topology.', 'this is also indicative that the topology used plays a major role in this feature.', 'more sophisticated techniques will be needed to identify the topology of individual articles']","['', 'however, here too, it is 91. 8 %.', 'for bless, as seen in  #AUTHOR_TAG, the performance of slqs is 87 %.', 'similar to slqs, our depth measure is motivated by distributional informativeness hypothesis  #TAUTHOR_TAG.', 'however, without using the extensive computation of context vectors and entropy, we are able to demonstrate good performance.', 'as can be seen by physically examining wikipedia articles, many of them tend to have a star topology.', 'this is also indicative that the topology used plays a major role in this feature.', 'more sophisticated techniques will be needed to identify the topology of individual articles']",5
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",5
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",5
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",5
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",5
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",5
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",4
"['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for']","['given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for']","['this experiment, we aim to discriminate pairs of words connected by the hypernym relation, from words connected by other relations ( meronym, coord, attribute, event, antonym, synonym ).', 'for each pair, we evaluate our scoring function given in expression ( 2 ).', 'we compared our numbers with those given in  #TAUTHOR_TAG.', 'in that paper, multiple measures are used, and the best performing measure for every row of the table is presented.', 'we conducted the experiments for both, star as well as the linear topology.', 'however, the results for star topology were slightly better, hence we present these in table ( 2 ).', '']",6
"['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified']","['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified']","['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.', 'in this paper, we will investigate the case']","['', 'the ud guidelines however have been written with the intent to maximize crosslinguistic parallelism and this constraint has forced the guidelines developers to sometimes choose representations that are known to be worse for parsing ( de  #AUTHOR_TAG.', 'for that reason, de  #AUTHOR_TAG suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation.', 'transforming tree representations for the purpose of parsing is not a new idea.', 'it has been done for constituency parsing for example by  #AUTHOR_TAG but also for dependency parsing for example by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.', 'in this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by  #TAUTHOR_TAG on ud treebanks to find out whether or not the alternative representation is useful for parsing with ud.', ""have shown that modifying coordination constructions and verb groups from their representation in the prague dependency treebank ( henceforth pdt ) to a representation described in melcuk ( 1988 ) ( mel'cuk style, henceforth ms ) improves dependency parsing for czech."", 'the procedure they follow is as follows']",0
"['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified']","['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified']","['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.', 'in this paper, we will investigate the case']","['', 'the ud guidelines however have been written with the intent to maximize crosslinguistic parallelism and this constraint has forced the guidelines developers to sometimes choose representations that are known to be worse for parsing ( de  #AUTHOR_TAG.', 'for that reason, de  #AUTHOR_TAG suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation.', 'transforming tree representations for the purpose of parsing is not a new idea.', 'it has been done for constituency parsing for example by  #AUTHOR_TAG but also for dependency parsing for example by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.', 'in this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by  #TAUTHOR_TAG on ud treebanks to find out whether or not the alternative representation is useful for parsing with ud.', ""have shown that modifying coordination constructions and verb groups from their representation in the prague dependency treebank ( henceforth pdt ) to a representation described in melcuk ( 1988 ) ( mel'cuk style, henceforth ms ) improves dependency parsing for czech."", 'the procedure they follow is as follows']",0
"[').', ' #TAUTHOR_TAG have shown that these']","['standard ).', ' #TAUTHOR_TAG have shown that these']","['', ' #TAUTHOR_TAG have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.', ' #AUTHOR_TAG conducted a study over the alternative representations']","['', ' #TAUTHOR_TAG have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.', ' #AUTHOR_TAG conducted a study over the alternative representations of 6 constructions across 5 parsing models for english and found that some of them are easier to parse than others.', 'their results were consistent across parsing models.', 'the motivations behind those two types of studies are different.', 'have originally a representation that is more semantically oriented and potentially useful for nlp applications which they therefore wish their output to have, the pdt style, and change it to a representation that is more syntactically oriented, the ms style, because it is easier to parse.', 'by contrast,  #AUTHOR_TAG have no a priori preference for any of the different alternatives of the constructions they study and instead study the effect of the different representations on parsing for the purpose of choosing one representation over the other.', 'their methodology is therefore different, they evaluate the different representations on their respective gold standard.', 'they argue that accuracy within a representation is a good indicator of the learnability of that representation and they argue that learnability is a good criterion for selecting a syntactic representation among alternatives.', 'in any case, these studies seem to show that such transformations can affect parsing for various languages and for various parsing models.', ' #AUTHOR_TAG were the first to obtain negative results from such transformations.', 'they attempted to modify certain constructions in a ud treebank to improve parsing for english but failed to show any improvement.', 'some transformations even decreased parsing accuracy.', '']",0
"['1.', ' #TAUTHOR_TAG show that making the auxiliary the head of']","['1.', ' #TAUTHOR_TAG show that making the auxiliary the head of']","['auxiliary dependencies, as in figure 1.', ' #TAUTHOR_TAG show that making the auxiliary the head of']","['the pdt, main verbs are the head of auxiliary dependencies, as in figure 1.', ' #TAUTHOR_TAG show that making the auxiliary the head of the dependency as in figure 2 is useful for parsing czech and slovenian.', ' #AUTHOR_TAG verb groups are easier to parse when the auxiliary is the head ( as in pdt ) than when the verb is the head ( as in ms ).', 'since ud adopts the pdt style representation of verb groups, it would be interesting to find out whether or not transforming them to ms could also improve parsing.', 'this is what will be attempted in this study.', 'describe algorithms for such a transformation as well as its back transformation.', 'however, their back transformation algorithm assumes that the auxiliary appears to the left of the verb which is not always the case.', 'in addition, it is unclear what they do with the cases in which there are two auxiliaries in a verb group.', 'for these reasons, we will use a slightly modified version of this algorithm that we describe in section 3']",0
['by  #TAUTHOR_TAG that has shown that'],['by  #TAUTHOR_TAG that has shown that'],"['this paper, we have attempted to reproduce a study by  #TAUTHOR_TAG that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port']","['this paper, we have attempted to reproduce a study by  #TAUTHOR_TAG that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with universal dependencies.', 'contrary to expectations, the study has given evidence that main verbs should stay heads of auxiliary dependency relations for parsing with ud.', 'the benefits of error analyses for such a study have been highlighted because they allow us to shed more light on the different ways in which the transformations affect the parsing output.', 'experiments suggest that gains obtained from verb group transformations in previous studies have been obtained mainly because those transformations help disambiguating between main verbs and auxiliaries.', 'it is however still an open question why the vg transformation hurts parsing accuracy in the case of ud.', 'it seems that the transformation makes the construction harder to learn which might be because it makes it less flat.', 'future work could carry out an error analysis that is more detailed than was the case in this study.', 'repeating those experiments with other tree transformations that have been shown to be successful in the past, such as making prepositions the head of prepositional phrases, as well as looking at other parsing models would provide more insight into the relationship between tree transformations and parsing']",0
"['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified']","['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified']","['by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.', 'in this paper, we will investigate the case']","['', 'the ud guidelines however have been written with the intent to maximize crosslinguistic parallelism and this constraint has forced the guidelines developers to sometimes choose representations that are known to be worse for parsing ( de  #AUTHOR_TAG.', 'for that reason, de  #AUTHOR_TAG suggest that those representations could be modified for the purpose of parsing, thus creating a parsing representation.', 'transforming tree representations for the purpose of parsing is not a new idea.', 'it has been done for constituency parsing for example by  #AUTHOR_TAG but also for dependency parsing for example by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG modified the representation of several constructions in several languages and obtained a consistent improvement in parsing accuracy.', 'in this paper, we will investigate the case of the verb group construction and attempt to reproduce the study by  #TAUTHOR_TAG on ud treebanks to find out whether or not the alternative representation is useful for parsing with ud.', ""have shown that modifying coordination constructions and verb groups from their representation in the prague dependency treebank ( henceforth pdt ) to a representation described in melcuk ( 1988 ) ( mel'cuk style, henceforth ms ) improves dependency parsing for czech."", 'the procedure they follow is as follows']",5
"['will follow the methodology from  #TAUTHOR_TAG,']","['will follow the methodology from  #TAUTHOR_TAG,']","['will follow the methodology from  #TAUTHOR_TAG,']","['will follow the methodology from  #TAUTHOR_TAG, that is, to transform, parse and then detransform the data so as to compare the original and the transformed model on the original gold standard.', 'the method from  #AUTHOR_TAG which consists in comparing the baseline and the transformed data on their respective gold standard is less relevant here because ud is believed to be a useful representation and that the aim will be to improve parsing within that representation.', 'however, as was argued in that study, their method can give an indication of the learnability of a construction and can potentially be used to understand the results obtained by the parse - transform - detransform method.', 'for this reason, this method will also be attempted.', 'in addition, the original parsed data will also be transformed into the ms gold standard for comparison with the ms parsed data on the ms gold standard.', 'comparing the two can potentially help find out if the error amplifications described in the background section are strongly influencing the results.', 'as a matter of fact, if the transformed model is penalized by error amplifications on the original gold standard, it is expected that the original model will be penalized in the same way on the transformed gold standard']",5
"['treebanks.', 'for comparability with the study in  #TAUTHOR_TAG, and']","['treebanks.', 'for comparability with the study in  #TAUTHOR_TAG, and']","['out of the 37 treebanks.', 'for comparability with the study in  #TAUTHOR_TAG, and']","['ran all experiments on ud 1. 2  #AUTHOR_TAG.', 'treebanks that had 0. 1 % or less of auxiliary dependency relations were discarded.', 'japanese was also discarded because the japanese treebank is not open source.', 'dutch was discarded because the back transformation accuracy was low ( 90 % ).', 'this is due to inconsistencies in the annotation : verb groups are annotated as a chain of dependency relations.', 'this leaves us with a total of 25 out of the 37 treebanks.', 'for comparability with the study in  #TAUTHOR_TAG, and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the czech and slovenian treebanks that they worked on, respectively version 1. 0 of the pdt  #AUTHOR_TAG and the 2006 version of sdt  #AUTHOR_TAG.', 'overview of the data used for the experiments']",5
['by  #TAUTHOR_TAG that has shown that'],['by  #TAUTHOR_TAG that has shown that'],"['this paper, we have attempted to reproduce a study by  #TAUTHOR_TAG that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port']","['this paper, we have attempted to reproduce a study by  #TAUTHOR_TAG that has shown that making auxiliaries heads in verb groups improves parsing but failed to show that those results port to parsing with universal dependencies.', 'contrary to expectations, the study has given evidence that main verbs should stay heads of auxiliary dependency relations for parsing with ud.', 'the benefits of error analyses for such a study have been highlighted because they allow us to shed more light on the different ways in which the transformations affect the parsing output.', 'experiments suggest that gains obtained from verb group transformations in previous studies have been obtained mainly because those transformations help disambiguating between main verbs and auxiliaries.', 'it is however still an open question why the vg transformation hurts parsing accuracy in the case of ud.', 'it seems that the transformation makes the construction harder to learn which might be because it makes it less flat.', 'future work could carry out an error analysis that is more detailed than was the case in this study.', 'repeating those experiments with other tree transformations that have been shown to be successful in the past, such as making prepositions the head of prepositional phrases, as well as looking at other parsing models would provide more insight into the relationship between tree transformations and parsing']",5
"[').', ' #TAUTHOR_TAG have shown that these']","['standard ).', ' #TAUTHOR_TAG have shown that these']","['', ' #TAUTHOR_TAG have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.', ' #AUTHOR_TAG conducted a study over the alternative representations']","['', ' #TAUTHOR_TAG have shown that these same modifications as well as the modification of nonprojective structures helps parsing in four languages.', ' #AUTHOR_TAG conducted a study over the alternative representations of 6 constructions across 5 parsing models for english and found that some of them are easier to parse than others.', 'their results were consistent across parsing models.', 'the motivations behind those two types of studies are different.', 'have originally a representation that is more semantically oriented and potentially useful for nlp applications which they therefore wish their output to have, the pdt style, and change it to a representation that is more syntactically oriented, the ms style, because it is easier to parse.', 'by contrast,  #AUTHOR_TAG have no a priori preference for any of the different alternatives of the constructions they study and instead study the effect of the different representations on parsing for the purpose of choosing one representation over the other.', 'their methodology is therefore different, they evaluate the different representations on their respective gold standard.', 'they argue that accuracy within a representation is a good indicator of the learnability of that representation and they argue that learnability is a good criterion for selecting a syntactic representation among alternatives.', 'in any case, these studies seem to show that such transformations can affect parsing for various languages and for various parsing models.', ' #AUTHOR_TAG were the first to obtain negative results from such transformations.', 'they attempted to modify certain constructions in a ud treebank to improve parsing for english but failed to show any improvement.', 'some transformations even decreased parsing accuracy.', '']",7
"['treebanks.', 'for comparability with the study in  #TAUTHOR_TAG, and']","['treebanks.', 'for comparability with the study in  #TAUTHOR_TAG, and']","['out of the 37 treebanks.', 'for comparability with the study in  #TAUTHOR_TAG, and']","['ran all experiments on ud 1. 2  #AUTHOR_TAG.', 'treebanks that had 0. 1 % or less of auxiliary dependency relations were discarded.', 'japanese was also discarded because the japanese treebank is not open source.', 'dutch was discarded because the back transformation accuracy was low ( 90 % ).', 'this is due to inconsistencies in the annotation : verb groups are annotated as a chain of dependency relations.', 'this leaves us with a total of 25 out of the 37 treebanks.', 'for comparability with the study in  #TAUTHOR_TAG, and because we used a slightly modified version of their algorithm, we also tested the approach on the versions of the czech and slovenian treebanks that they worked on, respectively version 1. 0 of the pdt  #AUTHOR_TAG and the 2006 version of sdt  #AUTHOR_TAG.', 'overview of the data used for the experiments']",4
['prior systems of  #TAUTHOR_TAG and  #AUTHOR_TAG'],['prior systems of  #TAUTHOR_TAG and  #AUTHOR_TAG'],"['on multiple entity linking datasets, outperforming the prior systems of  #TAUTHOR_TAG and  #AUTHOR_TAG']","['key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts.', ""we present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity."", 'these convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n - grams characterize different topics.', 'we combine these networks with a sparse linear model to achieve state - of - the - art performance on multiple entity linking datasets, outperforming the prior systems of  #TAUTHOR_TAG and  #AUTHOR_TAG']",1
"[' #TAUTHOR_TAG.', 'but an even simpler approach']","[' #TAUTHOR_TAG.', 'but an even simpler approach']","['- similar entities, which can give us clues about the identity of the mention currently being resolved  #TAUTHOR_TAG.', 'but an even simpler approach']","['of the major challenges of entity linking is resolving contextually polysemous mentions.', ""for example, germany may refer to a nation, to that nation's government, or even to a soccer team."", 'past approaches to such cases have often focused on collective entity linking : nearby mentions in a document might be expected to link to topically - similar entities, which can give us clues about the identity of the mention currently being resolved  #TAUTHOR_TAG.', 'but an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context.', 'in past work, these approaches have typically relied on heuristics such as tf - idf ( ratinov et 1 source available at github. com / matthewfl / nlp - entity - convnet al., 2011 ), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning - based methods.', ""in this work, we model semantic similarity between a mention's source document context and its potential entity targets using convolutional neural networks ( cnns )."", 'cnns have been shown to be effective for sentence classification tasks  #AUTHOR_TAG and for capturing similarity in models for entity linking  #AUTHOR_TAG and other related tasks  #AUTHOR_TAG, so we expect them to be effective at isolating the relevant topic semantics for entity linking.', 'we show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context.', 'finally, we show how to integrate these networks with a preexisting entity linking system  #TAUTHOR_TAG.', 'through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state - ofthe - art performance across several datasets']",1
['frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. this suggests that the system of  #TAUTHOR_TAG has the power'],['frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. this suggests that the system of  #TAUTHOR_TAG has the power'],['those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. this suggests that the system of  #TAUTHOR_TAG has the power'],"['the sparse feature system, the highest weighted features are typically those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. this suggests that the system of  #TAUTHOR_TAG has the power to pick the right span of a mention to resolve, but then is left to generally pick the most common link target in wikipedia, which is not always correct.', 'by contrast, the full system has a greater ability to pick less common link targets if the topic indicators distilled from the cnns indicate that it should do so']",1
"[' #TAUTHOR_TAG.', 'but an even simpler approach']","[' #TAUTHOR_TAG.', 'but an even simpler approach']","['- similar entities, which can give us clues about the identity of the mention currently being resolved  #TAUTHOR_TAG.', 'but an even simpler approach']","['of the major challenges of entity linking is resolving contextually polysemous mentions.', ""for example, germany may refer to a nation, to that nation's government, or even to a soccer team."", 'past approaches to such cases have often focused on collective entity linking : nearby mentions in a document might be expected to link to topically - similar entities, which can give us clues about the identity of the mention currently being resolved  #TAUTHOR_TAG.', 'but an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context.', 'in past work, these approaches have typically relied on heuristics such as tf - idf ( ratinov et 1 source available at github. com / matthewfl / nlp - entity - convnet al., 2011 ), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning - based methods.', ""in this work, we model semantic similarity between a mention's source document context and its potential entity targets using convolutional neural networks ( cnns )."", 'cnns have been shown to be effective for sentence classification tasks  #AUTHOR_TAG and for capturing similarity in models for entity linking  #AUTHOR_TAG and other related tasks  #AUTHOR_TAG, so we expect them to be effective at isolating the relevant topic semantics for entity linking.', 'we show that convolutions over multiple granularities of the input document are useful for providing different notions of semantic context.', 'finally, we show how to integrate these networks with a preexisting entity linking system  #TAUTHOR_TAG.', 'through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state - ofthe - art performance across several datasets']",0
"['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['each article on wikipedia. this information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a', 'mention. for example, we may have never observed president barack obama as a linked string on wikipedia, even though we have seen the substring barack obama and it', 'unambiguously indicates the correct answer. following  #TAUTHOR_TAG, we introduce a latent variable q to capture which subset of a mention ( known as a query', ') we resolve. query generation includes potentially removing stop words, plural suffixes, punctuation, and leading or tailing words. this processes generates on average 9 queries for each mention. conveniently, this', '']",0
['frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. this suggests that the system of  #TAUTHOR_TAG has the power'],['frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. this suggests that the system of  #TAUTHOR_TAG has the power'],['those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. this suggests that the system of  #TAUTHOR_TAG has the power'],"['the sparse feature system, the highest weighted features are typically those indicating the frequency that a page was linked to and those indicating specific lexical items in the choice of the latent query variable q. this suggests that the system of  #TAUTHOR_TAG has the power to pick the right span of a mention to resolve, but then is left to generally pick the most common link target in wikipedia, which is not always correct.', 'by contrast, the full system has a greater ability to pick less common link targets if the topic indicators distilled from the cnns indicate that it should do so']",0
"['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['each article on wikipedia. this information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a', 'mention. for example, we may have never observed president barack obama as a linked string on wikipedia, even though we have seen the substring barack obama and it', 'unambiguously indicates the correct answer. following  #TAUTHOR_TAG, we introduce a latent variable q to capture which subset of a mention ( known as a query', ') we resolve. query generation includes potentially removing stop words, plural suffixes, punctuation, and leading or tailing words. this processes generates on average 9 queries for each mention. conveniently, this', '']",6
"['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['each article on wikipedia. this information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a', 'mention. for example, we may have never observed president barack obama as a linked string on wikipedia, even though we have seen the substring barack obama and it', 'unambiguously indicates the correct answer. following  #TAUTHOR_TAG, we introduce a latent variable q to capture which subset of a mention ( known as a query', ') we resolve. query generation includes potentially removing stop words, plural suffixes, punctuation, and leading or tailing words. this processes generates on average 9 queries for each mention. conveniently, this', '']",6
"['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['each article on wikipedia. this information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a', 'mention. for example, we may have never observed president barack obama as a linked string on wikipedia, even though we have seen the substring barack obama and it', 'unambiguously indicates the correct answer. following  #TAUTHOR_TAG, we introduce a latent variable q to capture which subset of a mention ( known as a query', ') we resolve. query generation includes potentially removing stop words, plural suffixes, punctuation, and leading or tailing words. this processes generates on average 9 queries for each mention. conveniently, this', '']",6
"[';  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['performed experiments on 4 different entity linking datasets.', '• ace ( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '• conll - yago  #AUTHOR_TAG : this corpus is based on the conll 2003 dataset ; the test set consists of 231 news articles and contains a number of rarer entities.', '• wp  #AUTHOR_TAG : this dataset consists of short snippets from wikipedia.', '• table 2 : performance of the system in this work ( full ) compared to two baselines from prior work and two ablations.', '']",6
"['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['. following  #TAUTHOR_TAG, we introduce a latent variable q to capture']","['each article on wikipedia. this information can serve as a useful prior, but only if we can leverage it effectively by targeting the most salient part of a', 'mention. for example, we may have never observed president barack obama as a linked string on wikipedia, even though we have seen the substring barack obama and it', 'unambiguously indicates the correct answer. following  #TAUTHOR_TAG, we introduce a latent variable q to capture which subset of a mention ( known as a query', ') we resolve. query generation includes potentially removing stop words, plural suffixes, punctuation, and leading or tailing words. this processes generates on average 9 queries for each mention. conveniently, this', '']",3
"[';  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['performed experiments on 4 different entity linking datasets.', '• ace ( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '• conll - yago  #AUTHOR_TAG : this corpus is based on the conll 2003 dataset ; the test set consists of 231 news articles and contains a number of rarer entities.', '• wp  #AUTHOR_TAG : this dataset consists of short snippets from wikipedia.', '• table 2 : performance of the system in this work ( full ) compared to two baselines from prior work and two ablations.', '']",3
"[';  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '•']","['performed experiments on 4 different entity linking datasets.', '• ace ( nist, 2005 ;  #AUTHOR_TAG :', 'this corpus was used in  #AUTHOR_TAG and  #TAUTHOR_TAG.', '• conll - yago  #AUTHOR_TAG : this corpus is based on the conll 2003 dataset ; the test set consists of 231 news articles and contains a number of rarer entities.', '• wp  #AUTHOR_TAG : this dataset consists of short snippets from wikipedia.', '• table 2 : performance of the system in this work ( full ) compared to two baselines from prior work and two ablations.', '']",5
"['seen in the training set [ 9, 11,  #TAUTHOR_TAG']","['of common n - grams and full captions seen in the training set [ 9, 11,  #TAUTHOR_TAG']","['of common n - grams and full captions seen in the training set [ 9, 11,  #TAUTHOR_TAG.', 'contributing to this problem is a combination of biased datasets and insufficient quality metrics.', 'while the main benchmarking dataset']","['', 'it is also common to include one or more attention layers to focus the captions on the most salient parts of an image.', 'the standard way of training is through maximum likelihood estimation ( mle ) by using a crossentropy loss to replicate ground - truth human - written captions for corresponding images.', 'recent image captioning models of this kind [ 1, 11, 12, 28 ] have shown impressive results, much thanks to the powerful language modelling capabilities of long short - term memory ( lstm ) [ 15 ] rnns.', 'however, although mle training enables models to confidently generate captions that have a high likelihood in the training set, it limits their capacity to generate novel descriptions.', 'their output exhibits a disproportionate replication of common n - grams and full captions seen in the training set [ 9, 11,  #TAUTHOR_TAG.', 'contributing to this problem is a combination of biased datasets and insufficient quality metrics.', 'while the main benchmarking dataset for image captioning, ms coco, makes available over 120k images with 5 human - annotated captions each [ 6 ], the selection process for the images suggests a lack of diversity in both content and composition [ 11, 20 ].', 'furthermore, the standard benchmarking metrics, based on ngram level overlap between generated captions and ground - truth captions, reward models with a bias towards common n - grams.', 'this leads to the ( indirect and unwanted ) consequence of incentivizing models that output generic captions that are likely to fit a range of similar images, despite missing the goal of describing the relevant aspects specific to each image.', 'in this paper, we propose a model that produces more diverse and specific captions by integrating a natural language understanding ( nlu ) component in our training which optimizes the specificity of our natural language generation ( nlg ) component.', 'our main contribution is an unsupervised specificity - guided training approach that improves the diversity and semantic accuracy of the generated']",0
"['of image captioning models [ 5,  #TAUTHOR_TAG.', 'benchmarking methods from machine translation [ 3, 19, 23 ] have been']","['of image captioning models [ 5,  #TAUTHOR_TAG.', 'benchmarking methods from machine translation [ 3, 19, 23 ] have been']","['subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of image captioning models [ 5,  #TAUTHOR_TAG.', 'benchmarking methods from machine translation [ 3, 19, 23 ] have been appropriated, while other somewhat similar methods']","['subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of image captioning models [ 5,  #TAUTHOR_TAG.', 'benchmarking methods from machine translation [ 3, 19, 23 ] have been appropriated, while other somewhat similar methods such as cider [ 27 ] have been proposed specifically for assessing the quality of image captions.', 'all these approaches unfortunately have a strong focus on replicating common n - grams from the ground - truth captions [ 5 ] and do not take into account the richness and diversity of human expression [ 9,  #TAUTHOR_TAG.', 'moreover, it has been found that this class of metrics suffers from poor correlations with human evaluation, with cider and meteor having the highest correlations among them [ 5 ].', 'with the recognition of these limitations, there has been a growing interest in developing metrics that measure other desirable qualities in captions.', 'spice [ 2 ] is a recent addition which measures the overlap of content by comparing automatically generated scene - graphs from the ground - truth and generated captions.', 'while being a relevant addition, it does not solve the problem of generic captions.', 'rare occurrences and more detailed descriptions are more likely to incur a penalty than common concepts ; e. g. correctly specifying a purple flower where the ground - truth text omits its color would register a false positive for the color.', ""this, again, encourages the'' safe'' generic captions that we want to move away from""]",0
"['of image captioning models [ 5,  #TAUTHOR_TAG.', 'benchmarking methods from machine translation [ 3, 19, 23 ] have been']","['of image captioning models [ 5,  #TAUTHOR_TAG.', 'benchmarking methods from machine translation [ 3, 19, 23 ] have been']","['subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of image captioning models [ 5,  #TAUTHOR_TAG.', 'benchmarking methods from machine translation [ 3, 19, 23 ] have been appropriated, while other somewhat similar methods']","['subjectivity in what defines a good caption, has made it difficult to identify a single metric for the overall quality of image captioning models [ 5,  #TAUTHOR_TAG.', 'benchmarking methods from machine translation [ 3, 19, 23 ] have been appropriated, while other somewhat similar methods such as cider [ 27 ] have been proposed specifically for assessing the quality of image captions.', 'all these approaches unfortunately have a strong focus on replicating common n - grams from the ground - truth captions [ 5 ] and do not take into account the richness and diversity of human expression [ 9,  #TAUTHOR_TAG.', 'moreover, it has been found that this class of metrics suffers from poor correlations with human evaluation, with cider and meteor having the highest correlations among them [ 5 ].', 'with the recognition of these limitations, there has been a growing interest in developing metrics that measure other desirable qualities in captions.', 'spice [ 2 ] is a recent addition which measures the overlap of content by comparing automatically generated scene - graphs from the ground - truth and generated captions.', 'while being a relevant addition, it does not solve the problem of generic captions.', 'rare occurrences and more detailed descriptions are more likely to incur a penalty than common concepts ; e. g. correctly specifying a purple flower where the ground - truth text omits its color would register a false positive for the color.', ""this, again, encourages the'' safe'' generic captions that we want to move away from""]",0
"['11 ] explores the concept of caption diversity.', 'more recently, this concept has been employed as the focus for training and evaluation  #TAUTHOR_TAG.', 'this']","['of generic captions produced by various image captioning models, [ 11 ] explores the concept of caption diversity.', 'more recently, this concept has been employed as the focus for training and evaluation  #TAUTHOR_TAG.', 'this']","['an effort to measure the amount of generic captions produced by various image captioning models, [ 11 ] explores the concept of caption diversity.', 'more recently, this concept has been employed as the focus for training and evaluation  #TAUTHOR_TAG.', '']","['an effort to measure the amount of generic captions produced by various image captioning models, [ 11 ] explores the concept of caption diversity.', 'more recently, this concept has been employed as the focus for training and evaluation  #TAUTHOR_TAG.', 'this research direction is still new and lacks clear benchmarks and standardized metrics.', 'we propose the following set of metrics to evaluate the diversity of a model :', '[UNK] novelty - percentage of generated captions where exact duplicates are not found in the training set [ 11,']",0
"['11 ] explores the concept of caption diversity.', 'more recently, this concept has been employed as the focus for training and evaluation  #TAUTHOR_TAG.', 'this']","['of generic captions produced by various image captioning models, [ 11 ] explores the concept of caption diversity.', 'more recently, this concept has been employed as the focus for training and evaluation  #TAUTHOR_TAG.', 'this']","['an effort to measure the amount of generic captions produced by various image captioning models, [ 11 ] explores the concept of caption diversity.', 'more recently, this concept has been employed as the focus for training and evaluation  #TAUTHOR_TAG.', '']","['an effort to measure the amount of generic captions produced by various image captioning models, [ 11 ] explores the concept of caption diversity.', 'more recently, this concept has been employed as the focus for training and evaluation  #TAUTHOR_TAG.', 'this research direction is still new and lacks clear benchmarks and standardized metrics.', 'we propose the following set of metrics to evaluate the diversity of a model :', '[UNK] novelty - percentage of generated captions where exact duplicates are not found in the training set [ 11,']",5
"['[ 11,  #TAUTHOR_TAG, using']","['[ 11,  #TAUTHOR_TAG, using']","['[ 11,  #TAUTHOR_TAG, using the single best caption after re - ranking for the latter.', 'we also report the specificity metrics used']","['models we compare to are the best models in terms of diversity from [ 11,  #TAUTHOR_TAG, using the single best caption after re - ranking for the latter.', 'we also report the specificity metrics used for our training goals.', 'the results for specificity would not be directly comparable to models using other external systems, but they are relevant when assessing our own models and verifying that our increase in diversity follows from an increase in specificity.', 'results from our contrastive models are averaged over 3 runs each.', 'the non - contrastive models are based on single runs.', 'as can be seen in table 1, our models demonstrate increased diversity and novelty, outperforming previously reported results.', 'the vocabulary size also increases but is lower than in  #TAUTHOR_TAG.', 'when it comes to the specificity metrics, our contrastive models have the advantage over our non - contrastive ones.', '']",5
"['[ 9,  #TAUTHOR_TAG']","['[ 9,  #TAUTHOR_TAG']","['recent applications of gan training in image captioning [ 9,  #TAUTHOR_TAG.', 'instead of allowing both systems to']","['', 'in this paper, we investigate whether the error signal from an image retrieval model can improve caption specificity in an image captioning model, and whether these more specific captions are also more diverse.', 'the training process is inspired by [ 22 ] where the task is to generate referring expressions that unambiguously refer to a region of an image ; their solution is to introduce a region discriminator that measures the quality of their generated expressions.', 'their method is in turn inspired by generative adversarial networks ( gans ) in which a generator and a discriminator are in constant competition - the discriminator aims to distinguish between real and generated data, while the generator aims to generate data that the discriminator cannot tell apart from the real data [ 13 ].', 'in [ 22 ], the training is cooperative rather than competitive ; both systems adjust to the other to provide the best joint results.', 'we take a slightly different approach from both the joint training in [ 22 ] and recent applications of gan training in image captioning [ 9,  #TAUTHOR_TAG.', 'instead of allowing both systems to learn from each other, we freeze the nlu side and allow only the nlg to learn from the nlu ; the nlu model is pre - trained on ground - truth captions, without any input from the nlg.', 'consequently, we avoid one of the problems observed in [ 22 ] where both systems adapt to each other and develop their own protocol of communication which gradually degrades the resemblance to human language.', 'we also avoid the instability in training and difficulty in loss monitoring commonly seen in gans']",4
"['[ 11,  #TAUTHOR_TAG, using']","['[ 11,  #TAUTHOR_TAG, using']","['[ 11,  #TAUTHOR_TAG, using the single best caption after re - ranking for the latter.', 'we also report the specificity metrics used']","['models we compare to are the best models in terms of diversity from [ 11,  #TAUTHOR_TAG, using the single best caption after re - ranking for the latter.', 'we also report the specificity metrics used for our training goals.', 'the results for specificity would not be directly comparable to models using other external systems, but they are relevant when assessing our own models and verifying that our increase in diversity follows from an increase in specificity.', 'results from our contrastive models are averaged over 3 runs each.', 'the non - contrastive models are based on single runs.', 'as can be seen in table 1, our models demonstrate increased diversity and novelty, outperforming previously reported results.', 'the vocabulary size also increases but is lower than in  #TAUTHOR_TAG.', 'when it comes to the specificity metrics, our contrastive models have the advantage over our non - contrastive ones.', '']",4
"['', 'another example of gan training is  #TAUTHOR_TAG where the discriminator']","['halfpipe', 'another example of gan training is  #TAUTHOR_TAG where the discriminator classifies whether a multi - sample set of captions are human - written or generated.', 'in contrast, our evaluator only requires a single caption and uses a much simpler loss function.', 'furthermore, we let the nlu remain frozen during training,']","['', 'another example of gan training is  #TAUTHOR_TAG where the discriminator']","['', 'another example of gan training is  #TAUTHOR_TAG where the discriminator classifies whether a multi - sample set of captions are human - written or generated.', 'in contrast, our evaluator only requires a single caption and uses a much simpler loss function.', 'furthermore, we let the nlu remain frozen during training, making the training stable and producing more informative learning curves.', 'a similar approach can be found in [ 10 ] where contrastive learning is used in a gan - like setting.', 'in contrast to our approach which is unsupervised after pre - training, theirs require image - caption pairs both during and after']",4
"['[ 11,  #TAUTHOR_TAG, using']","['[ 11,  #TAUTHOR_TAG, using']","['[ 11,  #TAUTHOR_TAG, using the single best caption after re - ranking for the latter.', 'we also report the specificity metrics used']","['models we compare to are the best models in terms of diversity from [ 11,  #TAUTHOR_TAG, using the single best caption after re - ranking for the latter.', 'we also report the specificity metrics used for our training goals.', 'the results for specificity would not be directly comparable to models using other external systems, but they are relevant when assessing our own models and verifying that our increase in diversity follows from an increase in specificity.', 'results from our contrastive models are averaged over 3 runs each.', 'the non - contrastive models are based on single runs.', 'as can be seen in table 1, our models demonstrate increased diversity and novelty, outperforming previously reported results.', 'the vocabulary size also increases but is lower than in  #TAUTHOR_TAG.', 'when it comes to the specificity metrics, our contrastive models have the advantage over our non - contrastive ones.', '']",7
"['[ 11,  #TAUTHOR_TAG, using']","['[ 11,  #TAUTHOR_TAG, using']","['[ 11,  #TAUTHOR_TAG, using the single best caption after re - ranking for the latter.', 'we also report the specificity metrics used']","['models we compare to are the best models in terms of diversity from [ 11,  #TAUTHOR_TAG, using the single best caption after re - ranking for the latter.', 'we also report the specificity metrics used for our training goals.', 'the results for specificity would not be directly comparable to models using other external systems, but they are relevant when assessing our own models and verifying that our increase in diversity follows from an increase in specificity.', 'results from our contrastive models are averaged over 3 runs each.', 'the non - contrastive models are based on single runs.', 'as can be seen in table 1, our models demonstrate increased diversity and novelty, outperforming previously reported results.', 'the vocabulary size also increases but is lower than in  #TAUTHOR_TAG.', 'when it comes to the specificity metrics, our contrastive models have the advantage over our non - contrastive ones.', '']",3
['natural language systems including machine translation  #TAUTHOR_TAG'],['natural language systems including machine translation  #TAUTHOR_TAG'],['natural language systems including machine translation  #TAUTHOR_TAG'],"['rich languages like arabic ( beesley, k. 1996 ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes ( i. e. prefix, stem, suffix ).', 'by segmenting words into morphemes, we can improve the performance of natural language systems including machine translation  #TAUTHOR_TAG and information retrieval ( franz, m. and mccarley, s. 2002 ).', 'in this paper, we present a cross - lingual english - arabic search engine combined with an on demand arabicenglish statistical machine translation system that relies on source language analysis for both improved search and translation.', 'we developed novel statistical learning algorithms for performing arabic word segmentation ( lee, y. et al 2003 ) into morphemes and morphological source language ( arabic ) analysis ( lee, y. et al 2003b ).', 'these components improve both monolingual ( arabic ) search and cross - lingual ( english - arabic ) search and machine translation.', 'in addition, the system supports either document translation or convolutional models for cross - lingual search ( franz, m. and mccarley, s. 2002 ).', '']",6
['natural language systems including machine translation  #TAUTHOR_TAG'],['natural language systems including machine translation  #TAUTHOR_TAG'],['natural language systems including machine translation  #TAUTHOR_TAG'],"['rich languages like arabic ( beesley, k. 1996 ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes ( i. e. prefix, stem, suffix ).', 'by segmenting words into morphemes, we can improve the performance of natural language systems including machine translation  #TAUTHOR_TAG and information retrieval ( franz, m. and mccarley, s. 2002 ).', 'in this paper, we present a cross - lingual english - arabic search engine combined with an on demand arabicenglish statistical machine translation system that relies on source language analysis for both improved search and translation.', 'we developed novel statistical learning algorithms for performing arabic word segmentation ( lee, y. et al 2003 ) into morphemes and morphological source language ( arabic ) analysis ( lee, y. et al 2003b ).', 'these components improve both monolingual ( arabic ) search and cross - lingual ( english - arabic ) search and machine translation.', 'in addition, the system supports either document translation or convolutional models for cross - lingual search ( franz, m. and mccarley, s. 2002 ).', '']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['manual feature extraction  #TAUTHOR_TAG'],"['the massive increase in social interactions on online social networks, there has also been an increase of hateful activities that exploit such infrastructure.', 'on twitter, hateful tweets are those that contain abusive speech targeting individuals ( cyber - bullying, a politician, a celebrity, a product ) or particular groups ( a country, lgbt, a religion, gender, an organization, etc. ).', 'detecting such hateful speech is important for analyzing public sentiment of a group of users towards another group, and for discouraging associated wrongful activities.', 'it is also useful to filter tweets before content recommendation, or learning ai chatterbots from tweets 1.', 'the manual way of filtering out hateful tweets is not scalable, motivating researchers to identify automated ways.', 'in this work, we focus on the problem of classifying a tweet as racist, sexist or neither.', 'the task is quite challenging due to the inherent complexity of the natural language constructsdifferent forms of hatred, different kinds of targets, different ways of representing the same meaning.', 'most of the earlier work revolves either around manual feature extraction  #TAUTHOR_TAG or use representation learning methods followed by a linear classifier [ 1, 4 ] of complex problems in speech, vision and text applications.', 'to the best of our knowledge, we are the first to experiment with deep learning architectures for the hate speech detection task.', '']",0
['method  #TAUTHOR_TAG which uses character n - grams'],['state - ofthe - art method  #TAUTHOR_TAG which uses character n - grams'],[') char n - grams : it is the state - ofthe - art method  #TAUTHOR_TAG which uses character n - grams'],"['first discuss a few baseline methods and then discuss the proposed approach.', 'in all these methods, an embedding is generated for a tweet and is used as its feature representation with a classifier.', 'baseline methods : as baselines, we experiment with three broad representations.', '( 1 ) char n - grams : it is the state - ofthe - art method  #TAUTHOR_TAG which uses character n - grams for hate speech detection.', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['manual feature extraction  #TAUTHOR_TAG'],"['the massive increase in social interactions on online social networks, there has also been an increase of hateful activities that exploit such infrastructure.', 'on twitter, hateful tweets are those that contain abusive speech targeting individuals ( cyber - bullying, a politician, a celebrity, a product ) or particular groups ( a country, lgbt, a religion, gender, an organization, etc. ).', 'detecting such hateful speech is important for analyzing public sentiment of a group of users towards another group, and for discouraging associated wrongful activities.', 'it is also useful to filter tweets before content recommendation, or learning ai chatterbots from tweets 1.', 'the manual way of filtering out hateful tweets is not scalable, motivating researchers to identify automated ways.', 'in this work, we focus on the problem of classifying a tweet as racist, sexist or neither.', 'the task is quite challenging due to the inherent complexity of the natural language constructsdifferent forms of hatred, different kinds of targets, different ways of representing the same meaning.', 'most of the earlier work revolves either around manual feature extraction  #TAUTHOR_TAG or use representation learning methods followed by a linear classifier [ 1, 4 ] of complex problems in speech, vision and text applications.', 'to the best of our knowledge, we are the first to experiment with deep learning architectures for the hate speech detection task.', '']",5
['available by  #TAUTHOR_TAG'],['available by  #TAUTHOR_TAG'],"['available by  #TAUTHOR_TAG.', 'of the 16k tweets, 3383 are labeled as sex']","['experimented with a dataset of 16k annotated tweets made available by  #TAUTHOR_TAG.', 'of the 16k tweets, 3383 are labeled as sexist, 1972 as racist, and the remaining are marked as neither sexist nor racist.', 'for the embedding based methods, we used the glove [ 5 ] pre - trained word embeddings.', 'glove embeddings 2 have been trained on a large tweet corpus ( 2b tweets, 27b tokens, 1. 2m vocab, uncased ).', 'we experimented with multiple word embedding sizes for our task.', 'we observed similar results with different sizes, and hence due to lack of space we report results using embedding size = 200.', 'we performed 10 - fold cross validation and calculated weighted macro precision, recall and f1 - scores.', ""we use'adam'for cnn and lstm, and'rms - prop'for fasttext as our optimizer."", 'we perform training in batches of size 128 for cnn & lstm and 64 for fasttext.', 'more details on the experimental setup can be found from our publicly available source code 3.', 'table 1 shows the results of various methods on the hate speech detection task.', 'part a shows results for baseline methods.', 'parts b and c focus on the proposed methods where part b contains methods using neural networks only, while part c uses average of word embeddings learned by dnns as features for gbdts.', 'we experimented with mul - tiple classifiers but report results mostly for gbdts only, due to lack of space']",5
"['or by multi - dimensional continuous values  #TAUTHOR_TAG.', 'the categorical']","['represented categorically or by multi - dimensional continuous values  #TAUTHOR_TAG.', 'the categorical']","['or by multi - dimensional continuous values  #TAUTHOR_TAG.', 'the categorical approach aims']","['', 'however, they exist in the forms of comments in a live webcast, opinion sites, or social media, and often contain considerable amount of noise.', 'such characteristics pose obstacles to those who intend to collect this type of information efficiently.', 'it is the reason why opinion mining has recently become a topic of interest in both academia and business institutions.', 'sentiment analysis is a type of opinion mining where affective states are represented categorically or by multi - dimensional continuous values  #TAUTHOR_TAG.', '']",0
"['or by multi - dimensional continuous values  #TAUTHOR_TAG.', 'the categorical']","['represented categorically or by multi - dimensional continuous values  #TAUTHOR_TAG.', 'the categorical']","['or by multi - dimensional continuous values  #TAUTHOR_TAG.', 'the categorical approach aims']","['', 'however, they exist in the forms of comments in a live webcast, opinion sites, or social media, and often contain considerable amount of noise.', 'such characteristics pose obstacles to those who intend to collect this type of information efficiently.', 'it is the reason why opinion mining has recently become a topic of interest in both academia and business institutions.', 'sentiment analysis is a type of opinion mining where affective states are represented categorically or by multi - dimensional continuous values  #TAUTHOR_TAG.', '']",0
"['of valence - arousal methods  #TAUTHOR_TAG.', 'thus, we use two sets of methods']","['of valence - arousal methods  #TAUTHOR_TAG.', 'thus, we use two sets of methods']","['by aggregating the results of a number of valence - arousal methods  #TAUTHOR_TAG.', 'thus, we use two sets of methods']","['', 'in order to cope with the problem of unknown words, we separate words in wva into 4, 184 characters with valence - arousal ratings, called cva.', 'the valence - arousal score of the unknown word can be obtained by averaging the matched cva.', 'moreover, previous research suggested that it is possible to improve the performance by aggregating the results of a number of valence - arousal methods  #TAUTHOR_TAG.', 'thus, we use two sets of methods for the prediction of valence : ( 1 ) prediction based on wva and cva, and ( 2 ) a knn valence prediction method.', '']",0
"['of valence - arousal methods  #TAUTHOR_TAG.', 'thus, we use two sets of methods']","['of valence - arousal methods  #TAUTHOR_TAG.', 'thus, we use two sets of methods']","['by aggregating the results of a number of valence - arousal methods  #TAUTHOR_TAG.', 'thus, we use two sets of methods']","['', 'in order to cope with the problem of unknown words, we separate words in wva into 4, 184 characters with valence - arousal ratings, called cva.', 'the valence - arousal score of the unknown word can be obtained by averaging the matched cva.', 'moreover, previous research suggested that it is possible to improve the performance by aggregating the results of a number of valence - arousal methods  #TAUTHOR_TAG.', 'thus, we use two sets of methods for the prediction of valence : ( 1 ) prediction based on wva and cva, and ( 2 ) a knn valence prediction method.', '']",1
['by  #TAUTHOR_TAG returned'],['by  #TAUTHOR_TAG returned state - of - the - art'],['by  #TAUTHOR_TAG returned'],"['##tm - based language models have been shown effective in word sense disambiguation ( wsd ).', 'in particular, the technique proposed by  #TAUTHOR_TAG returned state - of - the - art performance in several benchmarks, but neither the training data nor the source code was released.', 'this paper presents the results of a reproduction study and analysis of this technique using only openly available datasets ( gigaword, semcor, omsti ) and software ( tensorflow ).', 'our study showed that similar results can be obtained with much less data than hinted at by  #TAUTHOR_TAG.', 'detailed analyses shed light on the strengths and weaknesses of this method.', 'first, adding more unannotated training data is useful, but is subject to diminishing returns.', 'second, the model can correctly identify both popular and unpopular meanings.', 'finally, the limited sense coverage in the annotated datasets is a major limitation.', 'all code and trained models are made freely available']",0
"[""results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1""]","[""results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1 ) reproducing""]","[""this could prevent other attempts from replicating the results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1""]","[""this could prevent other attempts from replicating the results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1 ) reproducing and making available the code, trained models, and results and 2 ) understanding which"", '']",0
['. label propagation.  #TAUTHOR_TAG argue that the averaging'],['. label propagation.  #TAUTHOR_TAG argue that the averaging'],"['to c t using cosine', 'as the similarity function. label propagation.  #TAUTHOR_TAG argue that the averaging procedure is suboptimal']","['', 'as the similarity function. label propagation.  #TAUTHOR_TAG argue that the averaging procedure is suboptimal because of two reasons. first,', 'the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. second, averaging reduces the representation', 'of occurrences of each sense to a single vector and therefore ignores sense prior. for this reason, they propose to use label propagation for inference as an alternative', 'to averaging. label', 'propagation  #AUTHOR_TAG is a classic semi - supervised algorithm that has been employed in wsd  #AUTHOR_TAG and other nlp tasks  #AUTHOR_TAG. the', 'procedure involves predicting senses for not only the target cases but also for unannotated words queried from a', 'corpus. it represents both the target cases and unannotated words as points in a vector space', 'and iteratively propagates classification labels from the target classes to the words. in this way, it can be used to construct non -', 'spherical clusters and to give more influence to frequent senses', '']",0
['. label propagation.  #TAUTHOR_TAG argue that the averaging'],['. label propagation.  #TAUTHOR_TAG argue that the averaging'],"['to c t using cosine', 'as the similarity function. label propagation.  #TAUTHOR_TAG argue that the averaging procedure is suboptimal']","['', 'as the similarity function. label propagation.  #TAUTHOR_TAG argue that the averaging procedure is suboptimal because of two reasons. first,', 'the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. second, averaging reduces the representation', 'of occurrences of each sense to a single vector and therefore ignores sense prior. for this reason, they propose to use label propagation for inference as an alternative', 'to averaging. label', 'propagation  #AUTHOR_TAG is a classic semi - supervised algorithm that has been employed in wsd  #AUTHOR_TAG and other nlp tasks  #AUTHOR_TAG. the', 'procedure involves predicting senses for not only the target cases but also for unannotated words queried from a', 'corpus. it represents both the target cases and unannotated words as points in a vector space', 'and iteratively propagates classification labels from the target classes to the words. in this way, it can be used to construct non -', 'spherical clusters and to give more influence to frequent senses', '']",0
['. label propagation.  #TAUTHOR_TAG argue that the averaging'],['. label propagation.  #TAUTHOR_TAG argue that the averaging'],"['to c t using cosine', 'as the similarity function. label propagation.  #TAUTHOR_TAG argue that the averaging procedure is suboptimal']","['', 'as the similarity function. label propagation.  #TAUTHOR_TAG argue that the averaging procedure is suboptimal because of two reasons. first,', 'the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. second, averaging reduces the representation', 'of occurrences of each sense to a single vector and therefore ignores sense prior. for this reason, they propose to use label propagation for inference as an alternative', 'to averaging. label', 'propagation  #AUTHOR_TAG is a classic semi - supervised algorithm that has been employed in wsd  #AUTHOR_TAG and other nlp tasks  #AUTHOR_TAG. the', 'procedure involves predicting senses for not only the target cases but also for unannotated words queried from a', 'corpus. it represents both the target cases and unannotated words as points in a vector space', 'and iteratively propagates classification labels from the target classes to the words. in this way, it can be used to construct non -', 'spherical clusters and to give more influence to frequent senses', '']",0
['by  #TAUTHOR_TAG returned'],['by  #TAUTHOR_TAG returned state - of - the - art'],['by  #TAUTHOR_TAG returned'],"['##tm - based language models have been shown effective in word sense disambiguation ( wsd ).', 'in particular, the technique proposed by  #TAUTHOR_TAG returned state - of - the - art performance in several benchmarks, but neither the training data nor the source code was released.', 'this paper presents the results of a reproduction study and analysis of this technique using only openly available datasets ( gigaword, semcor, omsti ) and software ( tensorflow ).', 'our study showed that similar results can be obtained with much less data than hinted at by  #TAUTHOR_TAG.', 'detailed analyses shed light on the strengths and weaknesses of this method.', 'first, adding more unannotated training data is useful, but is subject to diminishing returns.', 'second, the model can correctly identify both popular and unpopular meanings.', 'finally, the limited sense coverage in the annotated datasets is a major limitation.', 'all code and trained models are made freely available']",5
"[""results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1""]","[""results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1 ) reproducing""]","[""this could prevent other attempts from replicating the results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1""]","[""this could prevent other attempts from replicating the results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1 ) reproducing and making available the code, trained models, and results and 2 ) understanding which"", '']",5
"['by  #TAUTHOR_TAG, which we consider in this paper', ', belongs to']","['by  #TAUTHOR_TAG, which we consider in this paper', ', belongs to']","['by  #TAUTHOR_TAG, which we consider in this paper', ', belongs to']","['', '( 2017 ) use word embeddings as a starting point and then rely on the formal constraints in a lexical resource to create synset embeddings', '. recently, there has been a surge in wsd approaches that use unannotated', 'data but do not consider synset relations. one example is provided by  #AUTHOR_TAG, who investigated the role of word embeddings as features in a wsd system. four methods ( concatenation, average', ', fractional decay, and exponential decay ) are used to extract features from the sentential context using word embeddings. the features are then added to the default feature set of', 'ims  #AUTHOR_TAG.  #AUTHOR_TAG b ) present a number of end - to - end neural wsd architectures. the best performing one is based', 'on a bidirectional long short - term memory ( blstm ) with attention and two auxiliary loss functions ( part -', 'of - speech and the wordnet coarse - grained semantic labels ).  #AUTHOR_TAG also make use of unannotated data to train a', 'blstm. the work by  #TAUTHOR_TAG, which we consider in this paper', ', belongs to this last category. different from  #AUTHOR_TAG, it uses significantly more unann', '##otated data, the model contains more hidden units ( 2048 vs. 600 ), and the sense assignment is more elaborated. we describe this approach in more detail in the following section']",5
"['evaluation used by  #TAUTHOR_TAG while', '']","['evaluation used by  #TAUTHOR_TAG while', '"" competition "" refers to the']","['evaluation used by  #TAUTHOR_TAG while', '']","['', ', and spanish. this test set contains 13 articles from previous editions of the workshop on statistical machine translation. 4 the', 'articles contain table 1 : performance of our implementation compared to already published results. we', 'report the model / method used to perform wsd, the used annotated dataset and scorer, and f', '##1 for each test set. in the naming of our models, lstm indicates that', 'the averaging technique was used for the sense assignment, while lstmlp refers to the results obtained using label propagation ( see section 3 ). the', 'datasets following t : indicate the annotated corpus used to represent the senses while u : omsti stands for using om', '##sti as unlabeled sentences in case label propagation is used. p : semcor indicates that sense distributions from semcor are used', 'in the system architecture. three scorers are used : "" framework "" refers to the ws', '##d evaluation framework from  #AUTHOR_TAG a ) ; "" mapping to wn3. 0 "" refers to the evaluation used by  #TAUTHOR_TAG while', '"" competition "" refers to the scorer provided by the', 'competition itself ( e. g., semeval2013 ). 1, 644 test instances in total, which are all nouns. the application', 'of the mfs baseline on this dataset yields an f 1 score of 63. 0 %']",5
"['evaluation used by  #TAUTHOR_TAG while', '']","['evaluation used by  #TAUTHOR_TAG while', '"" competition "" refers to the']","['evaluation used by  #TAUTHOR_TAG while', '']","['', ', and spanish. this test set contains 13 articles from previous editions of the workshop on statistical machine translation. 4 the', 'articles contain table 1 : performance of our implementation compared to already published results. we', 'report the model / method used to perform wsd, the used annotated dataset and scorer, and f', '##1 for each test set. in the naming of our models, lstm indicates that', 'the averaging technique was used for the sense assignment, while lstmlp refers to the results obtained using label propagation ( see section 3 ). the', 'datasets following t : indicate the annotated corpus used to represent the senses while u : omsti stands for using om', '##sti as unlabeled sentences in case label propagation is used. p : semcor indicates that sense distributions from semcor are used', 'in the system architecture. three scorers are used : "" framework "" refers to the ws', '##d evaluation framework from  #AUTHOR_TAG a ) ; "" mapping to wn3. 0 "" refers to the evaluation used by  #TAUTHOR_TAG while', '"" competition "" refers to the scorer provided by the', 'competition itself ( e. g., semeval2013 ). 1, 644 test instances in total, which are all nouns. the application', 'of the mfs baseline on this dataset yields an f 1 score of 63. 0 %']",5
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",5
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",5
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",5
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",5
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",5
[' #TAUTHOR_TAG and an additional'],[' #TAUTHOR_TAG and an additional'],[' #TAUTHOR_TAG and an additional analysis'],"['paper reports the results of a reproduction study of the model proposed by  #TAUTHOR_TAG and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.', '']",5
['by  #TAUTHOR_TAG returned'],['by  #TAUTHOR_TAG returned state - of - the - art'],['by  #TAUTHOR_TAG returned'],"['##tm - based language models have been shown effective in word sense disambiguation ( wsd ).', 'in particular, the technique proposed by  #TAUTHOR_TAG returned state - of - the - art performance in several benchmarks, but neither the training data nor the source code was released.', 'this paper presents the results of a reproduction study and analysis of this technique using only openly available datasets ( gigaword, semcor, omsti ) and software ( tensorflow ).', 'our study showed that similar results can be obtained with much less data than hinted at by  #TAUTHOR_TAG.', 'detailed analyses shed light on the strengths and weaknesses of this method.', 'first, adding more unannotated training data is useful, but is subject to diminishing returns.', 'second, the model can correctly identify both popular and unpopular meanings.', 'finally, the limited sense coverage in the annotated datasets is a major limitation.', 'all code and trained models are made freely available']",4
"[""results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1""]","[""results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1 ) reproducing""]","[""this could prevent other attempts from replicating the results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1""]","[""this could prevent other attempts from replicating the results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1 ) reproducing and making available the code, trained models, and results and 2 ) understanding which"", '']",4
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",4
[' #TAUTHOR_TAG and an additional'],[' #TAUTHOR_TAG and an additional'],[' #TAUTHOR_TAG and an additional analysis'],"['paper reports the results of a reproduction study of the model proposed by  #TAUTHOR_TAG and an additional analysis to gain a deeper understanding of the impact of various factors on its performance.', '']",4
"[""results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1""]","[""results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1 ) reproducing""]","[""this could prevent other attempts from replicating the results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1""]","[""this could prevent other attempts from replicating the results. to address these issues, we reimplemented  #TAUTHOR_TAG's method with the goal of : 1 ) reproducing and making available the code, trained models, and results and 2 ) understanding which"", '']",6
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",3
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",3
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",3
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",7
"['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '-']","['. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying']","['and more data to train language models, hoping that better word embeddings would translate into improved wsd performance. the fact that  #TAUTHOR_TAG used a 100', '- billion - token corpus only reinforces this intuition. we empirically evaluate the effectiveness of unlabeled data by varying the size of the corpus used to train lstm models and measure the corresponding wsd performance. more in particular, the size of the training data was set at 1 %, 10', '']",7
"['translation pairs, and  #TAUTHOR_TAG construct a vocabulary of overlapping words to represent']","['translation pairs, and  #TAUTHOR_TAG construct a vocabulary of overlapping words to represent']","['the task of detecting document translation pairs, and  #TAUTHOR_TAG construct a vocabulary of overlapping words to represent']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG detect parallel sentences by training ibm model 1 and maximum entropy classifiers, respectively.', 'in later work on detecting sentence and phrase translation pairs,  #AUTHOR_TAG and  #AUTHOR_TAG use smt systems to translate candidate documents ;  #AUTHOR_TAG use parallel data to train a translation equivalence model ; and  #AUTHOR_TAG use a translation lexicon to build a scoring function for parallel documents.', 'more recently,  #AUTHOR_TAG trained ibm model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts.', 'abdul -  #AUTHOR_TAG,  #AUTHOR_TAG, and gahbiche -  #AUTHOR_TAG, rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the target language with a baseline smt system and then search for corresponding documents.', 'on the other hand, there exist approaches that mine comparable corpora without any prior translation information or parallel data.', 'examples of this approach are rarer, and we briefly mention two :  #AUTHOR_TAG use singleton words ( hapax legomena ) to represent documents in a bilingual collection for the task of detecting document translation pairs, and  #TAUTHOR_TAG construct a vocabulary of overlapping words to represent documents in multilingual collections.', 'the latter approach demonstrates high precision vs. recall values on various language pairs from different languages and writing systems when detecting translation pairs on a document level such as europarl sessions.', 'recently proposed approaches, such as  #AUTHOR_TAG use monolingual corpora to estimate phrase - based smt parameters.', 'unlike our paper, however, they do not demonstrate an end - toend smt system trained without any parallel data.', 'our approach differs from these and other previous approaches by not relying on any initial translation dictionary or any bitext to train a seed smt system.', 'therefore, the primary experimental comparison that we perform is between no bitext at all and a system trained with some bitext']",0
"['orthography,  #TAUTHOR_TAG showed that this']","['orthography,  #TAUTHOR_TAG showed that this']","['orthography,  #TAUTHOR_TAG showed that']","['a given comparable corpus, ocd assumes that there is a set of words that exist in both languages that could be used as features in order to discriminate between documents that are translations of each other, documents that carry similar content, and documents that are not related.', 'firstly, for each language in the collection a vocabulary is created which consists of all word types seen in the corpora of that language.', 'words found in both source ( s ) and target ( t ) languages are extracted and the overlapping list of words are then used as dimensions for constructing a feature vector template.', 'documents in both languages are then represented using the template vector whose dimensions are the tf · idf values computed on the overlapping words which we now consider as features.', 'while the number of overlapping words is dependent on the families of the source and target languages and their orthography,  #TAUTHOR_TAG showed that this approach yields good results across language pairs from different families and writing systems such as english - greek, english - bulgarian and englisharabic where, as one would expect, most shared words are numbers and named entities.', 'we compare these vector representations efficiently using cosine ( cos ) distance and locality sensitive hashing  #AUTHOR_TAG.', 'this results in a single ranked list of all document pairs.', '']",0
"['translation pairs, and  #TAUTHOR_TAG construct a vocabulary of overlapping words to represent']","['translation pairs, and  #TAUTHOR_TAG construct a vocabulary of overlapping words to represent']","['the task of detecting document translation pairs, and  #TAUTHOR_TAG construct a vocabulary of overlapping words to represent']","['', ' #AUTHOR_TAG and  #AUTHOR_TAG detect parallel sentences by training ibm model 1 and maximum entropy classifiers, respectively.', 'in later work on detecting sentence and phrase translation pairs,  #AUTHOR_TAG and  #AUTHOR_TAG use smt systems to translate candidate documents ;  #AUTHOR_TAG use parallel data to train a translation equivalence model ; and  #AUTHOR_TAG use a translation lexicon to build a scoring function for parallel documents.', 'more recently,  #AUTHOR_TAG trained ibm model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts.', 'abdul -  #AUTHOR_TAG,  #AUTHOR_TAG, and gahbiche -  #AUTHOR_TAG, rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the target language with a baseline smt system and then search for corresponding documents.', 'on the other hand, there exist approaches that mine comparable corpora without any prior translation information or parallel data.', 'examples of this approach are rarer, and we briefly mention two :  #AUTHOR_TAG use singleton words ( hapax legomena ) to represent documents in a bilingual collection for the task of detecting document translation pairs, and  #TAUTHOR_TAG construct a vocabulary of overlapping words to represent documents in multilingual collections.', 'the latter approach demonstrates high precision vs. recall values on various language pairs from different languages and writing systems when detecting translation pairs on a document level such as europarl sessions.', 'recently proposed approaches, such as  #AUTHOR_TAG use monolingual corpora to estimate phrase - based smt parameters.', 'unlike our paper, however, they do not demonstrate an end - toend smt system trained without any parallel data.', 'our approach differs from these and other previous approaches by not relying on any initial translation dictionary or any bitext to train a seed smt system.', 'therefore, the primary experimental comparison that we perform is between no bitext at all and a system trained with some bitext']",4
[') approach of  #TAUTHOR_TAG as'],['( ocd ) approach of  #TAUTHOR_TAG as'],[') approach of  #TAUTHOR_TAG as'],"['bootstrapping approach ( figure 1 ) is a twostage system that used the overlapping cosine distance ( ocd ) approach of  #TAUTHOR_TAG as its first step.', 'ocd outputs a ranked list of candidate document pairs, which are then fed through a sentence - alignment system  #AUTHOR_TAG.', 'a polylingual topic model ( pltm )  #AUTHOR_TAG is then trained on the aligned portions of these documents.', 'using the trained model, we infer topics on the whole comparable training set.', 'once represented as points in the topic space, documents are then compared for similarity using divergence based metrics such as hellinger ( he ) distance.', 'results from these comparisons create a single ranked list of text translation pairs, which are on a sub docu - ment length level.', 'from this single ranked list, using thresholding, we again extract the top n candidate translation pairs that are then fed to an aligner for further refinement']",5
"['described in  #TAUTHOR_TAG.', 'it contains 19 wizard of oz dialogues with a virtual human called amani.', 'the user plays the role of an']","['described in  #TAUTHOR_TAG.', 'it contains 19 wizard of oz dialogues with a virtual human called amani.', 'the user plays the role of an']","['our experiments we use the dataset described in  #TAUTHOR_TAG.', 'it contains 19 wizard of oz dialogues with a virtual human called amani.', 'the user plays the role of an army commander']","['our experiments we use the dataset described in  #TAUTHOR_TAG.', 'it contains 19 wizard of oz dialogues with a virtual human called amani.', 'the user plays the role of an army commander whose unit has been attacked by a sniper.', 'the user interviews amani, who was a witness to the incident and has some information about the sniper.', 'amani is willing to tell the interviewer what she knows, but she will only reveal certain information in exchange for promises of safety, secrecy, and money ).', 'each dialogue turn in the data set includes a single user utterance followed by the response chosen by a human amani role player.', 'there are a total of 296 turns, for an average of 15. 6 turns / dialogue.', 'user utterances are modeled using 46 distinct speech act ( sa ) labels.', 'the dataset also defines a different set of 96 unique sas ( responses ) for amani.', 'six external referees analyzed each user utterance and selected a single character response out of the 96 sas.', 'thus the dataset defines a one - to - many mapping between user utterances and alternative system sas']",5
['79 ; see  #TAUTHOR_TAG'],"['the others, to establish a performance ceiling for this metric.', 'this score is. 79 ; see  #TAUTHOR_TAG']",['79 ; see  #TAUTHOR_TAG'],"['', 'we then count the proportion of the correct sas among all the sas produced across all 19 dialogues, and use this measure of weak accuracy to score dialogue policies.', 'we can use the weak accuracy of one referee, measured against all the others, to establish a performance ceiling for this metric.', 'this score is. 79 ; see  #TAUTHOR_TAG']",5
"['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that (']","['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that ( 1 ) we are likely project our biases in the text that we produce']","['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that (']","['work demonstrated that word embeddings induced from large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that ( 1 ) we are likely project our biases in the text that we produce and ( 2 ) these biases in text are bound to be encoded in word vectors due to the distributional nature  #AUTHOR_TAG of the word embedding models  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'for illustration, consider the famous analogy - based gender bias example from  #AUTHOR_TAG : "" man is to computer programmer as woman is to homemaker "".', 'this bias will be reflected in the text ( i. e., the word man will cooccur more often with words like programmer or engineer, whereas woman will more often appear next to homemaker or nurse ), and will, in turn, be captured by word embeddings built from such biased texts.', 'while biases encoded in word embeddings can be a useful data source for diachronic analyses of societal biases ( e. g.,  #AUTHOR_TAG, they may cause ethical problems for many downstream applications and nlp models.', 'in order to measure the extent to which various societal biases are captured by word embeddings,  #TAUTHOR_TAG proposed the word embedding association test ( weat ).', 'weat measures semantic similarity, computed through word embeddings, between two sets of target words ( e. g., insects vs. flowers ) and two sets of attribute words ( e. g., pleasant vs. unpleasant words ).', 'while they test a number of biases, the analysis is limited in scope to english as the only language, glove  #AUTHOR_TAG as the embedding model, and common crawl as the type of text.', 'following the same methodology, mc  #AUTHOR_TAG extend the analysis to three more languages ( german, dutch, spanish ), but test only for gender bias.', 'in this work, we present the most comprehensive study of biases captured by distributional word vector to date.', 'we create xweat, a collection of multilingual and cross - lingual versions of the weat dataset, by translating weat to six other languages and offer a comparative analysis of biases over seven diverse languages.', 'furthermore, we measure the consistency of weat biases across different embedding models and types of corpora.', 'what is more, given the recent surge of models for inducing cross - lingual embedding spaces  #AUTHOR_TAG a ;  #AUTHOR_TAG inter alia ) and their ubiquitous application in cross - lingual transfer of nlp models for downstream tasks, we investigate cross - lingual biases encoded in cross - lingual embedding spaces and compare them to bias effects present of corresponding monolingual']",0
"['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that (']","['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that ( 1 ) we are likely project our biases in the text that we produce']","['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that (']","['work demonstrated that word embeddings induced from large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that ( 1 ) we are likely project our biases in the text that we produce and ( 2 ) these biases in text are bound to be encoded in word vectors due to the distributional nature  #AUTHOR_TAG of the word embedding models  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'for illustration, consider the famous analogy - based gender bias example from  #AUTHOR_TAG : "" man is to computer programmer as woman is to homemaker "".', 'this bias will be reflected in the text ( i. e., the word man will cooccur more often with words like programmer or engineer, whereas woman will more often appear next to homemaker or nurse ), and will, in turn, be captured by word embeddings built from such biased texts.', 'while biases encoded in word embeddings can be a useful data source for diachronic analyses of societal biases ( e. g.,  #AUTHOR_TAG, they may cause ethical problems for many downstream applications and nlp models.', 'in order to measure the extent to which various societal biases are captured by word embeddings,  #TAUTHOR_TAG proposed the word embedding association test ( weat ).', 'weat measures semantic similarity, computed through word embeddings, between two sets of target words ( e. g., insects vs. flowers ) and two sets of attribute words ( e. g., pleasant vs. unpleasant words ).', 'while they test a number of biases, the analysis is limited in scope to english as the only language, glove  #AUTHOR_TAG as the embedding model, and common crawl as the type of text.', 'following the same methodology, mc  #AUTHOR_TAG extend the analysis to three more languages ( german, dutch, spanish ), but test only for gender bias.', 'in this work, we present the most comprehensive study of biases captured by distributional word vector to date.', 'we create xweat, a collection of multilingual and cross - lingual versions of the weat dataset, by translating weat to six other languages and offer a comparative analysis of biases over seven diverse languages.', 'furthermore, we measure the consistency of weat biases across different embedding models and types of corpora.', 'what is more, given the recent surge of models for inducing cross - lingual embedding spaces  #AUTHOR_TAG a ;  #AUTHOR_TAG inter alia ) and their ubiquitous application in cross - lingual transfer of nlp models for downstream tasks, we investigate cross - lingual biases encoded in cross - lingual embedding spaces and compare them to bias effects present of corresponding monolingual']",0
[')  #TAUTHOR_TAG is an'],['embedding association test ( weat )  #TAUTHOR_TAG is an'],[')  #TAUTHOR_TAG is an adaptation of'],"['word embedding association test ( weat )  #TAUTHOR_TAG is an adaptation of the implicit association test ( iat )  #AUTHOR_TAG.', 'whereas iat measures biases based on response times of human subjects to provided stimuli, weat quantifies the biases using semantic similarities between word embeddings of the same stimuli.', 'for each bias test, weat specifies four stimuli sets : two sets of target words and two sets of attribute words.', 'the sets of target words represent stimuli between which we want to measure the bias ( e. g., for gender biases, one target set could contain male names and the other females names ).', 'the attribute words, on the other hand, represent stimuli towards which the bias should be measured ( e. g., one list could contain pleasant stimuli like health and love and the other negative war and death ).', 'the weat dataset defines ten bias tests, each containing two target and two attribute sets.', '1 table 1 enumerates the weat tests and provides examples of the respective target and attribute words']",0
"['from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora -']","['from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we']","['adopt the general bias - testing framework from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we']","['adopt the general bias - testing framework from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we analyze the consistency of biases across distributional vectors induced from different types of text ; ( 2 ) embedding models - we compare biases across distributional vectors induced by different embedding models ( on the same corpora ) ; and ( 3 ) languageswe measure biases for word embeddings of different languages, trained from comparable corpora.', 'furthermore, unlike  #AUTHOR_TAG, we test whether biases depend on the selection of the similarity metric.', 'finally, given the ubiquitous adoption of cross - lingual embeddings  #AUTHOR_TAG glavas et al., 2019 ), we investigate biases in a variety of bilingual embedding spaces.', 'bias - testing framework.', 'we first describe the weat framework  #TAUTHOR_TAG.', 'let x and y be two sets of targets, and a and b two sets of attributes ( see § 2. 1 ).', 'the tested statistic is the difference between x and y in average similarity of their terms with terms from a and b :', 'with association difference for term t computed as :', 'where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work  #TAUTHOR_TAG.', 'the effect size, that is, the "" amount of bias "", is computed as the normalized measure of separation between association distributions :', 'where [UNK] denotes the mean and σ standard deviation']",0
"['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that (']","['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that ( 1 ) we are likely project our biases in the text that we produce']","['large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that (']","['work demonstrated that word embeddings induced from large text collections encode many human biases ( e. g.,  #TAUTHOR_TAG.', 'this finding is not particularly surprising given that ( 1 ) we are likely project our biases in the text that we produce and ( 2 ) these biases in text are bound to be encoded in word vectors due to the distributional nature  #AUTHOR_TAG of the word embedding models  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'for illustration, consider the famous analogy - based gender bias example from  #AUTHOR_TAG : "" man is to computer programmer as woman is to homemaker "".', 'this bias will be reflected in the text ( i. e., the word man will cooccur more often with words like programmer or engineer, whereas woman will more often appear next to homemaker or nurse ), and will, in turn, be captured by word embeddings built from such biased texts.', 'while biases encoded in word embeddings can be a useful data source for diachronic analyses of societal biases ( e. g.,  #AUTHOR_TAG, they may cause ethical problems for many downstream applications and nlp models.', 'in order to measure the extent to which various societal biases are captured by word embeddings,  #TAUTHOR_TAG proposed the word embedding association test ( weat ).', 'weat measures semantic similarity, computed through word embeddings, between two sets of target words ( e. g., insects vs. flowers ) and two sets of attribute words ( e. g., pleasant vs. unpleasant words ).', 'while they test a number of biases, the analysis is limited in scope to english as the only language, glove  #AUTHOR_TAG as the embedding model, and common crawl as the type of text.', 'following the same methodology, mc  #AUTHOR_TAG extend the analysis to three more languages ( german, dutch, spanish ), but test only for gender bias.', 'in this work, we present the most comprehensive study of biases captured by distributional word vector to date.', 'we create xweat, a collection of multilingual and cross - lingual versions of the weat dataset, by translating weat to six other languages and offer a comparative analysis of biases over seven diverse languages.', 'furthermore, we measure the consistency of weat biases across different embedding models and types of corpora.', 'what is more, given the recent surge of models for inducing cross - lingual embedding spaces  #AUTHOR_TAG a ;  #AUTHOR_TAG inter alia ) and their ubiquitous application in cross - lingual transfer of nlp models for downstream tasks, we investigate cross - lingual biases encoded in cross - lingual embedding spaces and compare them to bias effects present of corresponding monolingual']",1
"['dataset  #TAUTHOR_TAG and then describe xweat, our multilingual and cross - lingual extension of weat designed']","['first introduce the weat dataset  #TAUTHOR_TAG and then describe xweat, our multilingual and cross - lingual extension of weat designed']","['first introduce the weat dataset  #TAUTHOR_TAG and then describe xweat, our multilingual and cross - lingual extension of weat designed']","['first introduce the weat dataset  #TAUTHOR_TAG and then describe xweat, our multilingual and cross - lingual extension of weat designed for comparative bias analyses across languages and in cross - lingual embedding spaces']",5
"['from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora -']","['from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we']","['adopt the general bias - testing framework from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we']","['adopt the general bias - testing framework from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we analyze the consistency of biases across distributional vectors induced from different types of text ; ( 2 ) embedding models - we compare biases across distributional vectors induced by different embedding models ( on the same corpora ) ; and ( 3 ) languageswe measure biases for word embeddings of different languages, trained from comparable corpora.', 'furthermore, unlike  #AUTHOR_TAG, we test whether biases depend on the selection of the similarity metric.', 'finally, given the ubiquitous adoption of cross - lingual embeddings  #AUTHOR_TAG glavas et al., 2019 ), we investigate biases in a variety of bilingual embedding spaces.', 'bias - testing framework.', 'we first describe the weat framework  #TAUTHOR_TAG.', 'let x and y be two sets of targets, and a and b two sets of attributes ( see § 2. 1 ).', 'the tested statistic is the difference between x and y in average similarity of their terms with terms from a and b :', 'with association difference for term t computed as :', 'where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work  #TAUTHOR_TAG.', 'the effect size, that is, the "" amount of bias "", is computed as the normalized measure of separation between association distributions :', 'where [UNK] denotes the mean and σ standard deviation']",5
"['from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora -']","['from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we']","['adopt the general bias - testing framework from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we']","['adopt the general bias - testing framework from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we analyze the consistency of biases across distributional vectors induced from different types of text ; ( 2 ) embedding models - we compare biases across distributional vectors induced by different embedding models ( on the same corpora ) ; and ( 3 ) languageswe measure biases for word embeddings of different languages, trained from comparable corpora.', 'furthermore, unlike  #AUTHOR_TAG, we test whether biases depend on the selection of the similarity metric.', 'finally, given the ubiquitous adoption of cross - lingual embeddings  #AUTHOR_TAG glavas et al., 2019 ), we investigate biases in a variety of bilingual embedding spaces.', 'bias - testing framework.', 'we first describe the weat framework  #TAUTHOR_TAG.', 'let x and y be two sets of targets, and a and b two sets of attributes ( see § 2. 1 ).', 'the tested statistic is the difference between x and y in average similarity of their terms with terms from a and b :', 'with association difference for term t computed as :', 'where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work  #TAUTHOR_TAG.', 'the effect size, that is, the "" amount of bias "", is computed as the normalized measure of separation between association distributions :', 'where [UNK] denotes the mean and σ standard deviation']",5
"[', we have extended previous analyses based on the weat test  #TAUTHOR_TAG in multiple dimensions : across seven languages, four embedding models, and three different types of text.', 'we']","['largest study to date on biases encoded in distributional word vector spaces.', 'to this end, we have extended previous analyses based on the weat test  #TAUTHOR_TAG in multiple dimensions : across seven languages, four embedding models, and three different types of text.', 'we']","['this paper, we have presented the largest study to date on biases encoded in distributional word vector spaces.', 'to this end, we have extended previous analyses based on the weat test  #TAUTHOR_TAG in multiple dimensions : across seven languages, four embedding models, and three different types of text.', 'we find that different models']","['this paper, we have presented the largest study to date on biases encoded in distributional word vector spaces.', 'to this end, we have extended previous analyses based on the weat test  #TAUTHOR_TAG in multiple dimensions : across seven languages, four embedding models, and three different types of text.', 'we find that different models may produce embeddings with very different biases, which stresses the importance of embedding model selection when fair text representations are to be created.', 'surprisingly, we find that the user - generated texts, such as tweets, may be less biased than redacted content.', 'furthermore, we have investigated the bias effects in cross - lingual embedding spaces and have shown that they may be predicted from the biases of corresponding monolingual embeddings.', 'we make the xweat dataset and the testing code publicly available, 7 hoping to fuel further research on biases encoded in word representations.', 'table 12 : xweat t9 effect sizes for cross - lingual embedding spaces.', 'rows denote the target set language, column the attribute set language']",5
"['from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora -']","['from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we']","['adopt the general bias - testing framework from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we']","['adopt the general bias - testing framework from  #TAUTHOR_TAG, but we span our study over multiple dimensions : ( 1 ) corpora - we analyze the consistency of biases across distributional vectors induced from different types of text ; ( 2 ) embedding models - we compare biases across distributional vectors induced by different embedding models ( on the same corpora ) ; and ( 3 ) languageswe measure biases for word embeddings of different languages, trained from comparable corpora.', 'furthermore, unlike  #AUTHOR_TAG, we test whether biases depend on the selection of the similarity metric.', 'finally, given the ubiquitous adoption of cross - lingual embeddings  #AUTHOR_TAG glavas et al., 2019 ), we investigate biases in a variety of bilingual embedding spaces.', 'bias - testing framework.', 'we first describe the weat framework  #TAUTHOR_TAG.', 'let x and y be two sets of targets, and a and b two sets of attributes ( see § 2. 1 ).', 'the tested statistic is the difference between x and y in average similarity of their terms with terms from a and b :', 'with association difference for term t computed as :', 'where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work  #TAUTHOR_TAG.', 'the effect size, that is, the "" amount of bias "", is computed as the normalized measure of separation between association distributions :', 'where [UNK] denotes the mean and σ standard deviation']",6
"[', we have extended previous analyses based on the weat test  #TAUTHOR_TAG in multiple dimensions : across seven languages, four embedding models, and three different types of text.', 'we']","['largest study to date on biases encoded in distributional word vector spaces.', 'to this end, we have extended previous analyses based on the weat test  #TAUTHOR_TAG in multiple dimensions : across seven languages, four embedding models, and three different types of text.', 'we']","['this paper, we have presented the largest study to date on biases encoded in distributional word vector spaces.', 'to this end, we have extended previous analyses based on the weat test  #TAUTHOR_TAG in multiple dimensions : across seven languages, four embedding models, and three different types of text.', 'we find that different models']","['this paper, we have presented the largest study to date on biases encoded in distributional word vector spaces.', 'to this end, we have extended previous analyses based on the weat test  #TAUTHOR_TAG in multiple dimensions : across seven languages, four embedding models, and three different types of text.', 'we find that different models may produce embeddings with very different biases, which stresses the importance of embedding model selection when fair text representations are to be created.', 'surprisingly, we find that the user - generated texts, such as tweets, may be less biased than redacted content.', 'furthermore, we have investigated the bias effects in cross - lingual embedding spaces and have shown that they may be predicted from the biases of corresponding monolingual embeddings.', 'we make the xweat dataset and the testing code publicly available, 7 hoping to fuel further research on biases encoded in word representations.', 'table 12 : xweat t9 effect sizes for cross - lingual embedding spaces.', 'rows denote the target set language, column the attribute set language']",6
['by  #TAUTHOR_TAG. corpus t1 t2 t3 t'],"['), wikipedia ( i. e., encyclopedic the definition of a and vice versa  #AUTHOR_TAG. 5 this is consistent with the original results obtained by  #TAUTHOR_TAG. corpus t1 t2 t3 t4', '']","['by  #TAUTHOR_TAG. corpus t1 t2 t3 t4', '']","['results to be important as they indicate that embedding models may accentuate or', 'diminish biases expressed in text. corpora. in table 4 we compare the biases of embeddings trained with the same model ( glove ) but on different corpora : common crawl', '( i. e., noisy web content ), wikipedia ( i. e., encyclopedic the definition of a and vice versa  #AUTHOR_TAG. 5 this is consistent with the original results obtained by  #TAUTHOR_TAG. corpus t1 t2 t3 t4', '']",3
"['', 'very recently,  #TAUTHOR_TAG proposed']","['to new data sets and languages.', 'very recently,  #TAUTHOR_TAG proposed']","['', 'very recently,  #TAUTHOR_TAG proposed']","['', 'this pipeline approach can cause cascading errors, and in addition, since both stages rely on a syntactic parser and complicated handcraft features, it is difficult to generalize to new data sets and languages.', 'very recently,  #TAUTHOR_TAG proposed the first state - of - the - art end - to - end neural coreference resolution system.', '']",0
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. overall', '']","['200k updates, which is faster than that of  #TAUTHOR_TAG. overall', '']","['. 2 dropout. the batch size is 1 document. based on', 'the results on the development set, λ detection = 0. 1 works best from { 0. 05, 0. 1, 0. 5, 1. 0', '}. model is trained with adam optimizer  #AUTHOR_TAG and converges in around 200k updates, which is faster than that of  #TAUTHOR_TAG. overall', 'performance in table 1, we compare our model with previous', '']",0
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. overall', '']","['200k updates, which is faster than that of  #TAUTHOR_TAG. overall', '']","['. 2 dropout. the batch size is 1 document. based on', 'the results on the development set, λ detection = 0. 1 works best from { 0. 05, 0. 1, 0. 5, 1. 0', '}. model is trained with adam optimizer  #AUTHOR_TAG and converges in around 200k updates, which is faster than that of  #TAUTHOR_TAG. overall', 'performance in table 1, we compare our model with previous', '']",0
"[' #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', '( 3 ) entity - mention models learn classifiers']","[' #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', '( 3 ) entity - mention models learn classifiers']","[' #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', '( 3 ) entity - mention models learn classifiers']","['summarized by  #AUTHOR_TAG, learning - based coreference models can be categorized into three types : ( 1 ) mention - pair models train binary classifiers to determine if a pair of mentions are coreferent  #AUTHOR_TAG.', '( 2 ) mention - ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention  #AUTHOR_TAG b ;  #AUTHOR_TAG a ;  #TAUTHOR_TAG.', '( 3 ) entity - mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially - formed mention cluster  #AUTHOR_TAG b ).', 'in addition, we also note latent - antecedent models  #AUTHOR_TAG bjorkelund and  #AUTHOR_TAG.', ' #AUTHOR_TAG introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions.', 'recently, several neural coreference resolution systems have achieved impressive gains  #AUTHOR_TAG ( wiseman et al.,, 2016  #AUTHOR_TAG b, a ).', 'they utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of hand - crafted features.', 'for example,  #AUTHOR_TAG propose the first neural coreference resolution system by training a deep feed - forward neural network for mention ranking.', 'however, these models still employ the two - stage pipeline and require a syntactic parser or a separate designed hand - engineered mention detector.', 'finally, we also note the relevant work on joint mention detection and coreference resolution.', 'daume iii and  #AUTHOR_TAG propose to model both mention detection and coreference of the entity detection and tracking task simultaneously.', ' #AUTHOR_TAG a ) propose to use integer linear programming framework to model anaphoricity and coreference as a joint task']",0
['as in  #TAUTHOR_TAG using bidirectional ls'],['as in  #TAUTHOR_TAG using bidirectional lstms and'],"['as in  #TAUTHOR_TAG using bidirectional lstms and a headfinding attention.', 'thereafter, a feed']","['1 illustrates our model.', 'we adopt the same span representation approach as in  #TAUTHOR_TAG using bidirectional lstms and a headfinding attention.', '']",3
['as in  #TAUTHOR_TAG using bidirectional ls'],['as in  #TAUTHOR_TAG using bidirectional lstms and'],"['as in  #TAUTHOR_TAG using bidirectional lstms and a headfinding attention.', 'thereafter, a feed']","['1 illustrates our model.', 'we adopt the same span representation approach as in  #TAUTHOR_TAG using bidirectional lstms and a headfinding attention.', '']",3
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. overall', '']","['200k updates, which is faster than that of  #TAUTHOR_TAG. overall', '']","['. 2 dropout. the batch size is 1 document. based on', 'the results on the development set, λ detection = 0. 1 works best from { 0. 05, 0. 1, 0. 5, 1. 0', '}. model is trained with adam optimizer  #AUTHOR_TAG and converges in around 200k updates, which is faster than that of  #TAUTHOR_TAG. overall', 'performance in table 1, we compare our model with previous', '']",3
['as in  #TAUTHOR_TAG using bidirectional ls'],['as in  #TAUTHOR_TAG using bidirectional lstms and'],"['as in  #TAUTHOR_TAG using bidirectional lstms and a headfinding attention.', 'thereafter, a feed']","['1 illustrates our model.', 'we adopt the same span representation approach as in  #TAUTHOR_TAG using bidirectional lstms and a headfinding attention.', '']",5
['as in  #TAUTHOR_TAG using bidirectional ls'],['as in  #TAUTHOR_TAG using bidirectional lstms and'],"['as in  #TAUTHOR_TAG using bidirectional lstms and a headfinding attention.', 'thereafter, a feed']","['1 illustrates our model.', 'we adopt the same span representation approach as in  #TAUTHOR_TAG using bidirectional lstms and a headfinding attention.', '']",5
['of  #TAUTHOR_TAG'],"['of  #TAUTHOR_TAG. overall', '']","['200k updates, which is faster than that of  #TAUTHOR_TAG. overall', '']","['. 2 dropout. the batch size is 1 document. based on', 'the results on the development set, λ detection = 0. 1 works best from { 0. 05, 0. 1, 0. 5, 1. 0', '}. model is trained with adam optimizer  #AUTHOR_TAG and converges in around 200k updates, which is faster than that of  #TAUTHOR_TAG. overall', 'performance in table 1, we compare our model with previous', '']",5
"['platforms  #TAUTHOR_TAG, or']","['platforms  #TAUTHOR_TAG, or']","['twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', 'the examples in']","[""using an entity mention's context to disambiguate it is the crux of the entity linking task : in isolation, the mention richard wright could refer to three possible entities in wikipedia's knowledge base corresponding to an artist, a musician, or an author."", 'previous work in this area has distilled context information by exploiting tfidf features  #AUTHOR_TAG, global link coherence ( hoffart et al. ;  #AUTHOR_TAG, cues from coreference  #AUTHOR_TAG, convolutional neural networks ( sun et al. ; francis  #AUTHOR_TAG, or more sophisticated neural architectures  #AUTHOR_TAG.', 'these approaches typically focus on aggregating information from a mix of sources, including long - range information from the textual context or other linked entities.', 'while this approach is suitable for entity linking settings such as newswire  #AUTHOR_TAG and wikipedia  #AUTHOR_TAG, we cannot always rely on this information in other settings like twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', '']",0
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",0
"['platforms  #TAUTHOR_TAG, or']","['platforms  #TAUTHOR_TAG, or']","['twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', 'the examples in']","[""using an entity mention's context to disambiguate it is the crux of the entity linking task : in isolation, the mention richard wright could refer to three possible entities in wikipedia's knowledge base corresponding to an artist, a musician, or an author."", 'previous work in this area has distilled context information by exploiting tfidf features  #AUTHOR_TAG, global link coherence ( hoffart et al. ;  #AUTHOR_TAG, cues from coreference  #AUTHOR_TAG, convolutional neural networks ( sun et al. ; francis  #AUTHOR_TAG, or more sophisticated neural architectures  #AUTHOR_TAG.', 'these approaches typically focus on aggregating information from a mix of sources, including long - range information from the textual context or other linked entities.', 'while this approach is suitable for entity linking settings such as newswire  #AUTHOR_TAG and wikipedia  #AUTHOR_TAG, we cannot always rely on this information in other settings like twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', '']",3
"['platforms  #TAUTHOR_TAG, or']","['platforms  #TAUTHOR_TAG, or']","['twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', 'the examples in']","[""using an entity mention's context to disambiguate it is the crux of the entity linking task : in isolation, the mention richard wright could refer to three possible entities in wikipedia's knowledge base corresponding to an artist, a musician, or an author."", 'previous work in this area has distilled context information by exploiting tfidf features  #AUTHOR_TAG, global link coherence ( hoffart et al. ;  #AUTHOR_TAG, cues from coreference  #AUTHOR_TAG, convolutional neural networks ( sun et al. ; francis  #AUTHOR_TAG, or more sophisticated neural architectures  #AUTHOR_TAG.', 'these approaches typically focus on aggregating information from a mix of sources, including long - range information from the textual context or other linked entities.', 'while this approach is suitable for entity linking settings such as newswire  #AUTHOR_TAG and wikipedia  #AUTHOR_TAG, we cannot always rely on this information in other settings like twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', '']",3
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",3
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",3
"['platforms  #TAUTHOR_TAG, or']","['platforms  #TAUTHOR_TAG, or']","['twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', 'the examples in']","[""using an entity mention's context to disambiguate it is the crux of the entity linking task : in isolation, the mention richard wright could refer to three possible entities in wikipedia's knowledge base corresponding to an artist, a musician, or an author."", 'previous work in this area has distilled context information by exploiting tfidf features  #AUTHOR_TAG, global link coherence ( hoffart et al. ;  #AUTHOR_TAG, cues from coreference  #AUTHOR_TAG, convolutional neural networks ( sun et al. ; francis  #AUTHOR_TAG, or more sophisticated neural architectures  #AUTHOR_TAG.', 'these approaches typically focus on aggregating information from a mix of sources, including long - range information from the textual context or other linked entities.', 'while this approach is suitable for entity linking settings such as newswire  #AUTHOR_TAG and wikipedia  #AUTHOR_TAG, we cannot always rely on this information in other settings like twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', '']",5
"['platforms  #TAUTHOR_TAG, or']","['platforms  #TAUTHOR_TAG, or']","['twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', 'the examples in']","[""using an entity mention's context to disambiguate it is the crux of the entity linking task : in isolation, the mention richard wright could refer to three possible entities in wikipedia's knowledge base corresponding to an artist, a musician, or an author."", 'previous work in this area has distilled context information by exploiting tfidf features  #AUTHOR_TAG, global link coherence ( hoffart et al. ;  #AUTHOR_TAG, cues from coreference  #AUTHOR_TAG, convolutional neural networks ( sun et al. ; francis  #AUTHOR_TAG, or more sophisticated neural architectures  #AUTHOR_TAG.', 'these approaches typically focus on aggregating information from a mix of sources, including long - range information from the textual context or other linked entities.', 'while this approach is suitable for entity linking settings such as newswire  #AUTHOR_TAG and wikipedia  #AUTHOR_TAG, we cannot always rely on this information in other settings like twitter  #AUTHOR_TAG, snapchat  #AUTHOR_TAG, other web platforms  #TAUTHOR_TAG, or dialogue systems  #AUTHOR_TAG.', 'we need models that can make effective use of limited context windows in noisy settings.', 'in this work, we investigate this problem of effectively using context in the setting of the wikilinksned dataset from  #TAUTHOR_TAG.', '']",5
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",5
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",5
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",4
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",4
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",4
"['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over']","['unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to']","['p ( t | m, c l, c r ) = softmax t ( s t ). training because our model involves substantial computation for each possible title, we want to limit', 'the set of titles considered during training. for each example we consider, we construct a set t containing the gold title and 4 negative "" distractor "" titles from the candidate set. unlike  #TAUTHOR_TAG, we structure training as a multiclass', 'decision among these titles rather than a binary prediction problem over each title as gold or not. we run our model over the candidates t ∈ t to produce the distribution p ( t | m, c l', ', c r ) and train to maximize the log probablity log p ( t * | m, c l, c r ) of the gold title', '. results the model set forth in this section is the basis for the remaining models in this paper ; we call it the gru model as that is the only context encoding mechanism', 'it uses. as shown in table 1, this gru model gets a score of 73. 4 on the wikilinksned development set.', 'in the next section, we explore techniques for using the context in a more sophisticated way to improve further on this result.  #TAUTHOR_TAG, allowing the model to weight the importance of the outputs of the gru at each time step. each context', '( left and right ) has its own attention weights. for a given', 'side of context and candidate t, the attention first computes a transformation of the entity embedding e t as follows : q t = tanh ( w e t )', '. this allows the model to learn an attention query q t distinct from the candidate embedding e t. the model then computes attention probabilities α i for each gru output o', 'i, normalized over the entire sequence of gru outputs ( of length n ) : the resulting', 'probability distribution is used to take a weighted sum of gru outputs to get a representation a : we', 'compute a l and a r independently and symmetrically for the left and right', 'context. these vectors are then fed forward through the model as the final continuous representation of the left or right context, l', 'or r respectively. results in table 1, we see that our model with attention ( gru + attn ) outperforms our basic gru model by around 1 % absolute', '. it also outperforms the roughly similar model of  #TAUTHOR_TAG on the test set : this gain is due to a combination of factors', 'including the improved training procedure and some small modeling changes. 2 however, our attention scheme is not without its shortcomings, as we now discuss']",6
"['text modelling  #TAUTHOR_TAG.', 'in this paper, we present a simple architecture called holistic regularisation']","['text modelling  #TAUTHOR_TAG.', 'in this paper, we present a simple architecture called holistic regularisation']","['text modelling  #TAUTHOR_TAG.', 'in this paper, we present a simple architecture called holistic regularisation vae ( hr - va']","['autoencoder ( vae ) is a powerful method for learning representations of highdimensional data.', 'however, vaes can suffer from an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', 'such an issue is particularly prevalent when employing vae - rnn architectures for text modelling  #TAUTHOR_TAG.', 'in this paper, we present a simple architecture called holistic regularisation vae ( hr - vae ), which can effectively avoid latent variable collapse.', 'compared to existing vae - rnn architectures, we show that our model can achieve much more stable training process and can generate text with significantly better quality']",1
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', 'various efforts have been made to alleviate the latent variable collapse issue.', ' #TAUTHOR_TAG uses kl annealing, where a variable weight is added to the kl term in the cost function at training time.', ' #AUTHOR_TAG discovered that there is a trade - off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated cnn as decoder which can vary the amount of conditioning context.', 'they also introduced a loss clipping strategy in order to make the model more robust.', '']",1
"['based models in previous works  #TAUTHOR_TAG.', 'that is,']","['mitigate the kl vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is,']","['mitigate the kl vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is,']","['this section, we discuss the technical details of the proposed holistic regularisation vae ( hr - vae ) model, a general architecture which can effectively mitigate the kl vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is, all these models, as shown in figure 1a, only impose a standard normal distribution prior on the last hidden state of the rnn encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to kl loss vanishing.', 'our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the rnn - based encoder ( see figure 1b ), which allows a better regularisation of the model learning process.', 'we implement the hr - vae model using a twolayer lstm for both the encoder and decoder.', 'however, one should note that our architecture can be readily applied to other types of rnn such as gru.', '']",1
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', 'various efforts have been made to alleviate the latent variable collapse issue.', ' #TAUTHOR_TAG uses kl annealing, where a variable weight is added to the kl term in the cost function at training time.', ' #AUTHOR_TAG discovered that there is a trade - off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated cnn as decoder which can vary the amount of conditioning context.', 'they also introduced a loss clipping strategy in order to make the model more robust.', '']",0
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', 'various efforts have been made to alleviate the latent variable collapse issue.', ' #TAUTHOR_TAG uses kl annealing, where a variable weight is added to the kl term in the cost function at training time.', ' #AUTHOR_TAG discovered that there is a trade - off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated cnn as decoder which can vary the amount of conditioning context.', 'they also introduced a loss clipping strategy in order to make the model more robust.', '']",0
"['for text generation  #TAUTHOR_TAG.', 'pt']","['for text generation  #TAUTHOR_TAG.', 'ptb consists of more than 40, 000 sentences from wall street journal articles whereas']","[') text generation corpus  #AUTHOR_TAG, which have been used in a number of previous works for text generation  #TAUTHOR_TAG.', 'pt']","['evaluate our model on two public datasets, namely, penn treebank ( ptb )  #AUTHOR_TAG and the end - to - end ( e2e ) text generation corpus  #AUTHOR_TAG, which have been used in a number of previous works for text generation  #TAUTHOR_TAG.', 'ptb consists of more than 40, 000 sentences from wall street journal articles whereas the e2e dataset contains over 50, 000 sen - tences of restaurant reviews.', 'the statistics of these two datasets are summarised in table 1']",0
"[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","['( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a']","[') 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding']","['autoencoder ( vae )  #AUTHOR_TAG is a powerful method for learning representations of high - dimensional data.', 'however, recent attempts of applying vaes to text modelling are still far less successful compared to its application to image and speech  #AUTHOR_TAG.', 'when applying vaes for text modelling, recurrent neural networks ( rnns ) 1 are commonly used as the architecture for both encoder and decoder  #TAUTHOR_TAG.', 'while such a vae - rnn based architecture allows encoding and generating sentences ( in the decoding phase ) with variablelength effectively, it is also vulnerable to an issue known as latent variable collapse ( or kl loss vanishing ), where the posterior collapses to the prior and the model will ignore the latent codes in generative tasks.', 'various efforts have been made to alleviate the latent variable collapse issue.', ' #TAUTHOR_TAG uses kl annealing, where a variable weight is added to the kl term in the cost function at training time.', ' #AUTHOR_TAG discovered that there is a trade - off between the contextual capacity of the decoder and effective use of encoding information, and developed a dilated cnn as decoder which can vary the amount of conditioning context.', 'they also introduced a loss clipping strategy in order to make the model more robust.', '']",4
"['based models in previous works  #TAUTHOR_TAG.', 'that is,']","['mitigate the kl vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is,']","['mitigate the kl vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is,']","['this section, we discuss the technical details of the proposed holistic regularisation vae ( hr - vae ) model, a general architecture which can effectively mitigate the kl vanishing phenomenon.', 'our model design is motivated by one noticeable defect shared by the vae - rnn based models in previous works  #TAUTHOR_TAG.', 'that is, all these models, as shown in figure 1a, only impose a standard normal distribution prior on the last hidden state of the rnn encoder, which potentially leads to learning a suboptimal representation of the latent variable and results in model vulnerable to kl loss vanishing.', 'our hypothesis is that to learn a good representation of data and a good generative model, it is crucial to impose the standard normal prior on all the hidden states of the rnn - based encoder ( see figure 1b ), which allows a better regularisation of the model learning process.', 'we implement the hr - vae model using a twolayer lstm for both the encoder and decoder.', 'however, one should note that our architecture can be readily applied to other types of rnn such as gru.', '']",4
"['for text generation  #TAUTHOR_TAG.', 'pt']","['for text generation  #TAUTHOR_TAG.', 'ptb consists of more than 40, 000 sentences from wall street journal articles whereas']","[') text generation corpus  #AUTHOR_TAG, which have been used in a number of previous works for text generation  #TAUTHOR_TAG.', 'pt']","['evaluate our model on two public datasets, namely, penn treebank ( ptb )  #AUTHOR_TAG and the end - to - end ( e2e ) text generation corpus  #AUTHOR_TAG, which have been used in a number of previous works for text generation  #TAUTHOR_TAG.', 'ptb consists of more than 40, 000 sentences from wall street journal articles whereas the e2e dataset contains over 50, 000 sen - tences of restaurant reviews.', 'the statistics of these two datasets are summarised in table 1']",5
"['test split following  #TAUTHOR_TAG.', 'for']","['split following  #TAUTHOR_TAG.', 'for']","['the ptb dataset, we used the train - test split following  #TAUTHOR_TAG.', 'for the e2e dataset, we used the train - test split from the original dataset  #AUTHOR_TAG and indexed the words with a frequency higher than 3.', 'we represent input data with 512 - dimensional word2vec embeddings  #AUTHOR_TAG.', 'we set the dimension']","['the ptb dataset, we used the train - test split following  #TAUTHOR_TAG.', 'for the e2e dataset, we used the train - test split from the original dataset  #AUTHOR_TAG and indexed the words with a frequency higher than 3.', 'we represent input data with 512 - dimensional word2vec embeddings  #AUTHOR_TAG.', 'we set the dimension of the hidden layers of both encoder and decoder to 256.', 'the adam optimiser  #AUTHOR_TAG was used for training with an initial learning rate of 0. 0001.', 'each utterance in a mini - batch was padded to the maximum length for that batch, and the maximum batch - size allowed is 128']",5
['##d the latent variable collapse issue  #TAUTHOR_TAG ; va'],"['model which uses lstm for both encoder and decoder.', 'kl annealing is used to tackled the latent variable collapse issue  #TAUTHOR_TAG ; vae - cnn']",['##d the latent variable collapse issue  #TAUTHOR_TAG ; va'],"['compare our hr - vae model with three strong baselines using vae for text modelling : vae - lstm - base 3 : a variational autoencoder model which uses lstm for both encoder and decoder.', 'kl annealing is used to tackled the latent variable collapse issue  #TAUTHOR_TAG ; vae - cnn 4 : a variational autoencoder model with a lstm encoder and a dilated cnn decoder  #AUTHOR_TAG ; vmf - vae 5 : a variational autoencoder model using lstm for both encoder and decoder where the prior distribution is the von mises - fisher ( vmf ) distribution rather than a gaussian distribution  #AUTHOR_TAG.', 'the decoder needs to predict the entire sequence with only the help of the given latent variable z.', 'in this way, a high - quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information.', '']",5
['##d the latent variable collapse issue  #TAUTHOR_TAG ; va'],"['model which uses lstm for both encoder and decoder.', 'kl annealing is used to tackled the latent variable collapse issue  #TAUTHOR_TAG ; vae - cnn']",['##d the latent variable collapse issue  #TAUTHOR_TAG ; va'],"['compare our hr - vae model with three strong baselines using vae for text modelling : vae - lstm - base 3 : a variational autoencoder model which uses lstm for both encoder and decoder.', 'kl annealing is used to tackled the latent variable collapse issue  #TAUTHOR_TAG ; vae - cnn 4 : a variational autoencoder model with a lstm encoder and a dilated cnn decoder  #AUTHOR_TAG ; vmf - vae 5 : a variational autoencoder model using lstm for both encoder and decoder where the prior distribution is the von mises - fisher ( vmf ) distribution rather than a gaussian distribution  #AUTHOR_TAG.', 'the decoder needs to predict the entire sequence with only the help of the given latent variable z.', 'in this way, a high - quality representation abstracting the information of the input sentence is much needed for the decoder, and hence enforcing z to learn the required information.', '']",5
['use xlnet  #TAUTHOR_TAG to attempt to capture long range language dependencies'],"['use xlnet  #TAUTHOR_TAG to attempt to capture long range language dependencies.', 'at the time of this writing, xlnet provides the best']","['use xlnet  #TAUTHOR_TAG to attempt to capture long range language dependencies.', 'at the time of this writing, xlnet provides the best accuracy']","['use xlnet  #TAUTHOR_TAG to attempt to capture long range language dependencies.', 'at the time of this writing, xlnet provides the best accuracy for many downstream tasks that require language modeling pre - training, including question - answering, text classification, and other natural language understanding tasks.', ""we also attempt to take advantage of a transformer's parallel properties to make some performance optimizations when re - scoring our lattices""]",5
"['15 ] and english wikipedia which have 13gb of plain text combined  #TAUTHOR_TAG.', 'we run a transfer learning step using pytorch on the ted - lium dataset.', 'we implement a grpc server that can run inference on our model over the local network.', 'in kal']","['on bookscorpus [ 15 ] and english wikipedia which have 13gb of plain text combined  #TAUTHOR_TAG.', 'we run a transfer learning step using pytorch on the ted - lium dataset.', 'we implement a grpc server that can run inference on our model over the local network.', 'in kaldi, we implement a deterministicon - demandfst that calls into our exported model.', 'our deter - ministicondemandfst maps kaldi word symbol identifiers to tokens']","['on bookscorpus [ 15 ] and english wikipedia which have 13gb of plain text combined  #TAUTHOR_TAG.', 'we run a transfer learning step using pytorch on the ted - lium dataset.', 'we implement a grpc server that can run inference on our model over the local network.', 'in kal']","['experiments were conducted using the ted - lium3 dataset [ 13 ].', 'ted - lium is an english speech recognition training corpus from ted talks.', 'this data - set was chosen due to its topical nature, usually in the form of 15 minutes or more worth of speech where the speaker is discussing a particular topic.', 'our ted - lium dataset contains a training set of 248 hours of speech with aligned transcription, with approximately 2 hours of development and 3 hours of test.', 'an acoustic model and n - gram language model are trained to provide a baseline word - error rate.', 'we use a library that contains a pre - trained version of xlnet, an implementation of the transformer - xl architecture [ 14 ].', 'the model is fairly large with 110m parameters.', 'it was previously trained on bookscorpus [ 15 ] and english wikipedia which have 13gb of plain text combined  #TAUTHOR_TAG.', 'we run a transfer learning step using pytorch on the ted - lium dataset.', 'we implement a grpc server that can run inference on our model over the local network.', 'in kaldi, we implement a deterministicon - demandfst that calls into our exported model.', 'our deter - ministicondemandfst maps kaldi word symbol identifiers to tokens that our model understands and vice - versa.', 'when re - scoring a lattice, we remove the first - pass fst values and compose our deterministicondemandfst ( see figure 7 ).', 'the segment embedding for the best path after lattice re - scoring is cached and passed as inputs for re - scoring future lattices from the same speech context.', 'we compare this technique to first - pass decoding lattices ( no - rescoring ) and to re - scoring with an rnnlm trained directly on the ted - lium data - set']",5
"['was pre - trained on 512 tpus  #TAUTHOR_TAG, we expect that training for 20 epochs on']","[""was pre - trained on 512 tpus  #TAUTHOR_TAG, we expect that training for 20 epochs on ted - lium's text is not enough to overcome the differences between written text and conversational speech."", 'without fine - tuning, adding memory seems to have an adverse effect on the test set']","['- lium.', 'given the size of the model, and the fact that it was pre - trained on 512 tpus  #TAUTHOR_TAG, we expect that training for 20 epochs on']","['order to motivate the problem, we measure the oracle worderror rate which gives us the path with the minimum word error rate found within each lattice.', 'the oracle word error rate for the test set was found to be 1. 70 %.', 'if we were to flawlessly re - score a lattice, we could, in theory, achieve this word error rate.', 'in the lattice some very good answers exist.', 'however, our results in table 1 show how difficult it is to make a dent in the wer with such a large xlnet model.', 'the rnnlm still gives a much better score.', 'we suspect that this is due to a few things : firstly, the xlnet is 110m parameters and was trained on approximately 13gb of text compared to 25mb worth of text for ted - lium.', ""given the size of the model, and the fact that it was pre - trained on 512 tpus  #TAUTHOR_TAG, we expect that training for 20 epochs on ted - lium's text is not enough to overcome the differences between written text and conversational speech."", 'without fine - tuning, adding memory seems to have an adverse effect on the test set']",5
"['language modeling based on the transformer - xl architecture  #TAUTHOR_TAG.', 'this means that the outputs of xlnet depend strictly on the previous outputs.', 'this is different from']","['language modeling based on the transformer - xl architecture  #TAUTHOR_TAG.', 'this means that the outputs of xlnet depend strictly on the previous outputs.', 'this is different from']","['language modeling based on the transformer - xl architecture  #TAUTHOR_TAG.', 'this means that the outputs of xlnet depend strictly on the previous outputs.', 'this is different from other state - of - the - art language models']","['##net is a generalized auto - regressive model that can be used for language modeling based on the transformer - xl architecture  #TAUTHOR_TAG.', 'this means that the outputs of xlnet depend strictly on the previous outputs.', 'this is different from other state - of - the - art language models like bert ( bidirectional encoder representations from transformers ) which rely on conditioning the probabilities given surrounding words.', 'in bert, the model tries to predict a masked word by looking at all surrounding unmasked words ( figure 3 ).', 'the concept of fig. 3.', 'bert attempts to predict the masked word use both left and right contexts.', 'during training, a certain percentage of words are masked for use in prediction.', 'if both "" san "" and "" francisco "" were masked, bert would not be able to use information when decoding one of the words to help in decoding the other.', 'masking the input introduces a few disadvantages mentioned in  #TAUTHOR_TAG.', 'firstly, a masked token is rarely seen for most subsequent language modeling tasks, so there tends to be a discrepancy between the "" pre - taining "" step and the "" fine - tuning "" step.', 'typically, in the "" fine - tuning "" step, bert is adapted to attempt tasks like question - answering.', 'secondly, bert does not use information from one decoded masked token to help in decoding another masked token.', 'in other words, all masked tokens are assumed to be independent.', 'this is necessary in bert, because there is a strict separation between unmasked and masked tokens, as the masked tokens will be predicted.', 'in xlnet, however, the separation is a directional one : anything to the left of the word that is attempting to be predicted is fair game ( during training, words are reordered to get the benefit of surrounding context, but conceptually, orders are seen from left to right for the factorization order ).', 'this lends itself well to decoding in speech recognition as we typically re - score a lattice from left to right ( assuming you are visualizing a lattice in english ), while we prune low scoring results.', 'this does not mean, however, that the encodings for the words do not capture context from surrounding words.', 'with permutation language modeling, during training, the ordering of previous tokens can be modified ( figure 4 ).', 'with permutation language modeling, the network is trained on a regiment of random order word sequences.', 'for example, the phrase : "" i am going to san francisco to watch the warriors play basketball "" could be used to train xlnet by selecting a random factorization order, which is the order in which we']",0
"['language modeling based on the transformer - xl architecture  #TAUTHOR_TAG.', 'this means that the outputs of xlnet depend strictly on the previous outputs.', 'this is different from']","['language modeling based on the transformer - xl architecture  #TAUTHOR_TAG.', 'this means that the outputs of xlnet depend strictly on the previous outputs.', 'this is different from']","['language modeling based on the transformer - xl architecture  #TAUTHOR_TAG.', 'this means that the outputs of xlnet depend strictly on the previous outputs.', 'this is different from other state - of - the - art language models']","['##net is a generalized auto - regressive model that can be used for language modeling based on the transformer - xl architecture  #TAUTHOR_TAG.', 'this means that the outputs of xlnet depend strictly on the previous outputs.', 'this is different from other state - of - the - art language models like bert ( bidirectional encoder representations from transformers ) which rely on conditioning the probabilities given surrounding words.', 'in bert, the model tries to predict a masked word by looking at all surrounding unmasked words ( figure 3 ).', 'the concept of fig. 3.', 'bert attempts to predict the masked word use both left and right contexts.', 'during training, a certain percentage of words are masked for use in prediction.', 'if both "" san "" and "" francisco "" were masked, bert would not be able to use information when decoding one of the words to help in decoding the other.', 'masking the input introduces a few disadvantages mentioned in  #TAUTHOR_TAG.', 'firstly, a masked token is rarely seen for most subsequent language modeling tasks, so there tends to be a discrepancy between the "" pre - taining "" step and the "" fine - tuning "" step.', 'typically, in the "" fine - tuning "" step, bert is adapted to attempt tasks like question - answering.', 'secondly, bert does not use information from one decoded masked token to help in decoding another masked token.', 'in other words, all masked tokens are assumed to be independent.', 'this is necessary in bert, because there is a strict separation between unmasked and masked tokens, as the masked tokens will be predicted.', 'in xlnet, however, the separation is a directional one : anything to the left of the word that is attempting to be predicted is fair game ( during training, words are reordered to get the benefit of surrounding context, but conceptually, orders are seen from left to right for the factorization order ).', 'this lends itself well to decoding in speech recognition as we typically re - score a lattice from left to right ( assuming you are visualizing a lattice in english ), while we prune low scoring results.', 'this does not mean, however, that the encodings for the words do not capture context from surrounding words.', 'with permutation language modeling, during training, the ordering of previous tokens can be modified ( figure 4 ).', 'with permutation language modeling, the network is trained on a regiment of random order word sequences.', 'for example, the phrase : "" i am going to san francisco to watch the warriors play basketball "" could be used to train xlnet by selecting a random factorization order, which is the order in which we']",1
[' #TAUTHOR_TAG use an agent to control'],[' #TAUTHOR_TAG use an agent to control'],"['translation ubiquitous using real - time translation.', 'simultaneous machine translation aims to address this issue by interleaving reading the input with writing the output translation.', 'current simultaneous neural machine translation ( snmt ) systems  #TAUTHOR_TAG use an agent to control']","['of the next significant challenges in machine translation research is to make translation ubiquitous using real - time translation.', 'simultaneous machine translation aims to address this issue by interleaving reading the input with writing the output translation.', 'current simultaneous neural machine translation ( snmt ) systems  #TAUTHOR_TAG use an agent to control an incremental encoder - decoder ( or sequence to sequence ) nmt model.', 'each read adds more information to the encoder rnn, and each write produces more output using the decoder rnn.', 'in this paper, we propose adding a new action to the agent : a predict action that predicts what words might appear in the input stream.', 'prediction was previously proposed in simultaneous statistical machine translation ( grissom ii et al., 2014 ) but has not been studied in the context of neural machine translation ( nmt ).', 'in snmt systems, prediction of future words augments the encoder - decoder model with possible future contexts to produce output translations earlier ( minimize delay ) and / or produce better output translations ( improve translation quality ).', 'our experiments show that prediction improves snmt in both these measures']",0
['more input is a natural way to extend neural mt to simultaneous neural mt and has been explored in  #TAUTHOR_TAG which contains'],['more input is a natural way to extend neural mt to simultaneous neural mt and has been explored in  #TAUTHOR_TAG which contains'],['more input is a natural way to extend neural mt to simultaneous neural mt and has been explored in  #TAUTHOR_TAG which contains'],"['agent - based framework whose actions decide whether to translate or wait for more input is a natural way to extend neural mt to simultaneous neural mt and has been explored in  #TAUTHOR_TAG which contains two main components : the environment which receives the input words x = { x 1,..., x n } from the source language and incrementally generates translated words w = { w 1,..., w m } in the target language ; and the agent which decides an action for each time step, a t.', 'the agent generates an action sequence a = { a 1,..., a t } to control the environment.', 'previous models only include two actions : read and write.', 'we extend the model by adding the third action called predict.', 'action read is simply sending a new word to the en - vironment and generating a candidate word in the target language.', '']",0
[' #TAUTHOR_TAG use an agent to control'],[' #TAUTHOR_TAG use an agent to control'],"['translation ubiquitous using real - time translation.', 'simultaneous machine translation aims to address this issue by interleaving reading the input with writing the output translation.', 'current simultaneous neural machine translation ( snmt ) systems  #TAUTHOR_TAG use an agent to control']","['of the next significant challenges in machine translation research is to make translation ubiquitous using real - time translation.', 'simultaneous machine translation aims to address this issue by interleaving reading the input with writing the output translation.', 'current simultaneous neural machine translation ( snmt ) systems  #TAUTHOR_TAG use an agent to control an incremental encoder - decoder ( or sequence to sequence ) nmt model.', 'each read adds more information to the encoder rnn, and each write produces more output using the decoder rnn.', 'in this paper, we propose adding a new action to the agent : a predict action that predicts what words might appear in the input stream.', 'prediction was previously proposed in simultaneous statistical machine translation ( grissom ii et al., 2014 ) but has not been studied in the context of neural machine translation ( nmt ).', 'in snmt systems, prediction of future words augments the encoder - decoder model with possible future contexts to produce output translations earlier ( minimize delay ) and / or produce better output translations ( improve translation quality ).', 'our experiments show that prediction improves snmt in both these measures']",6
"['not work well in practice.', ' #AUTHOR_TAG introduced a trainable agent which they trained using deep q networks  #AUTHOR_TAG.', 'we modified the snmt trainable agent in  #TAUTHOR_TAG and added a new non - trivial predict action to the agent.', 'we compare to their model and show better results in delay and quality']","['not work well in practice.', ' #AUTHOR_TAG introduced a trainable agent which they trained using deep q networks  #AUTHOR_TAG.', 'we modified the snmt trainable agent in  #TAUTHOR_TAG and added a new non - trivial predict action to the agent.', 'we compare to their model and show better results in delay and quality']","['always prefers to read more words from the input and this approach does not work well in practice.', ' #AUTHOR_TAG introduced a trainable agent which they trained using deep q networks  #AUTHOR_TAG.', 'we modified the snmt trainable agent in  #TAUTHOR_TAG and added a new non - trivial predict action to the agent.', 'we compare to their model and show better results in delay and quality']","['work in snmt was done in speech, where the incoming signals were segmented based on acoustic or statistical cues  #AUTHOR_TAG fugen et al., 2007  #AUTHOR_TAG use a separate segmentation step and incrementally translate each segment using a standard phrase - based mt system.', ' #AUTHOR_TAG applied pattern matching to predict target - side verbs in japanese to english translation.', '( grissom ii et al., 2014 ) used reinforcement learning to predict the next word and the sentencefinal verb in a statistical mt model.', 'these models reduce the delay but are not trained end - toend like our agent - based snmt system.', ' #AUTHOR_TAG proposed a non - trainable heuristic agent which is not able to trade - off quality with delay.', 'it always prefers to read more words from the input and this approach does not work well in practice.', ' #AUTHOR_TAG introduced a trainable agent which they trained using deep q networks  #AUTHOR_TAG.', 'we modified the snmt trainable agent in  #TAUTHOR_TAG and added a new non - trivial predict action to the agent.', 'we compare to their model and show better results in delay and quality']",6
"['', 'the agent in the greedy decoding framework  #TAUTHOR_TAG was trained using reinforcement']","['on the actions that lead to better translation quality and lower delay.', 'the agent in the greedy decoding framework  #TAUTHOR_TAG was trained using reinforcement']","['on the actions that lead to better translation quality and lower delay.', 'the agent in the greedy decoding framework  #TAUTHOR_TAG was trained using reinforcement learning with the policy gradient algorithm  #AUTHOR_TAG, which observes the current state of the environment at time step']","['agent is a separate component which examines the environment at each time step and decides on the actions that lead to better translation quality and lower delay.', 'the agent in the greedy decoding framework  #TAUTHOR_TAG was trained using reinforcement learning with the policy gradient algorithm  #AUTHOR_TAG, which observes the current state of the environment at time step t as o t where o t = [ c t ; s t ; w m ].', 'a rnn with one hidden layer passed through a softmax function generates the probability distribution over the actions a t at each step.', 'therefore, policy [UNK] [UNK] will be computed as :', ""where u t is the hidden state of the agent's rnn""]",5
"['[UNK]  #TAUTHOR_TAG :', '[UNK] c prediction rewards for quality and delay alone do not motivate the agent to choose prediction and in preliminary experiments, after a number of steps,']","['[UNK]  #TAUTHOR_TAG :', '[UNK] c prediction rewards for quality and delay alone do not motivate the agent to choose prediction and in preliminary experiments, after a number of steps,']","['( for any other actions, s ( t ) would be zero ).', 'the delay reward is smoothed using a target delay which is a scalar constant denoted by d [UNK]  #TAUTHOR_TAG :', '[UNK] c prediction rewards for quality and delay alone do not motivate the agent to choose prediction and in preliminary experiments, after a number of steps']","['', 'all the evaluation metrics have been modified to be computed for every time step.', 'quality : we use a modified smoothed version of bleu score  #AUTHOR_TAG multiplied by brevity penalty  #AUTHOR_TAG for evaluating the impact of each action on translation quality.', 'at each point in time, the reward for translation quality is :', 'the bleu ( t ) is the difference between bleu score of the translated sentence at the previous time step and the current time step ; bleu ( t ) = bleu ( w t, w [UNK] ) bleu ( w t 1, w [UNK] ) ; where w t is the prefix of the translated sentence at time t. delay : the delay reward is used to motivate the agent to minimize delay.', 'we use average proportion ( ap )  #AUTHOR_TAG for this purpose, which is the average number of source words needed when translating each word.', 'given the source words x and translated words w, ap can be computed as :', 'where s ( t ) denotes the number of source words the write action uses at time step t ( for any other actions, s ( t ) would be zero ).', 'the delay reward is smoothed using a target delay which is a scalar constant denoted by d [UNK]  #TAUTHOR_TAG :', '[UNK] c prediction rewards for quality and delay alone do not motivate the agent to choose prediction and in preliminary experiments, after a number of steps, the number of prediction actions became zero.', 'we address this problem by defining prediction quality ( pq ) which rewards the agent for changes in bleu score after each prediction action.', 'by initializing r p 0 = 0, the prediction reward can be written as :', 'the final reward function is calculated as the combination of quality, delay, and prediction rewards :', '( 1 ) the trade - off between better translation quality and minimal delay is achieved by modifying the parameters [UNK],, and.', 'reinforcement learning is used to train the agent using a policy gradient algorithm  #TAUTHOR_TAG which searches for the maximum in', 'the gradient for a sentence is the cumulative sum of gradients at each time step.', 'we pre - train the environment on full sentences using log - loss log p ( y | x )']",5
"['[UNK]  #TAUTHOR_TAG :', '[UNK] c prediction rewards for quality and delay alone do not motivate the agent to choose prediction and in preliminary experiments, after a number of steps,']","['[UNK]  #TAUTHOR_TAG :', '[UNK] c prediction rewards for quality and delay alone do not motivate the agent to choose prediction and in preliminary experiments, after a number of steps,']","['( for any other actions, s ( t ) would be zero ).', 'the delay reward is smoothed using a target delay which is a scalar constant denoted by d [UNK]  #TAUTHOR_TAG :', '[UNK] c prediction rewards for quality and delay alone do not motivate the agent to choose prediction and in preliminary experiments, after a number of steps']","['', 'all the evaluation metrics have been modified to be computed for every time step.', 'quality : we use a modified smoothed version of bleu score  #AUTHOR_TAG multiplied by brevity penalty  #AUTHOR_TAG for evaluating the impact of each action on translation quality.', 'at each point in time, the reward for translation quality is :', 'the bleu ( t ) is the difference between bleu score of the translated sentence at the previous time step and the current time step ; bleu ( t ) = bleu ( w t, w [UNK] ) bleu ( w t 1, w [UNK] ) ; where w t is the prefix of the translated sentence at time t. delay : the delay reward is used to motivate the agent to minimize delay.', 'we use average proportion ( ap )  #AUTHOR_TAG for this purpose, which is the average number of source words needed when translating each word.', 'given the source words x and translated words w, ap can be computed as :', 'where s ( t ) denotes the number of source words the write action uses at time step t ( for any other actions, s ( t ) would be zero ).', 'the delay reward is smoothed using a target delay which is a scalar constant denoted by d [UNK]  #TAUTHOR_TAG :', '[UNK] c prediction rewards for quality and delay alone do not motivate the agent to choose prediction and in preliminary experiments, after a number of steps, the number of prediction actions became zero.', 'we address this problem by defining prediction quality ( pq ) which rewards the agent for changes in bleu score after each prediction action.', 'by initializing r p 0 = 0, the prediction reward can be written as :', 'the final reward function is calculated as the combination of quality, delay, and prediction rewards :', '( 1 ) the trade - off between better translation quality and minimal delay is achieved by modifying the parameters [UNK],, and.', 'reinforcement learning is used to train the agent using a policy gradient algorithm  #TAUTHOR_TAG which searches for the maximum in', 'the gradient for a sentence is the cumulative sum of gradients at each time step.', 'we pre - train the environment on full sentences using log - loss log p ( y | x )']",5
['model in  #TAUTHOR_TAG and set'],['model in  #TAUTHOR_TAG and set'],"['worked the best for the greedy decoding model in  #TAUTHOR_TAG and set the target delay d [UNK] for the agent to 0. 7.', '']","['train and evaluate our model on englishgerman ( en - de ) in both directions.', 'we use wmt 2015 for training and newstest 2013 for validation and testing.', 'all sentences have been tokenized and the words are segmented using byte pair encoding ( bpe )  #AUTHOR_TAG model configuration for a fair comparison, we follow the settings that worked the best for the greedy decoding model in  #TAUTHOR_TAG and set the target delay d [UNK] for the agent to 0. 7.', 'the en - vironment consists of two unidirectional layers with 1028 gru units for encoder and decoder.', 'we train the network using adadelta optimizer, a batch of size 32 and a fixed learning rate of 0. 0001 without decay.', 'we use softmax policy via recurrent networks with 512 gru units and a softmax function for the agent and train it using adam optimizer  #AUTHOR_TAG.', 'the batch size for the agent is 10, and the learning rate is 2e - 6.', 'the word predictor is a two layer rnn language model which consists of two layers of 1024 units, followed by a softmax layer.', 'the batch size is 64 with a learning rate of 2e - 5.', ""the predictor has been trained on the wmt'16 dataset and tested on newstest'16 corpora for both languages."", 'the perplexity of our language model is reported in table 1.', 'we set [UNK] = 1, = 0. 5 and = 0. 5.', 'we tried different settings for these hyperparameters during training and picked values that gave us the best quality and delay on the training data.', 'results and analysis figure 4 shows that as the sentence length increases, prediction helps translation quality due to complex reordering and multiclausal sentences ; however, for shorter samples where the structure of the sentences are simpler, the prediction action cannot improve translation quality.', 'table 2 compares our model with the greedy decoding ( gd ) model in terms of translation quality and latency.', 'it shows that the prediction mechanism outperforms the gd model in terms of bleu and average proportion ( ap ).', 'the delay evaluation measure ( ap ) counts about the same number of reads and writes.', 'it does not account for less delay as longer sentences are produced.', 'a better measure than ap might be needed to emphasize delay differences.', 'therefore we also report the average segment length ( [UNK] ), which is computed as the average number of consecutive reads in each sentence.', 'in both en! de and de! en experiments, our model constantly decreases the segment length by around 1 word which results in less latency.', 'in order to evaluate the effectiveness']",5
"[' #TAUTHOR_TAG,', 'where']","[' #TAUTHOR_TAG,', 'where']",[' #TAUTHOR_TAG'],"[""in the task are required to apply the labels bad and ok, either to words or phrases. in this paper we describe the approach behind the submissions of the universitat d'alacant team to these sub - tasks. for our word - level submissions we have applied the approach proposed by  #TAUTHOR_TAG,"", 'where we used black - box bilingual on - line resources. the new task tackles mtqe for translating english into german. for this task we have combined two on - line - available mt systems, 1 lucy lt kwik translator 2 and google translate, 3 and the bilingual concordancer reverso context 4 to spot sub - segment correspondences between a', 'sentence s in the source language ( sl ) and a given translation hypothesis t in the target language ( tl ). as described by  #TAUTHOR_TAG, a collection of features is obtained from these correspondences and then used', 'by a binary classifier to determine the final word - level mtqe labels. we have repeated the approach proposed in wmt 2015 for word - level sub - tasks, and have proposed', '']",4
"[' #TAUTHOR_TAG,', 'where']","[' #TAUTHOR_TAG,', 'where']",[' #TAUTHOR_TAG'],"[""in the task are required to apply the labels bad and ok, either to words or phrases. in this paper we describe the approach behind the submissions of the universitat d'alacant team to these sub - tasks. for our word - level submissions we have applied the approach proposed by  #TAUTHOR_TAG,"", 'where we used black - box bilingual on - line resources. the new task tackles mtqe for translating english into german. for this task we have combined two on - line - available mt systems, 1 lucy lt kwik translator 2 and google translate, 3 and the bilingual concordancer reverso context 4 to spot sub - segment correspondences between a', 'sentence s in the source language ( sl ) and a given translation hypothesis t in the target language ( tl ). as described by  #TAUTHOR_TAG, a collection of features is obtained from these correspondences and then used', 'by a binary classifier to determine the final word - level mtqe labels. we have repeated the approach proposed in wmt 2015 for word - level sub - tasks, and have proposed', '']",4
['by  #TAUTHOR_TAG. provided by the organisation were used to'],"['task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word']","['by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels,']","['with a single hidden layer containing the same number of nodes as the number of features ; this was the best performing', 'architecture in the preliminary experiments. 7 the training sets 5 the list of features can be found in the file features list in the package http : / / www. quest. dcs. shef.', 'ac. uk / wmt16 _ files _ qe / task2 _ en - de _ test. tar. gz 6 the list', 'of features can be found in the file features list in the package http : / / www. quest. dcs. shef. ac. uk / wmt16 _ files _ qe / task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error', 'was computed, in order to minimise the risk of overfitting. the binary classifiers for the sub - task on phrase - level mtqe was trained to optimise the main comparison metric : f bad 1 · f ok 1,', '']",4
['by  #TAUTHOR_TAG that combining'],['by  #TAUTHOR_TAG that combining'],['by  #TAUTHOR_TAG that combining the baseline features with'],[' #TAUTHOR_TAG'],4
"[' #TAUTHOR_TAG,', 'where']","[' #TAUTHOR_TAG,', 'where']",[' #TAUTHOR_TAG'],"[""in the task are required to apply the labels bad and ok, either to words or phrases. in this paper we describe the approach behind the submissions of the universitat d'alacant team to these sub - tasks. for our word - level submissions we have applied the approach proposed by  #TAUTHOR_TAG,"", 'where we used black - box bilingual on - line resources. the new task tackles mtqe for translating english into german. for this task we have combined two on - line - available mt systems, 1 lucy lt kwik translator 2 and google translate, 3 and the bilingual concordancer reverso context 4 to spot sub - segment correspondences between a', 'sentence s in the source language ( sl ) and a given translation hypothesis t in the target language ( tl ). as described by  #TAUTHOR_TAG, a collection of features is obtained from these correspondences and then used', 'by a binary classifier to determine the final word - level mtqe labels. we have repeated the approach proposed in wmt 2015 for word - level sub - tasks, and have proposed', '']",6
"[' #TAUTHOR_TAG,', 'where']","[' #TAUTHOR_TAG,', 'where']",[' #TAUTHOR_TAG'],"[""in the task are required to apply the labels bad and ok, either to words or phrases. in this paper we describe the approach behind the submissions of the universitat d'alacant team to these sub - tasks. for our word - level submissions we have applied the approach proposed by  #TAUTHOR_TAG,"", 'where we used black - box bilingual on - line resources. the new task tackles mtqe for translating english into german. for this task we have combined two on - line - available mt systems, 1 lucy lt kwik translator 2 and google translate, 3 and the bilingual concordancer reverso context 4 to spot sub - segment correspondences between a', 'sentence s in the source language ( sl ) and a given translation hypothesis t in the target language ( tl ). as described by  #TAUTHOR_TAG, a collection of features is obtained from these correspondences and then used', 'by a binary classifier to determine the final word - level mtqe labels. we have repeated the approach proposed in wmt 2015 for word - level sub - tasks, and have proposed', '']",6
['by  #TAUTHOR_TAG. provided by the organisation were used to'],"['task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word']","['by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels,']","['with a single hidden layer containing the same number of nodes as the number of features ; this was the best performing', 'architecture in the preliminary experiments. 7 the training sets 5 the list of features can be found in the file features list in the package http : / / www. quest. dcs. shef.', 'ac. uk / wmt16 _ files _ qe / task2 _ en - de _ test. tar. gz 6 the list', 'of features can be found in the file features list in the package http : / / www. quest. dcs. shef. ac. uk / wmt16 _ files _ qe / task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error', 'was computed, in order to minimise the risk of overfitting. the binary classifiers for the sub - task on phrase - level mtqe was trained to optimise the main comparison metric : f bad 1 · f ok 1,', '']",6
['by  #TAUTHOR_TAG that combining'],['by  #TAUTHOR_TAG that combining'],['by  #TAUTHOR_TAG that combining the baseline features with'],[' #TAUTHOR_TAG'],6
['by  #TAUTHOR_TAG. provided by the organisation were used to'],"['task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word']","['by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels,']","['with a single hidden layer containing the same number of nodes as the number of features ; this was the best performing', 'architecture in the preliminary experiments. 7 the training sets 5 the list of features can be found in the file features list in the package http : / / www. quest. dcs. shef.', 'ac. uk / wmt16 _ files _ qe / task2 _ en - de _ test. tar. gz 6 the list', 'of features can be found in the file features list in the package http : / / www. quest. dcs. shef. ac. uk / wmt16 _ files _ qe / task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error', 'was computed, in order to minimise the risk of overfitting. the binary classifiers for the sub - task on phrase - level mtqe was trained to optimise the main comparison metric : f bad 1 · f ok 1,', '']",5
['by  #TAUTHOR_TAG. provided by the organisation were used to'],"['task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word']","['by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels,']","['with a single hidden layer containing the same number of nodes as the number of features ; this was the best performing', 'architecture in the preliminary experiments. 7 the training sets 5 the list of features can be found in the file features list in the package http : / / www. quest. dcs. shef.', 'ac. uk / wmt16 _ files _ qe / task2 _ en - de _ test. tar. gz 6 the list', 'of features can be found in the file features list in the package http : / / www. quest. dcs. shef. ac. uk / wmt16 _ files _ qe / task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error', 'was computed, in order to minimise the risk of overfitting. the binary classifiers for the sub - task on phrase - level mtqe was trained to optimise the main comparison metric : f bad 1 · f ok 1,', '']",3
['by  #TAUTHOR_TAG. provided by the organisation were used to'],"['task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word']","['by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels,']","['with a single hidden layer containing the same number of nodes as the number of features ; this was the best performing', 'architecture in the preliminary experiments. 7 the training sets 5 the list of features can be found in the file features list in the package http : / / www. quest. dcs. shef.', 'ac. uk / wmt16 _ files _ qe / task2 _ en - de _ test. tar. gz 6 the list', 'of features can be found in the file features list in the package http : / / www. quest. dcs. shef. ac. uk / wmt16 _ files _ qe / task2p _ en - de _ test. tar. gz 7 the', 'rest of parameters of the classifiers were also kept as in the approach by  #TAUTHOR_TAG. provided by the organisation were used to train the binary classifiers, both for word and phrase levels, while the development sets were used as validation sets on which the training error', 'was computed, in order to minimise the risk of overfitting. the binary classifiers for the sub - task on phrase - level mtqe was trained to optimise the main comparison metric : f bad 1 · f ok 1,', '']",3
"[' #TAUTHOR_TAG, choose a different data split on']","[' #TAUTHOR_TAG, choose a different data split on the pos', '']","['( 13862 vs 14987 sentences ). different from  #AUTHOR_TAG and  #TAUTHOR_TAG, choose a different data split on']","['smaller dataset ( 13862 vs 14987 sentences ). different from  #AUTHOR_TAG and  #TAUTHOR_TAG, choose a different data split on the pos', 'dataset.  #AUTHOR_TAG and  #AUTHOR_TAG use different development sets for chunking. • preprocessing. a typical data preprocessing step is to normize digit characters  #AUTHOR_TAG.  #AUTHOR_TAG b )', 'use fine - grained representations for less frequent words.  #AUTHOR_TAG do not use preprocessing. • features.  #AUTHOR_TAG and  #AUTHOR_TAG apply word', 'spelling features and further integrate context features.  #AUTHOR_TAG and use neural features to represent external gazetteer information.  #AUTHOR_TAG and', ' #AUTHOR_TAG use end - to - end structure without handcrafted features. • hyperparameters including learning rate, dropout rate  #AUTHOR_TAG, number of layers, hidden size etc. can strongly affect the model performance.  #AUTHOR_TAG search for the', 'hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. however, existing models use different parameter settings, which affects the fair', 'comparison. • evaluation. some literature reports results using mean and standard deviation under different random seeds  #TAUTHOR_TAG. others report the best result among different trials', '']",4
"['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG,']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG,']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG, we adopt the standard splits']","['.', 'the ner dataset has been standardly split in  #AUTHOR_TAG  #TAUTHOR_TAG, we adopt the standard splits by using sections 0 - 18 as training set, sections 19 - 21 as development set and sections 22 - 24 as test set.', 'no preprocessing is performed on either dataset except for normalizing digits.', 'the dataset statistics are listed in table 2.', 'hyperparameters.', 'table 3 shows the hyperparameters used in our experiments, which mostly follow  #AUTHOR_TAG, including the learning rate η = 0. 015 for word lstm models.', 'for word cnn based models, a large η leads to convergence problem.', 'we take η = 0. 005 with more epochs ( 200 ) instead.', 'glove 100 - dimension  #AUTHOR_TAG is used to initialize word embeddings and character embeddings are randomly initialized.', 'we use mini - batch stochastic gradient descent ( sgd ) with a decayed learning rate to update parameters.', 'for ner and chunking, we the bioes tag scheme.', 'evaluation.', '']",4
"[' #TAUTHOR_TAG, choose a different data split on']","[' #TAUTHOR_TAG, choose a different data split on the pos', '']","['( 13862 vs 14987 sentences ). different from  #AUTHOR_TAG and  #TAUTHOR_TAG, choose a different data split on']","['smaller dataset ( 13862 vs 14987 sentences ). different from  #AUTHOR_TAG and  #TAUTHOR_TAG, choose a different data split on the pos', 'dataset.  #AUTHOR_TAG and  #AUTHOR_TAG use different development sets for chunking. • preprocessing. a typical data preprocessing step is to normize digit characters  #AUTHOR_TAG.  #AUTHOR_TAG b )', 'use fine - grained representations for less frequent words.  #AUTHOR_TAG do not use preprocessing. • features.  #AUTHOR_TAG and  #AUTHOR_TAG apply word', 'spelling features and further integrate context features.  #AUTHOR_TAG and use neural features to represent external gazetteer information.  #AUTHOR_TAG and', ' #AUTHOR_TAG use end - to - end structure without handcrafted features. • hyperparameters including learning rate, dropout rate  #AUTHOR_TAG, number of layers, hidden size etc. can strongly affect the model performance.  #AUTHOR_TAG search for the', 'hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. however, existing models use different parameter settings, which affects the fair', 'comparison. • evaluation. some literature reports results using mean and standard deviation under different random seeds  #TAUTHOR_TAG. others report the best result among different trials', '']",6
"[' #TAUTHOR_TAG, choose a different data split on']","[' #TAUTHOR_TAG, choose a different data split on the pos', '']","['( 13862 vs 14987 sentences ). different from  #AUTHOR_TAG and  #TAUTHOR_TAG, choose a different data split on']","['smaller dataset ( 13862 vs 14987 sentences ). different from  #AUTHOR_TAG and  #TAUTHOR_TAG, choose a different data split on the pos', 'dataset.  #AUTHOR_TAG and  #AUTHOR_TAG use different development sets for chunking. • preprocessing. a typical data preprocessing step is to normize digit characters  #AUTHOR_TAG.  #AUTHOR_TAG b )', 'use fine - grained representations for less frequent words.  #AUTHOR_TAG do not use preprocessing. • features.  #AUTHOR_TAG and  #AUTHOR_TAG apply word', 'spelling features and further integrate context features.  #AUTHOR_TAG and use neural features to represent external gazetteer information.  #AUTHOR_TAG and', ' #AUTHOR_TAG use end - to - end structure without handcrafted features. • hyperparameters including learning rate, dropout rate  #AUTHOR_TAG, number of layers, hidden size etc. can strongly affect the model performance.  #AUTHOR_TAG search for the', 'hyperparameters for each task and show that the system performance is sensitive to the choice of hyperparameters. however, existing models use different parameter settings, which affects the fair', 'comparison. • evaluation. some literature reports results using mean and standard deviation under different random seeds  #TAUTHOR_TAG. others report the best result among different trials', '']",0
"['extended by adding character - level lstm  #TAUTHOR_TAG, gru  #AUTHOR_TAG,']","['extended by adding character - level lstm  #TAUTHOR_TAG, gru  #AUTHOR_TAG,']","['has been extended by adding character - level lstm  #TAUTHOR_TAG, gru  #AUTHOR_TAG,']","['proposed a seminal neural architecture for sequence labeling.', 'it captures word sequence information with a one - layer cnn based on pretrained word embeddings and handcrafted neural features, followed with a crf output layer.', 'dos  #AUTHOR_TAG extended this model by integrating character - level cnn features.', ' #AUTHOR_TAG built a deeper dilated cnn architecture to capture larger local features.', ' #AUTHOR_TAG was the first to exploit lstm for sequence labeling.', 'built a bilstm - crf structure, which has been extended by adding character - level lstm  #TAUTHOR_TAG, gru  #AUTHOR_TAG, and cnn  #AUTHOR_TAG features.', ' #AUTHOR_TAG a ) proposed a neural reranking model to improve ner models.', 'these models achieve state - of - the - art results in the literature.', ' #AUTHOR_TAG b ) compared several word - based lstm models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value.', 'they investigated the influence of various hyperparameters and configurations.', 'our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects : 1 ) their experiments are based on a bilstm with handcrafted word features, while our experiments are based on end - to - end neural models without human knowledge.', '2 ) their system gives relatively low performances on standard benchmarks 2, while ours can give comparable or better results with state - of - the - art models, rendering our observations more informative for practitioners.', '3 ) our findings are more consistent with most previous work on configurations such as usefulness of character information  #AUTHOR_TAG, optimizer  #AUTHOR_TAG and tag scheme  #AUTHOR_TAG.', 'in contrast, many results of  #AUTHOR_TAG b ) contradict existing reports.', '4 ) we conduct a wider range of comparison for word sequence representations, including all combinations of character cnn / lstm and word cnn / lstm structures, while  #AUTHOR_TAG b ) studied the word lstm models only']",0
"['to character sequences in words, we can model word sequence information through lstm or cnn structures.', 'lstm has been widely used in sequence labeling  #TAUTHOR_TAG.', 'cnn can be much faster than lstm']","['to character sequences in words, we can model word sequence information through lstm or cnn structures.', 'lstm has been widely used in sequence labeling  #TAUTHOR_TAG.', 'cnn can be much faster than lstm']","['to character sequences in words, we can model word sequence information through lstm or cnn structures.', 'lstm has been widely used in sequence labeling  #TAUTHOR_TAG.', 'cnn can be much faster than lstm']","['to character sequences in words, we can model word sequence information through lstm or cnn structures.', 'lstm has been widely used in sequence labeling  #TAUTHOR_TAG.', 'cnn can be much faster than lstm due to the fact that convolution calculation can be parallel on the input sequence  #AUTHOR_TAG dos  #AUTHOR_TAG.', 'word cnn.', 'figure 3 ( a ) shows the multi - layer cnn on word sequence, where words are represented by embeddings.', 'if a character sequence representation layer is used, then word embeddings and character sequence representations are concatenated for word representations.', 'for each cnn layer, a window of size 3 slides along the sequence, extracting local features on the word inputs and a relu function  #AUTHOR_TAG is followed.', 'we follow  #AUTHOR_TAG by using 4 cnn layers.', 'batch normalization  #AUTHOR_TAG and dropout  #AUTHOR_TAG are applied following each cnn layer.', 'word lstm.', 'shown in figure 3 ( b ), word representations are fed into a forward lstm and backward lstm, respectively.', 'the forward lstm captures the word sequence information from left to right, while the backward lstm extracts information in a reversed direction.', 'the hidden states of the forward and backward lstms are concatenated at each word to give global information of the whole sequence']",0
"['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['##ing monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem (']","['bilingual lexicon induction ( ubli ) has been shown to benefit nlp tasks for low resource languages, including unsupervised nmt  #AUTHOR_TAG b, c ;  #AUTHOR_TAG a, b ), information retrieval ( vulic and  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG, and named entity recognition  #AUTHOR_TAG.', 'recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem ( e. g. englishitalian vs italian - english ).', 'however, most existing research considers mapping from one language to another without making use of symmetry.', 'our experiments show that separately learned ubli models are not always consistent in opposite directions.', 'as shown in figure 1a, when the model of  #TAUTHOR_TAG is applied to english and italian, the primal model maps the word "" three "" to the italian word "" tre "", but the dual model maps "" tre "" to "" two "" instead of "" three "".', 'we propose to address this issue by exploiting duality, encouraging forward and backward mappings to form a closed loop ( figure 1b ).', 'in particular, we extend the model of  #TAUTHOR_TAG by using a cycle consistency loss  #AUTHOR_TAG to regularize two models in opposite directions.', 'experiments on two benchmark datasets show that the simple method of enforcing consistency gives better results in both directions.', 'our model significantly outperforms competitive baselines, obtaining the best published results.', 'we release our code at xxx']",1
"['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['##ing monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem (']","['bilingual lexicon induction ( ubli ) has been shown to benefit nlp tasks for low resource languages, including unsupervised nmt  #AUTHOR_TAG b, c ;  #AUTHOR_TAG a, b ), information retrieval ( vulic and  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG, and named entity recognition  #AUTHOR_TAG.', 'recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem ( e. g. englishitalian vs italian - english ).', 'however, most existing research considers mapping from one language to another without making use of symmetry.', 'our experiments show that separately learned ubli models are not always consistent in opposite directions.', 'as shown in figure 1a, when the model of  #TAUTHOR_TAG is applied to english and italian, the primal model maps the word "" three "" to the italian word "" tre "", but the dual model maps "" tre "" to "" two "" instead of "" three "".', 'we propose to address this issue by exploiting duality, encouraging forward and backward mappings to form a closed loop ( figure 1b ).', 'in particular, we extend the model of  #TAUTHOR_TAG by using a cycle consistency loss  #AUTHOR_TAG to regularize two models in opposite directions.', 'experiments on two benchmark datasets show that the simple method of enforcing consistency gives better results in both directions.', 'our model significantly outperforms competitive baselines, obtaining the best published results.', 'we release our code at xxx']",1
"['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['##ing monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem (']","['bilingual lexicon induction ( ubli ) has been shown to benefit nlp tasks for low resource languages, including unsupervised nmt  #AUTHOR_TAG b, c ;  #AUTHOR_TAG a, b ), information retrieval ( vulic and  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG, and named entity recognition  #AUTHOR_TAG.', 'recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem ( e. g. englishitalian vs italian - english ).', 'however, most existing research considers mapping from one language to another without making use of symmetry.', 'our experiments show that separately learned ubli models are not always consistent in opposite directions.', 'as shown in figure 1a, when the model of  #TAUTHOR_TAG is applied to english and italian, the primal model maps the word "" three "" to the italian word "" tre "", but the dual model maps "" tre "" to "" two "" instead of "" three "".', 'we propose to address this issue by exploiting duality, encouraging forward and backward mappings to form a closed loop ( figure 1b ).', 'in particular, we extend the model of  #TAUTHOR_TAG by using a cycle consistency loss  #AUTHOR_TAG to regularize two models in opposite directions.', 'experiments on two benchmark datasets show that the simple method of enforcing consistency gives better results in both directions.', 'our model significantly outperforms competitive baselines, obtaining the best published results.', 'we release our code at xxx']",1
"['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG does not always work well.', 'to address this, we make a simple extension by calculating the weighted average of forward']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG does not always work well.', 'to address this, we make a simple extension by calculating the weighted average of forward and backward scores :', 'where λ is a hyperparameter to control the importance of the two objectives.', '1 here s first generates bilingual lexicons by learned mappings, and then computes the average cosine similarity of these translations']",1
"['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['##ing monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem (']","['bilingual lexicon induction ( ubli ) has been shown to benefit nlp tasks for low resource languages, including unsupervised nmt  #AUTHOR_TAG b, c ;  #AUTHOR_TAG a, b ), information retrieval ( vulic and  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG, and named entity recognition  #AUTHOR_TAG.', 'recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem ( e. g. englishitalian vs italian - english ).', 'however, most existing research considers mapping from one language to another without making use of symmetry.', 'our experiments show that separately learned ubli models are not always consistent in opposite directions.', 'as shown in figure 1a, when the model of  #TAUTHOR_TAG is applied to english and italian, the primal model maps the word "" three "" to the italian word "" tre "", but the dual model maps "" tre "" to "" two "" instead of "" three "".', 'we propose to address this issue by exploiting duality, encouraging forward and backward mappings to form a closed loop ( figure 1b ).', 'in particular, we extend the model of  #TAUTHOR_TAG by using a cycle consistency loss  #AUTHOR_TAG to regularize two models in opposite directions.', 'experiments on two benchmark datasets show that the simple method of enforcing consistency gives better results in both directions.', 'our model significantly outperforms competitive baselines, obtaining the best published results.', 'we release our code at xxx']",3
"[' #AUTHOR_TAG a, b ;  #TAUTHOR_TAG, matching the distributions of source and target']","[' #AUTHOR_TAG a, b ;  #TAUTHOR_TAG, matching the distributions of source and target']","['uses adversarial training  #AUTHOR_TAG a, b ;  #TAUTHOR_TAG, matching the distributions of source and target word embeddings through generative adversarial networks  #AUTHOR_TAG.', 'non - adversarial approaches have also been explored.', 'for instance,  #AUTHOR_TAG use squared - loss mutual information']","['##li.', 'a typical line of work uses adversarial training  #AUTHOR_TAG a, b ;  #TAUTHOR_TAG, matching the distributions of source and target word embeddings through generative adversarial networks  #AUTHOR_TAG.', 'non - adversarial approaches have also been explored.', 'for instance,  #AUTHOR_TAG use squared - loss mutual information to search for optimal cross - lingual word pairing.', ' #AUTHOR_TAG a ) and  #AUTHOR_TAG exploit the structural similarity of word embedding spaces to learn word mappings.', 'in this paper, we choose  #TAUTHOR_TAG as our baseline as it is theoretically attractive and gives strong results on large - scale datasets.', 'cycle consistency.', 'forward - backward consistency has been used to discover the correspondence between unpaired images  #AUTHOR_TAG.', 'in machine translation, similar ideas were exploited,  #AUTHOR_TAG,  #AUTHOR_TAG and use dual learning to train two opposite language translators by minimizing the reconstruction loss.', ' #AUTHOR_TAG consider back - translation, where a backward model is used to build synthetic parallel corpus and a forward model learns to generate genuine text based on the synthetic output.', 'closer to our method,  #AUTHOR_TAG jointly train two autoencoders to learn supervised bilingual word embeddings.', 'use sinkhorn distance  #AUTHOR_TAG and backtranslation to align word embeddings.', 'however, they cannot perform fully unsupervised training, relying on wgan  #AUTHOR_TAG for providing initial mappings.', 'concurrent with our work,  #AUTHOR_TAG build a adversarial autoencoder with cycle consistency loss and post - cycle reconstruction loss.', 'in contrast to these works, our method is fully unsupervised, simpler, and empirically more effective']",3
"['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their']","['##ing monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem (']","['bilingual lexicon induction ( ubli ) has been shown to benefit nlp tasks for low resource languages, including unsupervised nmt  #AUTHOR_TAG b, c ;  #AUTHOR_TAG a, b ), information retrieval ( vulic and  #AUTHOR_TAG, dependency parsing  #AUTHOR_TAG, and named entity recognition  #AUTHOR_TAG.', 'recent research has attempted to induce unsupervised bilingual lexicons by aligning monolingual word vector spaces  #AUTHOR_TAG a ;  #TAUTHOR_TAG a ; alvarez -  #AUTHOR_TAG.', 'given a pair of languages, their word alignment is inherently a bi - directional problem ( e. g. englishitalian vs italian - english ).', 'however, most existing research considers mapping from one language to another without making use of symmetry.', 'our experiments show that separately learned ubli models are not always consistent in opposite directions.', 'as shown in figure 1a, when the model of  #TAUTHOR_TAG is applied to english and italian, the primal model maps the word "" three "" to the italian word "" tre "", but the dual model maps "" tre "" to "" two "" instead of "" three "".', 'we propose to address this issue by exploiting duality, encouraging forward and backward mappings to form a closed loop ( figure 1b ).', 'in particular, we extend the model of  #TAUTHOR_TAG by using a cycle consistency loss  #AUTHOR_TAG to regularize two models in opposite directions.', 'experiments on two benchmark datasets show that the simple method of enforcing consistency gives better results in both directions.', 'our model significantly outperforms competitive baselines, obtaining the best published results.', 'we release our code at xxx']",4
"[' #TAUTHOR_TAG as our baseline, introducing a novel regularizer to enforce cycle consistency.', 'let']","[' #TAUTHOR_TAG as our baseline, introducing a novel regularizer to enforce cycle consistency.', 'let']","['take  #TAUTHOR_TAG as our baseline, introducing a novel regularizer to enforce cycle consistency.', 'let x = { x 1,...']","['take  #TAUTHOR_TAG as our baseline, introducing a novel regularizer to enforce cycle consistency.', 'let x = { x 1,..., x n } and y = { y 1,..., y m } be two sets of n and m word embeddings for a source and a target language, respectively.', 'the primal ubli task aims to learn a linear mapping f : x → y such that for each x i, f ( x i ) corresponds to its translation in y.', 'similarly, a linear mapping g : y → x is defined for the dual task.', 'in addition, we introduce two language discriminators d x and d y, which are trained to discriminate between the mapped word embeddings and the original word embeddings.', ' #AUTHOR_TAG align two word embedding spaces through generative adversarial networks, in which two networks are trained simultaneously.', 'specifically, take the primal ubli task as an example, the linear mapping f tries to generate "" fake "" word embeddings f ( x ) that look similar to word embeddings from y, while the discriminator d y aims to distinguish between "" fake "" and real word embeddings from y.', 'formally,']",5
"['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG does not always work well.', 'to address this, we make a simple extension by calculating the weighted average of forward']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG does not always work well.', 'to address this, we make a simple extension by calculating the weighted average of forward and backward scores :', 'where λ is a hyperparameter to control the importance of the two objectives.', '1 here s first generates bilingual lexicons by learned mappings, and then computes the average cosine similarity of these translations']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', '( ii )']","['and setup.', 'our datasets includes : ( i ) the multilingual unsupervised and supervised embeddings ( muse ) dataset released by  #TAUTHOR_TAG.', '( ii ) the more challenging vecmap dataset from  #AUTHOR_TAG and the extensions of  #AUTHOR_TAG.', 'we follow the evaluation setups of  #TAUTHOR_TAG, utilizing cross - domain similarity local scaling ( csls ) for retrieving the translation of given source words.', 'following a standard evaluation practice ( vulic and  #TAUTHOR_TAG, we report precision at 1 scores ( p @ 1 ).', 'given the instability of existing methods, we follow  #AUTHOR_TAG a ) to perform 10 runs for each method and report the best and the average accuracies']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', '( ii )']","['and setup.', 'our datasets includes : ( i ) the multilingual unsupervised and supervised embeddings ( muse ) dataset released by  #TAUTHOR_TAG.', '( ii ) the more challenging vecmap dataset from  #AUTHOR_TAG and the extensions of  #AUTHOR_TAG.', 'we follow the evaluation setups of  #TAUTHOR_TAG, utilizing cross - domain similarity local scaling ( csls ) for retrieving the translation of given source words.', 'following a standard evaluation practice ( vulic and  #TAUTHOR_TAG, we report precision at 1 scores ( p @ 1 ).', 'given the instability of existing methods, we follow  #AUTHOR_TAG a ) to perform 10 runs for each method and report the best and the average accuracies']",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', '( ii )']","['and setup.', 'our datasets includes : ( i ) the multilingual unsupervised and supervised embeddings ( muse ) dataset released by  #TAUTHOR_TAG.', '( ii ) the more challenging vecmap dataset from  #AUTHOR_TAG and the extensions of  #AUTHOR_TAG.', 'we follow the evaluation setups of  #TAUTHOR_TAG, utilizing cross - domain similarity local scaling ( csls ) for retrieving the translation of given source words.', 'following a standard evaluation practice ( vulic and  #TAUTHOR_TAG, we report precision at 1 scores ( p @ 1 ).', 'given the instability of existing methods, we follow  #AUTHOR_TAG a ) to perform 10 runs for each method and report the best and the average accuracies']",5
"[' #TAUTHOR_TAG as our baseline, introducing a novel regularizer to enforce cycle consistency.', 'let']","[' #TAUTHOR_TAG as our baseline, introducing a novel regularizer to enforce cycle consistency.', 'let']","['take  #TAUTHOR_TAG as our baseline, introducing a novel regularizer to enforce cycle consistency.', 'let x = { x 1,...']","['take  #TAUTHOR_TAG as our baseline, introducing a novel regularizer to enforce cycle consistency.', 'let x = { x 1,..., x n } and y = { y 1,..., y m } be two sets of n and m word embeddings for a source and a target language, respectively.', 'the primal ubli task aims to learn a linear mapping f : x → y such that for each x i, f ( x i ) corresponds to its translation in y.', 'similarly, a linear mapping g : y → x is defined for the dual task.', 'in addition, we introduce two language discriminators d x and d y, which are trained to discriminate between the mapped word embeddings and the original word embeddings.', ' #AUTHOR_TAG align two word embedding spaces through generative adversarial networks, in which two networks are trained simultaneously.', 'specifically, take the primal ubli task as an example, the linear mapping f tries to generate "" fake "" word embeddings f ( x ) that look similar to word embeddings from y, while the discriminator d y aims to distinguish between "" fake "" and real word embeddings from y.', 'formally,']",6
"['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG does not always work well.', 'to address this, we make a simple extension by calculating the weighted average of forward']","['follow  #TAUTHOR_TAG, using an unsupervised criterion to perform model selection.', 'in preliminary experiments, we find in adversarial training that the single - direction criterion s ( f, x, y ) by  #TAUTHOR_TAG does not always work well.', 'to address this, we make a simple extension by calculating the weighted average of forward and backward scores :', 'where λ is a hyperparameter to control the importance of the two objectives.', '1 here s first generates bilingual lexicons by learned mappings, and then computes the average cosine similarity of these translations']",6
"['compare our method with  #TAUTHOR_TAG ( adv - c ) under the same settings.', 'as 1 we find that λ = 0. 5 generally works well']","['compare our method with  #TAUTHOR_TAG ( adv - c ) under the same settings.', 'as 1 we find that λ = 0. 5 generally works well']","['compare our method with  #TAUTHOR_TAG ( adv - c ) under the same settings.', 'as 1 we find that λ = 0. 5 generally works well']","['compare our method with  #TAUTHOR_TAG ( adv - c ) under the same settings.', 'as 1 we find that λ = 0. 5 generally works well']",7
"['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these']","['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these']","['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these systems use supervised learning methods which only utilize annotated nl sentences.', 'however,']","['parsing is the task of mapping a natural language ( nl ) sentence into a complete, formal meaning representation ( mr ) which a computer program can execute to perform some task, like answering database queries or controlling a robot.', 'these mrs are expressed in domain - specific unambiguous formal meaning representation languages ( mrls ).', 'given a training corpus of nl sentences annotated with their correct mrs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct mrs.', 'several learning systems have been developed for semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these systems use supervised learning methods which only utilize annotated nl sentences.', 'however, it requires considerable human effort to annotate sentences.', 'in contrast, unannotated nl sentences are usually easily available.', 'semi - supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data  #AUTHOR_TAG.', 'in this paper we present, to our knowledge, the first semi - supervised learning system for semantic parsing.', 'we modify krisp, a supervised learning system for semantic parsing presented in  #TAUTHOR_TAG, to make a semi - supervised system we call semisup - krisp.', 'experiments on a realworld dataset show the improvements semisup - krisp obtains over krisp by utilizing unannotated sentences']",0
[')  #TAUTHOR_TAG is a supervised'],['system krisp ( kernel - based robust interpretation for semantic parsing )  #TAUTHOR_TAG is a supervised'],['system krisp ( kernel - based robust interpretation for semantic parsing )  #TAUTHOR_TAG is a supervised learning system for semantic parsing which takes nl sentences paired with'],"['system krisp ( kernel - based robust interpretation for semantic parsing )  #TAUTHOR_TAG is a supervised learning system for semantic parsing which takes nl sentences paired with their mrs as training data.', 'the productions of the formal mrl grammar are treated like semantic concepts.', 'for each of these productions, a support - vector machine ( svm ) ( cristianini and shawe -  #AUTHOR_TAG classifier is trained using string similarity as the kernel  #AUTHOR_TAG.', 'each classifier can then estimate the probability of any nl substring representing the semantic concept for its production.', 'during semantic parsing, the classifiers are called to estimate probabilities on different substrings of the sentence to compositionally build the most probable meaning representation ( mr ) of the sentence.', 'krisp trains the classifiers used in semantic parsing iteratively.', 'in each iteration, for every production in the mrl grammar, krisp collects positive and negative examples.', 'in the first iteration, the set of positive examples for production contains all sentences whose corresponding mrs use the production in their parse trees.', 'the set of negative examples includes all of the other training sentences.', 'using these positive and negative examples, an svm classifier is trained for each production using a string kernel.', 'in subsequent iterations, the parser learned from the previous iteration is applied to the training examples and more refined positive and negative examples, which are more specific substrings within the sentences, are collected for training.', 'iterations are continued until the classifiers converge, analogous to iterations in em  #AUTHOR_TAG.', 'experimentally, krisp compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data  #TAUTHOR_TAG']",0
[')  #TAUTHOR_TAG is a supervised'],['system krisp ( kernel - based robust interpretation for semantic parsing )  #TAUTHOR_TAG is a supervised'],['system krisp ( kernel - based robust interpretation for semantic parsing )  #TAUTHOR_TAG is a supervised learning system for semantic parsing which takes nl sentences paired with'],"['system krisp ( kernel - based robust interpretation for semantic parsing )  #TAUTHOR_TAG is a supervised learning system for semantic parsing which takes nl sentences paired with their mrs as training data.', 'the productions of the formal mrl grammar are treated like semantic concepts.', 'for each of these productions, a support - vector machine ( svm ) ( cristianini and shawe -  #AUTHOR_TAG classifier is trained using string similarity as the kernel  #AUTHOR_TAG.', 'each classifier can then estimate the probability of any nl substring representing the semantic concept for its production.', 'during semantic parsing, the classifiers are called to estimate probabilities on different substrings of the sentence to compositionally build the most probable meaning representation ( mr ) of the sentence.', 'krisp trains the classifiers used in semantic parsing iteratively.', 'in each iteration, for every production in the mrl grammar, krisp collects positive and negative examples.', 'in the first iteration, the set of positive examples for production contains all sentences whose corresponding mrs use the production in their parse trees.', 'the set of negative examples includes all of the other training sentences.', 'using these positive and negative examples, an svm classifier is trained for each production using a string kernel.', 'in subsequent iterations, the parser learned from the previous iteration is applied to the training examples and more refined positive and negative examples, which are more specific substrings within the sentences, are collected for training.', 'iterations are continued until the classifiers converge, analogous to iterations in em  #AUTHOR_TAG.', 'experimentally, krisp compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data  #TAUTHOR_TAG']",0
"['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these']","['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these']","['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these systems use supervised learning methods which only utilize annotated nl sentences.', 'however,']","['parsing is the task of mapping a natural language ( nl ) sentence into a complete, formal meaning representation ( mr ) which a computer program can execute to perform some task, like answering database queries or controlling a robot.', 'these mrs are expressed in domain - specific unambiguous formal meaning representation languages ( mrls ).', 'given a training corpus of nl sentences annotated with their correct mrs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct mrs.', 'several learning systems have been developed for semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these systems use supervised learning methods which only utilize annotated nl sentences.', 'however, it requires considerable human effort to annotate sentences.', 'in contrast, unannotated nl sentences are usually easily available.', 'semi - supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data  #AUTHOR_TAG.', 'in this paper we present, to our knowledge, the first semi - supervised learning system for semantic parsing.', 'we modify krisp, a supervised learning system for semantic parsing presented in  #TAUTHOR_TAG, to make a semi - supervised system we call semisup - krisp.', 'experiments on a realworld dataset show the improvements semisup - krisp obtains over krisp by utilizing unannotated sentences']",4
"['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these']","['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these']","['semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these systems use supervised learning methods which only utilize annotated nl sentences.', 'however,']","['parsing is the task of mapping a natural language ( nl ) sentence into a complete, formal meaning representation ( mr ) which a computer program can execute to perform some task, like answering database queries or controlling a robot.', 'these mrs are expressed in domain - specific unambiguous formal meaning representation languages ( mrls ).', 'given a training corpus of nl sentences annotated with their correct mrs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct mrs.', 'several learning systems have been developed for semantic parsing, many of them recently  #TAUTHOR_TAG.', 'these systems use supervised learning methods which only utilize annotated nl sentences.', 'however, it requires considerable human effort to annotate sentences.', 'in contrast, unannotated nl sentences are usually easily available.', 'semi - supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data  #AUTHOR_TAG.', 'in this paper we present, to our knowledge, the first semi - supervised learning system for semantic parsing.', 'we modify krisp, a supervised learning system for semantic parsing presented in  #TAUTHOR_TAG, to make a semi - supervised system we call semisup - krisp.', 'experiments on a realworld dataset show the improvements semisup - krisp obtains over krisp by utilizing unannotated sentences']",6
['based translation model in  #TAUTHOR_TAG falls'],['syntax - based translation model in  #TAUTHOR_TAG falls'],"['tree smt approaches.', 'although the syntax - based translation model in  #TAUTHOR_TAG falls']","['', 'although the syntax - based translation model in  #TAUTHOR_TAG falls under the string - to - tree category, i wonder why hierarchical phrasebased smt, or hiero ( chiang 2007 ), is not explicitly put under the string - to - string category, since hiero also uses "" unlabeled hierarchical phrases where there is no representation of linguistic categories. ""', 'chapter 2 focuses on how the statistical framework of a syntax - based smt approach learns its model from a word - aligned and parsed parallel text.', 'the first section explains how phrase pairs are extracted as translation rules from a word - aligned sentence pair in phrase - based smt ( koehn, och, and marcu 2003 ), highlighting the definition of a phrase as a sequence of words and the alignment - consistency property of a phrase pair as defined in  #AUTHOR_TAG.', 'the remainder of the chapter introduces three predominant instantiations of syntax - based models : hierarchical phrase - based smt ( hiero ) ( chiang 2007 ), which is a non - labeled syntax - based smt approach arising from the phrase - based approach ; syntax - augmented machine translation ( samt ), which introduces the notion of soft labels while keeping the nonlinguistic phrase notion ; and ghkm  #TAUTHOR_TAG, which only extracts translation rules consistent with constituency parse subtrees.', 'this chapter is nicely organized and it is easy to follow the gradual evolution from phrase - based smt to ghkm.', 'chapter 3 introduces the decoding formalism in the form of a directed hypergraph, defined as a set of vertices and a set of directed hyperedges.', 'the first section introduces the notion of a weighted parse forest represented in a weighted hypergraph, representing alternative parse trees of a sentence.', '']",0
['based translation model in  #TAUTHOR_TAG falls'],['syntax - based translation model in  #TAUTHOR_TAG falls'],"['tree smt approaches.', 'although the syntax - based translation model in  #TAUTHOR_TAG falls']","['', 'although the syntax - based translation model in  #TAUTHOR_TAG falls under the string - to - tree category, i wonder why hierarchical phrasebased smt, or hiero ( chiang 2007 ), is not explicitly put under the string - to - string category, since hiero also uses "" unlabeled hierarchical phrases where there is no representation of linguistic categories. ""', 'chapter 2 focuses on how the statistical framework of a syntax - based smt approach learns its model from a word - aligned and parsed parallel text.', 'the first section explains how phrase pairs are extracted as translation rules from a word - aligned sentence pair in phrase - based smt ( koehn, och, and marcu 2003 ), highlighting the definition of a phrase as a sequence of words and the alignment - consistency property of a phrase pair as defined in  #AUTHOR_TAG.', 'the remainder of the chapter introduces three predominant instantiations of syntax - based models : hierarchical phrase - based smt ( hiero ) ( chiang 2007 ), which is a non - labeled syntax - based smt approach arising from the phrase - based approach ; syntax - augmented machine translation ( samt ), which introduces the notion of soft labels while keeping the nonlinguistic phrase notion ; and ghkm  #TAUTHOR_TAG, which only extracts translation rules consistent with constituency parse subtrees.', 'this chapter is nicely organized and it is easy to follow the gradual evolution from phrase - based smt to ghkm.', 'chapter 3 introduces the decoding formalism in the form of a directed hypergraph, defined as a set of vertices and a set of directed hyperedges.', 'the first section introduces the notion of a weighted parse forest represented in a weighted hypergraph, representing alternative parse trees of a sentence.', '']",0
['relative to the standard  #TAUTHOR_TAG algorithm'],['relative to the standard  #TAUTHOR_TAG algorithm'],"['filler word usage.', 'cumulatively up to 20 % error reduction is achieved relative to the standard  #TAUTHOR_TAG algorithm']","['paper presents and evaluates several original techniques for the latent classification of biographic attributes such as gender, age and native language, in diverse genres ( conversation transcripts, email ) and languages ( arabic, english ).', 'first, we present a novel partner - sensitive model for extracting biographic attributes in conversations, given the differences in lexical usage and discourse style such as observed between same - gender and mixedgender conversations.', 'then, we explore a rich variety of novel sociolinguistic and discourse - based features, including mean utterance length, passive / active usage, percentage domination of the conversation, speaking rate and filler word usage.', 'cumulatively up to 20 % error reduction is achieved relative to the standard  #TAUTHOR_TAG algorithm for classifying individual conversations on switchboard, and accuracy for gender detection on the switchboard corpus ( aggregate ) and gulf arabic corpus exceeds 95 %']",4
"['respectively. as noted before, the standard reference', 'algorithm is  #TAUTHOR_TAG,']","['respectively. as noted before, the standard reference', 'algorithm is  #TAUTHOR_TAG,']","['respectively. as noted before, the standard reference', 'algorithm is  #TAUTHOR_TAG,']","['"" mhm "", "" um "", "" uh - huh "", "" uh "", "" hm "", "" hmm', '"", etc. 9. type - token ratio 10. mean inter - utterance time : avg. time taken between utterances of the same speaker. 11. % of "" yeah "" occurences. 12. % of wh - question words. 13. % mean word and utterance length. the above classes resulted in', 'a total of 16 sociolinguistic features which were added based on feature ablation studies as features in the meta svm classifier along with the 4 features as explained', 'previously in section 5. 3. the rows in table 4 labeled "" + ( any sociolinguistic feature ) "" show the performance gain using the respective features described', 'in this', 'section. each row indicates an additive effect in the feature ablation, showing the result of adding the current sociolinguistic feature with the set of features mentioned', 'in the rows above. table 4 combines the results of the experiments reported in the previous sections, assessed on both the', 'fisher and switchboard corpora for gender classification. the evaluation measure was the standard classifier accuracy, that is, the fraction of test conversation sides whose gender was correctly predicted. baseline performance ( always guessing', 'female ) yields 57. 47 % and 51. 6 % on fisher and switchboard respectively. as noted before, the standard reference', 'algorithm is  #TAUTHOR_TAG, and all cited relative error reductions are based on this established standard, as implemented in this paper. also, as a second reference, performance is also cited for the popular "" gender genie "",', 'an online gender - detector 7, based on the manually weighted word', '- level sociolinguistic features discussed in  #AUTHOR_TAG. the additional', 'table rows are described in sections 4 - 6, and cumulatively yield substantial improvements over the  #TAUTHOR_TAG, all of the above models can be easily extended to per - speaker evaluation by pooling in the predictions from', 'multiple conversations', 'of the same speaker. table', '5 shows the result of each model on a per - speaker basis using a majority vote', 'of the predictions made on the individual conversations of the respective', 'speaker. the consensus model when applied to switch', '##board corpus show larger gains as it has 9', '']",4
"['in younger speakers.', 'to give maximal consistency / benefit to the  #TAUTHOR_TAG n - gram - based model, we did not filter the self - reporting n - grams']","['in younger speakers.', 'to give maximal consistency / benefit to the  #TAUTHOR_TAG n - gram - based model, we did not filter the self - reporting n - grams']","['in younger speakers.', 'to give maximal consistency / benefit to the  #TAUTHOR_TAG n - gram - based model, we did not filter the self - reporting n - grams']","['primary motivation for using only the speaker transcripts as compared to also using acoustic properties of the speaker  #AUTHOR_TAG was to enable the application of the models to other new genres.', 'in order to empirically support this motivation, we also tested the performance of the models explored in this paper on the enron email corpus  #AUTHOR_TAG.', ""we manually annotated the sender's gender on a random collection of emails taken from the corpus."", ""the resulting training and test sets after preprocessing for header information, reply - to's, forwarded messages consisted of 1579 and 204 emails respectively."", 'in addition to ngram features, a subset of sociolinguistic features that could be extracted for email were also utilized.', 'based on the prior distribution, always guessing the most likely class ( "" male "" ) resulted in 63. 2 % accuracy.', 'we can see from table 7 that the  #AUTHOR_TAG model acc.', 'error reduc.', 'gulf arabic ( 52. 5 % sides are male ) ngram ( boulis & ostendorf, 05 ) results for age and native / non - native : based on the prior distribution, always guessing the most likely class for age ( age less - than - orequal - to 40 ) results in 62. 59 % accuracy and always guessing the most likely class for native language ( non - native ) yields 50. 59 % accuracy.', 'table 9 shows the results for age and native / nonnative speaker status.', 'we can see that the ngrambased approach for gender also gives reasonable performance on other speaker attributes, and more importantly, both the partner - model and sociolinguistic features help in reducing the error rate on age and native language substantially, indicating their usefulness not just on gender but also on other diverse latent attributes.', 'table 8 shows the most discriminative ngrams for binary classification of age, it is interesting to see the use of "" well "" right on top of the list for older speakers, also found in the sociolinguistic studies for age  #AUTHOR_TAG.', 'we also see that older speakers talk about their children ( "" my daughter "" ) and younger speakers talk about their parents ( "" my mom "" ), the use of words such as "" wow "", "" kinda "" and "" cool "" is also common in younger speakers.', 'to give maximal consistency / benefit to the  #TAUTHOR_TAG n - gram - based model, we did not filter the self - reporting n - grams such as "" im forty "" and "" im thirty "", putting our sociolinguisticliterature - based and discourse - style - based features at a']",4
"['relative to the standard  #TAUTHOR_TAG algorithm for classifying individual conversations on switchboard, and']","['relative to the standard  #TAUTHOR_TAG algorithm for classifying individual conversations on switchboard, and']","['genres.', 'cumulatively up to 20 % error reduction is achieved relative to the standard  #TAUTHOR_TAG algorithm for classifying individual conversations on switchboard, and']","['paper has presented and evaluated several original techniques for the latent classification of speaker gender, age and native language in diverse genres and languages.', 'a novel partner - sensitve model shows performance gains from the joint modeling of speaker attributes along with partner speaker attributes, given the differences in lexical usage and discourse style such as observed between same - gender and mixed - gender conversations.', 'the robustness of the partner - model is substantially supported based on the consistent performance gains achieved in diverse languages and attributes.', 'this paper has also explored a rich variety of novel sociolinguistic and discourse - based features, including mean utterance length, passive / active usage, percentage domination of the conversation, speaking rate and filler word usage.', 'in addition to these novel models, the paper also shows how these models and the previous work extend to new languages and genres.', 'cumulatively up to 20 % error reduction is achieved relative to the standard  #TAUTHOR_TAG algorithm for classifying individual conversations on switchboard, and accuracy for gender detection on the switchboard corpus ( aggregate ) and gulf arabic exceeds 95 %']",4
['of  #TAUTHOR_TAG and show how gender and other attributes'],['of  #TAUTHOR_TAG and show how gender and other attributes'],['of  #TAUTHOR_TAG and show how gender and other attributes can be accurately predicted based on the following original contributions :'],"[""/ etc. of the speaker's interlocutor, and hence improvements may be achived via dyadic modeling or stacked classifiers. there has been substantial work in the sociolinguistics literature investigating discourse style differences due to speaker properties such as gender  #AUTHOR_TAG eckert, mcconnell -  #AUTHOR_TAG. analyzing such differences is not only"", 'interesting from the sociolinguistic and psycholinguistic point of view of language understanding, but also from an engineering perspective, given', 'the goal of predicting latent author / speaker attributes in various practical applications such as user authenticaion, call routing, user and population profiling on social networking websites such as facebook, and gender / age conditioned language models for', 'machine translation and speech recogntition. while most of the prior work in sociolinguistics has been approached from a', 'non - computational perspective,  #AUTHOR_TAG employed the use of a linear model for gender classification with manually assigned weights for a set of', 'linguistically interesting words as features, focusing on a small development corpus. another computational study for gender classification using approximately 30 weblog entries was done by  #AUTHOR_TAG, making use of', 'a logistic regression model to study the effect of different features. while small - scale sociolinguistic studies on monologues have shed some light on important features, we focus on modeling attributes from spoken conversations, building upon the work of  #TAUTHOR_TAG and show how gender and other attributes can be accurately predicted based on the following original contributions : 1. modeling partner effect : a speaker may adapt his or her conversation style depending', 'on the partner and we show how conditioning on the predicted partner class using a stacked model can provide further performance gains in gender classification. 2. sociolinguistic features : the paper explores a rich set of lexical and non - lexical features motivated by the sociolinguistic', 'literature for gender classification, and show how they can effectively augment the standard ngrambased model of  #TAUTHOR_TAG. 3. application to arabic language : we also report results', 'for arabic language and show that the ngram model gives reasonably high accuracy for arabic as well. furthmore, we also get consistent performance gains due to partner effect and sociolingusic features, as observed in english. 4. application to email genre : we show', 'how the models explored in this paper extend to email genre, showing the wide applicability of general text - based features. 5. application to new attributes : we show how the lexical model of  #TAUTHOR_TAG can be extended to', 'age and', 'native vs. non - native prediction, with further improvements gained from our partner - sensitive models and novel sociolinguistic features']",6
['of  #TAUTHOR_TAG and show how gender and other attributes'],['of  #TAUTHOR_TAG and show how gender and other attributes'],['of  #TAUTHOR_TAG and show how gender and other attributes can be accurately predicted based on the following original contributions :'],"[""/ etc. of the speaker's interlocutor, and hence improvements may be achived via dyadic modeling or stacked classifiers. there has been substantial work in the sociolinguistics literature investigating discourse style differences due to speaker properties such as gender  #AUTHOR_TAG eckert, mcconnell -  #AUTHOR_TAG. analyzing such differences is not only"", 'interesting from the sociolinguistic and psycholinguistic point of view of language understanding, but also from an engineering perspective, given', 'the goal of predicting latent author / speaker attributes in various practical applications such as user authenticaion, call routing, user and population profiling on social networking websites such as facebook, and gender / age conditioned language models for', 'machine translation and speech recogntition. while most of the prior work in sociolinguistics has been approached from a', 'non - computational perspective,  #AUTHOR_TAG employed the use of a linear model for gender classification with manually assigned weights for a set of', 'linguistically interesting words as features, focusing on a small development corpus. another computational study for gender classification using approximately 30 weblog entries was done by  #AUTHOR_TAG, making use of', 'a logistic regression model to study the effect of different features. while small - scale sociolinguistic studies on monologues have shed some light on important features, we focus on modeling attributes from spoken conversations, building upon the work of  #TAUTHOR_TAG and show how gender and other attributes can be accurately predicted based on the following original contributions : 1. modeling partner effect : a speaker may adapt his or her conversation style depending', 'on the partner and we show how conditioning on the predicted partner class using a stacked model can provide further performance gains in gender classification. 2. sociolinguistic features : the paper explores a rich set of lexical and non - lexical features motivated by the sociolinguistic', 'literature for gender classification, and show how they can effectively augment the standard ngrambased model of  #TAUTHOR_TAG. 3. application to arabic language : we also report results', 'for arabic language and show that the ngram model gives reasonably high accuracy for arabic as well. furthmore, we also get consistent performance gains due to partner effect and sociolingusic features, as observed in english. 4. application to email genre : we show', 'how the models explored in this paper extend to email genre, showing the wide applicability of general text - based features. 5. application to new attributes : we show how the lexical model of  #TAUTHOR_TAG can be extended to', 'age and', 'native vs. non - native prediction, with further improvements gained from our partner - sensitive models and novel sociolinguistic features']",6
['of  #TAUTHOR_TAG and show how gender and other attributes'],['of  #TAUTHOR_TAG and show how gender and other attributes'],['of  #TAUTHOR_TAG and show how gender and other attributes can be accurately predicted based on the following original contributions :'],"[""/ etc. of the speaker's interlocutor, and hence improvements may be achived via dyadic modeling or stacked classifiers. there has been substantial work in the sociolinguistics literature investigating discourse style differences due to speaker properties such as gender  #AUTHOR_TAG eckert, mcconnell -  #AUTHOR_TAG. analyzing such differences is not only"", 'interesting from the sociolinguistic and psycholinguistic point of view of language understanding, but also from an engineering perspective, given', 'the goal of predicting latent author / speaker attributes in various practical applications such as user authenticaion, call routing, user and population profiling on social networking websites such as facebook, and gender / age conditioned language models for', 'machine translation and speech recogntition. while most of the prior work in sociolinguistics has been approached from a', 'non - computational perspective,  #AUTHOR_TAG employed the use of a linear model for gender classification with manually assigned weights for a set of', 'linguistically interesting words as features, focusing on a small development corpus. another computational study for gender classification using approximately 30 weblog entries was done by  #AUTHOR_TAG, making use of', 'a logistic regression model to study the effect of different features. while small - scale sociolinguistic studies on monologues have shed some light on important features, we focus on modeling attributes from spoken conversations, building upon the work of  #TAUTHOR_TAG and show how gender and other attributes can be accurately predicted based on the following original contributions : 1. modeling partner effect : a speaker may adapt his or her conversation style depending', 'on the partner and we show how conditioning on the predicted partner class using a stacked model can provide further performance gains in gender classification. 2. sociolinguistic features : the paper explores a rich set of lexical and non - lexical features motivated by the sociolinguistic', 'literature for gender classification, and show how they can effectively augment the standard ngrambased model of  #TAUTHOR_TAG. 3. application to arabic language : we also report results', 'for arabic language and show that the ngram model gives reasonably high accuracy for arabic as well. furthmore, we also get consistent performance gains due to partner effect and sociolingusic features, as observed in english. 4. application to email genre : we show', 'how the models explored in this paper extend to email genre, showing the wide applicability of general text - based features. 5. application to new attributes : we show how the lexical model of  #TAUTHOR_TAG can be extended to', 'age and', 'native vs. non - native prediction, with further improvements gained from our partner - sensitive models and novel sociolinguistic features']",6
"['respectively. as noted before, the standard reference', 'algorithm is  #TAUTHOR_TAG,']","['respectively. as noted before, the standard reference', 'algorithm is  #TAUTHOR_TAG,']","['respectively. as noted before, the standard reference', 'algorithm is  #TAUTHOR_TAG,']","['"" mhm "", "" um "", "" uh - huh "", "" uh "", "" hm "", "" hmm', '"", etc. 9. type - token ratio 10. mean inter - utterance time : avg. time taken between utterances of the same speaker. 11. % of "" yeah "" occurences. 12. % of wh - question words. 13. % mean word and utterance length. the above classes resulted in', 'a total of 16 sociolinguistic features which were added based on feature ablation studies as features in the meta svm classifier along with the 4 features as explained', 'previously in section 5. 3. the rows in table 4 labeled "" + ( any sociolinguistic feature ) "" show the performance gain using the respective features described', 'in this', 'section. each row indicates an additive effect in the feature ablation, showing the result of adding the current sociolinguistic feature with the set of features mentioned', 'in the rows above. table 4 combines the results of the experiments reported in the previous sections, assessed on both the', 'fisher and switchboard corpora for gender classification. the evaluation measure was the standard classifier accuracy, that is, the fraction of test conversation sides whose gender was correctly predicted. baseline performance ( always guessing', 'female ) yields 57. 47 % and 51. 6 % on fisher and switchboard respectively. as noted before, the standard reference', 'algorithm is  #TAUTHOR_TAG, and all cited relative error reductions are based on this established standard, as implemented in this paper. also, as a second reference, performance is also cited for the popular "" gender genie "",', 'an online gender - detector 7, based on the manually weighted word', '- level sociolinguistic features discussed in  #AUTHOR_TAG. the additional', 'table rows are described in sections 4 - 6, and cumulatively yield substantial improvements over the  #TAUTHOR_TAG, all of the above models can be easily extended to per - speaker evaluation by pooling in the predictions from', 'multiple conversations', 'of the same speaker. table', '5 shows the result of each model on a per - speaker basis using a majority vote', 'of the predictions made on the individual conversations of the respective', 'speaker. the consensus model when applied to switch', '##board corpus show larger gains as it has 9', '']",6
"['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabular']","['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabularies is extended', 'to include sub - words.']","['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabular']","['tools from a raw character corpus. the accuracy of word segmentation tools strongly depends on the richness', 'of dictionaries, and segmentation errors influence the performances or accuracies of subsequent processes  #AUTHOR_TAG', '. for example, poor dictionaries are undesirable when dealing', 'with sns data containing a vast amount of neologisms. however, rich dictionaries are not always available, and', 'maintaining them up - to - date to cover neologisms is also expensive. another problem of word embedding with explicit tokenization step is', 'the existence of out - of - vocabulary ( oov ) words.', 'due to tokenization error ( or wrong segmentation ), we may lose some words in training data.', 'in addition, newly given real - world datasets may include a lot of unseen words and phrases. practically, oov words in a', 'corpus are replaced with a special token representing oov. the larger oov rate', 'in a corpus affects the accuracies of downstream tasks  #AUTHOR_TAG. in recent years, an increasing number of studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabularies is extended', 'to include sub - words. by enriching the information of the word, sub - words are useful for capturing morphological changes  #TAUTHOR_TAG and the', '']",0
"['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabular']","['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabularies is extended', 'to include sub - words.']","['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabular']","['tools from a raw character corpus. the accuracy of word segmentation tools strongly depends on the richness', 'of dictionaries, and segmentation errors influence the performances or accuracies of subsequent processes  #AUTHOR_TAG', '. for example, poor dictionaries are undesirable when dealing', 'with sns data containing a vast amount of neologisms. however, rich dictionaries are not always available, and', 'maintaining them up - to - date to cover neologisms is also expensive. another problem of word embedding with explicit tokenization step is', 'the existence of out - of - vocabulary ( oov ) words.', 'due to tokenization error ( or wrong segmentation ), we may lose some words in training data.', 'in addition, newly given real - world datasets may include a lot of unseen words and phrases. practically, oov words in a', 'corpus are replaced with a special token representing oov. the larger oov rate', 'in a corpus affects the accuracies of downstream tasks  #AUTHOR_TAG. in recent years, an increasing number of studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabularies is extended', 'to include sub - words. by enriching the information of the word, sub - words are useful for capturing morphological changes  #TAUTHOR_TAG and the', '']",0
"['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabular']","['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabularies is extended', 'to include sub - words.']","['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabular']","['tools from a raw character corpus. the accuracy of word segmentation tools strongly depends on the richness', 'of dictionaries, and segmentation errors influence the performances or accuracies of subsequent processes  #AUTHOR_TAG', '. for example, poor dictionaries are undesirable when dealing', 'with sns data containing a vast amount of neologisms. however, rich dictionaries are not always available, and', 'maintaining them up - to - date to cover neologisms is also expensive. another problem of word embedding with explicit tokenization step is', 'the existence of out - of - vocabulary ( oov ) words.', 'due to tokenization error ( or wrong segmentation ), we may lose some words in training data.', 'in addition, newly given real - world datasets may include a lot of unseen words and phrases. practically, oov words in a', 'corpus are replaced with a special token representing oov. the larger oov rate', 'in a corpus affects the accuracies of downstream tasks  #AUTHOR_TAG. in recent years, an increasing number of studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabularies is extended', 'to include sub - words. by enriching the information of the word, sub - words are useful for capturing morphological changes  #TAUTHOR_TAG and the', '']",0
"['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabular']","['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabularies is extended', 'to include sub - words.']","['studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabular']","['tools from a raw character corpus. the accuracy of word segmentation tools strongly depends on the richness', 'of dictionaries, and segmentation errors influence the performances or accuracies of subsequent processes  #AUTHOR_TAG', '. for example, poor dictionaries are undesirable when dealing', 'with sns data containing a vast amount of neologisms. however, rich dictionaries are not always available, and', 'maintaining them up - to - date to cover neologisms is also expensive. another problem of word embedding with explicit tokenization step is', 'the existence of out - of - vocabulary ( oov ) words.', 'due to tokenization error ( or wrong segmentation ), we may lose some words in training data.', 'in addition, newly given real - world datasets may include a lot of unseen words and phrases. practically, oov words in a', 'corpus are replaced with a special token representing oov. the larger oov rate', 'in a corpus affects the accuracies of downstream tasks  #AUTHOR_TAG. in recent years, an increasing number of studies have investigated character - level models with sub', '##words in both unsupervised  #TAUTHOR_TAG and supervised learning  #AUTHOR_TAG. in these models, the notion of vocabularies is extended', 'to include sub - words. by enriching the information of the word, sub - words are useful for capturing morphological changes  #TAUTHOR_TAG and the', '']",3
"['of  #TAUTHOR_TAG.', '']","['of  #TAUTHOR_TAG.', '']","['of  #TAUTHOR_TAG.', 'two widely - used benchmark datasets are used : chinese word similarity dataset  #AUTHOR_TAG,']","['similarity task : our model and the baselines are trained on portions of wikipedia of increasing size to see the effect of the size of the training data.', 'for pairs of words, the cosine similarity between embeddings is compared to human judgment, and the quality is measured by spearman rank correlation.', 'most of the settings are the same as that of  #TAUTHOR_TAG.', 'two widely - used benchmark datasets are used : chinese word similarity dataset  #AUTHOR_TAG, which contains 297 pairs of words, and japanese word similarity dataset  #AUTHOR_TAG, which contains 4429 pairs of words.', 'conventional word embedding methods, c - bow, skip - gram, and sembei, cannot provide the embeddings of oov words in the test data.', 'in contrast, sisg and our model can compute representations for almost all words, since both methods learn compositional n - gram features.', 'in order to show comparable results, we use the null vector for these oov words following  #TAUTHOR_TAG.', 'noun category prediction task : we use 100mb of sns data, sina weibo for chinese and twitter for japanese and korean, as training corpora.', 'for evaluating the learned embeddings, noun words, including neologisms, and their categories are extracted from wikidata with the predetermined semantic category set 2.', '']",3
"['of  #TAUTHOR_TAG.', '']","['of  #TAUTHOR_TAG.', '']","['of  #TAUTHOR_TAG.', 'two widely - used benchmark datasets are used : chinese word similarity dataset  #AUTHOR_TAG,']","['similarity task : our model and the baselines are trained on portions of wikipedia of increasing size to see the effect of the size of the training data.', 'for pairs of words, the cosine similarity between embeddings is compared to human judgment, and the quality is measured by spearman rank correlation.', 'most of the settings are the same as that of  #TAUTHOR_TAG.', 'two widely - used benchmark datasets are used : chinese word similarity dataset  #AUTHOR_TAG, which contains 297 pairs of words, and japanese word similarity dataset  #AUTHOR_TAG, which contains 4429 pairs of words.', 'conventional word embedding methods, c - bow, skip - gram, and sembei, cannot provide the embeddings of oov words in the test data.', 'in contrast, sisg and our model can compute representations for almost all words, since both methods learn compositional n - gram features.', 'in order to show comparable results, we use the null vector for these oov words following  #TAUTHOR_TAG.', 'noun category prediction task : we use 100mb of sns data, sina weibo for chinese and twitter for japanese and korean, as training corpora.', 'for evaluating the learned embeddings, noun words, including neologisms, and their categories are extracted from wikidata with the predetermined semantic category set 2.', '']",3
['models  #TAUTHOR_TAG as well as by the segmentation - free character n - gram embedding for'],"['method scne is inspired by recent successful sub - word models  #TAUTHOR_TAG as well as by the segmentation - free character n - gram embedding for unsegmented languages  #AUTHOR_TAG.', 'vector representation']",['method scne is inspired by recent successful sub - word models  #TAUTHOR_TAG as well as by the segmentation - free character n - gram embedding for'],"['method scne is inspired by recent successful sub - word models  #TAUTHOR_TAG as well as by the segmentation - free character n - gram embedding for unsegmented languages  #AUTHOR_TAG.', 'vector representation of target n - gram is defined as follows.', 'let x 1 x 2 · · · x n be a raw unsegmented corpus of n characters.', 'for a range', 'we first count occurrences of n - gram s = s 1 s 2 · · · s n in the raw corpus as x t = s with length n = j−i + 1 ≤ n max.', '']",7
[')  #TAUTHOR_TAG'],"['use c - bow, skipgram  #AUTHOR_TAG, subword information skip - gram ( sisg )  #TAUTHOR_TAG']",[')  #TAUTHOR_TAG'],"['baseline systems, we use c - bow, skipgram  #AUTHOR_TAG, subword information skip - gram ( sisg )  #TAUTHOR_TAG and segmentation - free word embedding for unsegmented languages ( sembei )  #AUTHOR_TAG for the word - level tasks.', 'for the sentence - level task, baselines are pv - dbow, pv - dm  #AUTHOR_TAG and sent2vec  #AUTHOR_TAG.', 'in addition, we test sentence embedding baselines obtained by simple averaging of word embeddings over the sentence, denoted as c - bow *, skip - gram * and sisg *.', 'we also test a variant of sembei, denoted by sembei *, which calculates word or sentence embeddings by simple averaging of sub - n - gram embeddings, to see whether our model provides more effective compositional n - gram embeddings compared to the previously proposed non - compositional model']",5
"['of  #TAUTHOR_TAG.', '']","['of  #TAUTHOR_TAG.', '']","['of  #TAUTHOR_TAG.', 'two widely - used benchmark datasets are used : chinese word similarity dataset  #AUTHOR_TAG,']","['similarity task : our model and the baselines are trained on portions of wikipedia of increasing size to see the effect of the size of the training data.', 'for pairs of words, the cosine similarity between embeddings is compared to human judgment, and the quality is measured by spearman rank correlation.', 'most of the settings are the same as that of  #TAUTHOR_TAG.', 'two widely - used benchmark datasets are used : chinese word similarity dataset  #AUTHOR_TAG, which contains 297 pairs of words, and japanese word similarity dataset  #AUTHOR_TAG, which contains 4429 pairs of words.', 'conventional word embedding methods, c - bow, skip - gram, and sembei, cannot provide the embeddings of oov words in the test data.', 'in contrast, sisg and our model can compute representations for almost all words, since both methods learn compositional n - gram features.', 'in order to show comparable results, we use the null vector for these oov words following  #TAUTHOR_TAG.', 'noun category prediction task : we use 100mb of sns data, sina weibo for chinese and twitter for japanese and korean, as training corpora.', 'for evaluating the learned embeddings, noun words, including neologisms, and their categories are extracted from wikidata with the predetermined semantic category set 2.', '']",5
"['of  #TAUTHOR_TAG.', '']","['of  #TAUTHOR_TAG.', '']","['of  #TAUTHOR_TAG.', 'two widely - used benchmark datasets are used : chinese word similarity dataset  #AUTHOR_TAG,']","['similarity task : our model and the baselines are trained on portions of wikipedia of increasing size to see the effect of the size of the training data.', 'for pairs of words, the cosine similarity between embeddings is compared to human judgment, and the quality is measured by spearman rank correlation.', 'most of the settings are the same as that of  #TAUTHOR_TAG.', 'two widely - used benchmark datasets are used : chinese word similarity dataset  #AUTHOR_TAG, which contains 297 pairs of words, and japanese word similarity dataset  #AUTHOR_TAG, which contains 4429 pairs of words.', 'conventional word embedding methods, c - bow, skip - gram, and sembei, cannot provide the embeddings of oov words in the test data.', 'in contrast, sisg and our model can compute representations for almost all words, since both methods learn compositional n - gram features.', 'in order to show comparable results, we use the null vector for these oov words following  #TAUTHOR_TAG.', 'noun category prediction task : we use 100mb of sns data, sina weibo for chinese and twitter for japanese and korean, as training corpora.', 'for evaluating the learned embeddings, noun words, including neologisms, and their categories are extracted from wikidata with the predetermined semantic category set 2.', '']",5
"['in subwords according to the bpe  #TAUTHOR_TAG approach.', 'the models were trained with multi - domain']","['in subwords according to the bpe  #TAUTHOR_TAG approach.', 'the models were trained with multi - domain']","['to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #TAUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #AUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the']","['iadaatpa 1 project coded as n • 2016 - eu - ia - 0132 that ended in february 2019 is made for building of customized, domain - specific engines for public administrations from eu member states.', 'the consortium of the project decided to use neural machine translation at the beginning of the project.', 'this represented a challenge for all involved, and the positive aspect is that all public administrations engaged in the iadaatpa project were able to try, test and use state - of - the - art neural technology with a high level of satisfaction.', 'one of the main challenges faced by all partners was data availability.', 'although all public administrations had some data available, it was clearly insufficient for high - level customization.', 'in some cases, we had merely a few hundred words or several tens of thousand words.', 'each domain ( field ) has its own unique word distribution and neural machine translation systems are known to suffer a decrease in performance when data is out - of - domain.', 'pangeanic is a language service provider ( lsp ) specialised in natural language processing and machine translation.', 'it provides solutions to cognitive companies, institutions, translation professionals, and corporations.', 'the problem faced by the iadaatpa project at pangeanic was twofold : data acquisition for translation from spanish to russian there was no available in - domain data.', 'therefore, 2 translators were contracted as part of the project to create 30, 000 segments of in - domain data, translating public administrations websites.', 'they also cleaned united nations material and post - edited general - domain data that was previously filtered as indomain following the "" invitation model ""  #AUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', ""the main part of the training corpora ( approximately 75 % ) was part of pangeanic's own repository harvested through web crawling and also opensubtitles  #AUTHOR_TAG."", 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG engine customization the data was cleaned using the bicleaner tool ( sanchez -  #AUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in order to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #TAUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #AUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the']",5
"['in subwords according to the bpe  #TAUTHOR_TAG approach.', 'the models were trained with multi - domain']","['in subwords according to the bpe  #TAUTHOR_TAG approach.', 'the models were trained with multi - domain']","['to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #TAUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #AUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the source as the extra token was added at target - side and there was no need for apriori domain information.', 'this approach allowed the model to improve the quality for each domain']","['of the main challenges faced by all partners was data availability.', 'although all public administrations had some data available, it was clearly insufficient for high - level customization.', 'in some cases, we had merely a few hundred words or several tens of thousand words.', 'each domain ( field ) has its own unique word distribution and neural machine translation systems are known to suffer a decrease in performance when data is out - of - domain.', 'pangeanic is a language service provider ( lsp ) specialised in natural language processing and machine translation.', 'it provides solutions to cognitive companies, institutions, translation professionals, and corporations.', 'the problem faced by the iadaatpa project at pangeanic was twofold : data acquisition for translation from spanish to russian there was no available in - domain data.', 'therefore, 2 translators were contracted as part of the project to create 30, 000 segments of in - domain data, translating public administrations websites.', 'they also cleaned united nations material and post - edited general - domain data that was previously filtered as indomain following the "" invitation model ""  #AUTHOR_TAG.', 'for the other language pairs, the input material was 30, 000 post - edited segments.', ""the main part of the training corpora ( approximately 75 % ) was part of pangeanic's own repository harvested through web crawling and also opensubtitles  #AUTHOR_TAG."", 'the rest of the corpus was automatically validated synthetic material using general data from leipzig  #AUTHOR_TAG.', 'engine customization the data was cleaned using the bicleaner tool ( sanchez -  #AUTHOR_TAG.', 'the data was lowercased and extra embeddings were added in order to keep the case information.', 'the tokenization used was the one provided by opennmt 3 and words were divided in subwords according to the bpe  #TAUTHOR_TAG approach.', 'the models were trained with multi - domain data and we improved performance following a domainmixing approach  #AUTHOR_TAG.', 'the domain information was prepended with special tokens for each target sequence.', 'the domain prediction was based only on the source as the extra token was added at target - side and there was no need for apriori domain information.', 'this approach allowed the model to improve the quality for each domain']",5
"['15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved']","['##s ) [ 15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved recognition']","['15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved']","['', 'acoustic conditions. advanced variants of dnns, such as convolutional neural nets ( cnns ) [ 12 ], recurrent neural nets ( rnns ) [ 13 ], long short - term memory nets ( ls', '##tms ) [ 14 ], time - delay neural nets ( tdnns ) [ 15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved recognition performance, bringing them closer to human performance [ 9 ]. both abundance of data and sophistication of deep learning algorithms have symbiotically contributed', 'to the advancement of speech recognition performance. the role of acoustic features has not been explored in comparable detail, and their potential contribution to performance gains is unknown. this paper focuses on acoustic features and investigates how their selection improves recognition performance using benchmark training datasets : switchboard and fisher, when evaluated on the nist 2000 cts test set [ 2 ]. we investigated a traditional cnn model and explored the following : ( 1 ) use of multiple features both in isolation and in combination', '. our experiments demonstrated that the use of feature combinations helped to improve performance over individual features in isolation and over traditionally used mel - filterbank ( mfb ) features. articulatory features were found to be useful for improving recognition performance on both switchboard and callhome subsets of the nist 2000 cts test set. these findings indicate that the use of better acoustic features can help improve speech recognition performance when using', 'standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory.', ""for the sake of simplicity, we used a cnn acoustic model in our experiment, where the baseline system's performance"", 'is directly comparable to the state - of - the - art cnn performance reported in  #TAUTHOR_TAG. we expect our results using the cnn to carry over into other neural network architectures as well. the', 'outline of the paper is as follows. in section 2 we present the dataset and the recognition task. in section 3 we describe', 'the acoustic features and the articulatory features that were used in our experiments. section 4 presents the acoustic and language models used in our experiments, followed by experimental results', 'in section 5 and conclusion and future directions in section 6']",0
"['15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved']","['##s ) [ 15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved recognition']","['15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved']","['', 'acoustic conditions. advanced variants of dnns, such as convolutional neural nets ( cnns ) [ 12 ], recurrent neural nets ( rnns ) [ 13 ], long short - term memory nets ( ls', '##tms ) [ 14 ], time - delay neural nets ( tdnns ) [ 15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved recognition performance, bringing them closer to human performance [ 9 ]. both abundance of data and sophistication of deep learning algorithms have symbiotically contributed', 'to the advancement of speech recognition performance. the role of acoustic features has not been explored in comparable detail, and their potential contribution to performance gains is unknown. this paper focuses on acoustic features and investigates how their selection improves recognition performance using benchmark training datasets : switchboard and fisher, when evaluated on the nist 2000 cts test set [ 2 ]. we investigated a traditional cnn model and explored the following : ( 1 ) use of multiple features both in isolation and in combination', '. our experiments demonstrated that the use of feature combinations helped to improve performance over individual features in isolation and over traditionally used mel - filterbank ( mfb ) features. articulatory features were found to be useful for improving recognition performance on both switchboard and callhome subsets of the nist 2000 cts test set. these findings indicate that the use of better acoustic features can help improve speech recognition performance when using', 'standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory.', ""for the sake of simplicity, we used a cnn acoustic model in our experiment, where the baseline system's performance"", 'is directly comparable to the state - of - the - art cnn performance reported in  #TAUTHOR_TAG. we expect our results using the cnn to carry over into other neural network architectures as well. the', 'outline of the paper is as follows. in section 2 we present the dataset and the recognition task. in section 3 we describe', 'the acoustic features and the articulatory features that were used in our experiments. section 4 presents the acoustic and language models used in our experiments, followed by experimental results', 'in section 5 and conclusion and future directions in section 6']",0
"['swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG.']","['than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from']","['the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each']","['as a consequence the number of senone labels remained the same as the 360 - hour swb models. table 3 presents the', 'results from the 2000 hours cts trained models. the model configurations and their parameter size were kept the same as the 360 - hour swb models. figure 3 shows that the use of the additional fsh training data resulted in significant performance improvement', 'for both swb and the ch subsets of the nist 2000 cts test set. adding the fsh dataset resulted in relative wer reduction of 4. 4 % and 12 % respectively for swb and', 'ch subsets of the nist 2000 cts test set, using mfb + fmllr features. similar improvement was observed from the doc + fmllr features as well, where 8 % and 12 % relative reduction in wer for swb and ch subsets', 'was observed when fsh data was added to the training data. note that the ch subset of the nist', '2000 cts test set was more challenging than the swb subset, as it contains non - native', 'speakers of english, hence introducing accented speech into the evaluation set. the use of articulatory features helped to reduce the error rates for both swb and ch test sets, indicating their robustness to model spontaneous speech in both native ( swb ) and non - native ( ch ) speaking styles. the fsh corpus contains speech from quite a diverse set', 'of speakers, helping to reduce the wer of the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each individual system of different front - end features with fm', '##llr, i. e., mfb, doc, mfb + doc, mfb + doc', '+ tv, then conducting m - way combination of the subsystems using n - best rover [', '27 ] implemented in srilm [ 28 ]. in this system fusion experiment, all subsystems have equal weights for n - best rover. as can be seen from the table, n - best rover based 2 - way', 'and 3 - way system fusion produced a further 2 % and 4 % relative reduction in wer compared to the', 'best single system ( mfb + fmllr + doc + fmllr + tv ), for swb and ch evaluation sets respectively. note that the first row of table 4 is the last row of table 3, i. e', '., the best single system. the last row 4way fusion is from combining the 4 individual systems presented in table 3']",0
"['15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved']","['##s ) [ 15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved recognition']","['15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved']","['', 'acoustic conditions. advanced variants of dnns, such as convolutional neural nets ( cnns ) [ 12 ], recurrent neural nets ( rnns ) [ 13 ], long short - term memory nets ( ls', '##tms ) [ 14 ], time - delay neural nets ( tdnns ) [ 15, 29 ]', ',  #TAUTHOR_TAG, have significantly improved recognition performance, bringing them closer to human performance [ 9 ]. both abundance of data and sophistication of deep learning algorithms have symbiotically contributed', 'to the advancement of speech recognition performance. the role of acoustic features has not been explored in comparable detail, and their potential contribution to performance gains is unknown. this paper focuses on acoustic features and investigates how their selection improves recognition performance using benchmark training datasets : switchboard and fisher, when evaluated on the nist 2000 cts test set [ 2 ]. we investigated a traditional cnn model and explored the following : ( 1 ) use of multiple features both in isolation and in combination', '. our experiments demonstrated that the use of feature combinations helped to improve performance over individual features in isolation and over traditionally used mel - filterbank ( mfb ) features. articulatory features were found to be useful for improving recognition performance on both switchboard and callhome subsets of the nist 2000 cts test set. these findings indicate that the use of better acoustic features can help improve speech recognition performance when using', 'standard acoustic modeling techniques, and can demonstrate performance as good as those obtained from more sophisticated acoustic models that exploit temporal memory.', ""for the sake of simplicity, we used a cnn acoustic model in our experiment, where the baseline system's performance"", 'is directly comparable to the state - of - the - art cnn performance reported in  #TAUTHOR_TAG. we expect our results using the cnn to carry over into other neural network architectures as well. the', 'outline of the paper is as follows. in section 2 we present the dataset and the recognition task. in section 3 we describe', 'the acoustic features and the articulatory features that were used in our experiments. section 4 presents the acoustic and language models used in our experiments, followed by experimental results', 'in section 5 and conclusion and future directions in section 6']",5
"['swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG.']","['than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from']","['the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each']","['as a consequence the number of senone labels remained the same as the 360 - hour swb models. table 3 presents the', 'results from the 2000 hours cts trained models. the model configurations and their parameter size were kept the same as the 360 - hour swb models. figure 3 shows that the use of the additional fsh training data resulted in significant performance improvement', 'for both swb and the ch subsets of the nist 2000 cts test set. adding the fsh dataset resulted in relative wer reduction of 4. 4 % and 12 % respectively for swb and', 'ch subsets of the nist 2000 cts test set, using mfb + fmllr features. similar improvement was observed from the doc + fmllr features as well, where 8 % and 12 % relative reduction in wer for swb and ch subsets', 'was observed when fsh data was added to the training data. note that the ch subset of the nist', '2000 cts test set was more challenging than the swb subset, as it contains non - native', 'speakers of english, hence introducing accented speech into the evaluation set. the use of articulatory features helped to reduce the error rates for both swb and ch test sets, indicating their robustness to model spontaneous speech in both native ( swb ) and non - native ( ch ) speaking styles. the fsh corpus contains speech from quite a diverse set', 'of speakers, helping to reduce the wer of the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each individual system of different front - end features with fm', '##llr, i. e., mfb, doc, mfb + doc, mfb + doc', '+ tv, then conducting m - way combination of the subsystems using n - best rover [', '27 ] implemented in srilm [ 28 ]. in this system fusion experiment, all subsystems have equal weights for n - best rover. as can be seen from the table, n - best rover based 2 - way', 'and 3 - way system fusion produced a further 2 % and 4 % relative reduction in wer compared to the', 'best single system ( mfb + fmllr + doc + fmllr + tv ), for swb and ch evaluation sets respectively. note that the first row of table 4 is the last row of table 3, i. e', '., the best single system. the last row 4way fusion is from combining the 4 individual systems presented in table 3']",5
"['swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG.']","['than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from']","['the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each']","['as a consequence the number of senone labels remained the same as the 360 - hour swb models. table 3 presents the', 'results from the 2000 hours cts trained models. the model configurations and their parameter size were kept the same as the 360 - hour swb models. figure 3 shows that the use of the additional fsh training data resulted in significant performance improvement', 'for both swb and the ch subsets of the nist 2000 cts test set. adding the fsh dataset resulted in relative wer reduction of 4. 4 % and 12 % respectively for swb and', 'ch subsets of the nist 2000 cts test set, using mfb + fmllr features. similar improvement was observed from the doc + fmllr features as well, where 8 % and 12 % relative reduction in wer for swb and ch subsets', 'was observed when fsh data was added to the training data. note that the ch subset of the nist', '2000 cts test set was more challenging than the swb subset, as it contains non - native', 'speakers of english, hence introducing accented speech into the evaluation set. the use of articulatory features helped to reduce the error rates for both swb and ch test sets, indicating their robustness to model spontaneous speech in both native ( swb ) and non - native ( ch ) speaking styles. the fsh corpus contains speech from quite a diverse set', 'of speakers, helping to reduce the wer of the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each individual system of different front - end features with fm', '##llr, i. e., mfb, doc, mfb + doc, mfb + doc', '+ tv, then conducting m - way combination of the subsystems using n - best rover [', '27 ] implemented in srilm [ 28 ]. in this system fusion experiment, all subsystems have equal weights for n - best rover. as can be seen from the table, n - best rover based 2 - way', 'and 3 - way system fusion produced a further 2 % and 4 % relative reduction in wer compared to the', 'best single system ( mfb + fmllr + doc + fmllr + tv ), for swb and ch evaluation sets respectively. note that the first row of table 4 is the last row of table 3, i. e', '., the best single system. the last row 4way fusion is from combining the 4 individual systems presented in table 3']",5
"['swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG.']","['than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from']","['the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each']","['as a consequence the number of senone labels remained the same as the 360 - hour swb models. table 3 presents the', 'results from the 2000 hours cts trained models. the model configurations and their parameter size were kept the same as the 360 - hour swb models. figure 3 shows that the use of the additional fsh training data resulted in significant performance improvement', 'for both swb and the ch subsets of the nist 2000 cts test set. adding the fsh dataset resulted in relative wer reduction of 4. 4 % and 12 % respectively for swb and', 'ch subsets of the nist 2000 cts test set, using mfb + fmllr features. similar improvement was observed from the doc + fmllr features as well, where 8 % and 12 % relative reduction in wer for swb and ch subsets', 'was observed when fsh data was added to the training data. note that the ch subset of the nist', '2000 cts test set was more challenging than the swb subset, as it contains non - native', 'speakers of english, hence introducing accented speech into the evaluation set. the use of articulatory features helped to reduce the error rates for both swb and ch test sets, indicating their robustness to model spontaneous speech in both native ( swb ) and non - native ( ch ) speaking styles. the fsh corpus contains speech from quite a diverse set', 'of speakers, helping to reduce the wer of the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each individual system of different front - end features with fm', '##llr, i. e., mfb, doc, mfb + doc, mfb + doc', '+ tv, then conducting m - way combination of the subsystems using n - best rover [', '27 ] implemented in srilm [ 28 ]. in this system fusion experiment, all subsystems have equal weights for n - best rover. as can be seen from the table, n - best rover based 2 - way', 'and 3 - way system fusion produced a further 2 % and 4 % relative reduction in wer compared to the', 'best single system ( mfb + fmllr + doc + fmllr + tv ), for swb and ch evaluation sets respectively. note that the first row of table 4 is the last row of table 3, i. e', '., the best single system. the last row 4way fusion is from combining the 4 individual systems presented in table 3']",3
"['swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG.']","['than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from']","['the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each']","['as a consequence the number of senone labels remained the same as the 360 - hour swb models. table 3 presents the', 'results from the 2000 hours cts trained models. the model configurations and their parameter size were kept the same as the 360 - hour swb models. figure 3 shows that the use of the additional fsh training data resulted in significant performance improvement', 'for both swb and the ch subsets of the nist 2000 cts test set. adding the fsh dataset resulted in relative wer reduction of 4. 4 % and 12 % respectively for swb and', 'ch subsets of the nist 2000 cts test set, using mfb + fmllr features. similar improvement was observed from the doc + fmllr features as well, where 8 % and 12 % relative reduction in wer for swb and ch subsets', 'was observed when fsh data was added to the training data. note that the ch subset of the nist', '2000 cts test set was more challenging than the swb subset, as it contains non - native', 'speakers of english, hence introducing accented speech into the evaluation set. the use of articulatory features helped to reduce the error rates for both swb and ch test sets, indicating their robustness to model spontaneous speech in both native ( swb ) and non - native ( ch ) speaking styles. the fsh corpus contains speech from quite a diverse set', 'of speakers, helping to reduce the wer of the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each individual system of different front - end features with fm', '##llr, i. e., mfb, doc, mfb + doc, mfb + doc', '+ tv, then conducting m - way combination of the subsystems using n - best rover [', '27 ] implemented in srilm [ 28 ]. in this system fusion experiment, all subsystems have equal weights for n - best rover. as can be seen from the table, n - best rover based 2 - way', 'and 3 - way system fusion produced a further 2 % and 4 % relative reduction in wer compared to the', 'best single system ( mfb + fmllr + doc + fmllr + tv ), for swb and ch evaluation sets respectively. note that the first row of table 4 is the last row of table 3, i. e', '., the best single system. the last row 4way fusion is from combining the 4 individual systems presented in table 3']",3
"['swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG.']","['than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from']","['the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each']","['as a consequence the number of senone labels remained the same as the 360 - hour swb models. table 3 presents the', 'results from the 2000 hours cts trained models. the model configurations and their parameter size were kept the same as the 360 - hour swb models. figure 3 shows that the use of the additional fsh training data resulted in significant performance improvement', 'for both swb and the ch subsets of the nist 2000 cts test set. adding the fsh dataset resulted in relative wer reduction of 4. 4 % and 12 % respectively for swb and', 'ch subsets of the nist 2000 cts test set, using mfb + fmllr features. similar improvement was observed from the doc + fmllr features as well, where 8 % and 12 % relative reduction in wer for swb and ch subsets', 'was observed when fsh data was added to the training data. note that the ch subset of the nist', '2000 cts test set was more challenging than the swb subset, as it contains non - native', 'speakers of english, hence introducing accented speech into the evaluation set. the use of articulatory features helped to reduce the error rates for both swb and ch test sets, indicating their robustness to model spontaneous speech in both native ( swb ) and non - native ( ch ) speaking styles. the fsh corpus contains speech from quite a diverse set', 'of speakers, helping to reduce the wer of the ch subset more significantly than the swb subset, a trend reflected in results reported in the literature  #TAUTHOR_TAG. table 4 shows the system fusion results after dumping 2000 - best lists from the rescored lattices from each individual system of different front - end features with fm', '##llr, i. e., mfb, doc, mfb + doc, mfb + doc', '+ tv, then conducting m - way combination of the subsystems using n - best rover [', '27 ] implemented in srilm [ 28 ]. in this system fusion experiment, all subsystems have equal weights for n - best rover. as can be seen from the table, n - best rover based 2 - way', 'and 3 - way system fusion produced a further 2 % and 4 % relative reduction in wer compared to the', 'best single system ( mfb + fmllr + doc + fmllr + tv ), for swb and ch evaluation sets respectively. note that the first row of table 4 is the last row of table 3, i. e', '., the best single system. the last row 4way fusion is from combining the 4 individual systems presented in table 3']",3
"['##d neural nets ( tdnns ), long short - term memory neural nets ( lstms ), and the  #TAUTHOR_TAG,']","['investigate rnn or other neural network - based language modeling techniques that are known to perform better than word n - gram lms.', 'also, advanced acoustic modeling, through the use of timedelayed neural nets ( tdnns ), long short - term memory neural nets ( lstms ), and the  #TAUTHOR_TAG,']","['##d neural nets ( tdnns ), long short - term memory neural nets ( lstms ), and the  #TAUTHOR_TAG,']","['reported the results exploring multiple features for asr on english cts data.', 'we observed that the fmllr transform helped reduce the wer of the baseline system significantly.', 'we observed that using multiple acoustic features helped in improving the overall accuracy of the system.', 'use of robust features and articulatory features significantly reduced the wer for the more challenging callhome subset of the nist 2000 cts evaluation set, with accented speech in that subset.', 'we developed a fused - cnn - dnn architecture, where input convolution was only performed on the acoustic features and the articulatory features were process by a feed - forward layer.', 'we found this architecture effective for combining acoustic features and articulatory features.', 'the robust features and articulatory features capture complementary information, and the addition of them resulted in the best single system performance, with 12 % relative reduction of wer on swb and ch evaluation sets respectively, compared to the mfb + fmllr cnn baseline.', 'note that in this study the language model has not been optimized.', 'future studies should investigate rnn or other neural network - based language modeling techniques that are known to perform better than word n - gram lms.', 'also, advanced acoustic modeling, through the use of timedelayed neural nets ( tdnns ), long short - term memory neural nets ( lstms ), and the  #TAUTHOR_TAG, should also be explored as their performance has been mostly reported using mfb features, and the use of multi - view features can help further improve their performance']",2
[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research']",[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research direction. among different unsupervised word vector postprocessing schemes, the all - but - the - top approach  #TAUTHOR_TAG is a prominent example. empirically studying the latent features encoded by principal components (', 'pcs ) of distributional word vectors,  #TAUTHOR_TAG found that the variances explained by the leading pcs "" encode the frequency of the word to a significant degree "".', 'since word frequencies are arguably unrelated to lexical semantics, they recommend removing such leading pcs from word vectors', 'using a pca reconstruction. the current work advances the findings of  #TAUTHOR_TAG and improves their post - processing scheme. instead of discarding a fixed number of', 'pcs, we softly filter word vectors using matrix conceptors ( jaeger 2014 ; 2017 ), which characterize the linear space of those word vector features having high variances - the features most contaminated by word frequencies according to  #TAUTHOR_TAG. the proposed approach is mathematically simple and computationally efficient, as it is founded on elementary linear algebra. besides these traits, it is also practically effective : using a standard set of', 'lexical - level intrinsic evaluation tasks and a deep neural network - based dialogue state tracking task', ', we show that conceptor - based post - processing considerably enhances linguistic regularities captured by word vectors. a more detailed list of our contributions are : 1. we propose an unsupervised algorithm that leverages boolean operations of conceptors to post - process word vectors. the resulting word vectors achieve up to', '18. 86 % and 28. 34 % improvement on the simlex -', '999 and simverb - 3500 dataset relative to the original word representations. 2. a closer look at the', 'proposed algorithm reveals commonalities across several existing post - processing techniques for neural - based word vectors and pointwise mutual information ( pmi ) matrix based word vectors. unlike the existing alternatives, the proposed approach', 'is flexible enough to remove lexically - unrelated noise, while general - purpose enough to handle word vectors induced by different learning', 'algorithms. the rest of the paper is organized as follows. we first briefly review', 'the principal component nulling approach for unsupervised word vector post - processing introduced in  #TAUTHOR_TAG, upon which our work is based. we then introduce our proposed approach, conceptor negation ( cn ). analytically, we reveal the links and differences between the cn approach and the existing alternatives', '. finally, we showcase the effectiveness of the cn method with numerical experiments 1']",0
[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research']",[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research direction. among different unsupervised word vector postprocessing schemes, the all - but - the - top approach  #TAUTHOR_TAG is a prominent example. empirically studying the latent features encoded by principal components (', 'pcs ) of distributional word vectors,  #TAUTHOR_TAG found that the variances explained by the leading pcs "" encode the frequency of the word to a significant degree "".', 'since word frequencies are arguably unrelated to lexical semantics, they recommend removing such leading pcs from word vectors', 'using a pca reconstruction. the current work advances the findings of  #TAUTHOR_TAG and improves their post - processing scheme. instead of discarding a fixed number of', 'pcs, we softly filter word vectors using matrix conceptors ( jaeger 2014 ; 2017 ), which characterize the linear space of those word vector features having high variances - the features most contaminated by word frequencies according to  #TAUTHOR_TAG. the proposed approach is mathematically simple and computationally efficient, as it is founded on elementary linear algebra. besides these traits, it is also practically effective : using a standard set of', 'lexical - level intrinsic evaluation tasks and a deep neural network - based dialogue state tracking task', ', we show that conceptor - based post - processing considerably enhances linguistic regularities captured by word vectors. a more detailed list of our contributions are : 1. we propose an unsupervised algorithm that leverages boolean operations of conceptors to post - process word vectors. the resulting word vectors achieve up to', '18. 86 % and 28. 34 % improvement on the simlex -', '999 and simverb - 3500 dataset relative to the original word representations. 2. a closer look at the', 'proposed algorithm reveals commonalities across several existing post - processing techniques for neural - based word vectors and pointwise mutual information ( pmi ) matrix based word vectors. unlike the existing alternatives, the proposed approach', 'is flexible enough to remove lexically - unrelated noise, while general - purpose enough to handle word vectors induced by different learning', 'algorithms. the rest of the paper is organized as follows. we first briefly review', 'the principal component nulling approach for unsupervised word vector post - processing introduced in  #TAUTHOR_TAG, upon which our work is based. we then introduce our proposed approach, conceptor negation ( cn ). analytically, we reveal the links and differences between the cn approach and the existing alternatives', '. finally, we showcase the effectiveness of the cn method with numerical experiments 1']",0
[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research']",[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research direction. among different unsupervised word vector postprocessing schemes, the all - but - the - top approach  #TAUTHOR_TAG is a prominent example. empirically studying the latent features encoded by principal components (', 'pcs ) of distributional word vectors,  #TAUTHOR_TAG found that the variances explained by the leading pcs "" encode the frequency of the word to a significant degree "".', 'since word frequencies are arguably unrelated to lexical semantics, they recommend removing such leading pcs from word vectors', 'using a pca reconstruction. the current work advances the findings of  #TAUTHOR_TAG and improves their post - processing scheme. instead of discarding a fixed number of', 'pcs, we softly filter word vectors using matrix conceptors ( jaeger 2014 ; 2017 ), which characterize the linear space of those word vector features having high variances - the features most contaminated by word frequencies according to  #TAUTHOR_TAG. the proposed approach is mathematically simple and computationally efficient, as it is founded on elementary linear algebra. besides these traits, it is also practically effective : using a standard set of', 'lexical - level intrinsic evaluation tasks and a deep neural network - based dialogue state tracking task', ', we show that conceptor - based post - processing considerably enhances linguistic regularities captured by word vectors. a more detailed list of our contributions are : 1. we propose an unsupervised algorithm that leverages boolean operations of conceptors to post - process word vectors. the resulting word vectors achieve up to', '18. 86 % and 28. 34 % improvement on the simlex -', '999 and simverb - 3500 dataset relative to the original word representations. 2. a closer look at the', 'proposed algorithm reveals commonalities across several existing post - processing techniques for neural - based word vectors and pointwise mutual information ( pmi ) matrix based word vectors. unlike the existing alternatives, the proposed approach', 'is flexible enough to remove lexically - unrelated noise, while general - purpose enough to handle word vectors induced by different learning', 'algorithms. the rest of the paper is organized as follows. we first briefly review', 'the principal component nulling approach for unsupervised word vector post - processing introduced in  #TAUTHOR_TAG, upon which our work is based. we then introduce our proposed approach, conceptor negation ( cn ). analytically, we reveal the links and differences between the cn approach and the existing alternatives', '. finally, we showcase the effectiveness of the cn method with numerical experiments 1']",0
[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research']",[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research direction. among different unsupervised word vector postprocessing schemes, the all - but - the - top approach  #TAUTHOR_TAG is a prominent example. empirically studying the latent features encoded by principal components (', 'pcs ) of distributional word vectors,  #TAUTHOR_TAG found that the variances explained by the leading pcs "" encode the frequency of the word to a significant degree "".', 'since word frequencies are arguably unrelated to lexical semantics, they recommend removing such leading pcs from word vectors', 'using a pca reconstruction. the current work advances the findings of  #TAUTHOR_TAG and improves their post - processing scheme. instead of discarding a fixed number of', 'pcs, we softly filter word vectors using matrix conceptors ( jaeger 2014 ; 2017 ), which characterize the linear space of those word vector features having high variances - the features most contaminated by word frequencies according to  #TAUTHOR_TAG. the proposed approach is mathematically simple and computationally efficient, as it is founded on elementary linear algebra. besides these traits, it is also practically effective : using a standard set of', 'lexical - level intrinsic evaluation tasks and a deep neural network - based dialogue state tracking task', ', we show that conceptor - based post - processing considerably enhances linguistic regularities captured by word vectors. a more detailed list of our contributions are : 1. we propose an unsupervised algorithm that leverages boolean operations of conceptors to post - process word vectors. the resulting word vectors achieve up to', '18. 86 % and 28. 34 % improvement on the simlex -', '999 and simverb - 3500 dataset relative to the original word representations. 2. a closer look at the', 'proposed algorithm reveals commonalities across several existing post - processing techniques for neural - based word vectors and pointwise mutual information ( pmi ) matrix based word vectors. unlike the existing alternatives, the proposed approach', 'is flexible enough to remove lexically - unrelated noise, while general - purpose enough to handle word vectors induced by different learning', 'algorithms. the rest of the paper is organized as follows. we first briefly review', 'the principal component nulling approach for unsupervised word vector post - processing introduced in  #TAUTHOR_TAG, upon which our work is based. we then introduce our proposed approach, conceptor negation ( cn ). analytically, we reveal the links and differences between the cn approach and the existing alternatives', '. finally, we showcase the effectiveness of the cn method with numerical experiments 1']",0
['##tt ) word vector post - processing approach introduced by  #TAUTHOR_TAG'],['( abtt ) word vector post - processing approach introduced by  #TAUTHOR_TAG'],"['##tt ) word vector post - processing approach introduced by  #TAUTHOR_TAG.', 'in brief, the abtt approach is based on two key observations of distributional word vectors.', 'first, using a pca,  #TAUTHOR_TAG revealed that word vectors are strongly']","['section is an overview of the all - but - the - top ( abtt ) word vector post - processing approach introduced by  #TAUTHOR_TAG.', 'in brief, the abtt approach is based on two key observations of distributional word vectors.', 'first, using a pca,  #TAUTHOR_TAG revealed that word vectors are strongly influenced by a few leading principal components ( pcs ).', 'second, they provided an interpretation of such leading pcs : they empirically demonstrated a correlation between the variances explained by the leading pcs and word frequencies.', 'since word frequencies are arguably unrelated to lexical semantics, they recommend eliminating top pcs from word vectors via a pca reconstruction.', 'this method is described in algorithm 1.', 'algorithm 1 : the all - but - the - top ( abtt ) algorithm for word vector post - processing.', 'input : ( i ) { v w ∈ r n : w ∈ v } : word vectors with a vocabulary v ; ( ii ) d : the number of pcs to be removed.', '1 center the word vectors : letv w : = v w − [UNK] for all w ∈ v, where [UNK] is the mean of the input word vectors.', 'in practice,  #TAUTHOR_TAG found that the improvements yielded by abtt are particularly impressive for word similarity tasks.', 'here, we provide a straightforward interpretation of the effects.', '']",0
['##tt ) word vector post - processing approach introduced by  #TAUTHOR_TAG'],['( abtt ) word vector post - processing approach introduced by  #TAUTHOR_TAG'],"['##tt ) word vector post - processing approach introduced by  #TAUTHOR_TAG.', 'in brief, the abtt approach is based on two key observations of distributional word vectors.', 'first, using a pca,  #TAUTHOR_TAG revealed that word vectors are strongly']","['section is an overview of the all - but - the - top ( abtt ) word vector post - processing approach introduced by  #TAUTHOR_TAG.', 'in brief, the abtt approach is based on two key observations of distributional word vectors.', 'first, using a pca,  #TAUTHOR_TAG revealed that word vectors are strongly influenced by a few leading principal components ( pcs ).', 'second, they provided an interpretation of such leading pcs : they empirically demonstrated a correlation between the variances explained by the leading pcs and word frequencies.', 'since word frequencies are arguably unrelated to lexical semantics, they recommend eliminating top pcs from word vectors via a pca reconstruction.', 'this method is described in algorithm 1.', 'algorithm 1 : the all - but - the - top ( abtt ) algorithm for word vector post - processing.', 'input : ( i ) { v w ∈ r n : w ∈ v } : word vectors with a vocabulary v ; ( ii ) d : the number of pcs to be removed.', '1 center the word vectors : letv w : = v w − [UNK] for all w ∈ v, where [UNK] is the mean of the input word vectors.', 'in practice,  #TAUTHOR_TAG found that the improvements yielded by abtt are particularly impressive for word similarity tasks.', 'here, we provide a straightforward interpretation of the effects.', '']",0
['##tt ) word vector post - processing approach introduced by  #TAUTHOR_TAG'],['( abtt ) word vector post - processing approach introduced by  #TAUTHOR_TAG'],"['##tt ) word vector post - processing approach introduced by  #TAUTHOR_TAG.', 'in brief, the abtt approach is based on two key observations of distributional word vectors.', 'first, using a pca,  #TAUTHOR_TAG revealed that word vectors are strongly']","['section is an overview of the all - but - the - top ( abtt ) word vector post - processing approach introduced by  #TAUTHOR_TAG.', 'in brief, the abtt approach is based on two key observations of distributional word vectors.', 'first, using a pca,  #TAUTHOR_TAG revealed that word vectors are strongly influenced by a few leading principal components ( pcs ).', 'second, they provided an interpretation of such leading pcs : they empirically demonstrated a correlation between the variances explained by the leading pcs and word frequencies.', 'since word frequencies are arguably unrelated to lexical semantics, they recommend eliminating top pcs from word vectors via a pca reconstruction.', 'this method is described in algorithm 1.', 'algorithm 1 : the all - but - the - top ( abtt ) algorithm for word vector post - processing.', 'input : ( i ) { v w ∈ r n : w ∈ v } : word vectors with a vocabulary v ; ( ii ) d : the number of pcs to be removed.', '1 center the word vectors : letv w : = v w − [UNK] for all w ∈ v, where [UNK] is the mean of the input word vectors.', 'in practice,  #TAUTHOR_TAG found that the improvements yielded by abtt are particularly impressive for word similarity tasks.', 'here, we provide a straightforward interpretation of the effects.', '']",0
"['the leading pcs of word vectors using the abtt algorithm described above is effective in practice, as seen in the elaborate experiments conducted by  #TAUTHOR_TAG.', 'however, the method comes with a potential limitation : for']","['the leading pcs of word vectors using the abtt algorithm described above is effective in practice, as seen in the elaborate experiments conducted by  #TAUTHOR_TAG.', 'however, the method comes with a potential limitation : for']","['the leading pcs of word vectors using the abtt algorithm described above is effective in practice, as seen in the elaborate experiments conducted by  #TAUTHOR_TAG.', 'however, the method comes with a potential limitation : for each latent feature taking form as a pc of the word vectors, abtt']","['the leading pcs of word vectors using the abtt algorithm described above is effective in practice, as seen in the elaborate experiments conducted by  #TAUTHOR_TAG.', 'however, the method comes with a potential limitation : for each latent feature taking form as a pc of the word vectors, abtt either completely removes the feature or keeps it intact.', 'for this reason,  #AUTHOR_TAG argued that abtt is liable either to not remove enough noise or to cause too much information loss.', 'the objective of this paper is to address the limitations of abtt.', 'more concretely, we propose to use matrix conceptors ( jaeger 2017 ) to gate away variances explained by the leading pcs of word vectors.', 'as will be seen later, the proposed conceptor negation method removes noise in a "" softer "" manner when compared to abtt.', 'we show that it shares the spirit of an eigenvalue weighting approach for pmi - based word vector post - processing.', 'we proceed by providing the technical background of conceptors']",0
"['c that describes the distribution of x using equation 4.', 'recall that  #TAUTHOR_TAG found that the directions with']","['}.', 'we can estimate a conceptor c that describes the distribution of x using equation 4.', 'recall that  #TAUTHOR_TAG found that the directions with']","['v w ∈ r n : w ∈ v }.', 'we can estimate a conceptor c that describes the distribution of x using equation 4.', 'recall that  #TAUTHOR_TAG found that the directions with']","['subsection explains how conceptors can be used to post - process word vectors.', 'the intuition behind our approach is simple.', 'consider a random variable x taking values on word vectors { v w ∈ r n : w ∈ v }.', 'we can estimate a conceptor c that describes the distribution of x using equation 4.', 'recall that  #TAUTHOR_TAG found that the directions with which x has the highest variances encode word frequencies, which are unrelated to word semantics.', 'to suppress such word - frequency related features, we can simply pass all word vectors through the negated conceptor ¬c, so that ¬c dampens the directions with which x has the highest variances.', 'this simple method is summarized in algorithm 2.', 'algorithm 2 : the conceptor negation ( cn ) algorithm for word vector post - processing.', 'input : ( i ) { v w ∈ r n : w ∈ v } : word vectors of a vocabulary v ; ( ii ) α ∈ r : a hyper - parameter 1 compute the conceptor c from word vectors :', 'the hyper - parameter α of algorithm 2 governs the "" sharpness "" of the suppressing effects on word vectors employed by ¬c. although in this work we are mostly interested in α ∈ ( 0, ∞ ), it is nonetheless illustrative to consider the extreme cases where α = 0 or ∞ : for α = 0, ¬c will be an identity matrix, meaning that word vectors will be kept intact ; for α = ∞, ¬c will be a zero matrix, meaning that all word vectors will be nulled to zero vectors.', 'the computational costs of the algorithm 2 are dominated by its step 1 : one needs to calculate the matrix product', 'n× | v | being the matrix whose columns are word vectors.', 'since modern word vectors usually come with a vocabulary of some millions of words ( e. g., google news word2vec contains 3 million tokens ), performing a matrix product on such large matrices [ v w ] w∈v is computationally laborious. but considering that there are many uninteresting words in the vast vocabulary, we find it is empirically beneficial to only use a subset of the vocabulary, whose words are not too peculiar 2.', 'specifically, borrowing the word list provided by  #AUTHOR_TAG 3, we use the words that appear at least 200 times in a wikipedia dump 2015 to estimate r. this greatly boosts the computation speed.', 'somewhat surprisingly, the trick also improves the performance of algorithm 2.', 'this might due to the higher quality of word vectors of common words compared with infrequent ones']",0
[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research']",[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research direction. among different unsupervised word vector postprocessing schemes, the all - but - the - top approach  #TAUTHOR_TAG is a prominent example. empirically studying the latent features encoded by principal components (', 'pcs ) of distributional word vectors,  #TAUTHOR_TAG found that the variances explained by the leading pcs "" encode the frequency of the word to a significant degree "".', 'since word frequencies are arguably unrelated to lexical semantics, they recommend removing such leading pcs from word vectors', 'using a pca reconstruction. the current work advances the findings of  #TAUTHOR_TAG and improves their post - processing scheme. instead of discarding a fixed number of', 'pcs, we softly filter word vectors using matrix conceptors ( jaeger 2014 ; 2017 ), which characterize the linear space of those word vector features having high variances - the features most contaminated by word frequencies according to  #TAUTHOR_TAG. the proposed approach is mathematically simple and computationally efficient, as it is founded on elementary linear algebra. besides these traits, it is also practically effective : using a standard set of', 'lexical - level intrinsic evaluation tasks and a deep neural network - based dialogue state tracking task', ', we show that conceptor - based post - processing considerably enhances linguistic regularities captured by word vectors. a more detailed list of our contributions are : 1. we propose an unsupervised algorithm that leverages boolean operations of conceptors to post - process word vectors. the resulting word vectors achieve up to', '18. 86 % and 28. 34 % improvement on the simlex -', '999 and simverb - 3500 dataset relative to the original word representations. 2. a closer look at the', 'proposed algorithm reveals commonalities across several existing post - processing techniques for neural - based word vectors and pointwise mutual information ( pmi ) matrix based word vectors. unlike the existing alternatives, the proposed approach', 'is flexible enough to remove lexically - unrelated noise, while general - purpose enough to handle word vectors induced by different learning', 'algorithms. the rest of the paper is organized as follows. we first briefly review', 'the principal component nulling approach for unsupervised word vector post - processing introduced in  #TAUTHOR_TAG, upon which our work is based. we then introduce our proposed approach, conceptor negation ( cn ). analytically, we reveal the links and differences between the cn approach and the existing alternatives', '. finally, we showcase the effectiveness of the cn method with numerical experiments 1']",6
[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research']",[';  #TAUTHOR_TAG. the current paper is in line with'],"['', 'levy, goldberg, and dagan 2015 ;  #TAUTHOR_TAG. the current paper is in line with the second, unsupervised, research direction. among different unsupervised word vector postprocessing schemes, the all - but - the - top approach  #TAUTHOR_TAG is a prominent example. empirically studying the latent features encoded by principal components (', 'pcs ) of distributional word vectors,  #TAUTHOR_TAG found that the variances explained by the leading pcs "" encode the frequency of the word to a significant degree "".', 'since word frequencies are arguably unrelated to lexical semantics, they recommend removing such leading pcs from word vectors', 'using a pca reconstruction. the current work advances the findings of  #TAUTHOR_TAG and improves their post - processing scheme. instead of discarding a fixed number of', 'pcs, we softly filter word vectors using matrix conceptors ( jaeger 2014 ; 2017 ), which characterize the linear space of those word vector features having high variances - the features most contaminated by word frequencies according to  #TAUTHOR_TAG. the proposed approach is mathematically simple and computationally efficient, as it is founded on elementary linear algebra. besides these traits, it is also practically effective : using a standard set of', 'lexical - level intrinsic evaluation tasks and a deep neural network - based dialogue state tracking task', ', we show that conceptor - based post - processing considerably enhances linguistic regularities captured by word vectors. a more detailed list of our contributions are : 1. we propose an unsupervised algorithm that leverages boolean operations of conceptors to post - process word vectors. the resulting word vectors achieve up to', '18. 86 % and 28. 34 % improvement on the simlex -', '999 and simverb - 3500 dataset relative to the original word representations. 2. a closer look at the', 'proposed algorithm reveals commonalities across several existing post - processing techniques for neural - based word vectors and pointwise mutual information ( pmi ) matrix based word vectors. unlike the existing alternatives, the proposed approach', 'is flexible enough to remove lexically - unrelated noise, while general - purpose enough to handle word vectors induced by different learning', 'algorithms. the rest of the paper is organized as follows. we first briefly review', 'the principal component nulling approach for unsupervised word vector post - processing introduced in  #TAUTHOR_TAG, upon which our work is based. we then introduce our proposed approach, conceptor negation ( cn ). analytically, we reveal the links and differences between the cn approach and the existing alternatives', '. finally, we showcase the effectiveness of the cn method with numerical experiments 1']",6
"['by  #TAUTHOR_TAG.', 'word similarity we test the']","['by  #TAUTHOR_TAG.', 'word similarity we test the']","['has been suggested by  #TAUTHOR_TAG.', 'word similarity we test the performance of cn on seven benchmarks']","['evaluate the post - processed word vectors on a variety of lexical - level intrinsic tasks and a down - stream deep learning task.', 'we use the publicly available pre - trained google news word2vec ( mikolov et al. 2013 ) 5 and common crawl glove 6 ( pennington, socher, and manning 2014 ) to perform lexical - level experiments.', 'for cn, we fix α = 2 for word2vec and glove throughout the experiments 7.', 'for abtt, we set d = 3 for word2vec and d = 2 for glove, as what has been suggested by  #TAUTHOR_TAG.', 'word similarity we test the performance of cn on seven benchmarks that have been widely used to measure word similarity : the rg65 ( rubenstein and goodenough 1965 ), the wordsim - 353 ( ws ) ( finkelstein et al. 2002 ), the rarewords ( rw ) ( luong, socher, and manning 2013 ), the men dataset ( bruni, tran, and baroni 2014 ), the mturk ( radinsky et al. 2011 ), the simlex - 999 ( simlex ) ( hill, reichart, and korhonen 2015 ), and the simverb - 3500 ( gerz et al. 2016 ).', '']",5
"['by  #TAUTHOR_TAG.', 'word similarity we test the']","['by  #TAUTHOR_TAG.', 'word similarity we test the']","['has been suggested by  #TAUTHOR_TAG.', 'word similarity we test the performance of cn on seven benchmarks']","['evaluate the post - processed word vectors on a variety of lexical - level intrinsic tasks and a down - stream deep learning task.', 'we use the publicly available pre - trained google news word2vec ( mikolov et al. 2013 ) 5 and common crawl glove 6 ( pennington, socher, and manning 2014 ) to perform lexical - level experiments.', 'for cn, we fix α = 2 for word2vec and glove throughout the experiments 7.', 'for abtt, we set d = 3 for word2vec and d = 2 for glove, as what has been suggested by  #TAUTHOR_TAG.', 'word similarity we test the performance of cn on seven benchmarks that have been widely used to measure word similarity : the rg65 ( rubenstein and goodenough 1965 ), the wordsim - 353 ( ws ) ( finkelstein et al. 2002 ), the rarewords ( rw ) ( luong, socher, and manning 2013 ), the men dataset ( bruni, tran, and baroni 2014 ), the mturk ( radinsky et al. 2011 ), the simlex - 999 ( simlex ) ( hill, reichart, and korhonen 2015 ), and the simverb - 3500 ( gerz et al. 2016 ).', '']",5
,,,,5
,,,,5
,,,,5
"['by  #TAUTHOR_TAG.', 'word similarity we test the']","['by  #TAUTHOR_TAG.', 'word similarity we test the']","['has been suggested by  #TAUTHOR_TAG.', 'word similarity we test the performance of cn on seven benchmarks']","['evaluate the post - processed word vectors on a variety of lexical - level intrinsic tasks and a down - stream deep learning task.', 'we use the publicly available pre - trained google news word2vec ( mikolov et al. 2013 ) 5 and common crawl glove 6 ( pennington, socher, and manning 2014 ) to perform lexical - level experiments.', 'for cn, we fix α = 2 for word2vec and glove throughout the experiments 7.', 'for abtt, we set d = 3 for word2vec and d = 2 for glove, as what has been suggested by  #TAUTHOR_TAG.', 'word similarity we test the performance of cn on seven benchmarks that have been widely used to measure word similarity : the rg65 ( rubenstein and goodenough 1965 ), the wordsim - 353 ( ws ) ( finkelstein et al. 2002 ), the rarewords ( rw ) ( luong, socher, and manning 2013 ), the men dataset ( bruni, tran, and baroni 2014 ), the mturk ( radinsky et al. 2011 ), the simlex - 999 ( simlex ) ( hill, reichart, and korhonen 2015 ), and the simverb - 3500 ( gerz et al. 2016 ).', '']",3
"['.', 'using this technique, the system of  #TAUTHOR_TAG processes unrestricted non - projective structures with']","['dependency structure.', 'using this technique, the system of  #TAUTHOR_TAG processes unrestricted non - projective structures with state - ofthe - art']","['by swapping adjacent words of the input sentence while building the dependency structure.', 'using this technique, the system of  #TAUTHOR_TAG processes unrestricted non - projective structures with']","['', 'this makes the treatment of non - projectivity central for accurate dependency parsing.', 'unfortunately, parsing with unrestricted non - projective structures is a hard problem, for which exact inference is not possible in polynomial time except under drastic independence assumptions ( mc  #AUTHOR_TAG, and most data - driven parsers therefore use approximate methods mc  #AUTHOR_TAG.', 'one recently explored approach is to perform online reordering by swapping adjacent words of the input sentence while building the dependency structure.', 'using this technique, the system of  #TAUTHOR_TAG processes unrestricted non - projective structures with state - ofthe - art accuracy in observed linear time.', 'the normal procedure for training a transitionbased parser is to use an oracle that predicts an optimal transition sequence for every dependency tree in the training set, and then approximate this oracle by a classifier.', 'in this paper, we show that the oracle used for training by  #TAUTHOR_TAG is suboptimal because it eagerly swaps words as early as possible and therefore makes a large number of unnecessary transitions, which potentially affects both efficiency and accuracy.', 'we propose an alternative oracle that reduces the number of transitions by building larger structures before swapping, but still handles arbitrary non - projective structures']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'the first three transitions of this system (']","['', 'the basic idea in online reordering is to allow the parser to swap input words so that all dependency arcs can be constructed between adjacent subtrees.', 'this idea is implemented in the transition system proposed by  #TAUTHOR_TAG.', 'the first three transitions of this system ( left - arc, right - arc, and shift ) are familiar from many systems for transition - based dependency parsing  #AUTHOR_TAG.', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[' #TAUTHOR_TAG.', 'the first three transitions of this system (']","['', 'the basic idea in online reordering is to allow the parser to swap input words so that all dependency arcs can be constructed between adjacent subtrees.', 'this idea is implemented in the transition system proposed by  #TAUTHOR_TAG.', 'the first three transitions of this system ( left - arc, right - arc, and shift ) are familiar from many systems for transition - based dependency parsing  #AUTHOR_TAG.', '']",0
"['.', 'using this technique, the system of  #TAUTHOR_TAG processes unrestricted non - projective structures with']","['dependency structure.', 'using this technique, the system of  #TAUTHOR_TAG processes unrestricted non - projective structures with state - ofthe - art']","['by swapping adjacent words of the input sentence while building the dependency structure.', 'using this technique, the system of  #TAUTHOR_TAG processes unrestricted non - projective structures with']","['', 'this makes the treatment of non - projectivity central for accurate dependency parsing.', 'unfortunately, parsing with unrestricted non - projective structures is a hard problem, for which exact inference is not possible in polynomial time except under drastic independence assumptions ( mc  #AUTHOR_TAG, and most data - driven parsers therefore use approximate methods mc  #AUTHOR_TAG.', 'one recently explored approach is to perform online reordering by swapping adjacent words of the input sentence while building the dependency structure.', 'using this technique, the system of  #TAUTHOR_TAG processes unrestricted non - projective structures with state - ofthe - art accuracy in observed linear time.', 'the normal procedure for training a transitionbased parser is to use an oracle that predicts an optimal transition sequence for every dependency tree in the training set, and then approximate this oracle by a classifier.', 'in this paper, we show that the oracle used for training by  #TAUTHOR_TAG is suboptimal because it eagerly swaps words as early as possible and therefore makes a large number of unnecessary transitions, which potentially affects both efficiency and accuracy.', 'we propose an alternative oracle that reduces the number of transitions by building larger structures before swapping, but still handles arbitrary non - projective structures']",4
"['τ 1 proposed by  #TAUTHOR_TAG.', 'this']","['τ 1 proposed by  #TAUTHOR_TAG.', 'this']","['2 defines the original training oracle τ 1 proposed by  #TAUTHOR_TAG.', 'this oracle follows an eager reordering strategy ; it predicts swap in every configuration where this is possible.', 'the basic insight in this paper is that, by postponing swaps']","['2 defines the original training oracle τ 1 proposed by  #TAUTHOR_TAG.', 'this oracle follows an eager reordering strategy ; it predicts swap in every configuration where this is possible.', 'the basic insight in this paper is that, by postponing swaps and building as much of the tree structure as possible before swapping, we can significantly decrease the length of the transition sequence for a given sentence and tree.', 'this may benefit the efficiency of the parser trained using the oracle, as each transition takes a certain time to predict and to execute.', 'longer transition sequences may also be harder to learn than shorter ones, which potentially affects the accuracy of the parser']",7
"['transition system originally presented in  #TAUTHOR_TAG.', 'this oracle postpones swapping as long as possible but still fulfills the correctness criterion.', 'our experimental results show that the new training oracle can']","['transition system originally presented in  #TAUTHOR_TAG.', 'this oracle postpones swapping as long as possible but still fulfills the correctness criterion.', 'our experimental results show that the new training oracle can']","['have presented a new training oracle for the transition system originally presented in  #TAUTHOR_TAG.', 'this oracle postpones swapping as long as possible but still fulfills the correctness criterion.', 'our experimental results show that the new training oracle can reduce']","['have presented a new training oracle for the transition system originally presented in  #TAUTHOR_TAG.', 'this oracle postpones swapping as long as possible but still fulfills the correctness criterion.', '']",7
['five data sets as  #TAUTHOR_TAG'],['five data sets as  #TAUTHOR_TAG'],"['now test the hypothesis that the new training oracle can improve both the accuracy and the efficiency of a transition - based dependency parser.', 'our experiments are based on the same five data sets as  #TAUTHOR_TAG.', 'the training sets vary in size from 28, 750 tokens ( 1,']","['now test the hypothesis that the new training oracle can improve both the accuracy and the efficiency of a transition - based dependency parser.', 'our experiments are based on the same five data sets as  #TAUTHOR_TAG.', 'the training sets vary in size from 28, 750 tokens ( 1, 534 sentences ) for slovene to 1, 249, 408 tokens ( 72, 703 sentences ) for czech, while the test sets all consist of about 5, 000 tokens']",5
['five data sets as  #TAUTHOR_TAG'],['five data sets as  #TAUTHOR_TAG'],"['now test the hypothesis that the new training oracle can improve both the accuracy and the efficiency of a transition - based dependency parser.', 'our experiments are based on the same five data sets as  #TAUTHOR_TAG.', 'the training sets vary in size from 28, 750 tokens ( 1,']","['now test the hypothesis that the new training oracle can improve both the accuracy and the efficiency of a transition - based dependency parser.', 'our experiments are based on the same five data sets as  #TAUTHOR_TAG.', 'the training sets vary in size from 28, 750 tokens ( 1, 534 sentences ) for slovene to 1, 249, 408 tokens ( 72, 703 sentences ) for czech, while the test sets all consist of about 5, 000 tokens']",3
"['set of  #TAUTHOR_TAG.', 'the test']","['set of  #TAUTHOR_TAG.', 'the test']","['perform the correlation analysis on the flickr8k data set of  #AUTHOR_TAG, and the data set of  #TAUTHOR_TAG.', 'the test data of the flickr8k data set contains 1']","['perform the correlation analysis on the flickr8k data set of  #AUTHOR_TAG, and the data set of  #TAUTHOR_TAG.', 'the test data of the flickr8k data set contains 1, 000 images paired with five reference descriptions.', '']",4
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"[', meteor is most strongly correlated measure against human judgements.', 'a similar pattern is observed in the  #TAUTHOR_TAG data set, though the correlations are lower across all measures.', 'this could be caused by the smaller sample size']","['', 'on the flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant.', 'ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models.', 'an analysis of the distribution of ter scores in figure 2 ( a ) shows that differences in candidate and reference length are prevalent in the image description task.', 'unigram bleu is also only weakly correlated against human judgements, even though it has been reported extensively for this task.', 'finally, meteor is most strongly correlated measure against human judgements.', 'a similar pattern is observed in the  #TAUTHOR_TAG data set, though the correlations are lower across all measures.', 'this could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human - written descriptions containing the goldstandard text, as in the flickr8k data set.', 'figure 3 shows two images from the test collection of the flickr8k data set with a low meteor score and a maximum human judgement of semantic correctness.', 'the main difference between the candidates and references are in deciding what to describe ( content selection ), and how to describe it ( realisation ).', 'we can hypothesise that in both translation and summarisation, the source text acts as a lexical and semantic framework within which the translation or summarisation process takes place']",3
"['emerging topics of discussion.', 'in this study we propose a method for simulating various types of transcription errors.', 'we then test the robustness of a popular topic modelling algorithm, latent dirichlet allocation ( lda ) using a topic stability measure introduced by  #TAUTHOR_TAG over a variety of corpora']","['emerging topics of discussion.', 'in this study we propose a method for simulating various types of transcription errors.', 'we then test the robustness of a popular topic modelling algorithm, latent dirichlet allocation ( lda ) using a topic stability measure introduced by  #TAUTHOR_TAG over a variety of corpora']","['emerging topics of discussion.', 'in this study we propose a method for simulating various types of transcription errors.', 'we then test the robustness of a popular topic modelling algorithm, latent dirichlet allocation ( lda ) using a topic stability measure introduced by  #TAUTHOR_TAG over a variety of corpora']","['modelling techniques are widely applied in text retrieval tasks.', 'such techniques have been previously applied to news sources  #AUTHOR_TAG, ocr  #AUTHOR_TAG, blogs  #AUTHOR_TAG etc.', 'in which the quality of the source text is high with low error rates ( missing, misspelled, or incorrect terms or phrases ).', 'however with the improvements in terms of accuracy and the reduction in the cost of automatic speech transcription and optical character recognition ( ocr ) technologies, the range of sources that topic modelling can now be applied to is growing.', 'one artefact of such new text sources is their inherent noise.', 'in speech to text transcriptions, humans in general manage a wer of 2 % to 4 %  #AUTHOR_TAG.', 'when transcribing with a vocabulary size of 200, 5000 and 100000, the word error rates are 3 %, 7 % and 45 % respectively.', 'the best accuracy for broadcast news transcription 13 %  #AUTHOR_TAG, but this drops below 25. 7 % in conference transcription and gets worse in casual conversation  #AUTHOR_TAG.', 'these records show that the difficulty of automatic speech recognition rises with vocabulary size, speaker dependency and level of crosstalk.', 'noise aside, many of these newly available sources contain rich and valuable information that can be analysed through topic modelling.', 'for example, automatic speech transcription applied to call centre audio recordings is able to capture a level of detail that is otherwise unavailable unless the call audio is manually reviewed which is infeasible for large call volumes.', 'in this case topic modelling can be applied to transcribed text to extract the key issues and emerging topics of discussion.', 'in this study we propose a method for simulating various types of transcription errors.', 'we then test the robustness of a popular topic modelling algorithm, latent dirichlet allocation ( lda ) using a topic stability measure introduced by  #TAUTHOR_TAG over a variety of corpora']",5
"['by  #TAUTHOR_TAG for measuring topic model agreement.', 'we']","['by  #TAUTHOR_TAG for measuring topic model agreement.', 'we']","['the evaluation of topic models, we follow the approach by  #TAUTHOR_TAG for measuring topic model agreement.', '']","['##i et al.  #AUTHOR_TAG introduced latent dirichlet allocation ( lda ) as a generative probabilistic model for text corpora.', 'lda regulates the probabilistic distributions between document, topic and word and it is an unsupervised learning model.', 'for the evaluation of topic models, we follow the approach by  #TAUTHOR_TAG for measuring topic model agreement.', 'we can denote a topic list as s = { r 1,..., r k }, where r i is a topic with rank i. an individual topic can be described as r = { t 1,..., t m }, where t l is a term with rank l belong to the topic.', 'jaccard index  #AUTHOR_TAG compares the number of identical items in two sets, but it neglects ranking order.', 'average jaccard ( aj ) similarity is a top - weighted version of the jaccard index used to accommodate ranking information.', 'aj calculates the average of the jaccard scores between every pair of subsets in two lists.', 'based on aj, we can evaluate the agreement of two sets of ranked lists ( topic models ).', 'the topic model agreement score between s 1 and s 2 is a mean value of the top similarity scores between each cross pair of r. the agreement score is solved using the hungarian method  #AUTHOR_TAG and is constrained in the range [ 0, 1 ], where a perfect match between two identical k - way ranked sets results in 1 and a score 0 for nonoverlapping sets.', '']",5
"['and wikilow  #TAUTHOR_TAG with different document size and corpus size.', 'the bbc corpus includes general bbc news']","['this paper, we explore two datasets bbc and wikilow  #TAUTHOR_TAG with different document size and corpus size.', 'the bbc corpus includes general bbc news articles.', 'this corpus contains 2225']","['and wikilow  #TAUTHOR_TAG with different document size and corpus size.', 'the bbc corpus includes general bbc news articles.', 'this corpus contains 222']","['this paper, we explore two datasets bbc and wikilow  #TAUTHOR_TAG with different document size and corpus size.', 'the bbc corpus includes general bbc news articles.', 'this corpus contains 2225 documents in 5 topics and it uses 3121 terms.', 'the wikilow corpus is a subset of wikipedia and articles are labeled with fine - grained wikiproject sub - groups.', 'there are totally 4986 documents in 10 topics and it uses 15411 terms.', 'in both datasets the topics consist of distinct vocabularies which we expect lda to detect.', 'for example, the topics in bbc datasets are business, entertainment, politics, sport and technology']",5
['by  #TAUTHOR_TAG who investigated how topic stability'],['by  #TAUTHOR_TAG who investigated how topic stability'],['by  #TAUTHOR_TAG who investigated how topic stability'],"['investigated how transcription errors affect the quality and robustness of topic models produced over a range of corpora, using a topic stability measure introduced a priori.', 'we simulate transcription errors from the perspective of word error rate and generated noisy corpora with deletion, insertion and metaphone replacement.', 'topic models produced by lda show high tolerance to deletion noise up to 50 % but low tolerance to insertion and metaphone replacement errors.', 'we find the robustness of topic models is mainly determined by the extent to which the distribution of original texts is modified.', 'deletion noise is introduced randomly and its effect on topic models is minor.', 'insertion and metaphone replacement noise is systematic and undermines topic model stability to a large extent.', 'moreover, the number of topics selected also affects topic agreement.', 'with random noise or low - level systematic errors ( below 20 % ), a correct or approximately correct number of topics brings the highest topic agreement scores.', 'with high level systematic errors, topic models with 3 times the correct number of topics are most robust.', 'in some corpora, redundant number of topics helps the lda model through severe systematic errors ( figure 1 ( b ) ).', 'this complements previous work by  #TAUTHOR_TAG who investigated how topic stability is influenced by number of topics over noisefree corpora.', 'this suggests that transcribers should perhaps consider omitting words when the uncertainty is high.', 'the topic model is less influenced with a random missing term than an erroneous replacement.', 'for human consumption this may not be optimal, but in the case of output specifically intended for topic extraction this approach makes sense']",3
"[')  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling']","[')  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features that are frequent in both domains and are important']","['( scl )  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features']","['in the pre - nn era, a prominent approach to domain adaptation in nlp, and particularly in sentiment', 'classification, has been structural correspondence learning ( scl )  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features that are frequent in both domains and are important for the nlp task. non - pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between', 'the domains. elegant and well motivated as it may be, scl does not form the state - of - the - art since the neural approaches took over. in this paper we marry these approaches, proposing nn models inspired by ideas from both. particularly, our basic model receives the nonpi', '']",0
"[')  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling']","[')  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features that are frequent in both domains and are important']","['( scl )  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features']","['in the pre - nn era, a prominent approach to domain adaptation in nlp, and particularly in sentiment', 'classification, has been structural correspondence learning ( scl )  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features that are frequent in both domains and are important for the nlp task. non - pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between', 'the domains. elegant and well motivated as it may be, scl does not form the state - of - the - art since the neural approaches took over. in this paper we marry these approaches, proposing nn models inspired by ideas from both. particularly, our basic model receives the nonpi', '']",0
"['', 'pivot and non - pivot features the definitions of this approach are given in  #AUTHOR_TAG  #TAUTHOR_TAG, where sc']","['our contribution in light of these approaches.', 'pivot and non - pivot features the definitions of this approach are given in  #AUTHOR_TAG  #TAUTHOR_TAG, where scl is presented in the context of pos tagging and sentiment classification, respectively.', 'fundamentally, the method divides the shared feature']","['our contribution in light of these approaches.', 'pivot and non - pivot features the definitions of this approach are given in  #AUTHOR_TAG  #TAUTHOR_TAG, where sc']","['', 'each input example of the task classifier, at both training and test, is first run through the representation model of the first step and the induced representation is fed to the classifier.', 'recently, end - to - end models that jointly learn to represent the data and to perform the classification task have also been proposed.', 'we compare our models to one such method ( msda - dan,  #AUTHOR_TAG ).', 'below, we first discuss two prominent ideas in feature representation learning : pivot features and autoencoder neural networks.', 'we then summarize our contribution in light of these approaches.', 'pivot and non - pivot features the definitions of this approach are given in  #AUTHOR_TAG  #TAUTHOR_TAG, where scl is presented in the context of pos tagging and sentiment classification, respectively.', 'fundamentally, the method divides the shared feature space of both the source and the target domains to the set of pivot features that are frequent in both domains and are prominent in the nlp task, and a complementary set of non - pivot features.', 'in this section we abstract away from the actual feature space and its division to pivot and non - pivot subsets.', 'in section 4 we discuss this issue in the context of sentiment classification.', 'for representation learning']",0
"['of  #TAUTHOR_TAG, is that']","['of  #TAUTHOR_TAG, is that']","['important observation of  #TAUTHOR_TAG, is that some pivot features are similar to each other']","['important observation of  #TAUTHOR_TAG, is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task.', 'for example, in sentiment classification with word unigram features, the words ( unigrams ) great and excellent are likely to serve as pivot features, as the meaning of each of them is preserved across domains.', 'at the same time, both features convey very similar ( positive ) sentiment information to the level that a sentiment classifier should treat them as equals.', 'the ae - scl - sr model is based on two crucial observations.', 'first, in many nlp tasks the pivot features can be pre - embeded into a vector space where pivots with similar meaning have similar vectors.', 'second, the set f p x i of pivot features that appear in an example x i is typically much smaller than the setf p x i of pivot features that do not appear in it.', '']",0
['- validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development'],"['at training time. we experiment', 'with a 5 - fold cross - validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development']","['- validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development.', 'the test set for each target']","['', 'the sentiment classifier ( this concatenation is a standard convention in the baseline methods ). for', 'msda - dan all the above holds, except from one exception. msda - dan gets an input representation that consists of a concatenation of the original and the msda -', 'induced feature sets. as this is an end - to - end model that predicts the sentiment class', 'jointly with the new feature representation, we do not employ any additional sentiment classifier. as in the other models, msda - dan utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time. we experiment', 'with a 5 - fold cross - validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development.', 'the test set for each target domain of  #TAUTHOR_TAG consists of all 2000 labeled reviews of that domain, and for the blog domain it consists of the 7086 labeled sentences provided with the task dataset. in all five folds half of the training examples and half of the development examples are', 'randomly selected from the positive', 'reviews and the other halves from the negative reviews. we report average results across these five folds, employing the same', 'folds for all models. hyper - parameter tuning the details of the hyper - parameter tuning process for all models ( including data', 'splits to training, development and test sets ) are described in the appendices. here we provide a summary']",0
"[')  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling']","[')  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features that are frequent in both domains and are important']","['( scl )  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features']","['in the pre - nn era, a prominent approach to domain adaptation in nlp, and particularly in sentiment', 'classification, has been structural correspondence learning ( scl )  #TAUTHOR_TAG. following the auxiliary', 'problems approach to semi - supervised learning  #AUTHOR_TAG, this method identifies correspondences among features from different domains by modeling their correlations with pivot', 'features : features that are frequent in both domains and are important for the nlp task. non - pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between', 'the domains. elegant and well motivated as it may be, scl does not form the state - of - the - art since the neural approaches took over. in this paper we marry these approaches, proposing nn models inspired by ideas from both. particularly, our basic model receives the nonpi', '']",6
"['( e. g.  #TAUTHOR_TAG our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.', 'in']","['( e. g.  #TAUTHOR_TAG our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.', 'in']","['a nonlinear prediction function from x np to x p.', 'as discussed in section 4 the task we experiment with is cross - domain sentiment classification.', 'following previous work ( e. g.  #TAUTHOR_TAG our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.', 'in']","['denote the feature set in our problem with f, the subset of pivot features with f p ⊆ { 1,..., | f | } and the subset of non - pivot features with f np ⊆ { 1,..., | f | } such that f p ∪ f np = { 1,..., | f | } and f p ∩ f np = ∅. we further denote the feature representation of an input example x with x. following this notation, the vector of pivot features of x is denoted with x p while the vector of non - pivot features is denoted with x np.', 'in order to learn a robust and compact feature representation for x we will aim to learn a nonlinear prediction function from x np to x p.', 'as discussed in section 4 the task we experiment with is cross - domain sentiment classification.', 'following previous work ( e. g.  #TAUTHOR_TAG our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.', 'in what follows we hence assume that the feature representation x of an example x is a binary vector, and hence so are x p and x np']",5
['- validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development'],"['at training time. we experiment', 'with a 5 - fold cross - validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development']","['- validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development.', 'the test set for each target']","['', 'the sentiment classifier ( this concatenation is a standard convention in the baseline methods ). for', 'msda - dan all the above holds, except from one exception. msda - dan gets an input representation that consists of a concatenation of the original and the msda -', 'induced feature sets. as this is an end - to - end model that predicts the sentiment class', 'jointly with the new feature representation, we do not employ any additional sentiment classifier. as in the other models, msda - dan utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time. we experiment', 'with a 5 - fold cross - validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development.', 'the test set for each target domain of  #TAUTHOR_TAG consists of all 2000 labeled reviews of that domain, and for the blog domain it consists of the 7086 labeled sentences provided with the task dataset. in all five folds half of the training examples and half of the development examples are', 'randomly selected from the positive', 'reviews and the other halves from the negative reviews. we report average results across these five folds, employing the same', 'folds for all models. hyper - parameter tuning the details of the hyper - parameter tuning process for all models ( including data', 'splits to training, development and test sets ) are described in the appendices. here we provide a summary']",5
['- validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development'],"['at training time. we experiment', 'with a 5 - fold cross - validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development']","['- validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development.', 'the test set for each target']","['', 'the sentiment classifier ( this concatenation is a standard convention in the baseline methods ). for', 'msda - dan all the above holds, except from one exception. msda - dan gets an input representation that consists of a concatenation of the original and the msda -', 'induced feature sets. as this is an end - to - end model that predicts the sentiment class', 'jointly with the new feature representation, we do not employ any additional sentiment classifier. as in the other models, msda - dan utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time. we experiment', 'with a 5 - fold cross - validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development.', 'the test set for each target domain of  #TAUTHOR_TAG consists of all 2000 labeled reviews of that domain, and for the blog domain it consists of the 7086 labeled sentences provided with the task dataset. in all five folds half of the training examples and half of the development examples are', 'randomly selected from the positive', 'reviews and the other halves from the negative reviews. we report average results across these five folds, employing the same', 'folds for all models. hyper - parameter tuning the details of the hyper - parameter tuning process for all models ( including data', 'splits to training, development and test sets ) are described in the appendices. here we provide a summary']",5
['- validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development'],"['at training time. we experiment', 'with a 5 - fold cross - validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development']","['- validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development.', 'the test set for each target']","['', 'the sentiment classifier ( this concatenation is a standard convention in the baseline methods ). for', 'msda - dan all the above holds, except from one exception. msda - dan gets an input representation that consists of a concatenation of the original and the msda -', 'induced feature sets. as this is an end - to - end model that predicts the sentiment class', 'jointly with the new feature representation, we do not employ any additional sentiment classifier. as in the other models, msda - dan utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time. we experiment', 'with a 5 - fold cross - validation on the source domain  #TAUTHOR_TAG : 1600 reviews for training and 400 reviews for development.', 'the test set for each target domain of  #TAUTHOR_TAG consists of all 2000 labeled reviews of that domain, and for the blog domain it consists of the 7086 labeled sentences provided with the task dataset. in all five folds half of the training examples and half of the development examples are', 'randomly selected from the positive', 'reviews and the other halves from the negative reviews. we report average results across these five folds, employing the same', 'folds for all models. hyper - parameter tuning the details of the hyper - parameter tuning process for all models ( including data', 'splits to training, development and test sets ) are described in the appendices. here we provide a summary']",5
"['- mi, following  #TAUTHOR_TAG we tuned']","['bigram representations are in the appendices.', 'baselines : for scl - mi, following  #TAUTHOR_TAG we tuned']","['- mi, following  #TAUTHOR_TAG we tuned']","['the stochastic gradient descent ( sgd ) training algorithm we set the learning rate to 0. 1, momentum to 0. 9 and weightdecay regularization to 10 −5.', 'the number of pivots was chosen among { 100, 200,..., 500 } and the dimensionality of h among { 100, 300, 500 }. for the features induced by these models we take their w h x np vector.', 'for ae - scl - sr, embeddings for the unigram and bigram features were learned with word2vec  #AUTHOR_TAG.', 'details about the software and the way we learn bigram representations are in the appendices.', 'baselines : for scl - mi, following  #TAUTHOR_TAG we tuned the number of pivot features  #AUTHOR_TAG between 500 and 1000 and the svd dimensions among 50, 100 and 150.', 'for msda we tuned the number of reconstructed features among { 500, 1000, 2000, 5000, 10000 }, the number of model layers among { 1, 3, 5 } and the corruption probability among { 0. 1, 0. 2,..., 0. 5 }. for msda - dan, we followed  #AUTHOR_TAG : the λ adaptation parameter is chosen among 9 values between 10 −2 and 1 on a logarithmic scale, the hidden layer size l is chosen among { 50, 100, 200 } and the learning rate [UNK] is 10 −3']",5
"['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', '']",5
"['', 'in the  #TAUTHOR_TAG task ( top']","['1 presents our results.', 'in the  #TAUTHOR_TAG task ( top tables ), ae - scl - sr is the best performing model in 9 of 12 setups and on']","['', 'in the  #TAUTHOR_TAG task ( top tables ), ae - scl - sr is the best performing model in 9 of 12 setups and on a unified test set consisting of the test sets of all 12 setups ( the test - all column ).', '']","['1 presents our results.', 'in the  #TAUTHOR_TAG task ( top tables ), ae - scl - sr is the best performing model in 9 of 12 setups and on a unified test set consisting of the test sets of all 12 setups ( the test - all column ).', '']",7
"['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', '']",4
"['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', 'we use the one']","['of the product review data there are two releases of the datasets of the  #TAUTHOR_TAG cross - domain product review task.', '']",4
['information status  #TAUTHOR_TAG is restricted'],['information status  #TAUTHOR_TAG is restricted'],['on classifying information status  #TAUTHOR_TAG is restricted'],"['work on classifying information status  #TAUTHOR_TAG is restricted to coarse - grained classification and focuses on conversational dialogue.', 'we here introduce the task of classifying finegrained information status and work on written text.', 'we add a fine - grained information status layer to the wall street journal portion of the ontonotes corpus.', 'we claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions.', 'our approach strongly outperforms reimplementations of previous work']",0
['on learning is  #TAUTHOR_TAG is restricted in several'],"['on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in']",['. previous work on learning is  #TAUTHOR_TAG is restricted in several'],"['##s have not been mentioned before but are also not autonomous, i. e., they can only be correctly interpreted by reference to another mention or to prior world knowledge', '. all other mentions are new. is can be beneficial for a number of nlp tasks, though the results have been mixed.  #AUTHOR_TAG used is as a feature for generating pitch accent in conversational', 'speech. as is is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. for determining constituent order of german sentences,  #AUTHOR_TAG incorporate features', 'modeling is to good effect.  #AUTHOR_TAG showed that is is a useful feature for core', '##ference resolution. previous work on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in particular with the corpus annotated by  #AUTHOR_TAG. however, many applications that', 'can profit from is concentrate on written', 'texts, such as summarization. for example,  #AUTHOR_TAG show that solving the is subproblem of', 'whether a person proper name is already known to the reader improves automatic summarization', 'of news. therefore, we here model is in written text, creating a new dataset which adds an is layer to the already existing comprehensive', 'annotation in the ontonotes corpus  #AUTHOR_TAG. we also report the first results on fine', '']",0
"['person proper names.', ' #AUTHOR_TAG and  #TAUTHOR_TAG both present algorithms']","['person proper names.', ' #AUTHOR_TAG and  #TAUTHOR_TAG both present algorithms']","['person proper names.', ' #AUTHOR_TAG and  #TAUTHOR_TAG both present algorithms']","['annotation schemes and corpora.', 'we enhance the approach in  #AUTHOR_TAG in two major ways ( see also section 3. 1 ).', 'first, comparative anaphora are not specifically handled in  #AUTHOR_TAG ( and follow - on work such as  #AUTHOR_TAG and  #AUTHOR_TAG ), although some of them might be included in their respective bridging subcategories.', 'second, we apply the annotation scheme reliably to a new genre, namely news.', 'this is a non - trivial extension :  #AUTHOR_TAG applied a variation of the  #AUTHOR_TAG scheme to a small set of 220 nps in a german news / commentary corpus but found that reliability then dropped significantly to the range of κ = 0. 55 to 0. 60.', 'they attributed this to the higher syntactic complexity and semantic vagueness in the commentary corpus.', ' #AUTHOR_TAG annotated a german news corpus marginally reliable ( κ = 0. 66 ) for their overall scheme but their confusion matrix shows even lower reliability for several subcategories, most importantly deixis and bridging.', 'while standard coreference corpora do not contain is annotation, some corpora annotated for bridging are emerging  #AUTHOR_TAG korzen and buch -  #AUTHOR_TAG but they are ( i ) not annotated for comparative anaphora or other is categories, ( ii ) often not tested for reliability or reach only low reliability, ( iii ) often very small  #AUTHOR_TAG.', 'to the best of our knowledge, we therefore present the first english corpus reliably annotated for a wide range of is categories as well as full anaphoric information for three main anaphora types ( coreference, bridging, comparative ).', 'automatic recognition of is.', ' #AUTHOR_TAG describe heuristics for processing definite descriptions in news text.', 'as their approach is restricted to definites, they only analyse a subset of the mentions we consider carrying is.', ' #AUTHOR_TAG also concentrate on a subproblem of is only, namely the hearer - old / hearer - new distinctions for person proper names.', "" #AUTHOR_TAG and  #TAUTHOR_TAG both present algorithms for is detection on nissim et al.'s ( 2004 ) switchboard corpus."", 'both papers treat is classification as a local classification problem whereas we look at dependencies between the is status of different mentions, leading to collective classification.', 'in addition, they only distinguish the three main categories old, mediated and new.', 'finally, we work on news corpora which poses different problems from dialogue.', 'anaphoricity determination  #AUTHOR_TAG identifies many or most old mentions.', 'however, no distinction between mediated and new mentions is made.', 'most approaches to bridging resolution  #AUTHOR_TAG or comparative anaphora  #AUTHOR_TAG address only the selection of the antecedent for the bridging / comparative anaphor, not its recognition.', ' #AUTHOR_TAG do also tackle bridging recognition,']",0
['and  #TAUTHOR_TAG classify'],['and  #TAUTHOR_TAG classify'],"['and  #TAUTHOR_TAG classify each mention individually in a standard supervised ml setting, not considering potential dependencies between the is categories of different mentions.', 'however, collective or joint classification has made substantial impact in other nlp tasks,']","['and  #TAUTHOR_TAG classify each mention individually in a standard supervised ml setting, not considering potential dependencies between the is categories of different mentions.', 'however, collective or joint classification has made substantial impact in other nlp tasks, such as opinion mining  #AUTHOR_TAG, text categorization  #AUTHOR_TAG and the related task of coreference resolution  #AUTHOR_TAG.', 'we investigate two types of relations between mentions that might impact on is classification.', '']",0
"['so that we need to train several binary networks in a one - vs - all paradigm ( see also  #TAUTHOR_TAG, which will not']","['so that we need to train several binary networks in a one - vs - all paradigm ( see also  #TAUTHOR_TAG, which will not']","[', if the local classifier is a tree kernel svm so is the relational one.', 'one problem when using the svm tree kernel as relational classifier is that it allows only for binary classification so that we need to train several binary networks in a one - vs - all paradigm ( see also  #TAUTHOR_TAG, which will not']","['incorporating our inter - mention links, we use a variant of iterative collective classification ( ica ), which has shown good performance over a variety of tasks  #AUTHOR_TAG and has been used in nlp for example for opinion mining  #AUTHOR_TAG.', 'ica is normally faster than gibbs sampling and - in initial experiments - did not yield significantly different results from it.', 'ica initializes each mention with its most likely is, according to the local classifier and features.', 'it then iterates a relational classifier, which uses both local and relational features ( our haschild and precedes features ) taking is assignments to neighbouring mentions into account.', 'we use the exist aggregator to define the dependence between mentions.', 'we use netkit  #AUTHOR_TAG with its standard ica settings for collective inference, as it allows direct comparison between local and collective classification.', 'the relational classifiers are always exactly the same classifiers as the local ones with the relational features added : thus, if the local classifier is a tree kernel svm so is the relational one.', 'one problem when using the svm tree kernel as relational classifier is that it allows only for binary classification so that we need to train several binary networks in a one - vs - all paradigm ( see also  #TAUTHOR_TAG, which will not be able to use the multiclass dependencies of the relational features to optimum effect.', ""table 7 shows the comparison of collective classification to local classification, using nissim's framework and features, and table 8 the equivalent table for rahman and ng's approach."", 'the improvements using the additional local features over the original local classifiers are statistically significant in all cases']",0
['information status  #TAUTHOR_TAG is restricted'],['information status  #TAUTHOR_TAG is restricted'],['on classifying information status  #TAUTHOR_TAG is restricted'],"['work on classifying information status  #TAUTHOR_TAG is restricted to coarse - grained classification and focuses on conversational dialogue.', 'we here introduce the task of classifying finegrained information status and work on written text.', 'we add a fine - grained information status layer to the wall street journal portion of the ontonotes corpus.', 'we claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions.', 'our approach strongly outperforms reimplementations of previous work']",1
['on learning is  #TAUTHOR_TAG is restricted in several'],"['on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in']",['. previous work on learning is  #TAUTHOR_TAG is restricted in several'],"['##s have not been mentioned before but are also not autonomous, i. e., they can only be correctly interpreted by reference to another mention or to prior world knowledge', '. all other mentions are new. is can be beneficial for a number of nlp tasks, though the results have been mixed.  #AUTHOR_TAG used is as a feature for generating pitch accent in conversational', 'speech. as is is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. for determining constituent order of german sentences,  #AUTHOR_TAG incorporate features', 'modeling is to good effect.  #AUTHOR_TAG showed that is is a useful feature for core', '##ference resolution. previous work on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in particular with the corpus annotated by  #AUTHOR_TAG. however, many applications that', 'can profit from is concentrate on written', 'texts, such as summarization. for example,  #AUTHOR_TAG show that solving the is subproblem of', 'whether a person proper name is already known to the reader improves automatic summarization', 'of news. therefore, we here model is in written text, creating a new dataset which adds an is layer to the already existing comprehensive', 'annotation in the ontonotes corpus  #AUTHOR_TAG. we also report the first results on fine', '']",1
['on learning is  #TAUTHOR_TAG is restricted in several'],"['on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in']",['. previous work on learning is  #TAUTHOR_TAG is restricted in several'],"['##s have not been mentioned before but are also not autonomous, i. e., they can only be correctly interpreted by reference to another mention or to prior world knowledge', '. all other mentions are new. is can be beneficial for a number of nlp tasks, though the results have been mixed.  #AUTHOR_TAG used is as a feature for generating pitch accent in conversational', 'speech. as is is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. for determining constituent order of german sentences,  #AUTHOR_TAG incorporate features', 'modeling is to good effect.  #AUTHOR_TAG showed that is is a useful feature for core', '##ference resolution. previous work on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in particular with the corpus annotated by  #AUTHOR_TAG. however, many applications that', 'can profit from is concentrate on written', 'texts, such as summarization. for example,  #AUTHOR_TAG show that solving the is subproblem of', 'whether a person proper name is already known to the reader improves automatic summarization', 'of news. therefore, we here model is in written text, creating a new dataset which adds an is layer to the already existing comprehensive', 'annotation in the ontonotes corpus  #AUTHOR_TAG. we also report the first results on fine', '']",4
['on learning is  #TAUTHOR_TAG is restricted in several'],"['on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in']",['. previous work on learning is  #TAUTHOR_TAG is restricted in several'],"['##s have not been mentioned before but are also not autonomous, i. e., they can only be correctly interpreted by reference to another mention or to prior world knowledge', '. all other mentions are new. is can be beneficial for a number of nlp tasks, though the results have been mixed.  #AUTHOR_TAG used is as a feature for generating pitch accent in conversational', 'speech. as is is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. for determining constituent order of german sentences,  #AUTHOR_TAG incorporate features', 'modeling is to good effect.  #AUTHOR_TAG showed that is is a useful feature for core', '##ference resolution. previous work on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in particular with the corpus annotated by  #AUTHOR_TAG. however, many applications that', 'can profit from is concentrate on written', 'texts, such as summarization. for example,  #AUTHOR_TAG show that solving the is subproblem of', 'whether a person proper name is already known to the reader improves automatic summarization', 'of news. therefore, we here model is in written text, creating a new dataset which adds an is layer to the already existing comprehensive', 'annotation in the ontonotes corpus  #AUTHOR_TAG. we also report the first results on fine', '']",4
"['person proper names.', ' #AUTHOR_TAG and  #TAUTHOR_TAG both present algorithms']","['person proper names.', ' #AUTHOR_TAG and  #TAUTHOR_TAG both present algorithms']","['person proper names.', ' #AUTHOR_TAG and  #TAUTHOR_TAG both present algorithms']","['annotation schemes and corpora.', 'we enhance the approach in  #AUTHOR_TAG in two major ways ( see also section 3. 1 ).', 'first, comparative anaphora are not specifically handled in  #AUTHOR_TAG ( and follow - on work such as  #AUTHOR_TAG and  #AUTHOR_TAG ), although some of them might be included in their respective bridging subcategories.', 'second, we apply the annotation scheme reliably to a new genre, namely news.', 'this is a non - trivial extension :  #AUTHOR_TAG applied a variation of the  #AUTHOR_TAG scheme to a small set of 220 nps in a german news / commentary corpus but found that reliability then dropped significantly to the range of κ = 0. 55 to 0. 60.', 'they attributed this to the higher syntactic complexity and semantic vagueness in the commentary corpus.', ' #AUTHOR_TAG annotated a german news corpus marginally reliable ( κ = 0. 66 ) for their overall scheme but their confusion matrix shows even lower reliability for several subcategories, most importantly deixis and bridging.', 'while standard coreference corpora do not contain is annotation, some corpora annotated for bridging are emerging  #AUTHOR_TAG korzen and buch -  #AUTHOR_TAG but they are ( i ) not annotated for comparative anaphora or other is categories, ( ii ) often not tested for reliability or reach only low reliability, ( iii ) often very small  #AUTHOR_TAG.', 'to the best of our knowledge, we therefore present the first english corpus reliably annotated for a wide range of is categories as well as full anaphoric information for three main anaphora types ( coreference, bridging, comparative ).', 'automatic recognition of is.', ' #AUTHOR_TAG describe heuristics for processing definite descriptions in news text.', 'as their approach is restricted to definites, they only analyse a subset of the mentions we consider carrying is.', ' #AUTHOR_TAG also concentrate on a subproblem of is only, namely the hearer - old / hearer - new distinctions for person proper names.', "" #AUTHOR_TAG and  #TAUTHOR_TAG both present algorithms for is detection on nissim et al.'s ( 2004 ) switchboard corpus."", 'both papers treat is classification as a local classification problem whereas we look at dependencies between the is status of different mentions, leading to collective classification.', 'in addition, they only distinguish the three main categories old, mediated and new.', 'finally, we work on news corpora which poses different problems from dialogue.', 'anaphoricity determination  #AUTHOR_TAG identifies many or most old mentions.', 'however, no distinction between mediated and new mentions is made.', 'most approaches to bridging resolution  #AUTHOR_TAG or comparative anaphora  #AUTHOR_TAG address only the selection of the antecedent for the bridging / comparative anaphor, not its recognition.', ' #AUTHOR_TAG do also tackle bridging recognition,']",4
['and  #TAUTHOR_TAG classify'],['and  #TAUTHOR_TAG classify'],"['and  #TAUTHOR_TAG classify each mention individually in a standard supervised ml setting, not considering potential dependencies between the is categories of different mentions.', 'however, collective or joint classification has made substantial impact in other nlp tasks,']","['and  #TAUTHOR_TAG classify each mention individually in a standard supervised ml setting, not considering potential dependencies between the is categories of different mentions.', 'however, collective or joint classification has made substantial impact in other nlp tasks, such as opinion mining  #AUTHOR_TAG, text categorization  #AUTHOR_TAG and the related task of coreference resolution  #AUTHOR_TAG.', 'we investigate two types of relations between mentions that might impact on is classification.', '']",4
[' #TAUTHOR_TAG'],[' #AUTHOR_TAG and 5 %  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],4
['on learning is  #TAUTHOR_TAG is restricted in several'],"['on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in']",['. previous work on learning is  #TAUTHOR_TAG is restricted in several'],"['##s have not been mentioned before but are also not autonomous, i. e., they can only be correctly interpreted by reference to another mention or to prior world knowledge', '. all other mentions are new. is can be beneficial for a number of nlp tasks, though the results have been mixed.  #AUTHOR_TAG used is as a feature for generating pitch accent in conversational', 'speech. as is is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. for determining constituent order of german sentences,  #AUTHOR_TAG incorporate features', 'modeling is to good effect.  #AUTHOR_TAG showed that is is a useful feature for core', '##ference resolution. previous work on learning is  #TAUTHOR_TAG is restricted in several ways. it deals with conversational dialogue, in particular with the corpus annotated by  #AUTHOR_TAG. however, many applications that', 'can profit from is concentrate on written', 'texts, such as summarization. for example,  #AUTHOR_TAG show that solving the is subproblem of', 'whether a person proper name is already known to the reader improves automatic summarization', 'of news. therefore, we here model is in written text, creating a new dataset which adds an is layer to the already existing comprehensive', 'annotation in the ontonotes corpus  #AUTHOR_TAG. we also report the first results on fine', '']",5
"['use the following local features, including the features in  #AUTHOR_TAG and  #TAUTHOR_TAG to be able to gauge how their systems']","['use the following local features, including the features in  #AUTHOR_TAG and  #TAUTHOR_TAG to be able to gauge how their systems']","['use the following local features, including the features in  #AUTHOR_TAG and  #TAUTHOR_TAG to be able to gauge how their']","['use the following local features, including the features in  #AUTHOR_TAG and  #TAUTHOR_TAG to be able to gauge how their systems fare on our corpus and as a comparison point for our novel collective classification approach.', 'the features developed by  #AUTHOR_TAG are shown in table 4.', 'nissim shows clearly that these features are useful for is classification.', 'thus, subjects are more likely to be old as assumed by, e. g., centering theory ( grosz et 1995 ).', 'also, previously unmentioned proper names are more likely to be hearer - old and therefore mediated / knowledge, although their exact status will depend on how well known a particular proper name is.', ' #AUTHOR_TAG add all unigrams appearing in any mention in the training set as features.', ""they also integrated ( via a convolution tree - kernel svm  #AUTHOR_TAG ) partial parse trees that capture the generalised syntactic context of a mention e and include the mention's parent and sibling nodes without lexical leaves."", 'however, they use no structure underneath the mention node e itself, assuming that "" any np - internal information has presumably been captured by the flat features "".', 'to these feature sets, we add a small set of other local features otherlocal.', 'these track partial previous mentions by also counting partial previous mention time as well as the previous mention of content words only.', ""we also add a mention's number as one of singular, plural or unknown, and whether the mention is modified by an adjective."", 'another feature encapsulates whether the mention is modified by a comparative marker, using a small set of 10 markers such as another, such, similar... and the presence of adjectives or adverbs in the comparative.', ""finally, we include the mention's semantic class as one of 12 coarse - grained classes, including location, organisation, person and several classes for numbers ( such as date, money or percent )""]",5
"['experiments.', ' #AUTHOR_TAG and  #TAUTHOR_TAG,']","['experiments.', ' #AUTHOR_TAG and  #TAUTHOR_TAG,']","['use our gold standard corpus ( see section 3. 3 ) via 10 - fold cross - validation on documents for all experiments.', ' #AUTHOR_TAG and  #TAUTHOR_TAG,']","['use our gold standard corpus ( see section 3. 3 ) via 10 - fold cross - validation on documents for all experiments.', ' #AUTHOR_TAG and  #TAUTHOR_TAG, we perform all experiments on gold standard mentions and use the human wsj syntactic annotation for feature extraction, when necessary.', 'for the extraction of semantic class, we use ontonotes entity type annotation for proper names and an automatic assignment of semantic class via wordnet hypernyms for common nouns.', 'coarse - grained versions of all algorithms distinguish only between the three old, mediated, new categories.', 'fine - grained versions distinguish between the categories old, the six mediated subtypes, and new.', '']",5
"['algorithms in  #AUTHOR_TAG and  #TAUTHOR_TAG as comparison baselines, using their feature and algorithm choices.', 'algorithm nissim is therefore a']","['algorithms in  #AUTHOR_TAG and  #TAUTHOR_TAG as comparison baselines, using their feature and algorithm choices.', 'algorithm nissim is therefore a']","['reimplemented the algorithms in  #AUTHOR_TAG and  #TAUTHOR_TAG as comparison baselines, using their feature and algorithm choices.', 'algorithm nissim is therefore a decision tree j48 with standard settings in']","['reimplemented the algorithms in  #AUTHOR_TAG and  #TAUTHOR_TAG as comparison baselines, using their feature and algorithm choices.', 'algorithm nissim is therefore a decision tree j48 with standard settings in weka with the features in table 4.', 'algorithm rahmanng is an svm with a composite kernel and one - vs - all training / testing ( toolkit svmlight ).', 'they use the features in table 4 plus unigram and tree kernel features, described in section 4. 1.', 'we add our additional set of otherlocal features to both baseline algorithms ( yielding nissim + ol and rahmanng + ol ) as they aim specifically at improving fine - grained classification']",5
[' #TAUTHOR_TAG'],[' #AUTHOR_TAG and 5 %  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],5
"['. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['the existence of various indonesian pretrained word embeddings, there are no publicly available indonesian analogy task datasets on which to evaluate these embeddings.', 'consequently, it is unknown if indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic or semantic information as measured by analogy tasks.', 'also, such embeddings are usually trained on indonesian wikipedia  #TAUTHOR_TAG whose size is relatively small, approximately 60m tokens.', 'therefore, in this work, we introduce kawat ( kata word analogy task ), an indonesian word analogy task dataset, and new indonesian word embeddings pretrained on 160m tokens of online news corpus.', 'we evaluated these embeddings on kawat, and also tested them on pos tagging and text summarization as representatives of syntactic and semantic downstream task respectively']",1
"['. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['the existence of various indonesian pretrained word embeddings, there are no publicly available indonesian analogy task datasets on which to evaluate these embeddings.', 'consequently, it is unknown if indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic or semantic information as measured by analogy tasks.', 'also, such embeddings are usually trained on indonesian wikipedia  #TAUTHOR_TAG whose size is relatively small, approximately 60m tokens.', 'therefore, in this work, we introduce kawat ( kata word analogy task ), an indonesian word analogy task dataset, and new indonesian word embeddings pretrained on 160m tokens of online news corpus.', 'we evaluated these embeddings on kawat, and also tested them on pos tagging and text summarization as representatives of syntactic and semantic downstream task respectively']",1
"['. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic']","['the existence of various indonesian pretrained word embeddings, there are no publicly available indonesian analogy task datasets on which to evaluate these embeddings.', 'consequently, it is unknown if indonesian word embeddings introduced in, e. g.,  #TAUTHOR_TAG and  #AUTHOR_TAG, capture syntactic or semantic information as measured by analogy tasks.', 'also, such embeddings are usually trained on indonesian wikipedia  #TAUTHOR_TAG whose size is relatively small, approximately 60m tokens.', 'therefore, in this work, we introduce kawat ( kata word analogy task ), an indonesian word analogy task dataset, and new indonesian word embeddings pretrained on 160m tokens of online news corpus.', 'we evaluated these embeddings on kawat, and also tested them on pos tagging and text summarization as representatives of syntactic and semantic downstream task respectively']",0
['##n wikipedia  #TAUTHOR_TAG and nlpl embedding'],['indonesian wikipedia  #TAUTHOR_TAG and nlpl embedding'],['##n wikipedia  #TAUTHOR_TAG and nlpl embedding'],"['', 'we also used another two pretrained embeddings : polyglot embedding trained on indonesian wikipedia  #TAUTHOR_TAG and nlpl embedding trained on the indonesian portion of conll 2017 corpus  #AUTHOR_TAG.', 'for training our word embeddings, we used online news corpus obtained from tempo.', '2 we used tempo newspaper and magazine articles up to year 2014.', '']",5
"['lexical simplification  #TAUTHOR_TAG,']","['lexical simplification  #TAUTHOR_TAG,']","['lexical simplification  #TAUTHOR_TAG,']","['frequency, form, and meaning, words also have several other less known words properties, such as imageability, concreteness, familiarity, subjective frequency, and age of acquisition ( aoa ), which are subjective psycholinguistic properties, as they depend on the personal experiences that individuals had using those words.', 'according to [ 15 ], word imageability is the ease and speed with which a word evokes a mental image ; concreteness is the degree to which words refer to objects, people, places, or things that can be experienced by the senses ; experiential familiarity is the degree to which individuals know and use words in their everyday life ; subjective frequency is the estimation of the number of times a word is encountered by individuals in its written or spoken form, and aoa is the estimation of the age at which a word was learned.', 'psycholinguistic properties have been used in various approaches, such as for lexical simplification  #TAUTHOR_TAG, for text simplification at the sentence level, with the aim of reducing the difficulty of informative text for language learners [ 18 ], to predict the reading times ( rts ) of each word in a sentence to assess sentence complexity [ 14 ] and also to create robust text level readability models [ 17 ], which is also one of the purposes of this paper.', 'because of its inherent costs, the measurement of subjective psycholinguistic properties is usually used in the creation of datasets of limited size [ 2, 7, 8, 15 ].', 'for the english language, the most well known database of this kind is the mrc psycholinguistic database 4, which contains 27 subjective psycholinguistic properties for 150, 837 words.', 'for bp for example, there is a psycholinguistic database 5 containing 21 columns of information for 215, 175 words, but no subjective psycholinguistic properties.', 'in this work we aim to overcome this gap by automatically inferring the psycholinguistic properties of imageability, concreteness, aoa and subjective frequency ( similar to familiarity ) for a large database of 26, 874 bp words using a resource - light regression approach.', 'as for the automatic inference, this work is strongly based on the results of  #TAUTHOR_TAG which proposed an automatic bootstrapping method for regression to populate the mrc database.', 'we explore here 3 research questions : ( 1 ) is it possible to achieve high pearson and spearman correlations values and low mse values with a regression method using only word embedding features to infer the psycholinguistic properties for bp? ( 2 ) which size a database with psycholinguistic properties should have to be used in']",0
"['[ 4,  #TAUTHOR_TAG.', 'in order to solve limitations resulting from using word databases with human ratings,']","['[ 4,  #TAUTHOR_TAG.', 'in order to solve limitations resulting from using word databases with human ratings, [ 4 ] proposes a computational model']","['automatically estimate missing psycholinguistic properties in the mrc database [ 4,  #TAUTHOR_TAG.', 'in order to solve limitations resulting from using word databases with human ratings, [ 4 ] proposes a computational model']","['the best of our knowledge there are only two studies that propose regression methods to automatically estimate missing psycholinguistic properties in the mrc database [ 4,  #TAUTHOR_TAG.', 'in order to solve limitations resulting from using word databases with human ratings, [ 4 ] proposes a computational model to predict word concreteness, by using linear regression with word attributes from wordnet [ 3 ], latent semantic analysis ( lsa ) and the celex database 6 and use these attributes to simulate human ratings in the mrc database.', 'word concreteness is among the most important indices provided by cohmetrix, as comprehension is facilitated by virtue of more concrete words.', 'the lexical features used were 19 lexical types from wordnet, 17 lsa dimensions, hypernymy information from wordnet, word frequencies from the celex database, and word length ( i. e., number of letters ), totalling 39 attributes.', 'the pearson correlation between the estimated concreteness score and the concreteness score in the test set was 0. 82.', '[  #TAUTHOR_TAG automatically estimate missing psycholinguistic properties in the mrc database through a bootstrapping algorithm for regression.', ""their method exploits word embedding models and 15 lexical features, including the number of senses, synonyms, hyper - nyms and hyponyms for word in wordnet and also minimum, maximum and average distance between the word's senses in wordnet and the thesaurus'root sense."", 'the pearson correlation between the estimated score and the inferred score for familiarity was 0. 846 ; 0. 862 for aoa ; 0. 823 for imagenery and 0. 869 for concretness, which is better than the results of [ 4 ]']",0
"['[ 4,  #TAUTHOR_TAG.', 'in order to solve limitations resulting from using word databases with human ratings,']","['[ 4,  #TAUTHOR_TAG.', 'in order to solve limitations resulting from using word databases with human ratings, [ 4 ] proposes a computational model']","['automatically estimate missing psycholinguistic properties in the mrc database [ 4,  #TAUTHOR_TAG.', 'in order to solve limitations resulting from using word databases with human ratings, [ 4 ] proposes a computational model']","['the best of our knowledge there are only two studies that propose regression methods to automatically estimate missing psycholinguistic properties in the mrc database [ 4,  #TAUTHOR_TAG.', 'in order to solve limitations resulting from using word databases with human ratings, [ 4 ] proposes a computational model to predict word concreteness, by using linear regression with word attributes from wordnet [ 3 ], latent semantic analysis ( lsa ) and the celex database 6 and use these attributes to simulate human ratings in the mrc database.', 'word concreteness is among the most important indices provided by cohmetrix, as comprehension is facilitated by virtue of more concrete words.', 'the lexical features used were 19 lexical types from wordnet, 17 lsa dimensions, hypernymy information from wordnet, word frequencies from the celex database, and word length ( i. e., number of letters ), totalling 39 attributes.', 'the pearson correlation between the estimated concreteness score and the concreteness score in the test set was 0. 82.', '[  #TAUTHOR_TAG automatically estimate missing psycholinguistic properties in the mrc database through a bootstrapping algorithm for regression.', ""their method exploits word embedding models and 15 lexical features, including the number of senses, synonyms, hyper - nyms and hyponyms for word in wordnet and also minimum, maximum and average distance between the word's senses in wordnet and the thesaurus'root sense."", 'the pearson correlation between the estimated score and the inferred score for familiarity was 0. 846 ; 0. 862 for aoa ; 0. 823 for imagenery and 0. 869 for concretness, which is better than the results of [ 4 ]']",0
"['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical resources as wordnet, led us to raise the question']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical resources as wordnet, led us to raise the question "" could we have a similar performance with a simpler set of features which are easily obtainable for most languages? "".', 'therefore we decided to build our regressors using only word length, frequency lists, lexical databases composed of school dictionaries and word embeddings models.', 'one critical difference between the strategy of  #TAUTHOR_TAG and ours is that they concatenate all features to train a regressor, while we take a different approach.', 'although simply combining all features is straightforward, it can lead to noise insertion, given that the features used greatly contrast among them ( e. g. word embeddings and word length ).', 'instead, we adopted a more elegant solution, called multi - view learning [ 19 ].', 'in a multi - view learning, multiple regressors / classifiers are trained over different feature spaces and then combined to produce a single result.', 'here, the fusion stage is made by averaging the values predicted by the regressors [ 19 ]']",0
"['lexical simplification  #TAUTHOR_TAG,']","['lexical simplification  #TAUTHOR_TAG,']","['lexical simplification  #TAUTHOR_TAG,']","['frequency, form, and meaning, words also have several other less known words properties, such as imageability, concreteness, familiarity, subjective frequency, and age of acquisition ( aoa ), which are subjective psycholinguistic properties, as they depend on the personal experiences that individuals had using those words.', 'according to [ 15 ], word imageability is the ease and speed with which a word evokes a mental image ; concreteness is the degree to which words refer to objects, people, places, or things that can be experienced by the senses ; experiential familiarity is the degree to which individuals know and use words in their everyday life ; subjective frequency is the estimation of the number of times a word is encountered by individuals in its written or spoken form, and aoa is the estimation of the age at which a word was learned.', 'psycholinguistic properties have been used in various approaches, such as for lexical simplification  #TAUTHOR_TAG, for text simplification at the sentence level, with the aim of reducing the difficulty of informative text for language learners [ 18 ], to predict the reading times ( rts ) of each word in a sentence to assess sentence complexity [ 14 ] and also to create robust text level readability models [ 17 ], which is also one of the purposes of this paper.', 'because of its inherent costs, the measurement of subjective psycholinguistic properties is usually used in the creation of datasets of limited size [ 2, 7, 8, 15 ].', 'for the english language, the most well known database of this kind is the mrc psycholinguistic database 4, which contains 27 subjective psycholinguistic properties for 150, 837 words.', 'for bp for example, there is a psycholinguistic database 5 containing 21 columns of information for 215, 175 words, but no subjective psycholinguistic properties.', 'in this work we aim to overcome this gap by automatically inferring the psycholinguistic properties of imageability, concreteness, aoa and subjective frequency ( similar to familiarity ) for a large database of 26, 874 bp words using a resource - light regression approach.', 'as for the automatic inference, this work is strongly based on the results of  #TAUTHOR_TAG which proposed an automatic bootstrapping method for regression to populate the mrc database.', 'we explore here 3 research questions : ( 1 ) is it possible to achieve high pearson and spearman correlations values and low mse values with a regression method using only word embedding features to infer the psycholinguistic properties for bp? ( 2 ) which size a database with psycholinguistic properties should have to be used in']",3
"['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical resources as wordnet, led us to raise the question']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical resources as wordnet, led us to raise the question "" could we have a similar performance with a simpler set of features which are easily obtainable for most languages? "".', 'therefore we decided to build our regressors using only word length, frequency lists, lexical databases composed of school dictionaries and word embeddings models.', 'one critical difference between the strategy of  #TAUTHOR_TAG and ours is that they concatenate all features to train a regressor, while we take a different approach.', 'although simply combining all features is straightforward, it can lead to noise insertion, given that the features used greatly contrast among them ( e. g. word embeddings and word length ).', 'instead, we adopted a more elegant solution, called multi - view learning [ 19 ].', 'in a multi - view learning, multiple regressors / classifiers are trained over different feature spaces and then combined to produce a single result.', 'here, the fusion stage is made by averaging the values predicted by the regressors [ 19 ]']",1
"['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical resources as wordnet, led us to raise the question']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical resources as wordnet, led us to raise the question "" could we have a similar performance with a simpler set of features which are easily obtainable for most languages? "".', 'therefore we decided to build our regressors using only word length, frequency lists, lexical databases composed of school dictionaries and word embeddings models.', 'one critical difference between the strategy of  #TAUTHOR_TAG and ours is that they concatenate all features to train a regressor, while we take a different approach.', 'although simply combining all features is straightforward, it can lead to noise insertion, given that the features used greatly contrast among them ( e. g. word embeddings and word length ).', 'instead, we adopted a more elegant solution, called multi - view learning [ 19 ].', 'in a multi - view learning, multiple regressors / classifiers are trained over different feature spaces and then combined to produce a single result.', 'here, the fusion stage is made by averaging the values predicted by the regressors [ 19 ]']",1
"['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical resources as wordnet, led us to raise the question']","['fact that the methods developed by [ 4 ] and  #TAUTHOR_TAG are based on a large, scarce lexical resources as wordnet, led us to raise the question "" could we have a similar performance with a simpler set of features which are easily obtainable for most languages? "".', 'therefore we decided to build our regressors using only word length, frequency lists, lexical databases composed of school dictionaries and word embeddings models.', 'one critical difference between the strategy of  #TAUTHOR_TAG and ours is that they concatenate all features to train a regressor, while we take a different approach.', 'although simply combining all features is straightforward, it can lead to noise insertion, given that the features used greatly contrast among them ( e. g. word embeddings and word length ).', 'instead, we adopted a more elegant solution, called multi - view learning [ 19 ].', 'in a multi - view learning, multiple regressors / classifiers are trained over different feature spaces and then combined to produce a single result.', 'here, the fusion stage is made by averaging the values predicted by the regressors [ 19 ]']",4
"['with l2 regularization, which is also known as ridge regression or tikhonov regularization [ 6 ].', 'we choose this regression method due to the promising results reported by  #TAUTHOR_TAG.', 'we trained three regressors in different feature spaces : lexical features, skip - gram embeddings, and glove embeddings']","['used a linear least squares regressor with l2 regularization, which is also known as ridge regression or tikhonov regularization [ 6 ].', 'we choose this regression method due to the promising results reported by  #TAUTHOR_TAG.', 'we trained three regressors in different feature spaces : lexical features, skip - gram embeddings, and glove embeddings']","['with l2 regularization, which is also known as ridge regression or tikhonov regularization [ 6 ].', 'we choose this regression method due to the promising results reported by  #TAUTHOR_TAG.', 'we trained three regressors in different feature spaces : lexical features, skip - gram embeddings, and glove embeddings']","['used a linear least squares regressor with l2 regularization, which is also known as ridge regression or tikhonov regularization [ 6 ].', 'we choose this regression method due to the promising results reported by  #TAUTHOR_TAG.', 'we trained three regressors in different feature spaces : lexical features, skip - gram embeddings, and glove embeddings']",5
"['unsupervised  #AUTHOR_TAG.', ' #TAUTHOR_TAG observe that']","['unsupervised  #AUTHOR_TAG.', ' #TAUTHOR_TAG observe that']","['unsupervised  #AUTHOR_TAG.', ' #TAUTHOR_TAG observe that']","['##abification is the process of dividing a word into syllables.', 'although in the strict linguistic sense syllables are composed of phonemes rather than letters, due to practical considerations we focus here on orthographic syllabification, which is also referred to as hyphenation.', 'some dictionaries include hyphenation information to indicate where words may be broken for end - of - line divisions, and to assist the reader in recovering the correct pronunciation.', 'in many languages the orthographic and phonological representations of a word are closely related.', 'orthographic syllabification has a number of computational applications.', 'incorporation of the syllable boundaries between letters benefits grapheme - to - phoneme conversion  #AUTHOR_TAG, and respelling generation  #AUTHOR_TAG.', 'hyphenation of out - of - dictionary words is also important in text processing  #AUTHOR_TAG.', 'because of the productive nature of language, a dictionary look - up process for syllabification is inadequate.', 'rule - based systems are generally outperformed on out - ofdictionary words by data - driven methods, such as those of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'morphological segmentation is the task of dividing words into morphemes, the smallest meaning - bearing units in the word  #AUTHOR_TAG.', 'for example the morpheme over occurs in words like hold + over, lay + over, and skip + over.', '1 roots combine with derivational ( e. g. refut + able ) and inflectional affixes ( e. g. hold + ing ).', 'computational segmentation approaches can be divided into rule - based  #AUTHOR_TAG, supervised  #AUTHOR_TAG, semi - supervised ( gronroos et al., 2014 ), and unsupervised  #AUTHOR_TAG.', ' #TAUTHOR_TAG observe that some of the errors made by their otherwise highly - accurate system, such as hol - dov - er and coad - ju - tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.', 'in this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology.', 'we augment the syllabification approach of  #TAUTHOR_TAG, with features encoding morphological segmentation of words.', 'we investigate the degree of overlap between the morphological and syllable boundaries.', 'the results of our experiments on english and german show that the incorporation of expert - annotated ( gold ) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low - resource settings.', 'we find that the accuracy gains tend to be preserved when unsupervised segmentation is used instead.', 'on the other hand, relying on a fully - supervised system appears to be much less']",0
"['original syllabification method of  #TAUTHOR_TAG, which serves as our baseline system, and']","['original syllabification method of  #TAUTHOR_TAG, which serves as our baseline system, and']","['this section, we describe the original syllabification method of  #TAUTHOR_TAG, which serves as our baseline system, and']","['this section, we describe the original syllabification method of  #TAUTHOR_TAG, which serves as our baseline system, and discuss various approaches to incorporating morphological information.', ' #TAUTHOR_TAG present a discriminative approach to automatic syllabification.', 'they formulate syllabification as a tagging problem, and learn a structured svm tagger from labeled data  #AUTHOR_TAG.', 'under the markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence  #AUTHOR_TAG.', 'a large - margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance.', 'the test instances are tagged using the viterbi decoding algorithm on the basis of the weighted features']",0
"['unsupervised  #AUTHOR_TAG.', ' #TAUTHOR_TAG observe that']","['unsupervised  #AUTHOR_TAG.', ' #TAUTHOR_TAG observe that']","['unsupervised  #AUTHOR_TAG.', ' #TAUTHOR_TAG observe that']","['##abification is the process of dividing a word into syllables.', 'although in the strict linguistic sense syllables are composed of phonemes rather than letters, due to practical considerations we focus here on orthographic syllabification, which is also referred to as hyphenation.', 'some dictionaries include hyphenation information to indicate where words may be broken for end - of - line divisions, and to assist the reader in recovering the correct pronunciation.', 'in many languages the orthographic and phonological representations of a word are closely related.', 'orthographic syllabification has a number of computational applications.', 'incorporation of the syllable boundaries between letters benefits grapheme - to - phoneme conversion  #AUTHOR_TAG, and respelling generation  #AUTHOR_TAG.', 'hyphenation of out - of - dictionary words is also important in text processing  #AUTHOR_TAG.', 'because of the productive nature of language, a dictionary look - up process for syllabification is inadequate.', 'rule - based systems are generally outperformed on out - ofdictionary words by data - driven methods, such as those of  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, and  #AUTHOR_TAG.', 'morphological segmentation is the task of dividing words into morphemes, the smallest meaning - bearing units in the word  #AUTHOR_TAG.', 'for example the morpheme over occurs in words like hold + over, lay + over, and skip + over.', '1 roots combine with derivational ( e. g. refut + able ) and inflectional affixes ( e. g. hold + ing ).', 'computational segmentation approaches can be divided into rule - based  #AUTHOR_TAG, supervised  #AUTHOR_TAG, semi - supervised ( gronroos et al., 2014 ), and unsupervised  #AUTHOR_TAG.', ' #TAUTHOR_TAG observe that some of the errors made by their otherwise highly - accurate system, such as hol - dov - er and coad - ju - tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification.', 'in this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology.', 'we augment the syllabification approach of  #TAUTHOR_TAG, with features encoding morphological segmentation of words.', 'we investigate the degree of overlap between the morphological and syllable boundaries.', 'the results of our experiments on english and german show that the incorporation of expert - annotated ( gold ) morphological boundaries extracted from lexical databases substantially reduces the syllabification error rate, particularly in low - resource settings.', 'we find that the accuracy gains tend to be preserved when unsupervised segmentation is used instead.', 'on the other hand, relying on a fully - supervised system appears to be much less']",6
"['a baseline, we replicate the experiments of  #TAUTHOR_TAG, and extend']","['a baseline, we replicate the experiments of  #TAUTHOR_TAG, and extend']","['a baseline, we replicate the experiments of  #TAUTHOR_TAG, and extend']","['a baseline, we replicate the experiments of  #TAUTHOR_TAG, and extend them to lowresource settings.', 'since the training sets are of slightly different sizes, we label each training size point as specified in table 3.', 'we see that correct syllabification of approximately half of the words is achieved with as few as 100 english and 50 german training examples']",6
"['original syllabification method of  #TAUTHOR_TAG, which serves as our baseline system, and']","['original syllabification method of  #TAUTHOR_TAG, which serves as our baseline system, and']","['this section, we describe the original syllabification method of  #TAUTHOR_TAG, which serves as our baseline system, and']","['this section, we describe the original syllabification method of  #TAUTHOR_TAG, which serves as our baseline system, and discuss various approaches to incorporating morphological information.', ' #TAUTHOR_TAG present a discriminative approach to automatic syllabification.', 'they formulate syllabification as a tagging problem, and learn a structured svm tagger from labeled data  #AUTHOR_TAG.', 'under the markov assumption that each tag is dependent on its previous n tags, the tagger predicts the optimal tag sequence  #AUTHOR_TAG.', 'a large - margin training objective is applied to learn a weight vector to separate the correct tag sequence from other possible sequences for each training instance.', 'the test instances are tagged using the viterbi decoding algorithm on the basis of the weighted features']",5
"['a baseline, we replicate the experiments of  #TAUTHOR_TAG, and extend']","['a baseline, we replicate the experiments of  #TAUTHOR_TAG, and extend']","['a baseline, we replicate the experiments of  #TAUTHOR_TAG, and extend']","['a baseline, we replicate the experiments of  #TAUTHOR_TAG, and extend them to lowresource settings.', 'since the training sets are of slightly different sizes, we label each training size point as specified in table 3.', 'we see that correct syllabification of approximately half of the words is achieved with as few as 100 english and 50 german training examples']",5
"['discuss the overlap between morphological and syllabic boundaries.', 'we investigate the quality of the morphological segmentations of produced by various methods, and replicate the syllabification results of  #TAUTHOR_TAG.', '']","['discuss the overlap between morphological and syllabic boundaries.', 'we investigate the quality of the morphological segmentations of produced by various methods, and replicate the syllabification results of  #TAUTHOR_TAG.', 'finally, we']","['discuss the overlap between morphological and syllabic boundaries.', 'we investigate the quality of the morphological segmentations of produced by various methods, and replicate the syllabification results of  #TAUTHOR_TAG.', '']","['this section, we introduce our data sets, and discuss the overlap between morphological and syllabic boundaries.', 'we investigate the quality of the morphological segmentations of produced by various methods, and replicate the syllabification results of  #TAUTHOR_TAG.', 'finally, we discuss the results of incorporating morphological information into the syllabification system']",7
"['tasks  #TAUTHOR_TAG.', 'this']","['tasks  #TAUTHOR_TAG.', 'this']","[""' s remarkable success is also embodied in machine translation tasks  #TAUTHOR_TAG."", 'this work proposes an end - to - end co - attentional neural structure, named crossed']","['has emerged as a prominent neural module extensively adopted in a wide range of deep learning research problems  #AUTHOR_TAG rocktaschel et al., 2015 ;  #AUTHOR_TAG such as vqa, reading comprehension, textual entailment, image captioning, speech recognition and so forth.', ""it's remarkable success is also embodied in machine translation tasks  #TAUTHOR_TAG."", 'this work proposes an end - to - end co - attentional neural structure, named crossed co - attention networks ( ccns ) to address machine translation, a typical sequence - to - sequence nlp task.', 'we customize the transformer  #TAUTHOR_TAG featured by non - local operations  #AUTHOR_TAG with two * the work was done when yaoyiran was working at living analytics research centre, singapore management university who is now a phd student at university of cambridge.', ""input branches and tailor the transformer's multihead attention mechanism to the needs of information exchange between these two parallel branches."", '']",0
['- attention has demonstrated its capacity in neural transduction models  #TAUTHOR_TAG'],[': multi - head self - attention has demonstrated its capacity in neural transduction models  #TAUTHOR_TAG'],[': multi - head self - attention has demonstrated its capacity in neural transduction models  #TAUTHOR_TAG'],"[': multi - head self - attention has demonstrated its capacity in neural transduction models  #TAUTHOR_TAG, language model pre - training  #AUTHOR_TAG and speech synthesis  #AUTHOR_TAG c ).', 'while the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers  #TAUTHOR_TAG, recent work points out that it may tend to overlook neighboring information  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'it is found that applying an adaptive attention span could be conducive to character level language modeling tasks  #AUTHOR_TAG.', 'yang et al. propose to model localness for self - attention which would be conducive to capturing local information by learning a gaussian bias predicting the region of local attention  #AUTHOR_TAG a ).', 'other work indicates that adding convolution layers would ameliorate the aforementioned issue  #AUTHOR_TAG b ( yang et al.,, 2019b.', 'multi - head attention can also be used in multi - modal scenarios when v, k and q gates take in data from different domains.', ' #AUTHOR_TAG  #AUTHOR_TAG.', 'other work shows that training on 128 gpus can significantly boost the experimental results and shorten the training time  #AUTHOR_TAG.', 'a novel research direction is semi - or un - supervised machine translation aimed at addressing low - resource languages where parallel data is usually unavailable  #AUTHOR_TAG']",0
['- attention has demonstrated its capacity in neural transduction models  #TAUTHOR_TAG'],[': multi - head self - attention has demonstrated its capacity in neural transduction models  #TAUTHOR_TAG'],[': multi - head self - attention has demonstrated its capacity in neural transduction models  #TAUTHOR_TAG'],"[': multi - head self - attention has demonstrated its capacity in neural transduction models  #TAUTHOR_TAG, language model pre - training  #AUTHOR_TAG and speech synthesis  #AUTHOR_TAG c ).', 'while the novel attention mechanism, eschewing recurrence, is famous for modeling global dependencies and considered faster than recurrent layers  #TAUTHOR_TAG, recent work points out that it may tend to overlook neighboring information  #AUTHOR_TAG a ;  #AUTHOR_TAG.', 'it is found that applying an adaptive attention span could be conducive to character level language modeling tasks  #AUTHOR_TAG.', 'yang et al. propose to model localness for self - attention which would be conducive to capturing local information by learning a gaussian bias predicting the region of local attention  #AUTHOR_TAG a ).', 'other work indicates that adding convolution layers would ameliorate the aforementioned issue  #AUTHOR_TAG b ( yang et al.,, 2019b.', 'multi - head attention can also be used in multi - modal scenarios when v, k and q gates take in data from different domains.', ' #AUTHOR_TAG  #AUTHOR_TAG.', 'other work shows that training on 128 gpus can significantly boost the experimental results and shorten the training time  #AUTHOR_TAG.', 'a novel research direction is semi - or un - supervised machine translation aimed at addressing low - resource languages where parallel data is usually unavailable  #AUTHOR_TAG']",0
"['tasks  #TAUTHOR_TAG.', 'this']","['tasks  #TAUTHOR_TAG.', 'this']","[""' s remarkable success is also embodied in machine translation tasks  #TAUTHOR_TAG."", 'this work proposes an end - to - end co - attentional neural structure, named crossed']","['has emerged as a prominent neural module extensively adopted in a wide range of deep learning research problems  #AUTHOR_TAG rocktaschel et al., 2015 ;  #AUTHOR_TAG such as vqa, reading comprehension, textual entailment, image captioning, speech recognition and so forth.', ""it's remarkable success is also embodied in machine translation tasks  #TAUTHOR_TAG."", 'this work proposes an end - to - end co - attentional neural structure, named crossed co - attention networks ( ccns ) to address machine translation, a typical sequence - to - sequence nlp task.', 'we customize the transformer  #TAUTHOR_TAG featured by non - local operations  #AUTHOR_TAG with two * the work was done when yaoyiran was working at living analytics research centre, singapore management university who is now a phd student at university of cambridge.', ""input branches and tailor the transformer's multihead attention mechanism to the needs of information exchange between these two parallel branches."", '']",6
"['on the transformer model  #TAUTHOR_TAG, we design a novel co - attention mechanism.', 'our proposed mechanism consists of two symmetrical']","['on the transformer model  #TAUTHOR_TAG, we design a novel co - attention mechanism.', 'our proposed mechanism consists of two symmetrical']","['on the transformer model  #TAUTHOR_TAG, we design a novel co - attention mechanism.', 'our proposed mechanism consists of two symmetrical branches']","['on the transformer model  #TAUTHOR_TAG, we design a novel co - attention mechanism.', '']",6
"['hierarchical phrasebased smt, or hiero  #TAUTHOR_TAG, is not explicitly put under the string -']","['hierarchical phrasebased smt, or hiero  #TAUTHOR_TAG, is not explicitly put under the string - to - string category,']","['hierarchical phrasebased smt, or hiero  #TAUTHOR_TAG, is not explicitly put under the string - to - string category,']","['', 'although the syntax - based translation model in  #AUTHOR_TAG falls under the string - to - tree category, i wonder why hierarchical phrasebased smt, or hiero  #TAUTHOR_TAG, is not explicitly put under the string - to - string category, since hiero also uses "" unlabeled hierarchical phrases where there is no representation of linguistic categories. ""', 'chapter 2 focuses on how the statistical framework of a syntax - based smt approach learns its model from a word - aligned and parsed parallel text.', 'the first section explains how phrase pairs are extracted as translation rules from a word - aligned sentence pair in phrase - based smt ( koehn, och, and marcu 2003 ), highlighting the definition of a phrase as a sequence of words and the alignment - consistency property of a phrase pair as defined in  #AUTHOR_TAG.', 'the remainder of the chapter introduces three predominant instantiations of syntax - based models : hierarchical phrase - based smt ( hiero )  #TAUTHOR_TAG, which is a non - labeled syntax - based smt approach arising from the phrase - based approach ; syntax - augmented machine translation ( samt ), which introduces the notion of soft labels while keeping the nonlinguistic phrase notion ; and ghkm ( galley et al. 2004 ), which only extracts translation rules consistent with constituency parse subtrees.', 'this chapter is nicely organized and it is easy to follow the gradual evolution from phrase - based smt to ghkm.', 'chapter 3 introduces the decoding formalism in the form of a directed hypergraph, defined as a set of vertices and a set of directed hyperedges.', 'the first section introduces the notion of a weighted parse forest represented in a weighted hypergraph, representing alternative parse trees of a sentence.', '']",7
"['hierarchical phrasebased smt, or hiero  #TAUTHOR_TAG, is not explicitly put under the string -']","['hierarchical phrasebased smt, or hiero  #TAUTHOR_TAG, is not explicitly put under the string - to - string category,']","['hierarchical phrasebased smt, or hiero  #TAUTHOR_TAG, is not explicitly put under the string - to - string category,']","['', 'although the syntax - based translation model in  #AUTHOR_TAG falls under the string - to - tree category, i wonder why hierarchical phrasebased smt, or hiero  #TAUTHOR_TAG, is not explicitly put under the string - to - string category, since hiero also uses "" unlabeled hierarchical phrases where there is no representation of linguistic categories. ""', 'chapter 2 focuses on how the statistical framework of a syntax - based smt approach learns its model from a word - aligned and parsed parallel text.', 'the first section explains how phrase pairs are extracted as translation rules from a word - aligned sentence pair in phrase - based smt ( koehn, och, and marcu 2003 ), highlighting the definition of a phrase as a sequence of words and the alignment - consistency property of a phrase pair as defined in  #AUTHOR_TAG.', 'the remainder of the chapter introduces three predominant instantiations of syntax - based models : hierarchical phrase - based smt ( hiero )  #TAUTHOR_TAG, which is a non - labeled syntax - based smt approach arising from the phrase - based approach ; syntax - augmented machine translation ( samt ), which introduces the notion of soft labels while keeping the nonlinguistic phrase notion ; and ghkm ( galley et al. 2004 ), which only extracts translation rules consistent with constituency parse subtrees.', 'this chapter is nicely organized and it is easy to follow the gradual evolution from phrase - based smt to ghkm.', 'chapter 3 introduces the decoding formalism in the form of a directed hypergraph, defined as a set of vertices and a set of directed hyperedges.', 'the first section introduces the notion of a weighted parse forest represented in a weighted hypergraph, representing alternative parse trees of a sentence.', '']",0
"['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbrevi']","['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbreviation and passivization frequently found in scientific papers.', 'experimental results show that']","['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbrevi']","['japanese to english is difficult because they belong to different language families.', 'naive phrase - based statistical machine translation ( smt ) often fails to address syntactic difference between japanese and english.', 'preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating japanese and english.', 'thus, we apply a predicate - argument structure - based preordering method to the japanese - english statistical machine translation task of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbreviation and passivization frequently found in scientific papers.', ""experimental results show that our proposed method improves performance of both  #TAUTHOR_TAG's system and our phrase - based smt baseline without preordering""]",5
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['##ordering method is one of the popular techniques in statistical machine translation.', 'preordering the word order of source language in advance can enhance alignments on a pair of languages with a large difference in syntax like japanese and english, and thus improve performance of machine translation system.', 'one of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off - the - shelf smt toolkit can be plugged in without any modification.', 'preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.', 'specifically, previous work in the literature uses morphological analysis ( katz -  #AUTHOR_TAG, dependency structure ( katz -  #AUTHOR_TAG and predicate - argument structure  #TAUTHOR_TAG for preordering in japanese - english statistical machine translation.', 'however, these preordering methods are tested on limited domains : travel  #AUTHOR_TAG and patent  #TAUTHOR_TAG corpora.', 'translating japanese to english in a different domain such as scientific papers is still a big challenge for preordering - based approach.', 'for example, academic writing in english traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the japanese translation of passive construction on the english side.', 'it is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference.', 'predicate - argument structure - based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.', 'predicate - argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.', 'following  #TAUTHOR_TAG, we perform predicate - argument structure analysis on the japanese side to preorder japanese sentences to form an svo - like word order.', 'we propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.', 'the main contribution of this work is as follows :', '• we propose an extension to  #TAUTHOR_TAG in order to deal with abbreviation and passivization frequently found in scientific papers']",5
"['phrase.', 'third,  #TAUTHOR_TAG proposed']","['phrase.', 'third,  #TAUTHOR_TAG proposed']","['all words in each phrase.', 'third,  #TAUTHOR_TAG proposed']","['', 'second, katz -  #AUTHOR_TAG presented two preordering methods for japaneseenglish patent translation based on morphological analysis and dependency structure, respectively.', 'morphological analysis - based method splits sentences into segments by punctuation and a topic marker ( "" "" ), and then reverses the segments.', 'dependency analysis - based method reorders segments into a head - initial sentence, and moves verbs to make an svo - like structure.', 'unlike  #AUTHOR_TAG, they also reverse all words in each phrase.', 'third,  #TAUTHOR_TAG proposed predicate - argument structure - based preordering rules in two - level for the japanese - english patent translation task.', '']",5
"['phrase.', 'third,  #TAUTHOR_TAG proposed']","['phrase.', 'third,  #TAUTHOR_TAG proposed']","['all words in each phrase.', 'third,  #TAUTHOR_TAG proposed']","['', 'second, katz -  #AUTHOR_TAG presented two preordering methods for japaneseenglish patent translation based on morphological analysis and dependency structure, respectively.', 'morphological analysis - based method splits sentences into segments by punctuation and a topic marker ( "" "" ), and then reverses the segments.', 'dependency analysis - based method reorders segments into a head - initial sentence, and moves verbs to make an svo - like structure.', 'unlike  #AUTHOR_TAG, they also reverse all words in each phrase.', 'third,  #TAUTHOR_TAG proposed predicate - argument structure - based preordering rules in two - level for the japanese - english patent translation task.', '']",5
"['- implementation of  #TAUTHOR_TAG, and', '• preordered']","['re - implementation of  #TAUTHOR_TAG, and', '• preordered']","['- implementation of  #TAUTHOR_TAG, and', '• preordered']","['compared translation performance using a standard phrase - based statistical machine translation technique with three kinds of data :', '• original data ( baseline ),', '• preordered data by our re - implementation of  #TAUTHOR_TAG, and', '• preordered data by our proposed methods.', 'we analyzed predicate - argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence.', 'also, following  #TAUTHOR_TAG, we did not consider event nouns as predicates']",5
"['- implementation of  #TAUTHOR_TAG, and', '• preordered']","['re - implementation of  #TAUTHOR_TAG, and', '• preordered']","['- implementation of  #TAUTHOR_TAG, and', '• preordered']","['compared translation performance using a standard phrase - based statistical machine translation technique with three kinds of data :', '• original data ( baseline ),', '• preordered data by our re - implementation of  #TAUTHOR_TAG, and', '• preordered data by our proposed methods.', 'we analyzed predicate - argument structure of only the last predicate for each sentence, regardless of the number of predicates in a sentence.', 'also, following  #TAUTHOR_TAG, we did not consider event nouns as predicates']",5
"['structure analyzer trained on a newswire table 1 : comparison of the preordering methods.', 'all the preordering models using  #TAUTHOR_TAG are our re - implementation of their paper']","['to the errors in predicate - argument structure analysis.', 'we found that it is hard for predicateargument structure analyzer trained on a newswire table 1 : comparison of the preordering methods.', 'all the preordering models using  #TAUTHOR_TAG are our re - implementation of their paper']","['to the errors in predicate - argument structure analysis.', 'we found that it is hard for predicateargument structure analyzer trained on a newswire table 1 : comparison of the preordering methods.', 'all the preordering models using  #TAUTHOR_TAG are our re - implementation of their paper']","['of the errors found in a translation result are due to the errors in predicate - argument structure analysis.', 'we found that it is hard for predicateargument structure analyzer trained on a newswire table 1 : comparison of the preordering methods.', 'all the preordering models using  #TAUTHOR_TAG are our re - implementation of their paper']",5
"['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbrevi']","['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbreviation and passivization frequently found in scientific papers.', 'experimental results show that']","['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbrevi']","['japanese to english is difficult because they belong to different language families.', 'naive phrase - based statistical machine translation ( smt ) often fails to address syntactic difference between japanese and english.', 'preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating japanese and english.', 'thus, we apply a predicate - argument structure - based preordering method to the japanese - english statistical machine translation task of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbreviation and passivization frequently found in scientific papers.', ""experimental results show that our proposed method improves performance of both  #TAUTHOR_TAG's system and our phrase - based smt baseline without preordering""]",6
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['##ordering method is one of the popular techniques in statistical machine translation.', 'preordering the word order of source language in advance can enhance alignments on a pair of languages with a large difference in syntax like japanese and english, and thus improve performance of machine translation system.', 'one of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off - the - shelf smt toolkit can be plugged in without any modification.', 'preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.', 'specifically, previous work in the literature uses morphological analysis ( katz -  #AUTHOR_TAG, dependency structure ( katz -  #AUTHOR_TAG and predicate - argument structure  #TAUTHOR_TAG for preordering in japanese - english statistical machine translation.', 'however, these preordering methods are tested on limited domains : travel  #AUTHOR_TAG and patent  #TAUTHOR_TAG corpora.', 'translating japanese to english in a different domain such as scientific papers is still a big challenge for preordering - based approach.', 'for example, academic writing in english traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the japanese translation of passive construction on the english side.', 'it is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference.', 'predicate - argument structure - based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.', 'predicate - argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.', 'following  #TAUTHOR_TAG, we perform predicate - argument structure analysis on the japanese side to preorder japanese sentences to form an svo - like word order.', 'we propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.', 'the main contribution of this work is as follows :', '• we propose an extension to  #TAUTHOR_TAG in order to deal with abbreviation and passivization frequently found in scientific papers']",6
"['phrase.', 'third,  #TAUTHOR_TAG proposed']","['phrase.', 'third,  #TAUTHOR_TAG proposed']","['all words in each phrase.', 'third,  #TAUTHOR_TAG proposed']","['', 'second, katz -  #AUTHOR_TAG presented two preordering methods for japaneseenglish patent translation based on morphological analysis and dependency structure, respectively.', 'morphological analysis - based method splits sentences into segments by punctuation and a topic marker ( "" "" ), and then reverses the segments.', 'dependency analysis - based method reorders segments into a head - initial sentence, and moves verbs to make an svo - like structure.', 'unlike  #AUTHOR_TAG, they also reverse all words in each phrase.', 'third,  #TAUTHOR_TAG proposed predicate - argument structure - based preordering rules in two - level for the japanese - english patent translation task.', '']",6
"['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbrevi']","['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbreviation and passivization frequently found in scientific papers.', 'experimental results show that']","['of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbrevi']","['japanese to english is difficult because they belong to different language families.', 'naive phrase - based statistical machine translation ( smt ) often fails to address syntactic difference between japanese and english.', 'preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating japanese and english.', 'thus, we apply a predicate - argument structure - based preordering method to the japanese - english statistical machine translation task of scientific papers.', 'our method is based on the method described in  #TAUTHOR_TAG rules to handle abbreviation and passivization frequently found in scientific papers.', ""experimental results show that our proposed method improves performance of both  #TAUTHOR_TAG's system and our phrase - based smt baseline without preordering""]",4
"[', our re - implementation of  #TAUTHOR_TAG is below']","['1 shows the experimental results.', 'in terms of bleu, our re - implementation of  #TAUTHOR_TAG is below']","[', our re - implementation of  #TAUTHOR_TAG is below the baseline method while']","['used 1m japanese - english parallel sentences extracted from scientific papers ( train - 1. txt ) from the asian scientific paper excerpt corpus ( aspec ) 3.', 'we varied the size of the training corpus and used the best size determined by preliminary experiments.', 'we identified predicate - argument structure in japanese by syncha 4 0. 3.', 'it uses mecab 5 0. 996 with ipadic 2. 7. 0 for morphological analysis and cabocha 6 0. 68 for dependency parsing.', 'we used srilm 7 1. 7. 0 for language model, giza + + 8 1. 0. 7 for word alignment, and moses 9 2. 1. 1 for decoding.', 'we set distortion limits to default value 6 for all systems 10.', 'translation quality is evaluated in terms of bleu  #AUTHOR_TAG and ribes  #AUTHOR_TAG, as determined by the workshop organizers  #AUTHOR_TAG.', 'we performed minimum error rate training  #AUTHOR_TAG optimized for bleu using the development set ( dev. txt ) of the aspec corpus.', 'we conducted all the experiments using the scripts distributed at kftt moses baseline v1. 4 11.', 'table 1 shows the experimental results.', 'in terms of bleu, our re - implementation of  #TAUTHOR_TAG is below the baseline method while our proposed methods better than the baseline.', 'in terms of ribes, all preordering methods outperform the baseline, and our proposed method archieve the highest score']",4
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['##ordering method is one of the popular techniques in statistical machine translation.', 'preordering the word order of source language in advance can enhance alignments on a pair of languages with a large difference in syntax like japanese and english, and thus improve performance of machine translation system.', 'one of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off - the - shelf smt toolkit can be plugged in without any modification.', 'preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.', 'specifically, previous work in the literature uses morphological analysis ( katz -  #AUTHOR_TAG, dependency structure ( katz -  #AUTHOR_TAG and predicate - argument structure  #TAUTHOR_TAG for preordering in japanese - english statistical machine translation.', 'however, these preordering methods are tested on limited domains : travel  #AUTHOR_TAG and patent  #TAUTHOR_TAG corpora.', 'translating japanese to english in a different domain such as scientific papers is still a big challenge for preordering - based approach.', 'for example, academic writing in english traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the japanese translation of passive construction on the english side.', 'it is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference.', 'predicate - argument structure - based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.', 'predicate - argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.', 'following  #TAUTHOR_TAG, we perform predicate - argument structure analysis on the japanese side to preorder japanese sentences to form an svo - like word order.', 'we propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.', 'the main contribution of this work is as follows :', '• we propose an extension to  #TAUTHOR_TAG in order to deal with abbreviation and passivization frequently found in scientific papers']",0
"['phrase.', 'third,  #TAUTHOR_TAG proposed']","['phrase.', 'third,  #TAUTHOR_TAG proposed']","['all words in each phrase.', 'third,  #TAUTHOR_TAG proposed']","['', 'second, katz -  #AUTHOR_TAG presented two preordering methods for japaneseenglish patent translation based on morphological analysis and dependency structure, respectively.', 'morphological analysis - based method splits sentences into segments by punctuation and a topic marker ( "" "" ), and then reverses the segments.', 'dependency analysis - based method reorders segments into a head - initial sentence, and moves verbs to make an svo - like structure.', 'unlike  #AUTHOR_TAG, they also reverse all words in each phrase.', 'third,  #TAUTHOR_TAG proposed predicate - argument structure - based preordering rules in two - level for the japanese - english patent translation task.', '']",0
"['. ).', ' #TAUTHOR_TAG proposed to']","['""..', '. was explained "" or "" it was explained that... "". ).', ' #TAUTHOR_TAG proposed to']","['it was explained that... "". ).', ' #TAUTHOR_TAG proposed to']","['scientific papers, zero - pronouns in japanese are often translated into passive construction in english.', 'the number of passive construction in the 1m training corpus is 166, 057 ( 17 % ), whereas the number of active construction starting with "" they... "" and "" it is... "" are 4, 700 and 17, 104 ( 2 % ), respectively.', 'hence, we move a predicate to the end of the sentence if there exists no subject in active voice.', 'figure 1c describes how this rule transforms a japanese sentence with a zero - pronoun.', 'even though the japanese side is in active voice, english translation is expressed in passive voice.', 'note that a japanese sentence in active voice may be translated into different expressions even in the same passive construction ( e. g. ""...', '( explained... ) "" can be either ""..', '. was explained "" or "" it was explained that... "". ).', ' #TAUTHOR_TAG proposed to move a predicate after the subject ( inter - chunk preordering ).', 'however, if a subject is modified by other phrases, this rule moves the predicate to the middle of a subjective phrase composed of multiple phrases.', 'thus, we move a predicate to the end of the subjective phrase.', 'table 1d depicts how subject preordering moves a predicate in a sentence.', 'as we can see, this rule prevents subjective phrase "" | ( ventilation increase of the copd patients ) "" to be split by the predicate movement']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['##ordering method is one of the popular techniques in statistical machine translation.', 'preordering the word order of source language in advance can enhance alignments on a pair of languages with a large difference in syntax like japanese and english, and thus improve performance of machine translation system.', 'one of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off - the - shelf smt toolkit can be plugged in without any modification.', 'preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.', 'specifically, previous work in the literature uses morphological analysis ( katz -  #AUTHOR_TAG, dependency structure ( katz -  #AUTHOR_TAG and predicate - argument structure  #TAUTHOR_TAG for preordering in japanese - english statistical machine translation.', 'however, these preordering methods are tested on limited domains : travel  #AUTHOR_TAG and patent  #TAUTHOR_TAG corpora.', 'translating japanese to english in a different domain such as scientific papers is still a big challenge for preordering - based approach.', 'for example, academic writing in english traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the japanese translation of passive construction on the english side.', 'it is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference.', 'predicate - argument structure - based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.', 'predicate - argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.', 'following  #TAUTHOR_TAG, we perform predicate - argument structure analysis on the japanese side to preorder japanese sentences to form an svo - like word order.', 'we propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.', 'the main contribution of this work is as follows :', '• we propose an extension to  #TAUTHOR_TAG in order to deal with abbreviation and passivization frequently found in scientific papers']",1
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],"['##ordering method is one of the popular techniques in statistical machine translation.', 'preordering the word order of source language in advance can enhance alignments on a pair of languages with a large difference in syntax like japanese and english, and thus improve performance of machine translation system.', 'one of the advantages of preordering is that it can incorporate rich linguistic information on the source side, whilst off - the - shelf smt toolkit can be plugged in without any modification.', 'preordering methods employ various kinds of linguistic information to achieve better alignment between source and target languages.', 'specifically, previous work in the literature uses morphological analysis ( katz -  #AUTHOR_TAG, dependency structure ( katz -  #AUTHOR_TAG and predicate - argument structure  #TAUTHOR_TAG for preordering in japanese - english statistical machine translation.', 'however, these preordering methods are tested on limited domains : travel  #AUTHOR_TAG and patent  #TAUTHOR_TAG corpora.', 'translating japanese to english in a different domain such as scientific papers is still a big challenge for preordering - based approach.', 'for example, academic writing in english traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the japanese translation of passive construction on the english side.', 'it is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference.', 'predicate - argument structure - based preordering is one of the promising approaches that can solve syntactic and stylistic difference between a language pair.', 'predicate - argument structure analysis identifies who does what to whom and generalizes grammatical relations such as active and passive construction.', 'following  #TAUTHOR_TAG, we perform predicate - argument structure analysis on the japanese side to preorder japanese sentences to form an svo - like word order.', 'we propose three modifications to the preordering rules to extend their model to better handle translation of scientific papers.', 'the main contribution of this work is as follows :', '• we propose an extension to  #TAUTHOR_TAG in order to deal with abbreviation and passivization frequently found in scientific papers']",1
"['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the']","['detection of fake from legitimate news in different formats such as headlines, tweets and full news articles has been approached in recent natural language processing literature  #TAUTHOR_TAG.', 'the most important challenge in automatic misinformation detection using modern nlp techniques, especially at the level of full news articles, is data.', 'most previous systems built to identify fake news articles rely on training data labeled with respect to the general reputation of the sources, i. e., domains / user accounts  #AUTHOR_TAG.', ""even though some of these studies try to identify fake news based on linguistic cues, the question is whether they learn publishers'general writing style ( e. g., common writing features of a few clickbaity websites ) or deceptive style ( similarities among news articles that contain misinformation )."", 'in this study, we collect two new datasets that include the full text of news articles and individually assigned veracity labels.', 'we then address the above question, by conducting a set of crossdomain experiments : training a text classification system on data collected in a batch manner from suspicious and reputable websites and then testing the system on news articles that have been assessed in a one - by - one fashion.', 'our experiments reveal that the generalization power of a model trained on reputation - based labeled data is not impressive on individually assessed articles.', 'therefore, we propose to collect and verify larger collections of news articles with reliably assigned labels that would be useful for building more robust fake news detection systems']",0
[' #TAUTHOR_TAG is'],[' #TAUTHOR_TAG is'],"[' #TAUTHOR_TAG is the first large dataset collected through reliable annotation, but it contains only short statements.', 'another recently']","['', 'the so - called suspicious sources, however, sometimes do publish facts and valid information, and reputable websites sometimes publish inaccurate information  #AUTHOR_TAG.', 'the key to collect more reliable data, then, is to not rely on the source but on the text of the article itself, and only after the text has been assessed by human annotators and determined to contain false information.', 'currently, there exists only small collections of reliably - labeled news articles  #AUTHOR_TAG because this type of annotation is laborious.', 'the liar dataset  #TAUTHOR_TAG is the first large dataset collected through reliable annotation, but it contains only short statements.', 'another recently published large dataset is fever  #AUTHOR_TAG, which contains both claims and texts from wikipedia pages that support or refute those claims.', 'this dataset, however, has been built to serve the slightly different purpose of stance detection  #AUTHOR_TAG the claims have been artificially generated, and texts are not news articles.', 'our objective is to elaborate on the distinction between classifying reputation - based labeled news articles and individually - assessed news articles.', 'we do so by collecting and using datasets of the second type in evaluation of a text classifier trained on the first type of data.', 'in this section, we first introduce one large collection of news text from previous studies that has been labeled according to the list of suspicious websites, and one small collection that was labeled manually for each and every news article, but only contains satirical and legitimate instances.', 'we then introduce two datasets that we have scraped from the web by leveraging links to news articles mentioned by fact - checking websites ( buzzfeed and snopes ).', 'the distinguishing feature of these new collections is that they contain not only the full text of real news articles found online, but also individually assigned veracity labels indicative of their misinformative content.', 'rashkin et al. dataset :  #AUTHOR_TAG published a collection of roughly 20k news articles from eight sources categorized into four classes : propaganda ( the natural news and activist report ), satire ( the onion, the borowitz report, and clickhole ), hoax ( american news and dc gazette']",0
"[' #TAUTHOR_TAG fore, we use this model to demonstrate how a']","[' #TAUTHOR_TAG fore, we use this model to demonstrate how a']","['fake news detection in previous work  #TAUTHOR_TAG fore, we use this model to demonstrate how a classifier trained on']","['text classification, convolutional neural networks ( cnns ) have been competing with the tf - idf model, a simple but strong baseline using scored n - grams  #AUTHOR_TAG.', ""these methods have been used for fake news detection in previous work  #TAUTHOR_TAG fore, we use this model to demonstrate how a classifier trained on data labeled according to publisher's reputation would identify misinformative news articles."", ""it is evident in the first section of figure 1, that the model performs well on similarly collected test items, i. e., hoax, satire, propaganda and trusted news articles within rashkin et al.'s test dataset."", ""however, when the model is applied to rubin et al.'s data, which was carefully assessed for satirical cues in each and every article, the performance drops considerably ( see the second section of the figure )."", ""although the classifier detects more of the satirical texts in rubin et al.'s data, the distribution of the given labels is not very different to that of legitimate texts."", ""one important feature of rubin et al.'s data is that topics of the legitimate instances were matched and balanced with topics of the satirical instances."", 'the results here suggest that similarities captured by the classifier can be very dependent on the topics of the news articles.', 'next we examine the same model on our collected datasets, buzzfeeduse and snopes312, as test material.', 'the buzzfeeduse data comes with 4 categories ( figure 1 ).', 'the classifier does seem to have some sensitivity to true vs. false information in this dataset, as more of the mostly true articles were labeled as trusted.', '']",0
"[' #TAUTHOR_TAG fore, we use this model to demonstrate how a']","[' #TAUTHOR_TAG fore, we use this model to demonstrate how a']","['fake news detection in previous work  #TAUTHOR_TAG fore, we use this model to demonstrate how a classifier trained on']","['text classification, convolutional neural networks ( cnns ) have been competing with the tf - idf model, a simple but strong baseline using scored n - grams  #AUTHOR_TAG.', ""these methods have been used for fake news detection in previous work  #TAUTHOR_TAG fore, we use this model to demonstrate how a classifier trained on data labeled according to publisher's reputation would identify misinformative news articles."", ""it is evident in the first section of figure 1, that the model performs well on similarly collected test items, i. e., hoax, satire, propaganda and trusted news articles within rashkin et al.'s test dataset."", ""however, when the model is applied to rubin et al.'s data, which was carefully assessed for satirical cues in each and every article, the performance drops considerably ( see the second section of the figure )."", ""although the classifier detects more of the satirical texts in rubin et al.'s data, the distribution of the given labels is not very different to that of legitimate texts."", ""one important feature of rubin et al.'s data is that topics of the legitimate instances were matched and balanced with topics of the satirical instances."", 'the results here suggest that similarities captured by the classifier can be very dependent on the topics of the news articles.', 'next we examine the same model on our collected datasets, buzzfeeduse and snopes312, as test material.', 'the buzzfeeduse data comes with 4 categories ( figure 1 ).', 'the classifier does seem to have some sensitivity to true vs. false information in this dataset, as more of the mostly true articles were labeled as trusted.', '']",5
"['', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features']","['', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features']","['singular suffix and several first person form ( of any number and tense ). in presence of fusion, like in russian and', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features are', 'treated as a list of atomic elements, not as entire label. a']","['quality of the winning approach was much lower on medium ( about 85 % ) and low ( below 50 % ) datasets. this lower quality', 'is easy to explain since in low resource setting the system might even see no examples of the required form 2 or observe just one or two inflection pairs which do not', 'cover all possible paradigms for this particular form. for example, russian verbs has several tens of', 'variants to produce the + pres + sg + 1 form. consequently, to improve the inflection accuracy the system should extract more information from', 'the whole language, not only the instances of the given form. this task is easier', 'for agglutinative languages with regular inflection paradigm : to predict, say, the + pres + sg + 1 form in turkish, the system has just to observe several singular verb form ( not necessarily of the first person )', 'to extract the singular suffix and several first person form ( of any number and tense ). in presence of fusion, like in russian and', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features are', 'treated as a list of atomic elements, not as entire label. a new source of information about the whole language are', 'the laws of its phonetics. for example, to detect the vowel in the suffix of the turkish verb one do not need to observe', 'any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. a natural way to capture the phonetic patterns are character language models. they were already applied to the problem of infl', '##ection in  #AUTHOR_TAG and produced a strong boost over the baseline system. the work of sorokin used simple ngram models, however, neural language models  #AUTHOR_TAG has shown their superiority over earlier approaches for various tasks. summarizing, our approach', 'was to enrich the model of  #TAUTHOR_TAG with the language model component. we followed the architecture of  #AUTHOR_TAG, whose approach is simply to concatenate', 'the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. we expected to improve performance especially in low and medium resource', 'setting, however, our approach does not have clear advantages : our joint system is only slightly ahead', 'the baseline system of  #TAUTHOR_TAG for most of the languages. we conclude that the language model job is already executed by the decoder', '. however, given the vitality of language model approach in other areas of modern nlp', ' #AUTHOR_TAG, we describe our attempts in detail to give other researchers', 'the ideas for future work in this direction']",0
"['', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features']","['', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features']","['singular suffix and several first person form ( of any number and tense ). in presence of fusion, like in russian and', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features are', 'treated as a list of atomic elements, not as entire label. a']","['quality of the winning approach was much lower on medium ( about 85 % ) and low ( below 50 % ) datasets. this lower quality', 'is easy to explain since in low resource setting the system might even see no examples of the required form 2 or observe just one or two inflection pairs which do not', 'cover all possible paradigms for this particular form. for example, russian verbs has several tens of', 'variants to produce the + pres + sg + 1 form. consequently, to improve the inflection accuracy the system should extract more information from', 'the whole language, not only the instances of the given form. this task is easier', 'for agglutinative languages with regular inflection paradigm : to predict, say, the + pres + sg + 1 form in turkish, the system has just to observe several singular verb form ( not necessarily of the first person )', 'to extract the singular suffix and several first person form ( of any number and tense ). in presence of fusion, like in russian and', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features are', 'treated as a list of atomic elements, not as entire label. a new source of information about the whole language are', 'the laws of its phonetics. for example, to detect the vowel in the suffix of the turkish verb one do not need to observe', 'any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. a natural way to capture the phonetic patterns are character language models. they were already applied to the problem of infl', '##ection in  #AUTHOR_TAG and produced a strong boost over the baseline system. the work of sorokin used simple ngram models, however, neural language models  #AUTHOR_TAG has shown their superiority over earlier approaches for various tasks. summarizing, our approach', 'was to enrich the model of  #TAUTHOR_TAG with the language model component. we followed the architecture of  #AUTHOR_TAG, whose approach is simply to concatenate', 'the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. we expected to improve performance especially in low and medium resource', 'setting, however, our approach does not have clear advantages : our joint system is only slightly ahead', 'the baseline system of  #TAUTHOR_TAG for most of the languages. we conclude that the language model job is already executed by the decoder', '. however, given the vitality of language model approach in other areas of modern nlp', ' #AUTHOR_TAG, we describe our attempts in detail to give other researchers', 'the ideas for future work in this direction']",0
"['', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features']","['', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features']","['singular suffix and several first person form ( of any number and tense ). in presence of fusion, like in russian and', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features are', 'treated as a list of atomic elements, not as entire label. a']","['quality of the winning approach was much lower on medium ( about 85 % ) and low ( below 50 % ) datasets. this lower quality', 'is easy to explain since in low resource setting the system might even see no examples of the required form 2 or observe just one or two inflection pairs which do not', 'cover all possible paradigms for this particular form. for example, russian verbs has several tens of', 'variants to produce the + pres + sg + 1 form. consequently, to improve the inflection accuracy the system should extract more information from', 'the whole language, not only the instances of the given form. this task is easier', 'for agglutinative languages with regular inflection paradigm : to predict, say, the + pres + sg + 1 form in turkish, the system has just to observe several singular verb form ( not necessarily of the first person )', 'to extract the singular suffix and several first person form ( of any number and tense ). in presence of fusion, like in russian and', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features are', 'treated as a list of atomic elements, not as entire label. a new source of information about the whole language are', 'the laws of its phonetics. for example, to detect the vowel in the suffix of the turkish verb one do not need to observe', 'any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. a natural way to capture the phonetic patterns are character language models. they were already applied to the problem of infl', '##ection in  #AUTHOR_TAG and produced a strong boost over the baseline system. the work of sorokin used simple ngram models, however, neural language models  #AUTHOR_TAG has shown their superiority over earlier approaches for various tasks. summarizing, our approach', 'was to enrich the model of  #TAUTHOR_TAG with the language model component. we followed the architecture of  #AUTHOR_TAG, whose approach is simply to concatenate', 'the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. we expected to improve performance especially in low and medium resource', 'setting, however, our approach does not have clear advantages : our joint system is only slightly ahead', 'the baseline system of  #TAUTHOR_TAG for most of the languages. we conclude that the language model job is already executed by the decoder', '. however, given the vitality of language model approach in other areas of modern nlp', ' #AUTHOR_TAG, we describe our attempts in detail to give other researchers', 'the ideas for future work in this direction']",6
"['', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features']","['', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features']","['singular suffix and several first person form ( of any number and tense ). in presence of fusion, like in russian and', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features are', 'treated as a list of atomic elements, not as entire label. a']","['quality of the winning approach was much lower on medium ( about 85 % ) and low ( below 50 % ) datasets. this lower quality', 'is easy to explain since in low resource setting the system might even see no examples of the required form 2 or observe just one or two inflection pairs which do not', 'cover all possible paradigms for this particular form. for example, russian verbs has several tens of', 'variants to produce the + pres + sg + 1 form. consequently, to improve the inflection accuracy the system should extract more information from', 'the whole language, not only the instances of the given form. this task is easier', 'for agglutinative languages with regular inflection paradigm : to predict, say, the + pres + sg + 1 form in turkish, the system has just to observe several singular verb form ( not necessarily of the first person )', 'to extract the singular suffix and several first person form ( of any number and tense ). in presence of fusion, like in russian and', 'other slavonic languages, the decomposition is not that easy or even impossible. however, this decomposition is already realised in model of  #TAUTHOR_TAG since the grammatical features are', 'treated as a list of atomic elements, not as entire label. a new source of information about the whole language are', 'the laws of its phonetics. for example, to detect the vowel in the suffix of the turkish verb one do not need to observe', 'any verbs at all, but to extract the vowel harmony patterns from the inflection of nouns. a natural way to capture the phonetic patterns are character language models. they were already applied to the problem of infl', '##ection in  #AUTHOR_TAG and produced a strong boost over the baseline system. the work of sorokin used simple ngram models, however, neural language models  #AUTHOR_TAG has shown their superiority over earlier approaches for various tasks. summarizing, our approach', 'was to enrich the model of  #TAUTHOR_TAG with the language model component. we followed the architecture of  #AUTHOR_TAG, whose approach is simply to concatenate', 'the state of the neural decoder with the state of the neural language model before passing it to the output projection layer. we expected to improve performance especially in low and medium resource', 'setting, however, our approach does not have clear advantages : our joint system is only slightly ahead', 'the baseline system of  #TAUTHOR_TAG for most of the languages. we conclude that the language model job is already executed by the decoder', '. however, given the vitality of language model approach in other areas of modern nlp', ' #AUTHOR_TAG, we describe our attempts in detail to give other researchers', 'the ideas for future work in this direction']",4
"['.  #TAUTHOR_TAG, the winner of previous sigmorphon shared task.', 'this system is based on earlier']","['makarov et al.  #TAUTHOR_TAG, the winner of previous sigmorphon shared task.', 'this system is based on earlier']","['.  #TAUTHOR_TAG, the winner of previous sigmorphon shared task.', 'this system is based on earlier work of ah']","['the state - of - the - art baseline we choose the model of makarov et al.  #TAUTHOR_TAG, the winner of previous sigmorphon shared task.', 'this system is based on earlier work of aharoni and goldberg  #AUTHOR_TAG.', 'we briefly describe the structure of baseline model ( we call it agm - model further ) and refer the reader to these two papers for more information.', 'agmmodel consists of encoder and decoder, where an encoder is just a bidirectional lstm.', 'each element of the input sequence contains a 0 - 1 encoding of a current letter and two lstms traverse this sequence in opposite directions.', 'after encoding, each element of obtained sequence contains information about current letter and its context.', 'the main feature of the encoder is that it operates on the level on alignments, not on the level of letter sequences.', 'assume a pair volver - vuelvo appears in the training set.', 'the natural alignment is as input the lower string of figure 1.', 'let i be the number of current timestep and j be current position in the input string.', 'on i - th step the decoder takes a concatenation of 3 vectors : x j - the j - th element in the output of the encoder, f = w f eat f - the embedding of the grammatical feature vector and g i = w emb y i−1 - the embedding of previous output symbol.', 'the feature vector is obtained as 0 / 1 - encoding of the list of grammatical features.', 'we actually take the concatenation of output vectors for d ≥ 1 previous output symbols as y i−1, in our experiments d was set to 4.', 'on each step the decoder produces a vector z i as output and propagates updated hidden state vector h i to the next timestep.', '']",5
"['.  #TAUTHOR_TAG, the winner of previous sigmorphon shared task.', 'this system is based on earlier']","['makarov et al.  #TAUTHOR_TAG, the winner of previous sigmorphon shared task.', 'this system is based on earlier']","['.  #TAUTHOR_TAG, the winner of previous sigmorphon shared task.', 'this system is based on earlier work of ah']","['the state - of - the - art baseline we choose the model of makarov et al.  #TAUTHOR_TAG, the winner of previous sigmorphon shared task.', 'this system is based on earlier work of aharoni and goldberg  #AUTHOR_TAG.', 'we briefly describe the structure of baseline model ( we call it agm - model further ) and refer the reader to these two papers for more information.', 'agmmodel consists of encoder and decoder, where an encoder is just a bidirectional lstm.', 'each element of the input sequence contains a 0 - 1 encoding of a current letter and two lstms traverse this sequence in opposite directions.', 'after encoding, each element of obtained sequence contains information about current letter and its context.', 'the main feature of the encoder is that it operates on the level on alignments, not on the level of letter sequences.', 'assume a pair volver - vuelvo appears in the training set.', 'the natural alignment is as input the lower string of figure 1.', 'let i be the number of current timestep and j be current position in the input string.', 'on i - th step the decoder takes a concatenation of 3 vectors : x j - the j - th element in the output of the encoder, f = w f eat f - the embedding of the grammatical feature vector and g i = w emb y i−1 - the embedding of previous output symbol.', 'the feature vector is obtained as 0 / 1 - encoding of the list of grammatical features.', 'we actually take the concatenation of output vectors for d ≥ 1 previous output symbols as y i−1, in our experiments d was set to 4.', 'on each step the decoder produces a vector z i as output and propagates updated hidden state vector h i to the next timestep.', '']",5
"['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG,']","['of  #TAUTHOR_TAG, the second equipped with language models.', 'the third one used only the language models : we extracted all possible abstract inflection paradigms for']","['submitted three systems, one replicating the algorithm of  #TAUTHOR_TAG, the second equipped with language models.', 'the third one used only the language models : we extracted all possible abstract inflection paradigms for a given set of grammatical features and created a set of possible candidate forms applying all paradigms to the lemma.', 'for example, consider the word делать and paradigms 1 + [UNK] # 1 + [UNK], 1 + [UNK] # 1 + ит, 1 + [UNK] # 1 and 1 + [UNK] # 1 + [UNK] ; the first three produce the forms делает, делит, делат, while the fourth yields nothing since the given word does not end in - [UNK].', 'then all these forms are ranked using sum of logarithmic probabilities from forward and backward language models.', 'our results are mostly negative, since our language - model based architecture produced only marginal improvement over the model of makarov et al. which it is based on.', 'moreover, for the lowresource setting the performance of both system was mediocre, even our third paradigm - based system was able to overperform them despite its obvious weakness.', 'the results are presented in table 1 5, m1 stands for the baseline model and m2 - for the lm - based one.', '']",5
"['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in']","['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is']","['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is']","['using solely public social media information. the advantage of such an effort is that the', 'resulting tool provides non - intrusive and cost - effective means to detect and warn at', ""- risk individuals early, before they visit a doctor's office, and possibly influence their decision to visit a doctor. previous work has demonstrated that intervention by social media has modest but significant success"", 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is made on aggregated data from', 'cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. our work takes the first steps towards transferring a classification', 'model that identifies communities that are more overweight than average to classifying overweight ( and thus at - risk for t2dm ) individuals. the contributions of our work are : 1. we introduce a random - forest (', ""rf ) model that classifies us states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. despite the model's simplicity, it outperforms  #TAUTHOR_TAG's best model by 2 % accuracy. 2. using this model, we introduce a novel semi - automated process that converts the decision nodes in the rf model into natural language questions"", '. we then use these questions to implement a quiz that mimics a 20 - questions - like game.', 'the quiz aims to detect if the person taking it is overweight or not based on', 'indirect questions related to food or use of food - related words. to our knowledge, we are the first to use a semiautomatically generated quiz for data acquisition. 3. we demonstrate that this quiz serves as a non - intrusive and engaging data collection process for individuals 2. the survey was posted online and evaluated with 945 participants,', 'of whom 926 voluntarily provided', 'supplemental data, such as information necessary to compute the body mass index ( bmi ), demographics, and twitter handle, demonstrating excellent engagement. the randomforest model backing the survey agreed with self - reported bmi in 78. 7 % of cases. more', 'importantly, the differences prompted a spirited reddit discussion, again supporting our hypothesis that this', 'quiz leads to higher participant engagement 3. this initial experiment suggests that it is possible to use easy - to -', 'access community data to acquire training data on individuals, which is much more expensive to obtain, yet is fundamental to building individualized public health tools. the anonymized data collected from', 'the quiz is publicly available']",0
[' #TAUTHOR_TAG. food - related tweets'],"['variants of', 'this problem have been considered  #TAUTHOR_TAG. food - related tweets']",[' #TAUTHOR_TAG. food - related tweets and use'],"['media. myslin et al. ( 2013 ) focus on understanding the perception of emerging tobacco products by analyzing tweets. social media, especially twitter, has been recently utilized as a popular source of data for public health', 'monitoring, such as tracking diseases  #AUTHOR_TAG yom  #AUTHOR_TAG, mining drug - related adverse events  #AUTHOR_TAG, predicting postpartum', 'psychological changes in new mothers  #AUTHOR_TAG, and detecting life satisfaction  #AUTHOR_TAG and obesity  #AUTHOR_TAG cohen -  #AUTHOR_TAG fern', '##andez -  #AUTHOR_TAG. we focus our attention on the language of food on social media to identify', 'overweight communities and individuals. in the last couple of years, several variants of', 'this problem have been considered  #TAUTHOR_TAG. food - related tweets and use it to predict several population characteristics, namely', 'diabetes rate, overweight rate and political tendency. generally, they use state - level populations, e. g., one of their classification tasks is to label whether a state is more overweight than the national median. overweight rate is the percentage of adults whose body mass index ( bmi ) is larger than a normal range defined', 'by nih. the classification task is to label whether a state is more overweight than the national median', "". individuals'tweets are localized at state level as a single instance to train several classifier"", 'models, and the performance of models is evaluated using leave - one - out cross - validation. importantly,  #TAUTHOR_TAG train', '']",0
"['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in']","['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is']","['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is']","['using solely public social media information. the advantage of such an effort is that the', 'resulting tool provides non - intrusive and cost - effective means to detect and warn at', ""- risk individuals early, before they visit a doctor's office, and possibly influence their decision to visit a doctor. previous work has demonstrated that intervention by social media has modest but significant success"", 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is made on aggregated data from', 'cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. our work takes the first steps towards transferring a classification', 'model that identifies communities that are more overweight than average to classifying overweight ( and thus at - risk for t2dm ) individuals. the contributions of our work are : 1. we introduce a random - forest (', ""rf ) model that classifies us states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. despite the model's simplicity, it outperforms  #TAUTHOR_TAG's best model by 2 % accuracy. 2. using this model, we introduce a novel semi - automated process that converts the decision nodes in the rf model into natural language questions"", '. we then use these questions to implement a quiz that mimics a 20 - questions - like game.', 'the quiz aims to detect if the person taking it is overweight or not based on', 'indirect questions related to food or use of food - related words. to our knowledge, we are the first to use a semiautomatically generated quiz for data acquisition. 3. we demonstrate that this quiz serves as a non - intrusive and engaging data collection process for individuals 2. the survey was posted online and evaluated with 945 participants,', 'of whom 926 voluntarily provided', 'supplemental data, such as information necessary to compute the body mass index ( bmi ), demographics, and twitter handle, demonstrating excellent engagement. the randomforest model backing the survey agreed with self - reported bmi in 78. 7 % of cases. more', 'importantly, the differences prompted a spirited reddit discussion, again supporting our hypothesis that this', 'quiz leads to higher participant engagement 3. this initial experiment suggests that it is possible to use easy - to -', 'access community data to acquire training data on individuals, which is much more expensive to obtain, yet is fundamental to building individualized public health tools. the anonymized data collected from', 'the quiz is publicly available']",6
"['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in']","['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is']","['has demonstrated that intervention by social media has modest but significant success', 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is']","['using solely public social media information. the advantage of such an effort is that the', 'resulting tool provides non - intrusive and cost - effective means to detect and warn at', ""- risk individuals early, before they visit a doctor's office, and possibly influence their decision to visit a doctor. previous work has demonstrated that intervention by social media has modest but significant success"", 'in decreasing obesity  #AUTHOR_TAG. furthermore, there is good evidence that detecting communities at risk using computational models trained on social media data is possible  #TAUTHOR_TAG. however, in all cases, classification is made on aggregated data from', 'cities, counties, or states, so these models are not immediately applicable to the task of classifying individuals. our work takes the first steps towards transferring a classification', 'model that identifies communities that are more overweight than average to classifying overweight ( and thus at - risk for t2dm ) individuals. the contributions of our work are : 1. we introduce a random - forest (', ""rf ) model that classifies us states as more or less overweight than average using only 7 decision trees with a maximum depth of 3. despite the model's simplicity, it outperforms  #TAUTHOR_TAG's best model by 2 % accuracy. 2. using this model, we introduce a novel semi - automated process that converts the decision nodes in the rf model into natural language questions"", '. we then use these questions to implement a quiz that mimics a 20 - questions - like game.', 'the quiz aims to detect if the person taking it is overweight or not based on', 'indirect questions related to food or use of food - related words. to our knowledge, we are the first to use a semiautomatically generated quiz for data acquisition. 3. we demonstrate that this quiz serves as a non - intrusive and engaging data collection process for individuals 2. the survey was posted online and evaluated with 945 participants,', 'of whom 926 voluntarily provided', 'supplemental data, such as information necessary to compute the body mass index ( bmi ), demographics, and twitter handle, demonstrating excellent engagement. the randomforest model backing the survey agreed with self - reported bmi in 78. 7 % of cases. more', 'importantly, the differences prompted a spirited reddit discussion, again supporting our hypothesis that this', 'quiz leads to higher participant engagement 3. this initial experiment suggests that it is possible to use easy - to -', 'access community data to acquire training data on individuals, which is much more expensive to obtain, yet is fundamental to building individualized public health tools. the anonymized data collected from', 'the quiz is publicly available']",4
[' #TAUTHOR_TAG. food - related tweets'],"['variants of', 'this problem have been considered  #TAUTHOR_TAG. food - related tweets']",[' #TAUTHOR_TAG. food - related tweets and use'],"['media. myslin et al. ( 2013 ) focus on understanding the perception of emerging tobacco products by analyzing tweets. social media, especially twitter, has been recently utilized as a popular source of data for public health', 'monitoring, such as tracking diseases  #AUTHOR_TAG yom  #AUTHOR_TAG, mining drug - related adverse events  #AUTHOR_TAG, predicting postpartum', 'psychological changes in new mothers  #AUTHOR_TAG, and detecting life satisfaction  #AUTHOR_TAG and obesity  #AUTHOR_TAG cohen -  #AUTHOR_TAG fern', '##andez -  #AUTHOR_TAG. we focus our attention on the language of food on social media to identify', 'overweight communities and individuals. in the last couple of years, several variants of', 'this problem have been considered  #TAUTHOR_TAG. food - related tweets and use it to predict several population characteristics, namely', 'diabetes rate, overweight rate and political tendency. generally, they use state - level populations, e. g., one of their classification tasks is to label whether a state is more overweight than the national median. overweight rate is the percentage of adults whose body mass index ( bmi ) is larger than a normal range defined', 'by nih. the classification task is to label whether a state is more overweight than the national median', "". individuals'tweets are localized at state level as a single instance to train several classifier"", 'models, and the performance of models is evaluated using leave - one - out cross - validation. importantly,  #TAUTHOR_TAG train', '']",4
"[""baseline 50. 89 svm  #TAUTHOR_TAG's best""]","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier :']","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier :']","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier : the first keeps numeric features ( e. g., word counts ) as is, whereas the second discretizes numeric features to three bins.', '']",4
[' #TAUTHOR_TAG. food - related tweets'],"['variants of', 'this problem have been considered  #TAUTHOR_TAG. food - related tweets']",[' #TAUTHOR_TAG. food - related tweets and use'],"['media. myslin et al. ( 2013 ) focus on understanding the perception of emerging tobacco products by analyzing tweets. social media, especially twitter, has been recently utilized as a popular source of data for public health', 'monitoring, such as tracking diseases  #AUTHOR_TAG yom  #AUTHOR_TAG, mining drug - related adverse events  #AUTHOR_TAG, predicting postpartum', 'psychological changes in new mothers  #AUTHOR_TAG, and detecting life satisfaction  #AUTHOR_TAG and obesity  #AUTHOR_TAG cohen -  #AUTHOR_TAG fern', '##andez -  #AUTHOR_TAG. we focus our attention on the language of food on social media to identify', 'overweight communities and individuals. in the last couple of years, several variants of', 'this problem have been considered  #TAUTHOR_TAG. food - related tweets and use it to predict several population characteristics, namely', 'diabetes rate, overweight rate and political tendency. generally, they use state - level populations, e. g., one of their classification tasks is to label whether a state is more overweight than the national median. overweight rate is the percentage of adults whose body mass index ( bmi ) is larger than a normal range defined', 'by nih. the classification task is to label whether a state is more overweight than the national median', "". individuals'tweets are localized at state level as a single instance to train several classifier"", 'models, and the performance of models is evaluated using leave - one - out cross - validation. importantly,  #TAUTHOR_TAG train', '']",1
['settings as  #TAUTHOR_TAG : we used the 88'],"['settings as  #TAUTHOR_TAG : we used the 887, 310 tweets they collected which were localizable to']","['as  #TAUTHOR_TAG : we used the 887, 310 tweets they collected which were localizable to a specific state']","['main data - collection idea is to use a playful 20 - questions - like survey, automatically generated from a community - based model, which can be widely deployed to acquire training data on individuals.', 'our approach is summarized in figure 1.', 'the first step is to develop an interpretable predictive model that identifies communities that are more overweight than average, in a way that can be converted into fun, engaging natural language questions.', 'to this end, we started with the same settings as  #TAUTHOR_TAG : we used the 887, 310 tweets they collected which were localizable to a specific state and contained at least one relevant hashtag, such as # breakfast or # dinner.', 'each state was assigned a binary label ( more or less overweight than the median ) by comparing the percentage of overweight adults against the median state.', 'for each state, we extracted features based on unigram ( i. e., single ) words and hashtags from all the above tweets localized to the corresponding state.', 'to mitigate sparsity, we also included topics generated using latent dirichlet allocation ( lda )  #AUTHOR_TAG and all tweets collected by fried et al. for example, one of the generated topics contains words that approximate the standard american diet ( e. g., chicken, potatoes, cheese, baked, beans, fried, mac ), which has already been shown to correlate with higher overweight and t2dm rates  #TAUTHOR_TAG figure 2 : a decision tree from the random forest classifier trained using state - level twitter data.', 'motivation for this decision was interpretability : as shown below, decision trees can be easily converted into a series of if... then... else...', 'statements, which form the building blocks of the quiz.', 'to minimize the number of questions, we trained a random forest with 7 trees with maximum depth of 3, and we ignored tokens that appear fewer than 3 times in the training data.', 'these parameter values were selected to make the quiz of reasonable length.', 'we aimed at 20 questions, as in the popular "" 20 questions "" game, in which one player must guess what object the other is thinking of by asking 20 or fewer yes - or - no questions.', 'further tuning confirmed that a small number of shallow trees are most effective in accurately partitioning the state - level data.', 'to further increase the interpretability of the model, word and hashtag counts were automatically discretized into three bins ( e. g., infrequent, somewhat frequent, and very frequent ) based on the quantiles of the training data.', 'figure 2 illustrates one of the']",5
['settings as  #TAUTHOR_TAG : we used the 88'],"['settings as  #TAUTHOR_TAG : we used the 887, 310 tweets they collected which were localizable to']","['as  #TAUTHOR_TAG : we used the 887, 310 tweets they collected which were localizable to a specific state']","['main data - collection idea is to use a playful 20 - questions - like survey, automatically generated from a community - based model, which can be widely deployed to acquire training data on individuals.', 'our approach is summarized in figure 1.', 'the first step is to develop an interpretable predictive model that identifies communities that are more overweight than average, in a way that can be converted into fun, engaging natural language questions.', 'to this end, we started with the same settings as  #TAUTHOR_TAG : we used the 887, 310 tweets they collected which were localizable to a specific state and contained at least one relevant hashtag, such as # breakfast or # dinner.', 'each state was assigned a binary label ( more or less overweight than the median ) by comparing the percentage of overweight adults against the median state.', 'for each state, we extracted features based on unigram ( i. e., single ) words and hashtags from all the above tweets localized to the corresponding state.', 'to mitigate sparsity, we also included topics generated using latent dirichlet allocation ( lda )  #AUTHOR_TAG and all tweets collected by fried et al. for example, one of the generated topics contains words that approximate the standard american diet ( e. g., chicken, potatoes, cheese, baked, beans, fried, mac ), which has already been shown to correlate with higher overweight and t2dm rates  #TAUTHOR_TAG figure 2 : a decision tree from the random forest classifier trained using state - level twitter data.', 'motivation for this decision was interpretability : as shown below, decision trees can be easily converted into a series of if... then... else...', 'statements, which form the building blocks of the quiz.', 'to minimize the number of questions, we trained a random forest with 7 trees with maximum depth of 3, and we ignored tokens that appear fewer than 3 times in the training data.', 'these parameter values were selected to make the quiz of reasonable length.', 'we aimed at 20 questions, as in the popular "" 20 questions "" game, in which one player must guess what object the other is thinking of by asking 20 or fewer yes - or - no questions.', 'further tuning confirmed that a small number of shallow trees are most effective in accurately partitioning the state - level data.', 'to further increase the interpretability of the model, word and hashtag counts were automatically discretized into three bins ( e. g., infrequent, somewhat frequent, and very frequent ) based on the quantiles of the training data.', 'figure 2 illustrates one of the']",5
"[""baseline 50. 89 svm  #TAUTHOR_TAG's best""]","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier :']","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier :']","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier : the first keeps numeric features ( e. g., word counts ) as is, whereas the second discretizes numeric features to three bins.', '']",5
"[""baseline 50. 89 svm  #TAUTHOR_TAG's best""]","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier :']","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier :']","[""baseline 50. 89 svm  #TAUTHOR_TAG's best classifier."", 'we include two versions of our classifier : the first keeps numeric features ( e. g., word counts ) as is, whereas the second discretizes numeric features to three bins.', '']",5
"[', the memory network  #AUTHOR_TAG, neural variational learning  #TAUTHOR_TAG, neural discrete representation  #AUTHOR_TAG van den  #AUTHOR_TAG', ', recurrent ladder']","[', the memory network  #AUTHOR_TAG, neural variational learning  #TAUTHOR_TAG, neural discrete representation  #AUTHOR_TAG van den  #AUTHOR_TAG', ', recurrent ladder']","['of advanced studies which illustrate how', 'deep bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding. in', 'particular, the memory network  #AUTHOR_TAG, neural variational learning  #TAUTHOR_TAG, neural discrete representation  #AUTHOR_TAG van den  #AUTHOR_TAG', ', recurrent ladder network  #AUTHOR_TAG premont -  #AUTHOR_TAG sø']","['', 'deep bayesian learning is developed to infer the sophisticated recurrent models for natural language understanding. in', 'particular, the memory network  #AUTHOR_TAG, neural variational learning  #TAUTHOR_TAG, neural discrete representation  #AUTHOR_TAG van den  #AUTHOR_TAG', ', recurrent ladder network  #AUTHOR_TAG premont -  #AUTHOR_TAG sønderby et al.,', '2016 ), stochastic neural network  #AUTHOR_TAG, markov recurrent neural network  #AUTHOR_TAG, sequence gan  #AUTHOR_TAG', 'and reinforcement learning  #AUTHOR_TAG are introduced in various deep models which', 'open a window to more practical tasks, e. g. reading comprehension, sentence generation, dialogue system, question answering and machine translation. in the final part, we spotlight on some future directions for deep', 'language understanding which can handle the challenges of big data, heterogeneous condition and dynamic system. in particular, deep learning, structural learning, temporal', 'modeling, long history representation and stochastic learning are emphasized. slides of this tutorial are available at http : / / chien. cm. nctu. edu. tw / home / coling /. apsipa 2013,', 'iscslp 2014, interspeech 2013, 2016 and icassp']",0
"['communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the']","['communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the']","['academic communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', '']","['generation ( qg ) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the state - of - the - art models mainly adopt neural approaches by training a neural network based on the sequence - to - sequence framework.', '']",1
"['communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the']","['communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the']","['academic communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', '']","['generation ( qg ) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the state - of - the - art models mainly adopt neural approaches by training a neural network based on the sequence - to - sequence framework.', '']",1
"['communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the']","['communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the']","['academic communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', '']","['generation ( qg ) task, which takes a context and an answer as input and generates a question that targets the given answer, have received tremendous interests in recent years from both industrial and academic communities  #AUTHOR_TAG  #AUTHOR_TAG  #TAUTHOR_TAG.', 'the state - of - the - art models mainly adopt neural approaches by training a neural network based on the sequence - to - sequence framework.', '']",0
"['models.', 'the  #TAUTHOR_TAG, and squad 81k is the setting of  #AUTHOR_TAG.', 'squ']","['bert - sqg models.', 'the  #TAUTHOR_TAG, and squad 81k is the setting of  #AUTHOR_TAG.', 'squad']","['bert - sqg models.', 'the  #TAUTHOR_TAG, and squad 81k is the setting of  #AUTHOR_TAG.', 'squad 73k 73240 11877 10570 squad 81k 81577 8964 8964 pre - trained model uses']","['use the pytorch version of bert 1 to train our bert - qg and bert - sqg models.', 'the  #TAUTHOR_TAG, and squad 81k is the setting of  #AUTHOR_TAG.', 'squad 73k 73240 11877 10570 squad 81k 81577 8964 8964 pre - trained model uses the officially provided bert base model ( 12 layers, 768 hidden dimensions, and 12 attention heads. ) with a vocab of 30522 words.', 'dropout probability is set to 0. 1 between transformer layers.', 'the adamax optimizer is applied during the training process, with an initial learning rate of 5e - 5.', 'the batch size for the update is set at 28.', 'all our models use two titan rtx gpus for 5 epochs training.', 'we use dev.', 'data for epoch model to make predictions and select the highest accuracy rate as our score evaluation model.', 'also, in our bert - sqg model, we use the beam search strategy for sequence decoding.', 'the beam size is set to 3']",0
['on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the'],"['on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', '']","['previous work on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', '']","['squad dataset contains 536 wikipedia articles and around 100k reading comprehension questions ( and the corresponding answers ) posed about the articles.', 'answers of the questions are text spans in the articles.', 'we follow the same data split settings as previous work on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', 'table 1 summarizes some statistics for the compared datasets.', '• squad 73k in this set, we follow the same setting as  #TAUTHOR_TAG ; the accessible parts of the squad training data are randomly divided into a training set ( 80 % ), a development set ( 10 % ), and a test set ( 10 % ).', 'we report results on the 10 % test set.', '• squad 81k in this set, we follow the same setting as  #AUTHOR_TAG ; the accessible squad development data set is divided into a development set ( 50 % ), and a test set ( 50 % )']",5
['on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the'],"['on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', '']","['previous work on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', '']","['squad dataset contains 536 wikipedia articles and around 100k reading comprehension questions ( and the corresponding answers ) posed about the articles.', 'answers of the questions are text spans in the articles.', 'we follow the same data split settings as previous work on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', 'table 1 summarizes some statistics for the compared datasets.', '• squad 73k in this set, we follow the same setting as  #TAUTHOR_TAG ; the accessible parts of the squad training data are randomly divided into a training set ( 80 % ), a development set ( 10 % ), and a test set ( 10 % ).', 'we report results on the 10 % test set.', '• squad 81k in this set, we follow the same setting as  #AUTHOR_TAG ; the accessible squad development data set is divided into a development set ( 50 % ), and a test set ( 50 % )']",5
['models  #TAUTHOR_TAG  #AUTHOR_TAG in the'],"['models  #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nqg - rc  #TAUTHOR_TAG : a seq2seq']","[' #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nq']","['this paper, we compare our models with the best performing models  #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nqg - rc  #TAUTHOR_TAG : a seq2seq question generation model based on bidirectional lstm.', '• plqg  #AUTHOR_TAG : a seq2seq network which contains a gated self - attention encoder and a maxout pointer decoder to enable the capability of handling long text input.', 'plqg model is the state - of - the - art models for qg tasks.', 'table 2 shows the comparison results using sentence - level context and table 3 shows the results on paragraph level context.', 'we compare the models using standard metric bleu and rouge - l (  #AUTHOR_TAG ).', 'we have the following findings to note about the results.', 'first, as can be observed, bert - qg offers poor performance.', 'in fact, the performance of bert - qg is far from the results by other models.', 'this result is expected as bert - qg generates the sentences without considering the previous decoded results.', 'however, when taking into account the previous decoded results ( bert - sqg ), we effectively utilize the power of bert and yield the state - of - the - art result compared with the existing rnn variants for qg.', 'as shown in table 2, bert - sqg outperforms the existing best performing model by 2 % on both benchmark datasets']",5
['on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the'],"['on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', '']","['previous work on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', '']","['squad dataset contains 536 wikipedia articles and around 100k reading comprehension questions ( and the corresponding answers ) posed about the articles.', 'answers of the questions are text spans in the articles.', 'we follow the same data split settings as previous work on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', 'table 1 summarizes some statistics for the compared datasets.', '• squad 73k in this set, we follow the same setting as  #TAUTHOR_TAG ; the accessible parts of the squad training data are randomly divided into a training set ( 80 % ), a development set ( 10 % ), and a test set ( 10 % ).', 'we report results on the 10 % test set.', '• squad 81k in this set, we follow the same setting as  #AUTHOR_TAG ; the accessible squad development data set is divided into a development set ( 50 % ), and a test set ( 50 % )']",3
['on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the'],"['on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', '']","['previous work on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', '']","['squad dataset contains 536 wikipedia articles and around 100k reading comprehension questions ( and the corresponding answers ) posed about the articles.', 'answers of the questions are text spans in the articles.', 'we follow the same data split settings as previous work on the qg tasks  #TAUTHOR_TAG  #AUTHOR_TAG to directly compare the state - of - theart results on qg tasks.', 'table 1 summarizes some statistics for the compared datasets.', '• squad 73k in this set, we follow the same setting as  #TAUTHOR_TAG ; the accessible parts of the squad training data are randomly divided into a training set ( 80 % ), a development set ( 10 % ), and a test set ( 10 % ).', 'we report results on the 10 % test set.', '• squad 81k in this set, we follow the same setting as  #AUTHOR_TAG ; the accessible squad development data set is divided into a development set ( 50 % ), and a test set ( 50 % )']",3
['models  #TAUTHOR_TAG  #AUTHOR_TAG in the'],"['models  #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nqg - rc  #TAUTHOR_TAG : a seq2seq']","[' #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nq']","['this paper, we compare our models with the best performing models  #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nqg - rc  #TAUTHOR_TAG : a seq2seq question generation model based on bidirectional lstm.', '• plqg  #AUTHOR_TAG : a seq2seq network which contains a gated self - attention encoder and a maxout pointer decoder to enable the capability of handling long text input.', 'plqg model is the state - of - the - art models for qg tasks.', 'table 2 shows the comparison results using sentence - level context and table 3 shows the results on paragraph level context.', 'we compare the models using standard metric bleu and rouge - l (  #AUTHOR_TAG ).', 'we have the following findings to note about the results.', 'first, as can be observed, bert - qg offers poor performance.', 'in fact, the performance of bert - qg is far from the results by other models.', 'this result is expected as bert - qg generates the sentences without considering the previous decoded results.', 'however, when taking into account the previous decoded results ( bert - sqg ), we effectively utilize the power of bert and yield the state - of - the - art result compared with the existing rnn variants for qg.', 'as shown in table 2, bert - sqg outperforms the existing best performing model by 2 % on both benchmark datasets']",3
['models  #TAUTHOR_TAG  #AUTHOR_TAG in the'],"['models  #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nqg - rc  #TAUTHOR_TAG : a seq2seq']","[' #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nq']","['this paper, we compare our models with the best performing models  #TAUTHOR_TAG  #AUTHOR_TAG in the literature.', 'the compared models in the experiment are :', '• nqg - rc  #TAUTHOR_TAG : a seq2seq question generation model based on bidirectional lstm.', '• plqg  #AUTHOR_TAG : a seq2seq network which contains a gated self - attention encoder and a maxout pointer decoder to enable the capability of handling long text input.', 'plqg model is the state - of - the - art models for qg tasks.', 'table 2 shows the comparison results using sentence - level context and table 3 shows the results on paragraph level context.', 'we compare the models using standard metric bleu and rouge - l (  #AUTHOR_TAG ).', 'we have the following findings to note about the results.', 'first, as can be observed, bert - qg offers poor performance.', 'in fact, the performance of bert - qg is far from the results by other models.', 'this result is expected as bert - qg generates the sentences without considering the previous decoded results.', 'however, when taking into account the previous decoded results ( bert - sqg ), we effectively utilize the power of bert and yield the state - of - the - art result compared with the existing rnn variants for qg.', 'as shown in table 2, bert - sqg outperforms the existing best performing model by 2 % on both benchmark datasets']",3
['in memory. in  #TAUTHOR_TAG we use'],['in memory. in  #TAUTHOR_TAG we use a'],['in memory. in  #TAUTHOR_TAG we use'],"['', 'be stored in memory. in  #TAUTHOR_TAG we use a cluster of 45 machines, together with a parallel implementation of the bfgs training', 'algorithm, to solve this problem. the need for cluster computing resources presents a barrier to the development of further ccg parsing models.  #AUTHOR_TAG describe', 'a generative model for ccg, which only requires a non - iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. in this paper we propose the perceptron algorithm as a solution', '. the perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. however, the key difference compared with the loglinear training is that the perceptron converges in many', 'fewer iterations, and so it is practical to read the training instances into memory one at a time. the difficulty in using the perceptron for training phrase -', 'structure parsing models is the need for an efficient decoder ( since perceptron training essentially is decoding ).', 'here we exploit the lexicalized nature of ccg by using the supertag', '##ger to restrict the size of the charts over which viterbi decoding', 'is performed, resulting in an extremely effcient decoder. in fact, the decoding is so fast that we can estimate a state - of - the - art discriminative', 'parsing model in only a few hours on a single machine']",0
['in memory. in  #TAUTHOR_TAG we use'],['in memory. in  #TAUTHOR_TAG we use a'],['in memory. in  #TAUTHOR_TAG we use'],"['', 'be stored in memory. in  #TAUTHOR_TAG we use a cluster of 45 machines, together with a parallel implementation of the bfgs training', 'algorithm, to solve this problem. the need for cluster computing resources presents a barrier to the development of further ccg parsing models.  #AUTHOR_TAG describe', 'a generative model for ccg, which only requires a non - iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. in this paper we propose the perceptron algorithm as a solution', '. the perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. however, the key difference compared with the loglinear training is that the perceptron converges in many', 'fewer iterations, and so it is practical to read the training instances into memory one at a time. the difficulty in using the perceptron for training phrase -', 'structure parsing models is the need for an efficient decoder ( since perceptron training essentially is decoding ).', 'here we exploit the lexicalized nature of ccg by using the supertag', '##ger to restrict the size of the charts over which viterbi decoding', 'is performed, resulting in an extremely effcient decoder. in fact, the decoding is so fast that we can estimate a state - of - the - art discriminative', 'parsing model in only a few hours on a single machine']",0
['in memory. in  #TAUTHOR_TAG we use'],['in memory. in  #TAUTHOR_TAG we use a'],['in memory. in  #TAUTHOR_TAG we use'],"['', 'be stored in memory. in  #TAUTHOR_TAG we use a cluster of 45 machines, together with a parallel implementation of the bfgs training', 'algorithm, to solve this problem. the need for cluster computing resources presents a barrier to the development of further ccg parsing models.  #AUTHOR_TAG describe', 'a generative model for ccg, which only requires a non - iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. in this paper we propose the perceptron algorithm as a solution', '. the perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. however, the key difference compared with the loglinear training is that the perceptron converges in many', 'fewer iterations, and so it is practical to read the training instances into memory one at a time. the difficulty in using the perceptron for training phrase -', 'structure parsing models is the need for an efficient decoder ( since perceptron training essentially is decoding ).', 'here we exploit the lexicalized nature of ccg by using the supertag', '##ger to restrict the size of the charts over which viterbi decoding', 'is performed, resulting in an extremely effcient decoder. in fact, the decoding is so fast that we can estimate a state - of - the - art discriminative', 'parsing model in only a few hours on a single machine']",0
['in memory. in  #TAUTHOR_TAG we use'],['in memory. in  #TAUTHOR_TAG we use a'],['in memory. in  #TAUTHOR_TAG we use'],"['', 'be stored in memory. in  #TAUTHOR_TAG we use a cluster of 45 machines, together with a parallel implementation of the bfgs training', 'algorithm, to solve this problem. the need for cluster computing resources presents a barrier to the development of further ccg parsing models.  #AUTHOR_TAG describe', 'a generative model for ccg, which only requires a non - iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. in this paper we propose the perceptron algorithm as a solution', '. the perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. however, the key difference compared with the loglinear training is that the perceptron converges in many', 'fewer iterations, and so it is practical to read the training instances into memory one at a time. the difficulty in using the perceptron for training phrase -', 'structure parsing models is the need for an efficient decoder ( since perceptron training essentially is decoding ).', 'here we exploit the lexicalized nature of ccg by using the supertag', '##ger to restrict the size of the charts over which viterbi decoding', 'is performed, resulting in an extremely effcient decoder. in fact, the decoding is so fast that we can estimate a state - of - the - art discriminative', 'parsing model in only a few hours on a single machine']",0
"['with only the feature information retained ( see  #AUTHOR_TAG and  #TAUTHOR_TAG for the details ).', 'dynamic programming algorithms can be used with the feature forests']","['with only the feature information retained ( see  #AUTHOR_TAG and  #TAUTHOR_TAG for the details ).', 'dynamic programming algorithms can be used with the feature forests']","['', 'a feature forest is essentially a packed chart with only the feature information retained ( see  #AUTHOR_TAG and  #TAUTHOR_TAG for the details ).', 'dynamic programming algorithms can be used with the feature forests']","['same decoder is used for both training and testing : the viterbi algorithm.', 'however, the packed representation of gen ( x ) in each case is different.', 'when running the parser, a lot of grammatical information is stored in order to produce linguistically meaningful output.', 'for training, all that is required is a packed representation of the features on each derivation in gen ( x ) for each sentence in the training data.', 'the feature forests described in  #AUTHOR_TAG provide such a representation.', ' #AUTHOR_TAG b ) describe how a set of ccg derivations can be represented as a feature forest.', 'the feature forests are created by first building packed charts for the training sentences, and then extracting the feature information.', 'packed charts group together equivalent chart entries.', 'entries are equivalent when they interact in the same manner with both the generation of subsequent parse structure and the numerical parse selection.', 'in practice, this means that equivalent entries have the same span, and form the same structures and generate the same features in any further parsing of the sentence.', 'back pointers to the daughters indicate how an individual entry was created, so that any derivation can be recovered from the chart.', 'a feature forest is essentially a packed chart with only the feature information retained ( see  #AUTHOR_TAG and  #TAUTHOR_TAG for the details ).', 'dynamic programming algorithms can be used with the feature forests for efficient estimation.', 'for the log - linear parsing model in  #TAUTHOR_TAG, the inside - outside algorithm is used to calculate feature expectations, which are then used by the bfgs algorithm to optimise the likelihood function.', 'for the perceptron, the viterbi algorithm finds the features corresponding to the highest scoring derivation, which are then used in a simple additive update process']",0
"['with only the feature information retained ( see  #AUTHOR_TAG and  #TAUTHOR_TAG for the details ).', 'dynamic programming algorithms can be used with the feature forests']","['with only the feature information retained ( see  #AUTHOR_TAG and  #TAUTHOR_TAG for the details ).', 'dynamic programming algorithms can be used with the feature forests']","['', 'a feature forest is essentially a packed chart with only the feature information retained ( see  #AUTHOR_TAG and  #TAUTHOR_TAG for the details ).', 'dynamic programming algorithms can be used with the feature forests']","['same decoder is used for both training and testing : the viterbi algorithm.', 'however, the packed representation of gen ( x ) in each case is different.', 'when running the parser, a lot of grammatical information is stored in order to produce linguistically meaningful output.', 'for training, all that is required is a packed representation of the features on each derivation in gen ( x ) for each sentence in the training data.', 'the feature forests described in  #AUTHOR_TAG provide such a representation.', ' #AUTHOR_TAG b ) describe how a set of ccg derivations can be represented as a feature forest.', 'the feature forests are created by first building packed charts for the training sentences, and then extracting the feature information.', 'packed charts group together equivalent chart entries.', 'entries are equivalent when they interact in the same manner with both the generation of subsequent parse structure and the numerical parse selection.', 'in practice, this means that equivalent entries have the same span, and form the same structures and generate the same features in any further parsing of the sentence.', 'back pointers to the daughters indicate how an individual entry was created, so that any derivation can be recovered from the chart.', 'a feature forest is essentially a packed chart with only the feature information retained ( see  #AUTHOR_TAG and  #TAUTHOR_TAG for the details ).', 'dynamic programming algorithms can be used with the feature forests for efficient estimation.', 'for the log - linear parsing model in  #TAUTHOR_TAG, the inside - outside algorithm is used to calculate feature expectations, which are then used by the bfgs algorithm to optimise the likelihood function.', 'for the perceptron, the viterbi algorithm finds the features corresponding to the highest scoring derivation, which are then used in a simple additive update process']",0
['in memory. in  #TAUTHOR_TAG we use'],['in memory. in  #TAUTHOR_TAG we use a'],['in memory. in  #TAUTHOR_TAG we use'],"['', 'be stored in memory. in  #TAUTHOR_TAG we use a cluster of 45 machines, together with a parallel implementation of the bfgs training', 'algorithm, to solve this problem. the need for cluster computing resources presents a barrier to the development of further ccg parsing models.  #AUTHOR_TAG describe', 'a generative model for ccg, which only requires a non - iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. in this paper we propose the perceptron algorithm as a solution', '. the perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. however, the key difference compared with the loglinear training is that the perceptron converges in many', 'fewer iterations, and so it is practical to read the training instances into memory one at a time. the difficulty in using the perceptron for training phrase -', 'structure parsing models is the need for an efficient decoder ( since perceptron training essentially is decoding ).', 'here we exploit the lexicalized nature of ccg by using the supertag', '##ger to restrict the size of the charts over which viterbi decoding', 'is performed, resulting in an extremely effcient decoder. in fact, the decoding is so fast that we can estimate a state - of - the - art discriminative', 'parsing model in only a few hours on a single machine']",5
"['feature representation φ ( x, y ) as in  #TAUTHOR_TAG, to allow comparison with the log - linear model.', 'the features are defined in terms of local subtrees in']","['feature representation φ ( x, y ) as in  #TAUTHOR_TAG, to allow comparison with the log - linear model.', 'the features are defined in terms of local subtrees in']","['the argmax in ( 1 ).', 'in this paper, y is the set of possible ccg derivations and gen ( x ) enumerates the set of derivations for sentence x. we use the same feature representation φ ( x, y ) as in  #TAUTHOR_TAG, to allow comparison with the log - linear model.', 'the features are defined in terms of local subtrees in the derivation, consisting of a parent category plus']","['parsing problem is to find a mapping from a set of sentences x ∈ x to a set of parses y ∈ y.', 'we assume that the mapping f is represented through a feature vector φ ( x, y ) ∈ r d and a parameter vector α ∈ r d in the following way  #AUTHOR_TAG :', 'where gen ( x ) denotes the set of possible parses for sentence x and φ ( x, y ) · α = i α i φ i ( x, y ) is the inner product.', 'the learning task is to set the parameter values ( the feature weights ) using the training set as evidence, where the training set consists of examples ( x i, y i ) for 1 ≤ i ≤ n.', 'the decoder is an algorithm which finds the argmax in ( 1 ).', 'in this paper, y is the set of possible ccg derivations and gen ( x ) enumerates the set of derivations for sentence x. we use the same feature representation φ ( x, y ) as in  #TAUTHOR_TAG, to allow comparison with the log - linear model.', 'the features are defined in terms of local subtrees in the derivation, consisting of a parent category plus one or two children.', 'some features are lexicalized, encoding word - word dependencies.', 'features are integervalued, counting the number of times some configuration occurs in a derivation.', 'gen ( x ) is defined by the ccg grammar, plus the supertagger, since the supertagger determines how many lexical categories are assigned to each word in x ( through the β parameter ).', 'rather than try to recreate the adaptive supertagging described in section 2 for training, we simply fix the the value of β so that gen ( x ) is the set of derivations licenced by the grammar for sentence x, given that value.', 'β is now a parameter of the training process which we determine experimentally using development data.', 'the β parameter can be thought of as determining the set of incorrect derivations which the training algorithm uses to "" discriminate against "", with a smaller value of β resulting in more derivations']",5
"['- scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG,']","['- scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG,']","['of the sentences in section 00.', 'the f - scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG, accuracy is']","['', 'were used for all the experiments. the parser provides an analysis for 99. 37 % of the sentences in section 00.', 'the f - scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG, accuracy is measured using f - score over the goldstandard predicate - argument dependencies in ccgbank. the table shows that the accuracy increases initially with', 'the number of iterations, but converges quickly after only 4 iterations. the accuracy after only one iteration is also surprisingly high. table 3 compares the accuracy of the percept', '##ron and log - linear models on the development data. lp is labelled precision, lr is labelled', 'recall, and cat is the lexical category accuracy. the same feature forests were', 'used for training the perceptron and log - linear models, and the same parser and decoding algorithm were used for', 'testing, so the results for the two models are directly comparable. the only difference in each case was the weights file used. table 3 : comparison of the perceptron and loglinear models on the', 'development data forest creation ( with the number of training iterations again optimised on the development data ). a smaller β', 'value results in larger forests, giving more incorrect derivations for the training algorithm to "" discriminate against "". increasing the size of', 'the forests is no problem for the perceptron, since the memory requirements are so modest, but this would cause problems for the', 'log - linear training which is already highly memory intensive. the table shows that increasing the', 'number of incorrect derivations gives a small improvement in performance for the perceptron. table 4 gives the accuracies for the', 'two models on the test data, section 23 of ccgb', '##ank. here the coverage of the parser is 99. 63 %, and again the accuracies are computed only for the sentences', 'with an analysis. the figures for the averaged perceptron were obtained using 6 iterations, with β = 0. 002. the perceptron', 'slightly outperforms the log - linear model ( although we have not carried out significance tests ). we justify the use of different β', 'values for the two models by arguing that the perceptron is much more flexible in terms of the size of the training forests it can handle']",5
"['- scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG,']","['- scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG,']","['of the sentences in section 00.', 'the f - scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG, accuracy is']","['', 'were used for all the experiments. the parser provides an analysis for 99. 37 % of the sentences in section 00.', 'the f - scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG, accuracy is measured using f - score over the goldstandard predicate - argument dependencies in ccgbank. the table shows that the accuracy increases initially with', 'the number of iterations, but converges quickly after only 4 iterations. the accuracy after only one iteration is also surprisingly high. table 3 compares the accuracy of the percept', '##ron and log - linear models on the development data. lp is labelled precision, lr is labelled', 'recall, and cat is the lexical category accuracy. the same feature forests were', 'used for training the perceptron and log - linear models, and the same parser and decoding algorithm were used for', 'testing, so the results for the two models are directly comparable. the only difference in each case was the weights file used. table 3 : comparison of the perceptron and loglinear models on the', 'development data forest creation ( with the number of training iterations again optimised on the development data ). a smaller β', 'value results in larger forests, giving more incorrect derivations for the training algorithm to "" discriminate against "". increasing the size of', 'the forests is no problem for the perceptron, since the memory requirements are so modest, but this would cause problems for the', 'log - linear training which is already highly memory intensive. the table shows that increasing the', 'number of incorrect derivations gives a small improvement in performance for the perceptron. table 4 gives the accuracies for the', 'two models on the test data, section 23 of ccgb', '##ank. here the coverage of the parser is 99. 63 %, and again the accuracies are computed only for the sentences', 'with an analysis. the figures for the averaged perceptron were obtained using 6 iterations, with β = 0. 002. the perceptron', 'slightly outperforms the log - linear model ( although we have not carried out significance tests ). we justify the use of different β', 'values for the two models by arguing that the perceptron is much more flexible in terms of the size of the training forests it can handle']",5
['in memory. in  #TAUTHOR_TAG we use'],['in memory. in  #TAUTHOR_TAG we use a'],['in memory. in  #TAUTHOR_TAG we use'],"['', 'be stored in memory. in  #TAUTHOR_TAG we use a cluster of 45 machines, together with a parallel implementation of the bfgs training', 'algorithm, to solve this problem. the need for cluster computing resources presents a barrier to the development of further ccg parsing models.  #AUTHOR_TAG describe', 'a generative model for ccg, which only requires a non - iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. in this paper we propose the perceptron algorithm as a solution', '. the perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. however, the key difference compared with the loglinear training is that the perceptron converges in many', 'fewer iterations, and so it is practical to read the training instances into memory one at a time. the difficulty in using the perceptron for training phrase -', 'structure parsing models is the need for an efficient decoder ( since perceptron training essentially is decoding ).', 'here we exploit the lexicalized nature of ccg by using the supertag', '##ger to restrict the size of the charts over which viterbi decoding', 'is performed, resulting in an extremely effcient decoder. in fact, the decoding is so fast that we can estimate a state - of - the - art discriminative', 'parsing model in only a few hours on a single machine']",1
['in memory. in  #TAUTHOR_TAG we use'],['in memory. in  #TAUTHOR_TAG we use a'],['in memory. in  #TAUTHOR_TAG we use'],"['', 'be stored in memory. in  #TAUTHOR_TAG we use a cluster of 45 machines, together with a parallel implementation of the bfgs training', 'algorithm, to solve this problem. the need for cluster computing resources presents a barrier to the development of further ccg parsing models.  #AUTHOR_TAG describe', 'a generative model for ccg, which only requires a non - iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. in this paper we propose the perceptron algorithm as a solution', '. the perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. however, the key difference compared with the loglinear training is that the perceptron converges in many', 'fewer iterations, and so it is practical to read the training instances into memory one at a time. the difficulty in using the perceptron for training phrase -', 'structure parsing models is the need for an efficient decoder ( since perceptron training essentially is decoding ).', 'here we exploit the lexicalized nature of ccg by using the supertag', '##ger to restrict the size of the charts over which viterbi decoding', 'is performed, resulting in an extremely effcient decoder. in fact, the decoding is so fast that we can estimate a state - of - the - art discriminative', 'parsing model in only a few hours on a single machine']",1
"['- scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG,']","['- scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG,']","['of the sentences in section 00.', 'the f - scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG, accuracy is']","['', 'were used for all the experiments. the parser provides an analysis for 99. 37 % of the sentences in section 00.', 'the f - scores are based only on the sentences for which there is an analysis. following  #TAUTHOR_TAG, accuracy is measured using f - score over the goldstandard predicate - argument dependencies in ccgbank. the table shows that the accuracy increases initially with', 'the number of iterations, but converges quickly after only 4 iterations. the accuracy after only one iteration is also surprisingly high. table 3 compares the accuracy of the percept', '##ron and log - linear models on the development data. lp is labelled precision, lr is labelled', 'recall, and cat is the lexical category accuracy. the same feature forests were', 'used for training the perceptron and log - linear models, and the same parser and decoding algorithm were used for', 'testing, so the results for the two models are directly comparable. the only difference in each case was the weights file used. table 3 : comparison of the perceptron and loglinear models on the', 'development data forest creation ( with the number of training iterations again optimised on the development data ). a smaller β', 'value results in larger forests, giving more incorrect derivations for the training algorithm to "" discriminate against "". increasing the size of', 'the forests is no problem for the perceptron, since the memory requirements are so modest, but this would cause problems for the', 'log - linear training which is already highly memory intensive. the table shows that increasing the', 'number of incorrect derivations gives a small improvement in performance for the perceptron. table 4 gives the accuracies for the', 'two models on the test data, section 23 of ccgb', '##ank. here the coverage of the parser is 99. 63 %, and again the accuracies are computed only for the sentences', 'with an analysis. the figures for the averaged perceptron were obtained using 6 iterations, with β = 0. 002. the perceptron', 'slightly outperforms the log - linear model ( although we have not carried out significance tests ). we justify the use of different β', 'values for the two models by arguing that the perceptron is much more flexible in terms of the size of the training forests it can handle']",4
"['in domain summaries, similar to  #TAUTHOR_TAG']","['in domain summaries, similar to  #TAUTHOR_TAG']","['in domain summaries, similar to  #TAUTHOR_TAG.', 'however, we do not rely on any source of external knowledge']","['', 'paraphrase templates containing concepts and typical strings were induced from comparable sentences in  #AUTHOR_TAG using multisentence alignment to discover "" variable "" and fixed structures.', 'linguistic patterns were applied to huge amounts of non - annotated pre - classified texts in  #AUTHOR_TAG to bootstrap information extraction patterns.', 'similarly, semi - supervised or unsupervised methods have been used to learn question / answering patterns  #AUTHOR_TAG or text schemas  #AUTHOR_TAG.', 'one current paradigm to learn from raw data is open information extraction  #AUTHOR_TAG, which without any prior knowledge aims at discovering all possible relations between pairs of entities occurring in text.', 'our work tries to learn the main concepts making up the template structure in domain summaries, similar to  #TAUTHOR_TAG.', 'however, we do not rely on any source of external knowledge']",3
[' #TAUTHOR_TAG has addressed'],[' #TAUTHOR_TAG has addressed'],[' #TAUTHOR_TAG has addressed'],"['', 'in the aviation domain, for example, numeric expressions constitute the extensions of different concepts including : number of victims, crew members, and number of survivors ; it is a rather common feature in the aviation domain to include these different concepts together in one sentence, making their "" separation "" complicated.', 'same explanations apply to other tested domains : for example locations playing the role of origin and destination of a given train or airplace are also sometimes confused.', 'our work demonstrates the possibility of learning conceptual information in several domains and languages, while previous work  #TAUTHOR_TAG has addressed sets of related domains ( e. g., muc - 4 templates ) in english.', 'learning full conceptualizations from raw data is a daunting and difficult enterprise  #AUTHOR_TAG.', 'here, we provide a short - cut by proposing a method able to learn the essential concepts of a domain by relying on summaries which are freely available on the web.', 'our method is able to produce conceptualizations from a few documents in each domain and language unlike recent open domain information extraction which requires massive amount of texts for relation learning  #AUTHOR_TAG.', 'our algorithm has a reasonable computational complexity, unlike alignment - based or clustering - based approaches  #AUTHOR_TAG, which are computationally expensive']",4
['transformer models  #TAUTHOR_TAG using sockey'],"['transformer models  #TAUTHOR_TAG using sockeye 1  #AUTHOR_TAG.', 'compared to rnn - based translation models  #AUTHOR_TAG, transformer models can be trained very fast due to parallelizable self - attention networks.', 'we applied several very useful techniques for effectively training our models']","['##nglish directions.', 'we trained transformer models  #TAUTHOR_TAG using sockeye 1  #AUTHOR_TAG.', 'compared to rn']","['paper describes the dfki - nmt submission to the wmt19 news translation task.', 'we participated in both english - to - german and german - toenglish directions.', 'we trained transformer models  #TAUTHOR_TAG using sockeye 1  #AUTHOR_TAG.', 'compared to rnn - based translation models  #AUTHOR_TAG, transformer models can be trained very fast due to parallelizable self - attention networks.', 'we applied several very useful techniques for effectively training our models']",5
['back - translating monolingual data are baseline transformers  #TAUTHOR_TAG trained on the bilingual'],['back - translating monolingual data are baseline transformers  #TAUTHOR_TAG trained on the bilingual'],"['back - translating monolingual data are baseline transformers  #TAUTHOR_TAG trained on the bilingual data after data selection as described before.', 'during back - translation, we used greedy search instead of beam search for efficiency']","['back - translated the 2018 part of the large monolingual in - domain news crawl data as additional training data for our translation systems.', ' #AUTHOR_TAG showed that it is more beneficial to back - translate sentences that contain difficult words.', 'in our experiments, we consider words which occur less than 1000 times in the bilingual training data as difficult words.', 'then we randomly selected 10m sentences which contain difficult words for back - translation.', 'the mod - els used for back - translating monolingual data are baseline transformers  #TAUTHOR_TAG trained on the bilingual data after data selection as described before.', 'during back - translation, we used greedy search instead of beam search for efficiency']",5
"[""as the baseline transformer in  #TAUTHOR_TAG's work."", 'for transformerbig,']","[""as the baseline transformer in  #TAUTHOR_TAG's work."", 'for transformerbig,']","[""as the baseline transformer in  #TAUTHOR_TAG's work."", 'for transformerbig, we changed word embedding size into 1024 and kept other parameters unchanged.', 'a joint vocabulary of 50k for german and english']","['trained two transformer models for each translation task as transformer - base and transformer - big.', ""the settings of transformerbase is the same as the baseline transformer in  #TAUTHOR_TAG's work."", 'for transformerbig, we changed word embedding size into 1024 and kept other parameters unchanged.', 'a joint vocabulary of 50k for german and english is learned by byte pair encoding ( bpe )  #AUTHOR_TAG b ).', '2 we set dropout to 0. 1 for both transformer - base and transformer - big.', 'we used adam  #AUTHOR_TAG for optimization.', 'we used newstest2018 as the validation set for model training.', '']",5
['model  #TAUTHOR_TAG and we removed the encoder and the attention layer in'],['model  #TAUTHOR_TAG and we removed the encoder and the attention layer in'],"['fast.', 'figure 1 ( a ) shows the structure of the standard transformer translation model  #TAUTHOR_TAG and we removed the encoder and the attention layer in the decoder from the transformer translation model to create our transformer language model as shown in figure 1 ( b ).', 'for training efficiency, we used byte pair encoding  #AUTHOR_TAG b ) to learn a vocabulary of 50k for english and german respectively']","['', 'equation 1 is used to score each sentence pair in the out - ofdomain corpus.', 'in equation 1, p s is the language model probability of the source sentence ; n s is the length of the source sentence ; p t is the language model probability of the target sentence ; n t is the length of the target sentence.', 'we selected the top 15m scored sentence pairs from out - ofdomain data for training our systems.', 'the neural language models trained for data selection in our experiments are based on selfattention networks which can be trained very fast.', 'figure 1 ( a ) shows the structure of the standard transformer translation model  #TAUTHOR_TAG and we removed the encoder and the attention layer in the decoder from the transformer translation model to create our transformer language model as shown in figure 1 ( b ).', 'for training efficiency, we used byte pair encoding  #AUTHOR_TAG b ) to learn a vocabulary of 50k for english and german respectively']",6
"[""as the baseline transformer in  #TAUTHOR_TAG's work."", 'for transformerbig,']","[""as the baseline transformer in  #TAUTHOR_TAG's work."", 'for transformerbig,']","[""as the baseline transformer in  #TAUTHOR_TAG's work."", 'for transformerbig, we changed word embedding size into 1024 and kept other parameters unchanged.', 'a joint vocabulary of 50k for german and english']","['trained two transformer models for each translation task as transformer - base and transformer - big.', ""the settings of transformerbase is the same as the baseline transformer in  #TAUTHOR_TAG's work."", 'for transformerbig, we changed word embedding size into 1024 and kept other parameters unchanged.', 'a joint vocabulary of 50k for german and english is learned by byte pair encoding ( bpe )  #AUTHOR_TAG b ).', '2 we set dropout to 0. 1 for both transformer - base and transformer - big.', 'we used adam  #AUTHOR_TAG for optimization.', 'we used newstest2018 as the validation set for model training.', '']",3
['three language pairs  #TAUTHOR_TAG'],['three language pairs  #TAUTHOR_TAG'],"['three language pairs  #TAUTHOR_TAG.', 'our bli model based on our novel bwes significantly']","['', 'however, all these models critically require at least sentence - aligned parallel data and / or readilyavailable translation dictionaries to induce bilingual word embeddings ( bwes ) that are consistent and closely aligned over languages in the same semantic space.', 'contributions in this work, we alleviate the requirements : ( 1 ) we present the first model that is able to induce bilingual word embeddings from non - parallel data without any other readily available translation resources such as pre - given bilingual lexicons ; ( 2 ) we demonstrate the utility of bwes induced by this simple yet effective model in the bli task from comparable wikipedia data on benchmarking datasets for three language pairs  #TAUTHOR_TAG.', 'our bli model based on our novel bwes significantly outperforms a series of strong baselines that reported previous best scores on these datasets in the same learning setting, as well as other bli models based on recently proposed bwe induction models  #AUTHOR_TAG.', 'the focus of the work is on learning lexicons from documentaligned comparable corpora ( e. g., wikipedia articles aligned through inter - wiki links ).', 'the architecture of our bwe skip - gram model for learning bilingual word embeddings from document - aligned comparable data.', 'source language words and documents are drawn as gray boxes, while target language words and documents are drawn as blue boxes.', 'the right side of the']",3
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",3
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",3
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",3
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",3
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",4
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",6
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",5
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",5
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",0
[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],[';  #TAUTHOR_TAG. all parameters'],"['topics  #AUTHOR_TAG by the bilingual lda model and represents words as probability distributions over these topics ( vulic et al., 2011 ). ( 2 ) assoc - bli - a bli model that represents words as vectors of association norms ( roller and schulte im  #AUTHOR_TAG over both vocabularies, where these norms are computed using a multilingual topic model ( vulic and  #AUTHOR_TAG a ). ( 3 ) ppmi + cos - a standard distributional model for bl', '##i relying on positive pointwise mutual information and cosine similarity  #AUTHOR_TAG. the seed lexicon is bootstrapped using the method from ( peirsman and', 'pado, 2011 ;  #TAUTHOR_TAG. all parameters of the baseline bli models ( i. e', '., topic models and their settings, the number of dimensions k,', 'feature pruning values, window size ) are set to their optimal values', 'according to suggestions in prior work  #AUTHOR_TAG vulic and  #AUTHOR_TAG a ;  #TAUTHOR_TAG. due to space constraints, for ( much ) more', 'details about the baselines we point to the relevant literature ( peirsman and pado, 2011 ;  #AUTHOR_TAG vulic and', '']",0
['##ned multilingual datasets as proposed in  #AUTHOR_TAG b ;  #TAUTHOR_TAG'],['from large unaligned multilingual datasets as proposed in  #AUTHOR_TAG b ;  #TAUTHOR_TAG'],"['representation learning from large unaligned multilingual datasets as proposed in  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'in the long run, this idea may lead to large - scale fully data - driven representation learning models from huge amounts of multilingual data without any "" pre - requirement "" for parallel data or manually built lexicons']","['have proposed bilingual word embeddings skip - gram ( bwesg ), a simple yet effective model that is able to learn bilingual word embeddings solely on the basis of document - aligned comparable data.', 'we have demonstrated its utility in the task of bilingual lexicon induction from such comparable data, where our new bwesg - based bli model outperforms state - of - the - art models for bli from document - aligned comparable data and related bwe induction models.', 'the low - cost bwes may be used in other ( semantic ) tasks besides the ones discussed here, and it would be interesting to experiment with other types of context aggregation and selection beyond random shuffling, and other objective functions.', 'preliminary studies also demonstrate the utility of the bwes in monolingual and cross - lingual information retrieval ( vulic and  #AUTHOR_TAG.', 'finally, we may use the knowledge of bwes obtained by bwesg from document - aligned data to learn bilingual correspondences ( e. g., word translation pairs or lists of semantically similar words across languages ) which may in turn be used for representation learning from large unaligned multilingual datasets as proposed in  #AUTHOR_TAG b ;  #TAUTHOR_TAG.', 'in the long run, this idea may lead to large - scale fully data - driven representation learning models from huge amounts of multilingual data without any "" pre - requirement "" for parallel data or manually built lexicons']",1
"['operation is applied repeatedly to all data points.', 'natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.', 'recently,  #TAUTHOR_TAG presented an']","['operation is applied repeatedly to all data points.', 'natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.', 'recently,  #TAUTHOR_TAG presented an']","['origin in computer graphics, graphics processing units ( gpus ) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points.', 'natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.', 'recently,  #TAUTHOR_TAG presented an approach to gpu parsing that']","['to their origin in computer graphics, graphics processing units ( gpus ) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points.', 'natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.', 'recently,  #TAUTHOR_TAG presented an approach to gpu parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute viterbi parses for a high - quality grammar at about 164 sentences per second on a mid - range gpu.', 'in this work, we reintroduce sparsity to gpu parsing by adapting a coarse - to - fine pruning approach to the constraints of a gpu.', 'the resulting system is capable of computing over 404 viterbi parses per second - more than a 2x speedup - on the same hardware.', 'moreover, our approach allows us to efficiently implement less gpu - friendly minimum bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning - nearly a 6x speedup']",0
[' #TAUTHOR_TAG proposed a gpu implementation of'],[' #TAUTHOR_TAG proposed a gpu implementation of a constituency parser that sacrifices all sparsity in'],[' #TAUTHOR_TAG proposed a gpu implementation of'],"[' #TAUTHOR_TAG proposed a gpu implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that gpus can provide. their system uses a grammar based on the berkeley parser  #AUTHOR_TAG (', 'which is particularly amenable to gpu processing ), "" compiling "" the grammar into a sequence of gpu kernels that are applied densely to every item in the parse chart. together these', 'kernels implement the viterbi inside algorithm. on a mid - range gpu, their system can compute viterbi derivations at 164 sentences per second on sentences of length', '40 or less ( see timing details below ). in this paper, we develop algorithms that can exploit sparsity on a', 'gpu by adapting coarse - tofine pruning to a gpu setting. on a cpu, pruning methods can give speedups of up to 100x. such extreme speedup', '##s over a dense gpu baseline currently seem unlikely because fine - grained sparsity appears to be directly at odds with dense parallelism. however, in this', 'paper, we present a system that finds a middle ground, where some level of sparsity can be maintained without losing the parallelism of the gpu. we use a', '']",0
[' #TAUTHOR_TAG proposed a gpu implementation of'],[' #TAUTHOR_TAG proposed a gpu implementation of a constituency parser that sacrifices all sparsity in'],[' #TAUTHOR_TAG proposed a gpu implementation of'],"[' #TAUTHOR_TAG proposed a gpu implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that gpus can provide. their system uses a grammar based on the berkeley parser  #AUTHOR_TAG (', 'which is particularly amenable to gpu processing ), "" compiling "" the grammar into a sequence of gpu kernels that are applied densely to every item in the parse chart. together these', 'kernels implement the viterbi inside algorithm. on a mid - range gpu, their system can compute viterbi derivations at 164 sentences per second on sentences of length', '40 or less ( see timing details below ). in this paper, we develop algorithms that can exploit sparsity on a', 'gpu by adapting coarse - tofine pruning to a gpu setting. on a cpu, pruning methods can give speedups of up to 100x. such extreme speedup', '##s over a dense gpu baseline currently seem unlikely because fine - grained sparsity appears to be directly at odds with dense parallelism. however, in this', 'paper, we present a system that finds a middle ground, where some level of sparsity can be maintained without losing the parallelism of the gpu. we use a', '']",0
"['architecture environment puts very different constraints on parsing algorithms from a cpu environment.', ' #TAUTHOR_TAG proposed an implementation of a pcfg parser that sacrifices standard sparse methods like coarse - to - fine']","['architecture environment puts very different constraints on parsing algorithms from a cpu environment.', ' #TAUTHOR_TAG proposed an implementation of a pcfg parser that sacrifices standard sparse methods like coarse - to - fine pruning, focusing instead on maximizing']","['architecture environment puts very different constraints on parsing algorithms from a cpu environment.', ' #TAUTHOR_TAG proposed an implementation of a pcfg parser that sacrifices standard sparse methods like coarse - to - fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.', 'they assume that they are parsing many sentences at once, with throughput being more']","['architecture environment puts very different constraints on parsing algorithms from a cpu environment.', ' #TAUTHOR_TAG proposed an implementation of a pcfg parser that sacrifices standard sparse methods like coarse - to - fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.', 'they assume that they are parsing many sentences at once, with throughput being more important than latency.', 'in this section, we describe their dense algorithm, which we take as the baseline for our work ; we present it in a way that sets up the changes to follow.', 'at the top level, the cpu and gpu communicate via a work queue of parse items of the form ( s, i, k, j ), where s is an identifier of a sentence, i is the start of a span, k is the split point, and j table 1 : performance numbers for computing viterbi inside charts on 20, 000 sentences of length ≤40 from the penn treebank.', 'all times are measured on an nvidia geforce gtx 680.', ""' reimpl'is our reimplementation of their approach."", 'speedups are measured in reference to this reimplementation.', '']",0
"['architecture environment puts very different constraints on parsing algorithms from a cpu environment.', ' #TAUTHOR_TAG proposed an implementation of a pcfg parser that sacrifices standard sparse methods like coarse - to - fine']","['architecture environment puts very different constraints on parsing algorithms from a cpu environment.', ' #TAUTHOR_TAG proposed an implementation of a pcfg parser that sacrifices standard sparse methods like coarse - to - fine pruning, focusing instead on maximizing']","['architecture environment puts very different constraints on parsing algorithms from a cpu environment.', ' #TAUTHOR_TAG proposed an implementation of a pcfg parser that sacrifices standard sparse methods like coarse - to - fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.', 'they assume that they are parsing many sentences at once, with throughput being more']","['architecture environment puts very different constraints on parsing algorithms from a cpu environment.', ' #TAUTHOR_TAG proposed an implementation of a pcfg parser that sacrifices standard sparse methods like coarse - to - fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser.', 'they assume that they are parsing many sentences at once, with throughput being more important than latency.', 'in this section, we describe their dense algorithm, which we take as the baseline for our work ; we present it in a way that sets up the changes to follow.', 'at the top level, the cpu and gpu communicate via a work queue of parse items of the form ( s, i, k, j ), where s is an identifier of a sentence, i is the start of a span, k is the split point, and j table 1 : performance numbers for computing viterbi inside charts on 20, 000 sentences of length ≤40 from the penn treebank.', 'all times are measured on an nvidia geforce gtx 680.', ""' reimpl'is our reimplementation of their approach."", 'speedups are measured in reference to this reimplementation.', '']",0
"[""important feature of  #TAUTHOR_TAG's system is grammar compilation."", 'because registers are so']","[""important feature of  #TAUTHOR_TAG's system is grammar compilation."", 'because registers are so']","[""important feature of  #TAUTHOR_TAG's system is grammar compilation."", 'because registers are so']","[""important feature of  #TAUTHOR_TAG's system is grammar compilation."", 'because registers are so much faster than thread - local memory, it is critical to keep as many variables in registers as possible.', 'one way to accomplish this is to unroll loops at compilation time.', 'therefore, they inlined the iteration over the grammar directly into the gpu kernels ( i. e. the code itself ), which allows the compiler to more effectively use all of its registers.', 'however, register space is limited on gpus.', 'because the berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills.', ' #TAUTHOR_TAG found they had to partition the grammar into multiple different kernels.', 'we discuss this partitioning in more detail in section 7.', 'however, in short, the entire grammar g is broken into multiple clusters g i where each rule belongs to exactly one cluster']",0
"[""important feature of  #TAUTHOR_TAG's system is grammar compilation."", 'because registers are so']","[""important feature of  #TAUTHOR_TAG's system is grammar compilation."", 'because registers are so']","[""important feature of  #TAUTHOR_TAG's system is grammar compilation."", 'because registers are so']","[""important feature of  #TAUTHOR_TAG's system is grammar compilation."", 'because registers are so much faster than thread - local memory, it is critical to keep as many variables in registers as possible.', 'one way to accomplish this is to unroll loops at compilation time.', 'therefore, they inlined the iteration over the grammar directly into the gpu kernels ( i. e. the code itself ), which allows the compiler to more effectively use all of its registers.', 'however, register space is limited on gpus.', 'because the berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills.', ' #TAUTHOR_TAG found they had to partition the grammar into multiple different kernels.', 'we discuss this partitioning in more detail in section 7.', 'however, in short, the entire grammar g is broken into multiple clusters g i where each rule belongs to exactly one cluster']",0
['relative to  #TAUTHOR_TAG'],"[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and']","[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and nearly 50']",[' #TAUTHOR_TAG'],0
"['from the model of  #TAUTHOR_TAG, there have been a few attempts at using gpus in nlp contexts before.', ' #AUTHOR_TAG and  #AUTHOR_TAG both had early attempts at porting parsing algorithms to the gpu.', 'however, they did not demonstrate significantly increased speed over a cpu implementation.', 'in machine translation,  #AUTHOR_TAG adapted algorithms designed for gpus in the computational biology literature to']","['from the model of  #TAUTHOR_TAG, there have been a few attempts at using gpus in nlp contexts before.', ' #AUTHOR_TAG and  #AUTHOR_TAG both had early attempts at porting parsing algorithms to the gpu.', 'however, they did not demonstrate significantly increased speed over a cpu implementation.', 'in machine translation,  #AUTHOR_TAG adapted algorithms designed for gpus in the computational biology literature to']","['from the model of  #TAUTHOR_TAG, there have been a few attempts at using gpus in nlp contexts before.', ' #AUTHOR_TAG and  #AUTHOR_TAG both had early attempts at porting parsing algorithms to the gpu.', 'however, they did not demonstrate significantly increased speed over a cpu implementation.', 'in machine translation,  #AUTHOR_TAG adapted algorithms designed for gpus in the computational biology literature to speed up on - demand phrase table extraction']","['from the model of  #TAUTHOR_TAG, there have been a few attempts at using gpus in nlp contexts before.', ' #AUTHOR_TAG and  #AUTHOR_TAG both had early attempts at porting parsing algorithms to the gpu.', 'however, they did not demonstrate significantly increased speed over a cpu implementation.', 'in machine translation,  #AUTHOR_TAG adapted algorithms designed for gpus in the computational biology literature to speed up on - demand phrase table extraction']",0
"['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so']","['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so']","['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so we tested']","['', 'unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20k sentences.', '1 we should note that our experimental condition differs from that of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so we tested it on batches of 1200 sentences.', 'our reimplementation is approximately the same speed for the same batch sizes.', 'for batches of 20k sentences, we used sentences from the training set.', 'we verified that there was no significant difference in speed for sentences from the training set and from the test set.', 'use two nvidia geforce gtx 690s - each of which is essentially a repackaging of two 680s - meaning that our system and experiments would run approximately four times faster on their hardware.', '( this expected 4x factor is empirically consistent with the result of running their system on our hardware.']",4
"['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so']","['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so']","['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so we tested']","['', 'unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20k sentences.', '1 we should note that our experimental condition differs from that of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so we tested it on batches of 1200 sentences.', 'our reimplementation is approximately the same speed for the same batch sizes.', 'for batches of 20k sentences, we used sentences from the training set.', 'we verified that there was no significant difference in speed for sentences from the training set and from the test set.', 'use two nvidia geforce gtx 690s - each of which is essentially a repackaging of two 680s - meaning that our system and experiments would run approximately four times faster on their hardware.', '( this expected 4x factor is empirically consistent with the result of running their system on our hardware.']",4
"['.', 'the original system of  #TAUTHOR_TAG only used']","['pass.', 'the original system of  #TAUTHOR_TAG only used']","['', 'the original system of  #TAUTHOR_TAG only used']","[', the standard coarse - to - fine approach does not naively translate to gpu architectures.', ""gpus work by executing thousands of threads at once, but impose the constraint that large blocks of threads must be executing the same ( 2013 )'s system."", 'the gpu and cpu communicate via a work queue, which ferries parse items from the cpu to the gpu.', 'our system uses a coarse - to - fine approach, where the coarse pass computes a pruning mask that is used by the cpu when deciding which items to queue during the fine pass.', 'the original system of  #TAUTHOR_TAG only used the fine pass, with no pruning.', 'instructions in lockstep, differing only in their input data.', 'thus sparsely skipping rules and symbols will not save any work.', 'indeed, it may actually slow the system down.', 'in this section, we provide an overview of gpu architectures, focusing on the details that are relevant to building an efficient parser.', 'the large number of threads that a gpu executes are packaged into blocks of 32 threads called warps.', '']",4
['of compiled kernel as in  #TAUTHOR_TAG'],['of compiled kernel as in  #TAUTHOR_TAG'],"['of compiled kernel as in  #TAUTHOR_TAG. because the', '']","['', 'gpu, parse items are processed using the same style of compiled kernel as in  #TAUTHOR_TAG. because the', 'entire partition ( though not necessarily the entire grammar ) is applied to each item in the queue, we still do not need to worry about warp divergence. at', 'the top level, our system first computes pruning masks with a coarse grammar. then it processes the same sentences with the fine grammar. however, to the extent that the signatures are small', ', items can be selectively queued only to certain queues. this approach is diagrammed in figure 3. we', 'tested our new pruning approach using an x - bar grammar as the coarse pass. the resulting speed', '']",4
['relative to  #TAUTHOR_TAG'],"[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and']","[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and nearly 50']",[' #TAUTHOR_TAG'],4
['relative to  #TAUTHOR_TAG'],"[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and']","[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and nearly 50']",[' #TAUTHOR_TAG'],4
['relative to  #TAUTHOR_TAG'],"[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and']","[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and nearly 50']",[' #TAUTHOR_TAG'],4
"['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so']","['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so']","['of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so we tested']","['', 'unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20k sentences.', '1 we should note that our experimental condition differs from that of  #TAUTHOR_TAG : they evaluate on sentences of length ≤ 30.', 'furthermore, they 1 the implementation of  #TAUTHOR_TAG cannot handle batches so large, and so we tested it on batches of 1200 sentences.', 'our reimplementation is approximately the same speed for the same batch sizes.', 'for batches of 20k sentences, we used sentences from the training set.', 'we verified that there was no significant difference in speed for sentences from the training set and from the test set.', 'use two nvidia geforce gtx 690s - each of which is essentially a repackaging of two 680s - meaning that our system and experiments would run approximately four times faster on their hardware.', '( this expected 4x factor is empirically consistent with the result of running their system on our hardware.']",5
['of compiled kernel as in  #TAUTHOR_TAG'],['of compiled kernel as in  #TAUTHOR_TAG'],"['of compiled kernel as in  #TAUTHOR_TAG. because the', '']","['', 'gpu, parse items are processed using the same style of compiled kernel as in  #TAUTHOR_TAG. because the', 'entire partition ( though not necessarily the entire grammar ) is applied to each item in the queue, we still do not need to worry about warp divergence. at', 'the top level, our system first computes pruning masks with a coarse grammar. then it processes the same sentences with the fine grammar. however, to the extent that the signatures are small', ', items can be selectively queued only to certain queues. this approach is diagrammed in figure 3. we', 'tested our new pruning approach using an x - bar grammar as the coarse pass. the resulting speed', '']",5
['relative to  #TAUTHOR_TAG'],"[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and']","[""in parsing performance relative to  #TAUTHOR_TAG '"", 's system, and nearly 50']",[' #TAUTHOR_TAG'],5
['of compiled kernel as in  #TAUTHOR_TAG'],['of compiled kernel as in  #TAUTHOR_TAG'],"['of compiled kernel as in  #TAUTHOR_TAG. because the', '']","['', 'gpu, parse items are processed using the same style of compiled kernel as in  #TAUTHOR_TAG. because the', 'entire partition ( though not necessarily the entire grammar ) is applied to each item in the queue, we still do not need to worry about warp divergence. at', 'the top level, our system first computes pruning masks with a coarse grammar. then it processes the same sentences with the fine grammar. however, to the extent that the signatures are small', ', items can be selectively queued only to certain queues. this approach is diagrammed in figure 3. we', 'tested our new pruning approach using an x - bar grammar as the coarse pass. the resulting speed', '']",3
"['translation derivation  #TAUTHOR_TAG and has been widely adopted in statistical machine translation ( smt ).', '']","['translation derivation  #TAUTHOR_TAG and has been widely adopted in statistical machine translation ( smt ).', 'typically, such models define two types of translation rules : hierarchical ( translation )']","['translation derivation  #TAUTHOR_TAG and has been widely adopted in statistical machine translation ( smt ).', '']","[""##ang's hierarchical phrase - based ( hpb ) translation model utilizes synchronous context free grammar ( scfg ) for translation derivation  #TAUTHOR_TAG and has been widely adopted in statistical machine translation ( smt )."", 'typically, such models define two types of translation rules : hierarchical ( translation ) rules which consist of both terminals and non - terminals, and glue ( grammar ) rules which combine translated phrases in a monotone fashion.', ""due to lack of linguistic knowledge, chiang's hpb model contains only one type of nonterminal symbol x, often making it difficult to select the most appropriate translation rules."", ""1 what is more, chiang's hpb model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules."", 'in addition, once a 1 another non - terminal symbol s is used in glue rules.', '']",0
"['and  #TAUTHOR_TAG, our hd']","['and  #TAUTHOR_TAG, our hd - hpb translation model adopts a synchronous context free grammar, a rewriting system which generates']","['and  #TAUTHOR_TAG, our hd']","['and  #TAUTHOR_TAG, our hd - hpb translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context - free grammar.', ""instead of collapsing all non - terminals in the source language into a single symbol x as in  #TAUTHOR_TAG, given a word sequence f i j from position i to position j, we first find heads and then concatenate the pos tags of these heads as f i j's non - terminal symbol."", 'specifically, we adopt unlabeled dependency structure to derive heads, which are defined as :', '']",3
"['and  #TAUTHOR_TAG, our hd']","['and  #TAUTHOR_TAG, our hd - hpb translation model adopts a synchronous context free grammar, a rewriting system which generates']","['and  #TAUTHOR_TAG, our hd']","['and  #TAUTHOR_TAG, our hd - hpb translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context - free grammar.', ""instead of collapsing all non - terminals in the source language into a single symbol x as in  #TAUTHOR_TAG, given a word sequence f i j from position i to position j, we first find heads and then concatenate the pos tags of these heads as f i j's non - terminal symbol."", 'specifically, we adopt unlabeled dependency structure to derive heads, which are defined as :', '']",3
"[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinf""]","[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we']","[""as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look']","['mentioned, a hd - hr has at least one terminal on both source and target sides.', ""this is the same as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look for initial phrase pairs that contain other phrases and then replace sub - phrases with pos tags corresponding to their heads.', 'given the word alignment in figure 1, table 1 demonstrates the difference between hierarchical rules in  #TAUTHOR_TAG and hd - hrs defined here.', ""similar to chiang's hpb model, our hd - hpb model will result in a large number of rules causing problems in decoding."", 'to alleviate these problems, we filter our hd - hrs according to the same constraints as described in  #TAUTHOR_TAG.', 'moreover, we discard rules that have non - terminals with more than four heads']",3
"[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinf""]","[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we']","[""as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look']","['mentioned, a hd - hr has at least one terminal on both source and target sides.', ""this is the same as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look for initial phrase pairs that contain other phrases and then replace sub - phrases with pos tags corresponding to their heads.', 'given the word alignment in figure 1, table 1 demonstrates the difference between hierarchical rules in  #TAUTHOR_TAG and hd - hrs defined here.', ""similar to chiang's hpb model, our hd - hpb model will result in a large number of rules causing problems in decoding."", 'to alleviate these problems, we filter our hd - hrs according to the same constraints as described in  #TAUTHOR_TAG.', 'moreover, we discard rules that have non - terminals with more than four heads']",3
"['in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of  #TAUTHOR_TAG, including :', '• p hd - hr ( t']","['in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of  #TAUTHOR_TAG, including :', '• p hd - hr ( t | s ) and p hd - hr ( s | t ),']","['in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of  #TAUTHOR_TAG, including :', '• p hd - hr ( t']","['e for the translation output in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of  #TAUTHOR_TAG, including :', '• p hd - hr ( t | s ) and p hd - hr ( s | t ), translation probabilities for hd - hrs ;', '• p lex ( t | s ) and p lex ( s | t ), lexical translation probabilities for hd - hrs ;', '• p ty hd - hr = exp ( −1 ), rule penalty for hd - hrs ;', '• p nrr ( t | s ), translation probability for nrrs ;', '• p ty nrr = exp ( −1 ), rule penalty for nrrs ;', '• p lm ( e ), language model ;', '• p ty word ( e ) = exp ( − | e | ), word penalty.', 'our decoder is based on cky - style chart parsing with beam search and searches for the best derivation bottom - up.', 'for a source span [ i, j ], it applies both types of hd - hrs and nrrs.', 'however, hdhrs are only applied to generate derivations spanning no more than k words - the initial phrase length limit used in training to extract hd - hrswhile nrrs are applied to derivations spanning any length.', ""unlike in chiang's hpb model, it is possible for a non - terminal generated by a nrr to be included afterwards by a hd - hr or another nrr""]",3
"['and  #TAUTHOR_TAG, our hd']","['and  #TAUTHOR_TAG, our hd - hpb translation model adopts a synchronous context free grammar, a rewriting system which generates']","['and  #TAUTHOR_TAG, our hd']","['and  #TAUTHOR_TAG, our hd - hpb translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context - free grammar.', ""instead of collapsing all non - terminals in the source language into a single symbol x as in  #TAUTHOR_TAG, given a word sequence f i j from position i to position j, we first find heads and then concatenate the pos tags of these heads as f i j's non - terminal symbol."", 'specifically, we adopt unlabeled dependency structure to derive heads, which are defined as :', '']",4
"[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinf""]","[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we']","[""as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look']","['mentioned, a hd - hr has at least one terminal on both source and target sides.', ""this is the same as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look for initial phrase pairs that contain other phrases and then replace sub - phrases with pos tags corresponding to their heads.', 'given the word alignment in figure 1, table 1 demonstrates the difference between hierarchical rules in  #TAUTHOR_TAG and hd - hrs defined here.', ""similar to chiang's hpb model, our hd - hpb model will result in a large number of rules causing problems in decoding."", 'to alleviate these problems, we filter our hd - hrs according to the same constraints as described in  #TAUTHOR_TAG.', 'moreover, we discard rules that have non - terminals with more than four heads']",4
"[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinf""]","[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we']","[""as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look']","['mentioned, a hd - hr has at least one terminal on both source and target sides.', ""this is the same as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look for initial phrase pairs that contain other phrases and then replace sub - phrases with pos tags corresponding to their heads.', 'given the word alignment in figure 1, table 1 demonstrates the difference between hierarchical rules in  #TAUTHOR_TAG and hd - hrs defined here.', ""similar to chiang's hpb model, our hd - hpb model will result in a large number of rules causing problems in decoding."", 'to alleviate these problems, we filter our hd - hrs according to the same constraints as described in  #TAUTHOR_TAG.', 'moreover, we discard rules that have non - terminals with more than four heads']",4
"[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinf""]","[""defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we']","[""as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look']","['mentioned, a hd - hr has at least one terminal on both source and target sides.', ""this is the same as the hierarchical rules defined in chiang's hpb model  #TAUTHOR_TAG, except that we use head posinformed non - terminal symbols in the source language."", 'we look for initial phrase pairs that contain other phrases and then replace sub - phrases with pos tags corresponding to their heads.', 'given the word alignment in figure 1, table 1 demonstrates the difference between hierarchical rules in  #TAUTHOR_TAG and hd - hrs defined here.', ""similar to chiang's hpb model, our hd - hpb model will result in a large number of rules causing problems in decoding."", 'to alleviate these problems, we filter our hd - hrs according to the same constraints as described in  #TAUTHOR_TAG.', 'moreover, we discard rules that have non - terminals with more than four heads']",5
['side are labeled as x ) :  #TAUTHOR_TAG'],['side are labeled as x ) :  #TAUTHOR_TAG'],['are labeled as x ) :  #TAUTHOR_TAG'],"['##rs are translation rules without terminals.', 'given an initial phrase pair on the source side, there are four possible positional relationships for their target side translations ( we use y as a variable for nonterminals on the source side while all non - terminals on the target side are labeled as x ) :  #TAUTHOR_TAG and hd - hrs.', 'indexed underlines indicate sub - phrases and corresponding non - terminal symbols.', 'the non - terminals in hd - hrs ( e. g., nn, vv, vv - nr ) capture the head ( s ) pos tags of the corresponding word sequence in the source language.', 'merging two neighboring non - terminals into a single non - terminal, nrrs enable the translation model to explore a wider search space.', 'during training, we extract four types of nrrs and calculate probabilities for each type.', 'to speed up decoding, we currently ( i ) only use monotone and swap nrrs and ( ii ) limit the number of non - terminals in a nrr to 2']",7
"[""implementation of chiang's hpb model  #TAUTHOR_TAG,""]","[""implementation of chiang's hpb model  #TAUTHOR_TAG,""]","[""implementation of chiang's hpb model  #TAUTHOR_TAG, a source - side samtstyle refined version of hpb ( samt - hpb ), and the moses implementation of hpb."", 'for fair comparison, we adopt the same parameter settings']","[""evaluate the performance of our hd - hpb model and compare it with our implementation of chiang's hpb model  #TAUTHOR_TAG, a source - side samtstyle refined version of hpb ( samt - hpb ), and the moses implementation of hpb."", 'for fair comparison, we adopt the same parameter settings for our hd - hpb and hpb systems, including initial phrase length ( as 10 ) in training, the maximum number of non - terminals ( as 2 ) in translation rules, maximum number of non - terminals plus terminals ( as 5 ) on the source, beam threshold β ( as 10 −5 ) ( to discard derivations with a score worse than β times the best score in the same chart cell ), beam size b ( as 200 ) ( i. e. each chart cell contains at most b derivations ).', 'for moses hpb, we use "" grow - diag - final - and "" to obtain symmetric word alignments, 10 for the maximum phrase length, and the recommended default values for all other parameters.', 'we train our model on a dataset [UNK]. 5m sentence pairs from the ldc dataset.', '']",7
[' #TAUTHOR_TAG we presented a frequency - based greedy attribute selection strategy submitted to'],[' #TAUTHOR_TAG we presented a frequency - based greedy attribute selection strategy submitted to'],['previous work  #TAUTHOR_TAG we presented a frequency - based greedy attribute selection strategy submitted to'],"['previous work  #TAUTHOR_TAG we presented a frequency - based greedy attribute selection strategy submitted to the tuna challenge 2008.', 'presently we further the issue by taking additional information into accountnamely, the trial condition information available from the tuna data - and report improved results for string - edit distance as required for the 2009 competition']",0
[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],"[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation.', 'a list p of attributes sorted by frequency is the centre piece of the following selection strategy :', '• select all attributes whose relative frequency falls above a threshold value t ( t was estimated to be 0. 8 for both furniture and people domains. ) • if the resulting description uniquely describes the target object, then finalizes.', '• if not, starting from the most frequent attribute in p, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context.', 'the overall effect obtained is twofold : on the one hand, in a complex situation of reference ( in which many attributes may rule out many distractors, but more than one will be required to achieve uniqueness ) the algorithm simply selects frequent attributes.', ""this may be comparable to a human speaker who has to single out the target object but who does not have the means to come up with the'right'attribute straight away."", 'on the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize.', '']",0
[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],"[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation.', 'a list p of attributes sorted by frequency is the centre piece of the following selection strategy :', '• select all attributes whose relative frequency falls above a threshold value t ( t was estimated to be 0. 8 for both furniture and people domains. ) • if the resulting description uniquely describes the target object, then finalizes.', '• if not, starting from the most frequent attribute in p, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context.', 'the overall effect obtained is twofold : on the one hand, in a complex situation of reference ( in which many attributes may rule out many distractors, but more than one will be required to achieve uniqueness ) the algorithm simply selects frequent attributes.', ""this may be comparable to a human speaker who has to single out the target object but who does not have the means to come up with the'right'attribute straight away."", 'on the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize.', '']",0
[' #TAUTHOR_TAG we presented a frequency - based greedy attribute selection strategy submitted to'],[' #TAUTHOR_TAG we presented a frequency - based greedy attribute selection strategy submitted to'],['previous work  #TAUTHOR_TAG we presented a frequency - based greedy attribute selection strategy submitted to'],"['previous work  #TAUTHOR_TAG we presented a frequency - based greedy attribute selection strategy submitted to the tuna challenge 2008.', 'presently we further the issue by taking additional information into accountnamely, the trial condition information available from the tuna data - and report improved results for string - edit distance as required for the 2009 competition']",6
"[' #TAUTHOR_TAG, now taking also']","[' #TAUTHOR_TAG, now taking also']","[' #TAUTHOR_TAG, now taking also the trial condition ( +']","['present work is a refined version of the original frequency - based greedy attribute selection strategy submitted to the tuna challenge 2008  #TAUTHOR_TAG, now taking also the trial condition ( + / - loc ) into account.', 'in the tuna data, + loc indicates the instances of the experiment in which participants were told that they were allowed to refer to the x, y coordinates of the screen ( i. e., selecting the x - and / or y - dimension attributes ), whereas - loc indicates the trials in which they were discouraged ( but not prevented ) to do so.', 'in practice, references in + loc trials are more likely to convey the x - and y - dimension attributes than those in which the - loc condition was applied.', 'our modified algorithm simply consists of computing separated frequency lists for + loc and - loc trial conditions, and then using the original frequency - based greedy approach with each list accordingly.', 'in practice, descriptions are now generated in two different ways, depending on the trial condition, which may promote the xand y - dimension attributes to higher positions in the list p when + loc applies.', 'using the tuna challenge 2009 development data set, the attribute selection task was performed as above.', 'for the surface realisation task, we have reused the english language surface realisation module provided by irene langkilde - geary for the tuna challenge 2008']",6
[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects'],"[' #TAUTHOR_TAG we presented a combined strategy based on attribute frequency and certain aspects of a greedy attribute selection strategy for referring expressions generation.', 'a list p of attributes sorted by frequency is the centre piece of the following selection strategy :', '• select all attributes whose relative frequency falls above a threshold value t ( t was estimated to be 0. 8 for both furniture and people domains. ) • if the resulting description uniquely describes the target object, then finalizes.', '• if not, starting from the most frequent attribute in p, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context.', 'the overall effect obtained is twofold : on the one hand, in a complex situation of reference ( in which many attributes may rule out many distractors, but more than one will be required to achieve uniqueness ) the algorithm simply selects frequent attributes.', ""this may be comparable to a human speaker who has to single out the target object but who does not have the means to come up with the'right'attribute straight away."", 'on the other hand, as the number of distractors decreases, a single attribute capable of ruling out all distractors will eventually emerge, forcing the algorithm to switch to a greedy strategy and finalize.', '']",1
['1 : considering that in  #TAUTHOR_TAG we reported'],['1 : considering that in  #TAUTHOR_TAG we reported'],['in the overall string - edit distance values in figure 1 : considering that in  #TAUTHOR_TAG we reported'],"['following figure 1 shows mean sting - edit distance and bleu - 3 scores computed using the evaluation tool provided by the tuna challenge team.', 'for ease of comparison with our previous work, we also present dice and masi scores computed as in the previous tuna challenge, although these scores were not required for the current competition.', 'the most relevant comparison with our previous work is observed in the overall string - edit distance values in figure 1 : considering that in  #TAUTHOR_TAG we reported 6. 12 editdistance for furniture and 7. 38 for people, the overall improvement ( driven by the descriptions in the furniture domain ) may be explained by the fact that the current version makes more accurate decisions as to when to use these attributes according to the instructions given to the participants of the tuna trials ( the trial condition + / - loc. )', 'on the other hand, the divide between + loc and - loc strategies does not have a significant effect on the results based on the semantics of the description ( i. e., dice and masi scores ), which remain the same as those obtained previously.', 'this may be explained by the fact that using location information inappropriately counts as one single error in dice / masi calculations, but it may have a much greater impact on the wording of the surface string ( e. g., one single use of the x - dimension attribute may be realized as "" on the far left "", adding four words to the descriptions.']",4
"['as  #TAUTHOR_TAG allow to study this task in more depth.', '']","['as  #TAUTHOR_TAG allow to study this task in more depth.', '']","['as  #TAUTHOR_TAG allow to study this task in more depth.', '']","['.', 'generating descriptions for videos has many applications including assisting blind people and human - robot interaction.', 'the recent advances in image captioning as well as the release of large - scale movie description datasets such as  #TAUTHOR_TAG allow to study this task in more depth.', 'many of the proposed methods for image captioning rely on pre - trained object classifier cnns and long - short term memory recurrent networks ( lstms ) for generating descriptions.', 'while image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description.', 'in this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions.', 'based on these visual classifiers we learn how to generate a description using an lstm.', 'we explore different design choices to build and train the lstm and achieve the best performance to date on the challenging  #TAUTHOR_TAG.', 'we compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task']",0
[' #TAUTHOR_TAG'],"['long - short term memory networks ( lstms ) [ 13 ].', 'in the meanwhile, two large - scale  #TAUTHOR_TAG']","['long - short term memory networks ( lstms ) [ 13 ].', 'in the meanwhile, two large - scale  #TAUTHOR_TAG']","['description of visual content has lately received a lot of interest in our community.', 'multiple works have successfully addressed the image captioning problem [ 6, 16, 17, 35 ].', 'many of the proposed methods rely on long - short term memory networks ( lstms ) [ 13 ].', 'in the meanwhile, two large - scale  #TAUTHOR_TAG and montreal video annotation dataset ( m - vad ) [ 31 ].', 'both are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people.', 'works addressing these datasets  #TAUTHOR_TAG are indeed challenging in terms of visual recognition and automatic description.', 'this results in a significantly lower performance then on simpler video datasets ( e. g. msvd [ 2 ] ), but a detailed analysis of the difficulties is missing.', 'in this work we address this by taking a closer look at the performance of existing methods on the movie description task.', 'this work contributes a ) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations ; b ) based on the visual classifiers we evaluate different design choices to train an lstm for generating descriptions.', 'this outperforms  #TAUTHOR_TAG, both using automatic and human evaluation ; c ) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task']",0
[' #TAUTHOR_TAG'],"['long - short term memory networks ( lstms ) [ 13 ].', 'in the meanwhile, two large - scale  #TAUTHOR_TAG']","['long - short term memory networks ( lstms ) [ 13 ].', 'in the meanwhile, two large - scale  #TAUTHOR_TAG']","['description of visual content has lately received a lot of interest in our community.', 'multiple works have successfully addressed the image captioning problem [ 6, 16, 17, 35 ].', 'many of the proposed methods rely on long - short term memory networks ( lstms ) [ 13 ].', 'in the meanwhile, two large - scale  #TAUTHOR_TAG and montreal video annotation dataset ( m - vad ) [ 31 ].', 'both are based on movies with associated textual descriptions and allow studying the problem how to generate movie description for visually disabled people.', 'works addressing these datasets  #TAUTHOR_TAG are indeed challenging in terms of visual recognition and automatic description.', 'this results in a significantly lower performance then on simpler video datasets ( e. g. msvd [ 2 ] ), but a detailed analysis of the difficulties is missing.', 'in this work we address this by taking a closer look at the performance of existing methods on the movie description task.', 'this work contributes a ) an approach to build robust visual classifiers which distinguish verbs, objects, and places extracted from weak sentence annotations ; b ) based on the visual classifiers we evaluate different design choices to train an lstm for generating descriptions.', 'this outperforms  #TAUTHOR_TAG, both using automatic and human evaluation ; c ) we perform a detailed analysis of prior work and our approach to understand the challenges of the movie description task']",0
"['movie description datasets have been proposed,  #TAUTHOR_TAG']","['two large - scale movie description datasets have been proposed,  #TAUTHOR_TAG']","['two large - scale movie description datasets have been proposed,  #TAUTHOR_TAG']","['two large - scale movie description datasets have been proposed,  #TAUTHOR_TAG and montreal video annotation dataset ( m - vad ) [ 31 ].', 'given that they are based on movies, they cover a much broader domain then previous video description datasets.', 'consequently they are much more varied and challenging with respect to the visual content and the associated description.', 'they also do not have any additional annotations, as e. g. tacos multi - level [ 27 ], thus one has to rely on the weak annotations of the sentence descriptions.', 'to handle this challenging scenario [ 39 ] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3 - d cnn and generates a sentence using an lstm.', '']",0
"['as  #TAUTHOR_TAG allow to study this task in more depth.', '']","['as  #TAUTHOR_TAG allow to study this task in more depth.', '']","['as  #TAUTHOR_TAG allow to study this task in more depth.', '']","['.', 'generating descriptions for videos has many applications including assisting blind people and human - robot interaction.', 'the recent advances in image captioning as well as the release of large - scale movie description datasets such as  #TAUTHOR_TAG allow to study this task in more depth.', 'many of the proposed methods for image captioning rely on pre - trained object classifier cnns and long - short term memory recurrent networks ( lstms ) for generating descriptions.', 'while image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description.', 'in this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions.', 'based on these visual classifiers we learn how to generate a description using an lstm.', 'we explore different design choices to build and train the lstm and achieve the best performance to date on the challenging  #TAUTHOR_TAG.', 'we compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task']",7
"['this section we first analyze our approach on the  #TAUTHOR_TAG dataset and explore different design choices.', 'then, we compare our best system to  #TAUTHOR_TAG']","['this section we first analyze our approach on the  #TAUTHOR_TAG dataset and explore different design choices.', 'then, we compare our best system to  #TAUTHOR_TAG']","['this section we first analyze our approach on the  #TAUTHOR_TAG dataset and explore different design choices.', 'then, we compare our best system to  #TAUTHOR_TAG']","['this section we first analyze our approach on the  #TAUTHOR_TAG dataset and explore different design choices.', 'then, we compare our best system to  #TAUTHOR_TAG']",7
"['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['setup.', 'we build on the labels discovered by our semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1, 263 labels.', 'the parser additionally tells us whether the label is a verb.', 'we use the visual features ( dt, lsda, places ) provided with the  #TAUTHOR_TAG.', 'the lstm output / hidden unit as well as memory cell have each 500 dimensions.', 'we train the svm classifiers on the training set ( 56, 861 clips ).', 'we evaluate our method on the validation set ( 4, 930 clips ) using the meteor [ 21 ] score, which, according to [ 7, 32 ], supersedes other popular measures, such as bleu [ 26 ], rouge [ 22 ], in terms of agreement with human judgments.', 'the authors of cider [ 32 ] showed that meteor also outperforms cider when the number of references is small and in the case of  #TAUTHOR_TAG we have typically only a single reference']",7
['movie description datasets  #TAUTHOR_TAG and m - va'],"['movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to']","['the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to']","['the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to take a closer look at  #TAUTHOR_TAG, s2vt [ 33 ] and ours, in order to understand where these methods succeed and where they fail.', 'in the following we evaluate  #TAUTHOR_TAG test set']",7
"['someone is a man, someone is a man.', 's2']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone looks at him, someone turns to someone.', 'our someone is standing in the crowd, a little man with a little smile.', 'reference someone, back in elf guise, is trying to calm the kids.', ' #TAUTHOR_TAG the car is a water of the water.', 's2vt [ 33 ] on the door, opens the door opens.', 'our', 'the fellowship are in the courtyard.', 'reference they cross the quadrangle below and run along the cloister.', ' #TAUTHOR_TAG someone is down the door, someone is a back of the door, and someone is a door.', 's2vt [ 33 ] someone shakes his head and looks at someone.', 'our someone takes a drink and pours it into the water.', 'reference someone grabs a vodka bottle standing open on the counter and liberally pours some on the hand']",7
"['someone is a man, someone is a man.', 's2']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone looks at him, someone turns to someone.', 'our someone is standing in the crowd, a little man with a little smile.', 'reference someone, back in elf guise, is trying to calm the kids.', ' #TAUTHOR_TAG the car is a water of the water.', 's2vt [ 33 ] on the door, opens the door opens.', 'our', 'the fellowship are in the courtyard.', 'reference they cross the quadrangle below and run along the cloister.', ' #TAUTHOR_TAG someone is down the door, someone is a back of the door, and someone is a door.', 's2vt [ 33 ] someone shakes his head and looks at someone.', 'our someone takes a drink and pours it into the water.', 'reference someone grabs a vodka bottle standing open on the counter and liberally pours some on the hand']",7
"['someone is a man, someone is a man.', 's2']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone']","['someone is a man, someone is a man.', 's2vt [ 33 ] someone looks at him, someone turns to someone.', 'our someone is standing in the crowd, a little man with a little smile.', 'reference someone, back in elf guise, is trying to calm the kids.', ' #TAUTHOR_TAG the car is a water of the water.', 's2vt [ 33 ] on the door, opens the door opens.', 'our', 'the fellowship are in the courtyard.', 'reference they cross the quadrangle below and run along the cloister.', ' #TAUTHOR_TAG someone is down the door, someone is a back of the door, and someone is a door.', 's2vt [ 33 ] someone shakes his head and looks at someone.', 'our someone takes a drink and pours it into the water.', 'reference someone grabs a vodka bottle standing open on the counter and liberally pours some on the hand']",7
"['on the  #TAUTHOR_TAG dataset as shown by all automatic evaluation measures and extensive human evaluation.', 'we analyze the challenges in the movie description task using our and  #TAUTHOR_TAG.', 'we']","['on the  #TAUTHOR_TAG dataset as shown by all automatic evaluation measures and extensive human evaluation.', 'we analyze the challenges in the movie description task using our and  #TAUTHOR_TAG.', 'we']","['on the  #TAUTHOR_TAG dataset as shown by all automatic evaluation measures and extensive human evaluation.', 'we analyze the challenges in the movie description task using our and  #TAUTHOR_TAG.', 'we find that the factors which']","['propose an approach to automatic movie description which trains visual classifiers and uses the classifier scores as input to lstm.', 'to handle the weak sentence annotations we rely on three main ingredients.', 'first, we distinguish three semantic groups of labels ( verbs, objects and places ), second we train them discriminatively, removing potentially noisy negatives, and third, we select only a small number of the most reliable classifiers.', 'for sentence generation we show the benefits of exploring different lstm architectures and learning configurations.', 'as the result we obtain the highest performance on the  #TAUTHOR_TAG dataset as shown by all automatic evaluation measures and extensive human evaluation.', 'we analyze the challenges in the movie description task using our and  #TAUTHOR_TAG.', 'we find that the factors which contribute to higher performance include : presence of frequent words, sentence length and simplicity as well as presence of "" visual "" verbs ( e. g. "" nod "", "" walk "", "" sit "", "" smile "" ).', 'textual and visual difficulties of sentences / clips strongly correlate with the performance of all methods.', 'we observe a high bias in the data towards humans as subjects and verbs similar to "" look "".', 'future work has to focus on dealing with less frequent words and handle less visual descriptions.', 'this potentially requires to consider external text corpora, modalities other than video, such as audio and dialog, and to look across multiple sentences.', 'this would allow exploiting long - and shortrange context and thus understanding and describing the story of the movie']",7
"['movie description datasets have been proposed,  #TAUTHOR_TAG']","['two large - scale movie description datasets have been proposed,  #TAUTHOR_TAG']","['two large - scale movie description datasets have been proposed,  #TAUTHOR_TAG']","['two large - scale movie description datasets have been proposed,  #TAUTHOR_TAG and montreal video annotation dataset ( m - vad ) [ 31 ].', 'given that they are based on movies, they cover a much broader domain then previous video description datasets.', 'consequently they are much more varied and challenging with respect to the visual content and the associated description.', 'they also do not have any additional annotations, as e. g. tacos multi - level [ 27 ], thus one has to rely on the weak annotations of the sentence descriptions.', 'to handle this challenging scenario [ 39 ] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3 - d cnn and generates a sentence using an lstm.', '']",5
"['', 'as in  #TAUTHOR_TAG we']","['of videos and weak sentence annotations.', 'as in  #TAUTHOR_TAG we']","['training we rely on a parallel corpus of videos and weak sentence annotations.', 'as in  #TAUTHOR_TAG we parse the sentences to']","['training we rely on a parallel corpus of videos and weak sentence annotations.', 'as in  #TAUTHOR_TAG we parse the sentences to obtain a set of labels ( single words or short phrases, e. g. look up ) to train our visual classifiers.', 'however, in contrast to  #TAUTHOR_TAG we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized.', 'avoiding parser failure.', '']",5
"['', 'as in  #TAUTHOR_TAG we']","['of videos and weak sentence annotations.', 'as in  #TAUTHOR_TAG we']","['training we rely on a parallel corpus of videos and weak sentence annotations.', 'as in  #TAUTHOR_TAG we parse the sentences to']","['training we rely on a parallel corpus of videos and weak sentence annotations.', 'as in  #TAUTHOR_TAG we parse the sentences to obtain a set of labels ( single words or short phrases, e. g. look up ) to train our visual classifiers.', 'however, in contrast to  #TAUTHOR_TAG we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized.', 'avoiding parser failure.', '']",5
"['this section we first analyze our approach on the  #TAUTHOR_TAG dataset and explore different design choices.', 'then, we compare our best system to  #TAUTHOR_TAG']","['this section we first analyze our approach on the  #TAUTHOR_TAG dataset and explore different design choices.', 'then, we compare our best system to  #TAUTHOR_TAG']","['this section we first analyze our approach on the  #TAUTHOR_TAG dataset and explore different design choices.', 'then, we compare our best system to  #TAUTHOR_TAG']","['this section we first analyze our approach on the  #TAUTHOR_TAG dataset and explore different design choices.', 'then, we compare our best system to  #TAUTHOR_TAG']",5
"['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['setup.', 'we build on the labels discovered by our semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1, 263 labels.', 'the parser additionally tells us whether the label is a verb.', 'we use the visual features ( dt, lsda, places ) provided with the  #TAUTHOR_TAG.', 'the lstm output / hidden unit as well as memory cell have each 500 dimensions.', 'we train the svm classifiers on the training set ( 56, 861 clips ).', 'we evaluate our method on the validation set ( 4, 930 clips ) using the meteor [ 21 ] score, which, according to [ 7, 32 ], supersedes other popular measures, such as bleu [ 26 ], rouge [ 22 ], in terms of agreement with human judgments.', 'the authors of cider [ 32 ] showed that meteor also outperforms cider when the number of references is small and in the case of  #TAUTHOR_TAG we have typically only a single reference']",5
"['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear']","['setup.', 'we build on the labels discovered by our semantic parser  #TAUTHOR_TAG failed to process.', 'to be able to learn classifiers we select the labels that appear at least 30 times, resulting in 1, 263 labels.', 'the parser additionally tells us whether the label is a verb.', 'we use the visual features ( dt, lsda, places ) provided with the  #TAUTHOR_TAG.', 'the lstm output / hidden unit as well as memory cell have each 500 dimensions.', 'we train the svm classifiers on the training set ( 56, 861 clips ).', 'we evaluate our method on the validation set ( 4, 930 clips ) using the meteor [ 21 ] score, which, according to [ 7, 32 ], supersedes other popular measures, such as bleu [ 26 ], rouge [ 22 ], in terms of agreement with human judgments.', 'the authors of cider [ 32 ] showed that meteor also outperforms cider when the number of references is small and in the case of  #TAUTHOR_TAG we have typically only a single reference']",5
"[') on the test set of', ' #TAUTHOR_TAG']","[') on the test set of', ' #TAUTHOR_TAG']","[') on the test set of', ' #TAUTHOR_TAG']","['', 'and 10k ), using the base - learning rate 0. 01. our results show that the step - based learning is superior to the polynomial learning. in most of experiments we', 'trained our networks for 25, 000 iterations. after looking at the meteor performance for', 'intermediate iterations we found that for the step size 4000 at iteration 15,', '000 we achieve best performance overall. additionally we train multiple lstms with different random orderings of the training data. in our experiments we', 'combine three in an ensemble, averaging the resulting word predictions. in most cases the ensemble', 'improves over the single networks in terms of meteor score ( see table 4 ).', 'to summarize, the most important aspects that decrease over - fitting and lead to a better sentence generation are :', '( a ) a correct learning rate and step size, ( b ) dropout after the lstm layer, ( c ) choosing the training iteration based on meteor score as opposed to only looking at the lstm accuracy / loss which can be misleading, and ( d ) building ensembles of multiple networks with different random initializations. in the following section we evaluate our best ensemble ( last line of table 4 ) on the test set of', ' #TAUTHOR_TAG']",5
"['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6,']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6, 578 clips ).', 'we report']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6,']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6, 578 clips ).', 'we report all popular automatic evaluation measures, cider [ 32 ], bleu [ 26 ], rouge [ 22 ] and meteor [ 21 ], computed using the evaluation code of [ 3 ].', 'we also perform a human evaluation, by randomly selecting 1300 video snippets and asking amt turkers to rank  #TAUTHOR_TAG.', 'results.', 'moreover, we improve over the recent approach of [ 33 ], which also uses lstm to generate video descriptions.', 'exploring different strategies to label selection and classifier training, as well as various lstm configurations allows to obtain best result to date on the  #TAUTHOR_TAG.', '']",5
"['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6,']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6, 578 clips ).', 'we report']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6,']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6, 578 clips ).', 'we report all popular automatic evaluation measures, cider [ 32 ], bleu [ 26 ], rouge [ 22 ] and meteor [ 21 ], computed using the evaluation code of [ 3 ].', 'we also perform a human evaluation, by randomly selecting 1300 video snippets and asking amt turkers to rank  #TAUTHOR_TAG.', 'results.', 'moreover, we improve over the recent approach of [ 33 ], which also uses lstm to generate video descriptions.', 'exploring different strategies to label selection and classifier training, as well as various lstm configurations allows to obtain best result to date on the  #TAUTHOR_TAG.', '']",5
['movie description datasets  #TAUTHOR_TAG and m - va'],"['movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to']","['the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to']","['the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to take a closer look at  #TAUTHOR_TAG, s2vt [ 33 ] and ours, in order to understand where these methods succeed and where they fail.', 'in the following we evaluate  #TAUTHOR_TAG test set']",5
['movie description datasets  #TAUTHOR_TAG and m - va'],"['movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to']","['the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to']","['the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets  #TAUTHOR_TAG and m - vad [ 31 ] ) remains relatively low.', 'in this section we want to take a closer look at  #TAUTHOR_TAG, s2vt [ 33 ] and ours, in order to understand where these methods succeed and where they fail.', 'in the following we evaluate  #TAUTHOR_TAG test set']",5
"['', 'we obtain the sense information from the semantic parser of  #TAUTHOR_TAG, thus senses']","['', 'we obtain the sense information from the semantic parser of  #TAUTHOR_TAG, thus senses']","['this we rely on wordnet topics ( high level entries in the wordnet ontology, e. g. "" motion "", "" perception "", "" competition "", "" emotion "" ), defined for most synsets in wordnet [ 10 ].', 'we obtain the sense information from the semantic parser of  #TAUTHOR_TAG, thus senses might be noisy']","['##net verb topics.', 'we closer analyze the test sentences with respect to different verbs.', 'for this we rely on wordnet topics ( high level entries in the wordnet ontology, e. g. "" motion "", "" perception "", "" competition "", "" emotion "" ), defined for most synsets in wordnet [ 10 ].', 'we obtain the sense information from the semantic parser of  #TAUTHOR_TAG, thus senses might be noisy.', '']",5
"['on the  #TAUTHOR_TAG dataset as shown by all automatic evaluation measures and extensive human evaluation.', 'we analyze the challenges in the movie description task using our and  #TAUTHOR_TAG.', 'we']","['on the  #TAUTHOR_TAG dataset as shown by all automatic evaluation measures and extensive human evaluation.', 'we analyze the challenges in the movie description task using our and  #TAUTHOR_TAG.', 'we']","['on the  #TAUTHOR_TAG dataset as shown by all automatic evaluation measures and extensive human evaluation.', 'we analyze the challenges in the movie description task using our and  #TAUTHOR_TAG.', 'we find that the factors which']","['propose an approach to automatic movie description which trains visual classifiers and uses the classifier scores as input to lstm.', 'to handle the weak sentence annotations we rely on three main ingredients.', 'first, we distinguish three semantic groups of labels ( verbs, objects and places ), second we train them discriminatively, removing potentially noisy negatives, and third, we select only a small number of the most reliable classifiers.', 'for sentence generation we show the benefits of exploring different lstm architectures and learning configurations.', 'as the result we obtain the highest performance on the  #TAUTHOR_TAG dataset as shown by all automatic evaluation measures and extensive human evaluation.', 'we analyze the challenges in the movie description task using our and  #TAUTHOR_TAG.', 'we find that the factors which contribute to higher performance include : presence of frequent words, sentence length and simplicity as well as presence of "" visual "" verbs ( e. g. "" nod "", "" walk "", "" sit "", "" smile "" ).', 'textual and visual difficulties of sentences / clips strongly correlate with the performance of all methods.', 'we observe a high bias in the data towards humans as subjects and verbs similar to "" look "".', 'future work has to focus on dealing with less frequent words and handle less visual descriptions.', 'this potentially requires to consider external text corpora, modalities other than video, such as audio and dialog, and to look across multiple sentences.', 'this would allow exploiting long - and shortrange context and thus understanding and describing the story of the movie']",5
"['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6,']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6, 578 clips ).', 'we report']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6,']","['setup.', 'we compare the best method of  #TAUTHOR_TAG ( 6, 578 clips ).', 'we report all popular automatic evaluation measures, cider [ 32 ], bleu [ 26 ], rouge [ 22 ] and meteor [ 21 ], computed using the evaluation code of [ 3 ].', 'we also perform a human evaluation, by randomly selecting 1300 video snippets and asking amt turkers to rank  #TAUTHOR_TAG.', 'results.', 'moreover, we improve over the recent approach of [ 33 ], which also uses lstm to generate video descriptions.', 'exploring different strategies to label selection and classifier training, as well as various lstm configurations allows to obtain best result to date on the  #TAUTHOR_TAG.', '']",3
"['', 'we obtain the sense information from the semantic parser of  #TAUTHOR_TAG, thus senses']","['', 'we obtain the sense information from the semantic parser of  #TAUTHOR_TAG, thus senses']","['this we rely on wordnet topics ( high level entries in the wordnet ontology, e. g. "" motion "", "" perception "", "" competition "", "" emotion "" ), defined for most synsets in wordnet [ 10 ].', 'we obtain the sense information from the semantic parser of  #TAUTHOR_TAG, thus senses might be noisy']","['##net verb topics.', 'we closer analyze the test sentences with respect to different verbs.', 'for this we rely on wordnet topics ( high level entries in the wordnet ontology, e. g. "" motion "", "" perception "", "" competition "", "" emotion "" ), defined for most synsets in wordnet [ 10 ].', 'we obtain the sense information from the semantic parser of  #TAUTHOR_TAG, thus senses might be noisy.', '']",3
"['in * sem shared task 2013  #TAUTHOR_TAG.', 'others followed the']","['in * sem shared task 2013  #TAUTHOR_TAG.', 'others followed the']","['in the semantic textual similarity ( sts ) task in * sem shared task 2013  #TAUTHOR_TAG.', 'others followed']","['', 'then we use various types of text similarity features including string features, knowledge based features, corpus based features, syntactic features, machine translation based features, multi - level text features and so on, to capture the semantic similarity between two texts.', 'some of these features are borrowed from our previous system in the semantic textual similarity ( sts ) task in * sem shared task 2013  #TAUTHOR_TAG.', 'others followed the previous work in ( saric et al., 2012 ) and  #AUTHOR_TAG.', '']",5
"['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['different words, they also calculated the weighted lsa vector for each word. in addition, we use the co - occurrence retrieval model (', 'crm ) feature from our previous work', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate it is to substitute word', 'w 1 in place of word w 2 in a suitable natural language task, the more', 'semantically similar they are. at last, 6 corpus based features are extracted. syntactic features. dependency relations of sentences often contain semantic information. in this work we follow two syntactic dependency similarity features presented in our previous work  #TAUTHOR_TAG', '']",5
"['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['different words, they also calculated the weighted lsa vector for each word. in addition, we use the co - occurrence retrieval model (', 'crm ) feature from our previous work', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate it is to substitute word', 'w 1 in place of word w 2 in a suitable natural language task, the more', 'semantically similar they are. at last, 6 corpus based features are extracted. syntactic features. dependency relations of sentences often contain semantic information. in this work we follow two syntactic dependency similarity features presented in our previous work  #TAUTHOR_TAG', '']",5
"['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['different words, they also calculated the weighted lsa vector for each word. in addition, we use the co - occurrence retrieval model (', 'crm ) feature from our previous work', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate it is to substitute word', 'w 1 in place of word w 2 in a suitable natural language task, the more', 'semantically similar they are. at last, 6 corpus based features are extracted. syntactic features. dependency relations of sentences often contain semantic information. in this work we follow two syntactic dependency similarity features presented in our previous work  #TAUTHOR_TAG', '']",5
"['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['different words, they also calculated the weighted lsa vector for each word. in addition, we use the co - occurrence retrieval model (', 'crm ) feature from our previous work', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate it is to substitute word', 'w 1 in place of word w 2 in a suitable natural language task, the more', 'semantically similar they are. at last, 6 corpus based features are extracted. syntactic features. dependency relations of sentences often contain semantic information. in this work we follow two syntactic dependency similarity features presented in our previous work  #TAUTHOR_TAG', '']",5
"['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['different words, they also calculated the weighted lsa vector for each word. in addition, we use the co - occurrence retrieval model (', 'crm ) feature from our previous work', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate it is to substitute word', 'w 1 in place of word w 2 in a suitable natural language task, the more', 'semantically similar they are. at last, 6 corpus based features are extracted. syntactic features. dependency relations of sentences often contain semantic information. in this work we follow two syntactic dependency similarity features presented in our previous work  #TAUTHOR_TAG', '']",0
"['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate']","['', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability,']","['different words, they also calculated the weighted lsa vector for each word. in addition, we use the co - occurrence retrieval model (', 'crm ) feature from our previous work', ' #TAUTHOR_TAG as another corpus - based feature. the crm is', 'calculated based on', 'a notion of substitutability, that is, the more appropriate it is to substitute word', 'w 1 in place of word w 2 in a suitable natural language task, the more', 'semantically similar they are. at last, 6 corpus based features are extracted. syntactic features. dependency relations of sentences often contain semantic information. in this work we follow two syntactic dependency similarity features presented in our previous work  #TAUTHOR_TAG', '']",6
"['recently, an attention based system  #TAUTHOR_TAG utilizing both']","['on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both']","['on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both']","['', 'attempt in  #AUTHOR_TAG concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener - ated coreference annotations. adding this', 'layer to the neural rc models improved performance on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both documentlevel and entity - level information achieved stateof - the - art results on wikihop data set, proving that techniques like co - attention and self - attention widely', '']",0
['shown by  #TAUTHOR_TAG'],['shown by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG. the'],"['', 'denotes column - wise normalization. we further encode the co - attended document context using a bidirectional rnn f with gru : the final co - attention context is the columnwise concatenation of c s and d s : we expect s ca carries query - aware contextual information of supporting documents as shown by  #TAUTHOR_TAG. the same co - attention module can also be applied to query and candidates, and query and entities ( as', 'shown in figure 2 ) to get c ca', '']",0
"['recently, an attention based system  #TAUTHOR_TAG utilizing both']","['on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both']","['on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both']","['', 'attempt in  #AUTHOR_TAG concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener - ated coreference annotations. adding this', 'layer to the neural rc models improved performance on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both documentlevel and entity - level information achieved stateof - the - art results on wikihop data set, proving that techniques like co - attention and self - attention widely', '']",5
"['multiple documents  #TAUTHOR_TAG.', 'the method presented in this paper is']","['multiple documents  #TAUTHOR_TAG.', 'the method presented in this paper is']","['multiple documents  #TAUTHOR_TAG.', 'the method presented in this paper is']","['study presented in this paper is directly related to existing research on multi - hop reading comprehension across multiple documents  #TAUTHOR_TAG.', 'the method presented in this paper is similar to previous studies using gnn for multi - hop reasoning  #AUTHOR_TAG.', 'our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.', 'the co - attention and self - attention based encoding of multi - level information presented in each input is also inspired by the cfc model  #TAUTHOR_TAG because they show the effectiveness of attention mechanisms.', 'our model is very different from the other two studies  #AUTHOR_TAG : these two studies both explicitly score the possible reasoning paths with extra ner or coreference resolution systems while our method does not require these modules and we do multi - hop reasoning over graphs.', 'besides these studies, our work is also related to the following research directions.', 'multi - hop rc : there exist several different data sets that require reasoning in multiple steps in literature, for example babi  #AUTHOR_TAG, multirc  #AUTHOR_TAG and openbookqa  #AUTHOR_TAG.', 'a lot of systems have been proposed to solve the multi - hop rc problem with these data sets  #AUTHOR_TAG.', '']",5
"['multiple documents  #TAUTHOR_TAG.', 'the method presented in this paper is']","['multiple documents  #TAUTHOR_TAG.', 'the method presented in this paper is']","['multiple documents  #TAUTHOR_TAG.', 'the method presented in this paper is']","['study presented in this paper is directly related to existing research on multi - hop reading comprehension across multiple documents  #TAUTHOR_TAG.', 'the method presented in this paper is similar to previous studies using gnn for multi - hop reasoning  #AUTHOR_TAG.', 'our novelty is that we propose to use a heterogeneous graph instead of a graph with single type of nodes to incorporate different granularity levels of information.', 'the co - attention and self - attention based encoding of multi - level information presented in each input is also inspired by the cfc model  #TAUTHOR_TAG because they show the effectiveness of attention mechanisms.', 'our model is very different from the other two studies  #AUTHOR_TAG : these two studies both explicitly score the possible reasoning paths with extra ner or coreference resolution systems while our method does not require these modules and we do multi - hop reasoning over graphs.', 'besides these studies, our work is also related to the following research directions.', 'multi - hop rc : there exist several different data sets that require reasoning in multiple steps in literature, for example babi  #AUTHOR_TAG, multirc  #AUTHOR_TAG and openbookqa  #AUTHOR_TAG.', 'a lot of systems have been proposed to solve the multi - hop rc problem with these data sets  #AUTHOR_TAG.', '']",5
['shown by  #TAUTHOR_TAG'],['shown by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG. the'],"['', 'denotes column - wise normalization. we further encode the co - attended document context using a bidirectional rnn f with gru : the final co - attention context is the columnwise concatenation of c s and d s : we expect s ca carries query - aware contextual information of supporting documents as shown by  #TAUTHOR_TAG. the same co - attention module can also be applied to query and candidates, and query and entities ( as', 'shown in figure 2 ) to get c ca', '']",5
['shown by  #TAUTHOR_TAG'],['shown by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG. the'],"['', 'denotes column - wise normalization. we further encode the co - attended document context using a bidirectional rnn f with gru : the final co - attention context is the columnwise concatenation of c s and d s : we expect s ca carries query - aware contextual information of supporting documents as shown by  #TAUTHOR_TAG. the same co - attention module can also be applied to query and candidates, and query and entities ( as', 'shown in figure 2 ) to get c ca', '']",5
['shown by  #TAUTHOR_TAG'],['shown by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG. the'],"['', 'denotes column - wise normalization. we further encode the co - attended document context using a bidirectional rnn f with gru : the final co - attention context is the columnwise concatenation of c s and d s : we expect s ca carries query - aware contextual information of supporting documents as shown by  #TAUTHOR_TAG. the same co - attention module can also be applied to query and candidates, and query and entities ( as', 'shown in figure 2 ) to get c ca', '']",5
['shown by  #TAUTHOR_TAG'],['shown by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG. the'],"['', 'denotes column - wise normalization. we further encode the co - attended document context using a bidirectional rnn f with gru : the final co - attention context is the columnwise concatenation of c s and d s : we expect s ca carries query - aware contextual information of supporting documents as shown by  #TAUTHOR_TAG. the same co - attention module can also be applied to query and candidates, and query and entities ( as', 'shown in figure 2 ) to get c ca', '']",5
"['recently, an attention based system  #TAUTHOR_TAG utilizing both']","['on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both']","['on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both']","['', 'attempt in  #AUTHOR_TAG concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener - ated coreference annotations. adding this', 'layer to the neural rc models improved performance on multi - hop tasks. recently, an attention based system  #TAUTHOR_TAG utilizing both documentlevel and entity - level information achieved stateof - the - art results on wikihop data set, proving that techniques like co - attention and self - attention widely', '']",4
['shown by  #TAUTHOR_TAG'],['shown by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG. the'],"['', 'denotes column - wise normalization. we further encode the co - attended document context using a bidirectional rnn f with gru : the final co - attention context is the columnwise concatenation of c s and d s : we expect s ca carries query - aware contextual information of supporting documents as shown by  #TAUTHOR_TAG. the same co - attention module can also be applied to query and candidates, and query and entities ( as', 'shown in figure 2 ) to get c ca', '']",4
['shown by  #TAUTHOR_TAG'],['shown by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG. the'],"['', 'denotes column - wise normalization. we further encode the co - attended document context using a bidirectional rnn f with gru : the final co - attention context is the columnwise concatenation of c s and d s : we expect s ca carries query - aware contextual information of supporting documents as shown by  #TAUTHOR_TAG. the same co - attention module can also be applied to query and candidates, and query and entities ( as', 'shown in figure 2 ) to get c ca', '']",4
['shown by  #TAUTHOR_TAG'],['shown by  #TAUTHOR_TAG'],['by  #TAUTHOR_TAG. the'],"['', 'denotes column - wise normalization. we further encode the co - attended document context using a bidirectional rnn f with gru : the final co - attention context is the columnwise concatenation of c s and d s : we expect s ca carries query - aware contextual information of supporting documents as shown by  #TAUTHOR_TAG. the same co - attention module can also be applied to query and candidates, and query and entities ( as', 'shown in figure 2 ) to get c ca', '']",4
[' #TAUTHOR_TAG to'],"[' #TAUTHOR_TAG to 70. 9 %.', 'compared to two previous studies using gnn for multi - hop reading comprehension  #AUTHOR_TAG, our model surpasses them by a large margin even though we']",['%  #TAUTHOR_TAG to'],"['table 1, we show the results of the our proposed hde graph based model on both development and test set and compare it with previously published results.', 'we show that our proposed hde graph based model improves the state - of - the - art accuracy on development set from 67. 1 %  #AUTHOR_TAG to 68. 1 %, on the blind test set from 70. 6 %  #TAUTHOR_TAG to 70. 9 %.', 'compared to two previous studies using gnn for multi - hop reading comprehension  #AUTHOR_TAG, our model surpasses them by a large margin even though we do not use better pre - trained contextual embedding elmo  #AUTHOR_TAG']",4
"[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","['translation ( mt ) is the automatic translation of text from one human language to another.', 'statistical mt accomplishes this through a probabilistic model of the translation process.', 'in phrase - based statistical mt ( psmt ), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model.', 'the distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence.', 'as such, although psmt has been very successful, it suffers from the lack of a principled mechanism for handling long - distance reordering phenomena due to word order differences between languages.', 'one method for addressing this difficulty is the reordering - as - preprocessing approach, exemplified by  #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG, where psmt is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order.', 'although this leads to improved performance overall,  #TAUTHOR_TAG show that the reordering - as - preprocessing system does not consistently provide better translations than the psmt baseline on a sentence - by - sentence basis.', '']",0
"[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","['translation ( mt ) is the automatic translation of text from one human language to another.', 'statistical mt accomplishes this through a probabilistic model of the translation process.', 'in phrase - based statistical mt ( psmt ), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model.', 'the distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence.', 'as such, although psmt has been very successful, it suffers from the lack of a principled mechanism for handling long - distance reordering phenomena due to word order differences between languages.', 'one method for addressing this difficulty is the reordering - as - preprocessing approach, exemplified by  #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG, where psmt is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order.', 'although this leads to improved performance overall,  #TAUTHOR_TAG show that the reordering - as - preprocessing system does not consistently provide better translations than the psmt baseline on a sentence - by - sentence basis.', '']",0
"[' #TAUTHOR_TAG.', 'working with german -']","[' #TAUTHOR_TAG.', 'working with german - to - english translation,  #TAUTHOR_TAG']","[' #TAUTHOR_TAG.', 'working with german -']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG.', 'working with german -']","[' #TAUTHOR_TAG.', 'working with german - to - english translation,  #TAUTHOR_TAG']","[' #TAUTHOR_TAG.', 'working with german -']",[' #TAUTHOR_TAG'],0
"['explore the  #TAUTHOR_TAG finding by examining whether machine learning techniques can be used to predict, on']","['explore the  #TAUTHOR_TAG finding by examining whether machine learning techniques can be used to predict, on']","['explore the  #TAUTHOR_TAG finding by examining whether machine learning techniques can be used to predict, on a sentence - by - sentence basis, whether the translation of']","['explore the  #TAUTHOR_TAG finding by examining whether machine learning techniques can be used to predict, on a sentence - by - sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative.', 'for features, they use sentence length, parse probability from the collins parser and unlinked fragment count from the link grammar parser on the english side of the translation.', 'the authors find that, when used on the source side ( in english - to - dutch translation ), these features provide no significant improvement in bleu score, while as target - side features ( in dutch - to - english translation ) they improve the bleu score by 1. 7 points over and above the 1. 3 point improvement from reordering.', 'our work has some similarities to that of  #AUTHOR_TAG but uses the log - linear model of the translation system itself to include features, rather than a separate classifier that does not permit interaction between the confidence features and features used during translation.', 'this idea of using linguistic features to improve statistical mt has appeared in a number of recent papers.', ' #AUTHOR_TAG demonstrate an improvement in hierarchical psmt and syntaxbased ( string - to - tree ) statistical mt through the addition of features pinpointing possible errors in the translation, for example the number of occurrences of a particular grammar production rule, or non - terminal in a rule.', ' #AUTHOR_TAG derive features from the link grammar parser, in combination with word posterior probabilities, to detect mt errors ( in order to subsequently improve translation quality ).', ' #AUTHOR_TAG, we work with psmt and use features that consider the parse tree as a whole or aspects of the reordering process itself.', ' #AUTHOR_TAG, we use these features directly in translation']",0
"['by  #TAUTHOR_TAG.', 'however, this is']","['by  #TAUTHOR_TAG.', 'however, this is']","['by  #TAUTHOR_TAG.', 'however, this is most likely']","['data we use the corpora provided for the 2010 workshop on statistical machine translation 7 translation task.', 'the number of sentence pairs in each corpus are given in table 2.', 'we trained 5 - gram language models with srilm  #AUTHOR_TAG using the three language model files listed in table 3 : bleu scores for every system the last containing the remainder.', 'one language model was produced for each file or subfile, giving a total of ten models.', 'the final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus.', 'table 3 gives the bleu score for each of our four systems and the approximate oracle.', 'we note that these numbers are lower than those reported by  #TAUTHOR_TAG.', 'however, this is most likely due to differences in the training and testing data ; our results are roughly in line with the numbers reported in the euromatrix project for this test set.', '8 interestingly, our reimplementation of the  #TAUTHOR_TAG baseline does not outperform the plain psmt baseline.', 'possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules.', 'it may also be that the inconsistency of improvement noted by  #TAUTHOR_TAG is the cause ; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here.', 'to explore this, we look at the approximate oracle']",0
"[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","['translation ( mt ) is the automatic translation of text from one human language to another.', 'statistical mt accomplishes this through a probabilistic model of the translation process.', 'in phrase - based statistical mt ( psmt ), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model.', 'the distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence.', 'as such, although psmt has been very successful, it suffers from the lack of a principled mechanism for handling long - distance reordering phenomena due to word order differences between languages.', 'one method for addressing this difficulty is the reordering - as - preprocessing approach, exemplified by  #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG, where psmt is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order.', 'although this leads to improved performance overall,  #TAUTHOR_TAG show that the reordering - as - preprocessing system does not consistently provide better translations than the psmt baseline on a sentence - by - sentence basis.', '']",1
"[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","['translation ( mt ) is the automatic translation of text from one human language to another.', 'statistical mt accomplishes this through a probabilistic model of the translation process.', 'in phrase - based statistical mt ( psmt ), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model.', 'the distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence.', 'as such, although psmt has been very successful, it suffers from the lack of a principled mechanism for handling long - distance reordering phenomena due to word order differences between languages.', 'one method for addressing this difficulty is the reordering - as - preprocessing approach, exemplified by  #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG, where psmt is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order.', 'although this leads to improved performance overall,  #TAUTHOR_TAG show that the reordering - as - preprocessing system does not consistently provide better translations than the psmt baseline on a sentence - by - sentence basis.', '']",5
"[' #TAUTHOR_TAG, but any reordering preprocessing step could equally be used.', 'further details of our systems are given in § 4']","[' #TAUTHOR_TAG, but any reordering preprocessing step could equally be used.', 'further details of our systems are given in § 4']","[' #TAUTHOR_TAG, but any reordering preprocessing step could equally be used.', 'further details of our systems are given in § 4']","['', 'in addition to the difference with the distortion model mentioned above, our work differs in that  #AUTHOR_TAG focuses on finding the best reordering using syntactic features plus a few surface and pos - tag features as a way of "" guarding against parsing errors "", whereas we also look at using features to represent confidence in a parse.', '3 dual - path psmt in this paper, we develop a dual - path psmt system.', '§ 3. 1 introduces the lattice input format, by which we provide the system with two variants of the input sentence : the original and the reordered alternative produced by the preprocessing step.', '§ 3. 2 outlines the confidence features that we include in the translation model to help the system choose between the two alternatives.', 'our system is built upon the psmt system moses.', 'for reordering, we use the berkeley parser  #AUTHOR_TAG and the rules given by  #TAUTHOR_TAG, but any reordering preprocessing step could equally be used.', 'further details of our systems are given in § 4']",5
['##implement the  #TAUTHOR_TAG reordering'],"['reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', '']","['the reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', 'we use the berkeley parser  #AUTHOR_TAG, repository revision 14, 6 to provide']","['baseline psmt system is moses, repository revision 3590.', '4 we run all of our experiments using the moses experiment management system ; configuration files and scripts to reproduce our experiments are available online.', '5 for the reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', 'we use the berkeley parser  #AUTHOR_TAG, repository revision 14, 6 to provide the parse trees for the reordering process.', 'since the german parsing model provided on the parser website does not include the function labels needed by the  #TAUTHOR_TAG rules, we trained a new parsing model on the tiger corpus ( version 1 ).', 'the reordering script and parsing model, along with details of how the parsing model was trained, are available online with the configuration files above.', 'we compare four systems on german - toenglish translation : the moses baseline ( moses ), the  #TAUTHOR_TAG baseline ( reorder ), the lattice system with just the reordering indicator feature ( lattice ), and the lattice system with all 3 it is possible that in practice the imbalance in number of non - zero features between the two paths could cause the system some difficulty in assigning the weights for each feature.', 'in future it would be interesting to investigate this possibility by introducing extra features to balance the two paths.', 'confidence features ( + features ).', 'we do not explore different subsets of the features here.', 'for evaluation we use the standard bleu metric  #AUTHOR_TAG, which measures n - gram overlap between the candidate translation and the given reference translation']",5
['##implement the  #TAUTHOR_TAG reordering'],"['reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', '']","['the reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', 'we use the berkeley parser  #AUTHOR_TAG, repository revision 14, 6 to provide']","['baseline psmt system is moses, repository revision 3590.', '4 we run all of our experiments using the moses experiment management system ; configuration files and scripts to reproduce our experiments are available online.', '5 for the reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', 'we use the berkeley parser  #AUTHOR_TAG, repository revision 14, 6 to provide the parse trees for the reordering process.', 'since the german parsing model provided on the parser website does not include the function labels needed by the  #TAUTHOR_TAG rules, we trained a new parsing model on the tiger corpus ( version 1 ).', 'the reordering script and parsing model, along with details of how the parsing model was trained, are available online with the configuration files above.', 'we compare four systems on german - toenglish translation : the moses baseline ( moses ), the  #TAUTHOR_TAG baseline ( reorder ), the lattice system with just the reordering indicator feature ( lattice ), and the lattice system with all 3 it is possible that in practice the imbalance in number of non - zero features between the two paths could cause the system some difficulty in assigning the weights for each feature.', 'in future it would be interesting to investigate this possibility by introducing extra features to balance the two paths.', 'confidence features ( + features ).', 'we do not explore different subsets of the features here.', 'for evaluation we use the standard bleu metric  #AUTHOR_TAG, which measures n - gram overlap between the candidate translation and the given reference translation']",5
"['by  #TAUTHOR_TAG.', 'however, this is']","['by  #TAUTHOR_TAG.', 'however, this is']","['by  #TAUTHOR_TAG.', 'however, this is most likely']","['data we use the corpora provided for the 2010 workshop on statistical machine translation 7 translation task.', 'the number of sentence pairs in each corpus are given in table 2.', 'we trained 5 - gram language models with srilm  #AUTHOR_TAG using the three language model files listed in table 3 : bleu scores for every system the last containing the remainder.', 'one language model was produced for each file or subfile, giving a total of ten models.', 'the final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus.', 'table 3 gives the bleu score for each of our four systems and the approximate oracle.', 'we note that these numbers are lower than those reported by  #TAUTHOR_TAG.', 'however, this is most likely due to differences in the training and testing data ; our results are roughly in line with the numbers reported in the euromatrix project for this test set.', '8 interestingly, our reimplementation of the  #TAUTHOR_TAG baseline does not outperform the plain psmt baseline.', 'possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules.', 'it may also be that the inconsistency of improvement noted by  #TAUTHOR_TAG is the cause ; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here.', 'to explore this, we look at the approximate oracle']",5
"['by  #TAUTHOR_TAG.', 'however, this is']","['by  #TAUTHOR_TAG.', 'however, this is']","['by  #TAUTHOR_TAG.', 'however, this is most likely']","['data we use the corpora provided for the 2010 workshop on statistical machine translation 7 translation task.', 'the number of sentence pairs in each corpus are given in table 2.', 'we trained 5 - gram language models with srilm  #AUTHOR_TAG using the three language model files listed in table 3 : bleu scores for every system the last containing the remainder.', 'one language model was produced for each file or subfile, giving a total of ten models.', 'the final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus.', 'table 3 gives the bleu score for each of our four systems and the approximate oracle.', 'we note that these numbers are lower than those reported by  #TAUTHOR_TAG.', 'however, this is most likely due to differences in the training and testing data ; our results are roughly in line with the numbers reported in the euromatrix project for this test set.', '8 interestingly, our reimplementation of the  #TAUTHOR_TAG baseline does not outperform the plain psmt baseline.', 'possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules.', 'it may also be that the inconsistency of improvement noted by  #TAUTHOR_TAG is the cause ; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here.', 'to explore this, we look at the approximate oracle']",5
"[' #TAUTHOR_TAG system, contrary to their']","[' #TAUTHOR_TAG system, contrary to their']","['choosing whether or not to use the syntactically - informed reordering.', 'while our reordering step is a reimplementation of the  #TAUTHOR_TAG system, contrary to their']","['', 'we then augment the translation model of our system with a number of features to express our confidence in the reordering.', 'while these features do not yield further improvement, a rough upper bound provided by our approximate oracle suggests that other features may still be found to guide the system in choosing whether or not to use the syntactically - informed reordering.', 'while our reordering step is a reimplementation of the  #TAUTHOR_TAG system, contrary to their findings we do not see an improvement using the reordering step alone.', 'this provides evidence against the idea that reordering improves translation performance absolutely.', 'however, our success with the lattice system highlights the fact that it is useful for some sentences, and that syntactic confidence features may provide a mechanism for identifying which sentences, thus incorporating syntactic information into phrase - based statistical machine translation in a useful way']",5
"[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","[' #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG,']","['translation ( mt ) is the automatic translation of text from one human language to another.', 'statistical mt accomplishes this through a probabilistic model of the translation process.', 'in phrase - based statistical mt ( psmt ), translation proceeds by dividing a sentence into sequences of adjacent words called phrases, then translating each phrase and reordering the phrases according to a distortion model.', 'the distortion model may be lexicalised but does not typically incorporate information about the syntactic structure of the sentence.', 'as such, although psmt has been very successful, it suffers from the lack of a principled mechanism for handling long - distance reordering phenomena due to word order differences between languages.', 'one method for addressing this difficulty is the reordering - as - preprocessing approach, exemplified by  #TAUTHOR_TAG and xia and mc  #AUTHOR_TAG, where psmt is coupled with a preprocessing step that reorders input sentences to more closely parallel the target language word order.', 'although this leads to improved performance overall,  #TAUTHOR_TAG show that the reordering - as - preprocessing system does not consistently provide better translations than the psmt baseline on a sentence - by - sentence basis.', '']",4
"['explore the  #TAUTHOR_TAG finding by examining whether machine learning techniques can be used to predict, on']","['explore the  #TAUTHOR_TAG finding by examining whether machine learning techniques can be used to predict, on']","['explore the  #TAUTHOR_TAG finding by examining whether machine learning techniques can be used to predict, on a sentence - by - sentence basis, whether the translation of']","['explore the  #TAUTHOR_TAG finding by examining whether machine learning techniques can be used to predict, on a sentence - by - sentence basis, whether the translation of the reordered sentence is to be preferred over the alternative.', 'for features, they use sentence length, parse probability from the collins parser and unlinked fragment count from the link grammar parser on the english side of the translation.', 'the authors find that, when used on the source side ( in english - to - dutch translation ), these features provide no significant improvement in bleu score, while as target - side features ( in dutch - to - english translation ) they improve the bleu score by 1. 7 points over and above the 1. 3 point improvement from reordering.', 'our work has some similarities to that of  #AUTHOR_TAG but uses the log - linear model of the translation system itself to include features, rather than a separate classifier that does not permit interaction between the confidence features and features used during translation.', 'this idea of using linguistic features to improve statistical mt has appeared in a number of recent papers.', ' #AUTHOR_TAG demonstrate an improvement in hierarchical psmt and syntaxbased ( string - to - tree ) statistical mt through the addition of features pinpointing possible errors in the translation, for example the number of occurrences of a particular grammar production rule, or non - terminal in a rule.', ' #AUTHOR_TAG derive features from the link grammar parser, in combination with word posterior probabilities, to detect mt errors ( in order to subsequently improve translation quality ).', ' #AUTHOR_TAG, we work with psmt and use features that consider the parse tree as a whole or aspects of the reordering process itself.', ' #AUTHOR_TAG, we use these features directly in translation']",4
['##implement the  #TAUTHOR_TAG reordering'],"['reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', '']","['the reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', 'we use the berkeley parser  #AUTHOR_TAG, repository revision 14, 6 to provide']","['baseline psmt system is moses, repository revision 3590.', '4 we run all of our experiments using the moses experiment management system ; configuration files and scripts to reproduce our experiments are available online.', '5 for the reordering preprocessing step we reimplement the  #TAUTHOR_TAG reordering - aspreprocessing system as our second baseline.', 'we use the berkeley parser  #AUTHOR_TAG, repository revision 14, 6 to provide the parse trees for the reordering process.', 'since the german parsing model provided on the parser website does not include the function labels needed by the  #TAUTHOR_TAG rules, we trained a new parsing model on the tiger corpus ( version 1 ).', 'the reordering script and parsing model, along with details of how the parsing model was trained, are available online with the configuration files above.', 'we compare four systems on german - toenglish translation : the moses baseline ( moses ), the  #TAUTHOR_TAG baseline ( reorder ), the lattice system with just the reordering indicator feature ( lattice ), and the lattice system with all 3 it is possible that in practice the imbalance in number of non - zero features between the two paths could cause the system some difficulty in assigning the weights for each feature.', 'in future it would be interesting to investigate this possibility by introducing extra features to balance the two paths.', 'confidence features ( + features ).', 'we do not explore different subsets of the features here.', 'for evaluation we use the standard bleu metric  #AUTHOR_TAG, which measures n - gram overlap between the candidate translation and the given reference translation']",4
"['by  #TAUTHOR_TAG.', 'however, this is']","['by  #TAUTHOR_TAG.', 'however, this is']","['by  #TAUTHOR_TAG.', 'however, this is most likely']","['data we use the corpora provided for the 2010 workshop on statistical machine translation 7 translation task.', 'the number of sentence pairs in each corpus are given in table 2.', 'we trained 5 - gram language models with srilm  #AUTHOR_TAG using the three language model files listed in table 3 : bleu scores for every system the last containing the remainder.', 'one language model was produced for each file or subfile, giving a total of ten models.', 'the final language model was produced by interpolation between these ten, with weights assigned based on the tuning corpus.', 'table 3 gives the bleu score for each of our four systems and the approximate oracle.', 'we note that these numbers are lower than those reported by  #TAUTHOR_TAG.', 'however, this is most likely due to differences in the training and testing data ; our results are roughly in line with the numbers reported in the euromatrix project for this test set.', '8 interestingly, our reimplementation of the  #TAUTHOR_TAG baseline does not outperform the plain psmt baseline.', 'possible explanations include variability due to differences in training data, noisier parser output in our system, or differing interpretation of the description of the reordering rules.', 'it may also be that the inconsistency of improvement noted by  #TAUTHOR_TAG is the cause ; sometimes the reordering produces better results and sometimes the baseline, with the effect just by chance favouring the baseline here.', 'to explore this, we look at the approximate oracle']",4
['those of  #TAUTHOR_TAG'],['those of  #TAUTHOR_TAG'],['those of  #TAUTHOR_TAG'],"['our experiment, the oracle preferred the baseline output in 848 cases and the reordered in 1, 070 cases.', '215 sentences were identical between the two systems, while in 392 cases the sentences differed but had equal numbers of n - gram overlaps.', ""the bleu score for the oracle is higher than that of both baselines ; from this and the distribution of the oracle's choices, we conclude that the difference between our findings and those of  #TAUTHOR_TAG is at least partly due to the inconsistency that they identified."", '']",4
"[' #TAUTHOR_TAG system, contrary to their']","[' #TAUTHOR_TAG system, contrary to their']","['choosing whether or not to use the syntactically - informed reordering.', 'while our reordering step is a reimplementation of the  #TAUTHOR_TAG system, contrary to their']","['', 'we then augment the translation model of our system with a number of features to express our confidence in the reordering.', 'while these features do not yield further improvement, a rough upper bound provided by our approximate oracle suggests that other features may still be found to guide the system in choosing whether or not to use the syntactically - informed reordering.', 'while our reordering step is a reimplementation of the  #TAUTHOR_TAG system, contrary to their findings we do not see an improvement using the reordering step alone.', 'this provides evidence against the idea that reordering improves translation performance absolutely.', 'however, our success with the lattice system highlights the fact that it is useful for some sentences, and that syntactic confidence features may provide a mechanism for identifying which sentences, thus incorporating syntactic information into phrase - based statistical machine translation in a useful way']",4
"[' #TAUTHOR_TAG.', 'working with german -']","[' #TAUTHOR_TAG.', 'working with german - to - english translation,  #TAUTHOR_TAG']","[' #TAUTHOR_TAG.', 'working with german -']",[' #TAUTHOR_TAG'],6
[' #TAUTHOR_TAG re'],[' #TAUTHOR_TAG reordering rules with a set of automatically - extracted reordering rules ( as in xia'],['the  #TAUTHOR_TAG reordering rules with a set of automatically - extracted reordering rules ( as in xia'],"['', 'this extension would be quite different from the lattice - based systems in § 2. 4, which are all based on a single parse.', 'for future systems, we would like to replace the  #TAUTHOR_TAG reordering rules with a set of automatically - extracted reordering rules ( as in xia and mc  #AUTHOR_TAG ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements.', 'the next major phase of this work is to extend and explore the feature space.', 'this entails examining subsets of confidence features to establish which are the most useful indicators of reliable reordering, and possibly replacing the mert tuning process with another algorithm, such as mira, to handle a greater quantity of features.', 'in addition, we wish to explore more fully our negative result with the reimplementation of the  #TAUTHOR_TAG system, to investigate the effect of balancing features in the lattice, and to examine the variability of the bleu scores for each system']",2
[' #TAUTHOR_TAG re'],[' #TAUTHOR_TAG reordering rules with a set of automatically - extracted reordering rules ( as in xia'],['the  #TAUTHOR_TAG reordering rules with a set of automatically - extracted reordering rules ( as in xia'],"['', 'this extension would be quite different from the lattice - based systems in § 2. 4, which are all based on a single parse.', 'for future systems, we would like to replace the  #TAUTHOR_TAG reordering rules with a set of automatically - extracted reordering rules ( as in xia and mc  #AUTHOR_TAG ) so that we may more easily explore the usefulness of our system and confidence features in new language pairs with a variety of reordering requirements.', 'the next major phase of this work is to extend and explore the feature space.', 'this entails examining subsets of confidence features to establish which are the most useful indicators of reliable reordering, and possibly replacing the mert tuning process with another algorithm, such as mira, to handle a greater quantity of features.', 'in addition, we wish to explore more fully our negative result with the reimplementation of the  #TAUTHOR_TAG system, to investigate the effect of balancing features in the lattice, and to examine the variability of the bleu scores for each system']",2
"['', 'more recently,  #TAUTHOR_TAG replaced']","['using a highway bi - lstm network.', 'more recently,  #TAUTHOR_TAG replaced']","['architecture using a highway bi - lstm network.', 'more recently,  #TAUTHOR_TAG replaced the common recurrent architecture with a self - attention network, directly capturing relationships between tokens']","['the first decade of the 21 st century, mapping from the syntactic analysis of a sentence to its semantic representation has received a central interest in the natural language processing ( nlp ) community.', 'semantic role labeling, which is a sentence - level semantic task aimed at identifying "" who did what to whom, and how, when and where? ""  #AUTHOR_TAG, has strengthened this focus.', 'recently, several neural mechanisms have been used to train end - to - end srl models that do not require task - specific feature engineering as the traditional srl models do.', ' #AUTHOR_TAG introduced the first deep end - to - end model for srl using a stacked bi - lstm network with a conditional random field ( crf ) as the top layer.', ' #AUTHOR_TAG simplified their architecture using a highway bi - lstm network.', 'more recently,  #TAUTHOR_TAG replaced the common recurrent architecture with a self - attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training.', 'the work in deep end - to - end srl has focused heavily on applying deep learning advances without considering the multilingual aspect.', 'however, language - specific characteristics and the available amount of training data highly influence the optimal model structure.', 'damesrl facilitates exploration and fair evaluation of new srl models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization.', 'beyond the existing state - of - the - art models  #TAUTHOR_TAG, we exploit character - level modeling, beneficial when considering multiple languages.', 'to demonstrate the merits of easy cross - lingual exploration and evaluation of model structures for srl provided by damesrl, we report performance of several distinct models integrated into our framework for english, german and arabic, as they have very different linguistic characteristics.', 'by w p.', 'here, words outside argument spans have the tag o, and words at the beginning and inside of argument spans with role r have the tags b r and i r, respectively.', 'for example, the sentence "" the cat chases the dog. "" should be annotated as "" the b−a0 cat i−a0 chases b−v the b−a1 dog i−a1. o "".', ""damesrl's architecture ( see fig. 1 ) facilitates the construction of models that prioritize certain language - dependent linguistic properties, such as the importance of word order and inflection, or that adapt to the amount of available training data."", 'the framework, implemented in python 3. 5 using tensorflow, can be used to']",0
"['', 'more recently,  #TAUTHOR_TAG replaced']","['using a highway bi - lstm network.', 'more recently,  #TAUTHOR_TAG replaced']","['architecture using a highway bi - lstm network.', 'more recently,  #TAUTHOR_TAG replaced the common recurrent architecture with a self - attention network, directly capturing relationships between tokens']","['the first decade of the 21 st century, mapping from the syntactic analysis of a sentence to its semantic representation has received a central interest in the natural language processing ( nlp ) community.', 'semantic role labeling, which is a sentence - level semantic task aimed at identifying "" who did what to whom, and how, when and where? ""  #AUTHOR_TAG, has strengthened this focus.', 'recently, several neural mechanisms have been used to train end - to - end srl models that do not require task - specific feature engineering as the traditional srl models do.', ' #AUTHOR_TAG introduced the first deep end - to - end model for srl using a stacked bi - lstm network with a conditional random field ( crf ) as the top layer.', ' #AUTHOR_TAG simplified their architecture using a highway bi - lstm network.', 'more recently,  #TAUTHOR_TAG replaced the common recurrent architecture with a self - attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training.', 'the work in deep end - to - end srl has focused heavily on applying deep learning advances without considering the multilingual aspect.', 'however, language - specific characteristics and the available amount of training data highly influence the optimal model structure.', 'damesrl facilitates exploration and fair evaluation of new srl models for different languages by providing flexible neural model construction on different modeling levels, the handling of various input and output formats, and clear output visualization.', 'beyond the existing state - of - the - art models  #TAUTHOR_TAG, we exploit character - level modeling, beneficial when considering multiple languages.', 'to demonstrate the merits of easy cross - lingual exploration and evaluation of model structures for srl provided by damesrl, we report performance of several distinct models integrated into our framework for english, german and arabic, as they have very different linguistic characteristics.', 'by w p.', 'here, words outside argument spans have the tag o, and words at the beginning and inside of argument spans with role r have the tags b r and i r, respectively.', 'for example, the sentence "" the cat chases the dog. "" should be annotated as "" the b−a0 cat i−a0 chases b−v the b−a1 dog i−a1. o "".', ""damesrl's architecture ( see fig. 1 ) facilitates the construction of models that prioritize certain language - dependent linguistic properties, such as the importance of word order and inflection, or that adapt to the amount of available training data."", 'the framework, implemented in python 3. 5 using tensorflow, can be used to']",0
"['important srl cues are given through inflections, such as case markings in german and arabic.', 'despite the foreseen importance, character - level embeddings have not been used in previous work  #TAUTHOR_TAG.', '']","['important srl cues are given through inflections, such as case markings in german and arabic.', 'despite the foreseen importance, character - level embeddings have not been used in previous work  #TAUTHOR_TAG.', '']","['important srl cues are given through inflections, such as case markings in german and arabic.', 'despite the foreseen importance, character - level embeddings have not been used in previous work  #TAUTHOR_TAG.', 'phase ii : as core sequence representation component, users can choose between a self - attention encoding  #TAUTHOR_TAG, a regular bi - lstm  #AUTHOR_TAG or a highway bi - lstm  #AUTHOR_TAG.', '']","['can be seen in fig. 1, the framework divides model construction in four phases : ( i ) word representation, ( ii ) sentence representation, ( iii ) output modeling, and ( iv ) inference.', 'phase i : the word representation of a word w i consist of three optional concatenated components : a word - embedding, a boolean indicating if w i is the predicate of the semantic frame ( w p ), and a character representation.', 'damesrl provides a bi - lstm network to learn character - level word representations helping for languages where important srl cues are given through inflections, such as case markings in german and arabic.', 'despite the foreseen importance, character - level embeddings have not been used in previous work  #TAUTHOR_TAG.', 'phase ii : as core sequence representation component, users can choose between a self - attention encoding  #TAUTHOR_TAG, a regular bi - lstm  #AUTHOR_TAG or a highway bi - lstm  #AUTHOR_TAG.', 'phase iii : to compute model probabilities, users can choose a regular softmax, or a linear chain crf as proposed by  #AUTHOR_TAG, which can be useful for languages where word order is an important srl cue, such as english, or when less training data is available ( shown in section 4 ).', 'phase iv : the inference phase provides two options for label inference from the computed model probabilities including greedy prediction and viterbi decoding']",0
"['important srl cues are given through inflections, such as case markings in german and arabic.', 'despite the foreseen importance, character - level embeddings have not been used in previous work  #TAUTHOR_TAG.', '']","['important srl cues are given through inflections, such as case markings in german and arabic.', 'despite the foreseen importance, character - level embeddings have not been used in previous work  #TAUTHOR_TAG.', '']","['important srl cues are given through inflections, such as case markings in german and arabic.', 'despite the foreseen importance, character - level embeddings have not been used in previous work  #TAUTHOR_TAG.', 'phase ii : as core sequence representation component, users can choose between a self - attention encoding  #TAUTHOR_TAG, a regular bi - lstm  #AUTHOR_TAG or a highway bi - lstm  #AUTHOR_TAG.', '']","['can be seen in fig. 1, the framework divides model construction in four phases : ( i ) word representation, ( ii ) sentence representation, ( iii ) output modeling, and ( iv ) inference.', 'phase i : the word representation of a word w i consist of three optional concatenated components : a word - embedding, a boolean indicating if w i is the predicate of the semantic frame ( w p ), and a character representation.', 'damesrl provides a bi - lstm network to learn character - level word representations helping for languages where important srl cues are given through inflections, such as case markings in german and arabic.', 'despite the foreseen importance, character - level embeddings have not been used in previous work  #TAUTHOR_TAG.', 'phase ii : as core sequence representation component, users can choose between a self - attention encoding  #TAUTHOR_TAG, a regular bi - lstm  #AUTHOR_TAG or a highway bi - lstm  #AUTHOR_TAG.', 'phase iii : to compute model probabilities, users can choose a regular softmax, or a linear chain crf as proposed by  #AUTHOR_TAG, which can be useful for languages where word order is an important srl cue, such as english, or when less training data is available ( shown in section 4 ).', 'phase iv : the inference phase provides two options for label inference from the computed model probabilities including greedy prediction and viterbi decoding']",0
['from comparable sentences in  #TAUTHOR_TAG using multisentence alignment'],['from comparable sentences in  #TAUTHOR_TAG using multisentence alignment'],['templates containing concepts and typical strings were induced from comparable sentences in  #TAUTHOR_TAG using multisentence alignment'],"['', 'paraphrase templates containing concepts and typical strings were induced from comparable sentences in  #TAUTHOR_TAG using multisentence alignment to discover "" variable "" and fixed structures.', 'linguistic patterns were applied to huge amounts of non - annotated pre - classified texts in  #AUTHOR_TAG to bootstrap information extraction patterns.', 'similarly, semi - supervised or unsupervised methods have been used to learn question / answering patterns  #AUTHOR_TAG or text schemas  #AUTHOR_TAG.', 'one current paradigm to learn from raw data is open information extraction  #AUTHOR_TAG, which without any prior knowledge aims at discovering all possible relations between pairs of entities occurring in text.', 'our work tries to learn the main concepts making up the template structure in domain summaries, similar to  #AUTHOR_TAG.', 'however, we do not rely on any source of external knowledge']",0
"[' #TAUTHOR_TAG, which are computationally expensive']","[' #TAUTHOR_TAG, which are computationally expensive']","['clustering - based approaches  #TAUTHOR_TAG, which are computationally expensive']","['', 'in the aviation domain, for example, numeric expressions constitute the extensions of different concepts including : number of victims, crew members, and number of survivors ; it is a rather common feature in the aviation domain to include these different concepts together in one sentence, making their "" separation "" complicated.', 'same explanations apply to other tested domains : for example locations playing the role of origin and destination of a given train or airplace are also sometimes confused.', 'our work demonstrates the possibility of learning conceptual information in several domains and languages, while previous work  #AUTHOR_TAG has addressed sets of related domains ( e. g., muc - 4 templates ) in english.', 'learning full conceptualizations from raw data is a daunting and difficult enterprise  #AUTHOR_TAG.', 'here, we provide a short - cut by proposing a method able to learn the essential concepts of a domain by relying on summaries which are freely available on the web.', 'our method is able to produce conceptualizations from a few documents in each domain and language unlike recent open domain information extraction which requires massive amount of texts for relation learning  #AUTHOR_TAG.', 'our algorithm has a reasonable computational complexity, unlike alignment - based or clustering - based approaches  #TAUTHOR_TAG, which are computationally expensive']",4
"['a joint space for images and text  #TAUTHOR_TAG.', '']","['a joint space for images and text  #TAUTHOR_TAG.', '']","['a joint space for images and text  #TAUTHOR_TAG.', '']","['recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images.', 'examples include text - based image retrieval, image description and visual question answering.', 'an increasing number of large image description datasets has become available  #AUTHOR_TAG and various systems have been proposed to handle the image description task as a generation problem  #AUTHOR_TAG.', 'there has also been a great deal of work on sentence - based image search or cross - modal retrieval where the objective is to learn a joint space for images and text  #TAUTHOR_TAG.', 'previous work on image description generation or learning a joint space for images and text has mostly focused on english due to the availability of english datasets.', 'recently there have been attempts to create image descriptions and models for other languages  #AUTHOR_TAG.', 'most work on learning a joint space for images and their descriptions is based on canonical correlation analysis ( cca ) or neural variants of cca over representations of image and its descriptions  #AUTHOR_TAG.', 'besides cca, a few others learn a visual - semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function  #TAUTHOR_TAG or by aligning image regions ( objects ) and segments of the description  #AUTHOR_TAG in a common space.', '']",0
"['a joint space for images and text  #TAUTHOR_TAG.', '']","['a joint space for images and text  #TAUTHOR_TAG.', '']","['a joint space for images and text  #TAUTHOR_TAG.', '']","['recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images.', 'examples include text - based image retrieval, image description and visual question answering.', 'an increasing number of large image description datasets has become available  #AUTHOR_TAG and various systems have been proposed to handle the image description task as a generation problem  #AUTHOR_TAG.', 'there has also been a great deal of work on sentence - based image search or cross - modal retrieval where the objective is to learn a joint space for images and text  #TAUTHOR_TAG.', 'previous work on image description generation or learning a joint space for images and text has mostly focused on english due to the availability of english datasets.', 'recently there have been attempts to create image descriptions and models for other languages  #AUTHOR_TAG.', 'most work on learning a joint space for images and their descriptions is based on canonical correlation analysis ( cca ) or neural variants of cca over representations of image and its descriptions  #AUTHOR_TAG.', 'besides cca, a few others learn a visual - semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function  #TAUTHOR_TAG or by aligning image regions ( objects ) and segments of the description  #AUTHOR_TAG in a common space.', '']",0
['models of  #TAUTHOR_TAG and  #AUTHOR_TAG and'],['models of  #TAUTHOR_TAG and  #AUTHOR_TAG and'],['of  #TAUTHOR_TAG and  #AUTHOR_TAG and'],"['', 'we report results for both english and german descriptions.', 'note that we have one single model for both languages.', 'in tables 1 and 2 we present the ranking results of the baseline models of  #TAUTHOR_TAG and  #AUTHOR_TAG and our proposed pivot and parallel models.', 'we do not compare our image - description ranking results with  #AUTHOR_TAG since they report results on half of validation set of multi30k whereas our results are on the publicly available test set of multi30k.', 'for english, pivot with asymmetric similarity is either competitive or better than monolingual models 4 https : / / github. com / ivendrov / order - embedding and symmetric similarity, especially in the r @ 10 category it obtains state - of - the - art.', 'for german, both pivot and parallel with the asymmetric scoring function outperform monolingual models and symmetric similarity.', 'we also observe that the german ranking experiments benefit the most from the multilingual signal.', 'a reason for this could be that the german description corpus has many singleton words ( more than 50 % of the vocabulary ) and english description mapping might have helped in learning better semantic embeddings.', 'these results suggest that the multilingual signal could be used to learn better multimodal embeddings, irrespective of the language.', 'our results also show that the asymmetric scoring function can help learn better embeddings.', 'in table 3 we present a few examples where pivot - asym and parallel - asym models performed better on both the languages compared to baseline order embedding model even using descriptions of very different lengths as queries']",0
[' #TAUTHOR_TAG vg'],['a pair of sen -  #AUTHOR_TAG − 83. 7 84. 5 85. 0 mlmme  #AUTHOR_TAG vgg19 − 72. 7 79. 7 vse  #TAUTHOR_TAG vgg19 80. 6 82. 7 89. 6'],['. 7 79. 7 vse  #TAUTHOR_TAG vgg19 80. 6 82. 7 89. 6'],"['the semantic textual similarity task ( sts ), we use the textual embeddings from our model to compute the similarity between a pair of sen -  #AUTHOR_TAG − 83. 7 84. 5 85. 0 mlmme  #AUTHOR_TAG vgg19 − 72. 7 79. 7 vse  #TAUTHOR_TAG vgg19 80. 6 82. 7 89. 6 oe  #AUTHOR_TAG vgg19 82.', 'tences ( image descriptions in this case ).', '']",0
"['multimodal embeddings in a single language  #TAUTHOR_TAG.', '']","['multimodal embeddings in a single language  #TAUTHOR_TAG.', '']","['learning multimodal embeddings in a single language  #TAUTHOR_TAG.', '']","['both pivot and parallel we use a deep convolutional neural network architecture ( cnn ) to represent the image i denoted by f i ( i ) = w i · cnn ( i ) where w i is a learned weight matrix and cnn ( i ) is the image vector representation.', 'for each language we define a recurrent neural network encoder f c ( c k ) = gru ( c k ) with gated recurrent units ( gru ) activations to encode the description c k.', 'in pivot, we use monolingual corpora from multiple languages of sentences aligned with images to learn the joint space.', 'the intuition of this model is that an image is a universal representation across all languages, and if we constrain a sentence representation to be closer to image, sentences in different languages may also come closer.', 'accordingly we design a loss function as follows :', 'where k stands for each language.', 'this loss function encourages the similarity s ( c k, i ) between gold - standard description c k and image i to be greater than any other irrelevant description c k by a margin α.', 'a similar loss function is useful for learning multimodal embeddings in a single language  #TAUTHOR_TAG.', 'for each minibatch, we obtain invalid descriptions by selecting descriptions of other images except the current image of interest and vice - versa.', 'in parallel, in addition to making an image similar to a description, we make multiple descriptions of the same image in different languages similar to each other, based on the assumption that these descriptions, although not parallel, share some commonalities.', 'accordingly we enhance the previous loss function with an additional term :', 'note that we are iterating over all pairs of descriptions ( c 1, c 2 ), and maximizing the similarity between descriptions of the same image and at the same time minimizing the similarity between descriptions of different images.', 'we learn models using two similarity functions : symmetric and asymmetric.', 'for the former we use cosine similarity and for the latter we use the metric of  #AUTHOR_TAG which is useful for learning embeddings that maintain an order, e. g., dog and cat are more closer to pet than animal while being distinct.', 'such ordering is shown to be useful in building effective multimodal space of images and texts.', 'an analogy in our setting would be two descriptions of an image are closer to the image while at the same time preserving the identity of each ( which is useful when sentences describe two different aspects of the image ).', 'the similarity metric is defined as :', 'where a and b are embeddings of image and description.', 'we call the symmetric similarity variants of our models as pivot - sym and parallel -']",1
"['##ish gaelic ( usually hereafter gaelic ) is a celtic language, rather closely related to irish, with around 59, 000 speakers as of the last uk census in 2011.', 'as opposed to the situation for irish gaelic  #AUTHOR_TAG a ;  #TAUTHOR_TAG there are no treebanks']","['##ish gaelic ( usually hereafter gaelic ) is a celtic language, rather closely related to irish, with around 59, 000 speakers as of the last uk census in 2011.', 'as opposed to the situation for irish gaelic  #AUTHOR_TAG a ;  #TAUTHOR_TAG there are no treebanks']","['##ish gaelic ( usually hereafter gaelic ) is a celtic language, rather closely related to irish, with around 59, 000 speakers as of the last uk census in 2011.', 'as opposed to the situation for irish gaelic  #AUTHOR_TAG a ;  #TAUTHOR_TAG there are no treebanks']","['##ish gaelic ( usually hereafter gaelic ) is a celtic language, rather closely related to irish, with around 59, 000 speakers as of the last uk census in 2011.', 'as opposed to the situation for irish gaelic  #AUTHOR_TAG a ;  #TAUTHOR_TAG there are no treebanks or tagging schemes for scottish gaelic, although there are machine - readable dictionaries and databases available from sabhal mor ostaig.', 'a single paper in the acl anthology  #AUTHOR_TAG mentions scottish gaelic in the context of computational dialectology of irish.', 'there is also an lrec workshop paper  #AUTHOR_TAG on machine translation between irish and scottish gaelic.', 'elsewhere in the celtic languages, welsh has an lfg grammar  #AUTHOR_TAG but no treebanks.', 'for breton there is a small amount of work on morphological analysis and constraint - grammar - based machine translation  #AUTHOR_TAG.', 'recent work on the grammar of scottish gaelic ( for example  #AUTHOR_TAG, but there are many more examples ) has largely focussed on theoretical syntactic issues somewhat distant from the more surfacy approaches popular in the field of natural language processing.', 'this paper explores grammatical issues in scottish gaelic by means of dependency tagging and combinatory categorial grammar ( ccg ), which we see as complementary approaches.', 'as such it is explicitly inspired by ccgbank  #AUTHOR_TAG, which consists of dependency structures and ccg derivations for over 99 % of the penn treebank.', 'it is hoped that this corpus will be a useful adjunct to currently on - going work in developing a part - of - speech tagset and tagger for scottish gaelic.', 'section 2 describes how the corpus was prepared, sections 3 and 4 give some context for the dependency scheme and categorial grammar annotations respectively, and the main part of the paper is section 5, which deals with language - specific features of the corpus']",0
"['irish  #AUTHOR_TAG a ;  #TAUTHOR_TAG, which is of']","['irish  #AUTHOR_TAG a ;  #TAUTHOR_TAG, which is of']","['irish  #AUTHOR_TAG a ;  #TAUTHOR_TAG, which is of']","['are four dependency schemes that we consulted while preparing the corpus.', 'the initial inspiration was provided by the c & c parser  #AUTHOR_TAG, which in addition to providing categorial grammar derivations for sentences provides a dependency structure in the gr ( grammatical representation ) scheme due to  #AUTHOR_TAG.', 'this contains 23 types and was developed originally for parser evaluation.', 'another popular scheme is the stanford dependency scheme ( de  #AUTHOR_TAG de  #AUTHOR_TAG, which is more finely - grained with over twice the number of dependency types to deal specifically with noisy data and to make it more accessible to non - linguists building information extraction applications.', 'a very important scheme is the dublin scheme for irish  #AUTHOR_TAG a ;  #TAUTHOR_TAG, which is of a similar size to the stanford scheme, but the reason for its size relative to gr is that it includes a large number of dependencies intended to handle grammatical features found in irish but not in english.', 'lastly we mention the universal dependency scheme developed in ( mc  #AUTHOR_TAG, which we have adopted, despite its being coarser - grained than the dublin scheme, on account of its simplicity and utility for cross - lingual comparisons and cross - training  #AUTHOR_TAG.', 'table 1 gives examples of the dependency relations used along with their mapping to the gr scheme']",0
"['scheme in  #TAUTHOR_TAG because of a difference between the two languages.', 'they treat the analogous']","['scheme in  #TAUTHOR_TAG because of a difference between the two languages.', 'they treat the analogous']","['described by a relative clause including the verb bi.', 'fig. 1 shows our dependency tree for this.', 'note that this is different from the scheme in  #TAUTHOR_TAG because of a difference between the two languages.', 'they treat the analogous sentence is tusa an muinte']","['', 'the other class of fused preposition - pronoun we need to consider is that in sentences like tha mi gad chluinntinn, "" i can hear you "", where gad is ag fused with do "" your "".', 'in this case it has type pp [ ag ] / s [ n ].', 'adjectives as in ccgbank are treated as clauses, s [ adj ].', 'the verbal noun is labelled s [ n ] by analogy with  #AUTHOR_TAG.', 'in addition to declarative and interrogative clauses, s [ dcl ] and s [ q ], we take our lead from the fourfold division of preverbal particles and add negative clauses s [ neg ], usually introduced by cha or chan, and negative interrogative clauses, s [ negq ], introduced by nach.', 'there are two verbs for "" to be "" in scottish gaelic, bi and is.', 'bi is used for predicate statements about nouns, to forming the present tense and to describe some psychological states.', 'it does not usually equate two nps, with an exception we will come to.', 'in the dublin scheme the prepositional phrase headed by ag in ta se ag iascaireacht ( "" he is fishing. "" ) is treated as being an externally - controlled complement of ta ( gaelic tha ) and we carry this analysis over into scottish gaelic where this is the most common way of expressing the present tense.', 'figure 1 demonstrates this, where dhachaigh is a non - clausal modifier of dol, the verbal noun for "" to go "".', 'is can be used as the copula between two nps, and to express psychological states such as liking and preference.', 'to say "" i am a teacher "", the gaelic is\'s e tidsear a th\'annam.', 'this, at least on the surface, equates pronoun e, with a noun described by a relative clause including the verb bi.', 'fig. 1 shows our dependency tree for this.', 'note that this is different from the scheme in  #TAUTHOR_TAG because of a difference between the two languages.', 'they treat the analogous sentence is tusa an muinte']",4
"['is available  #TAUTHOR_TAG.', 'recently, there']","['is available  #TAUTHOR_TAG.', 'recently, there']","['also be obtained from parallel corpora if such data is available  #TAUTHOR_TAG.', 'recently, there are also a number of studies that extract parap']","['the area of text mining for software engineering, paraphrases have been used in many tasks, e. g.,  #AUTHOR_TAG.', 'however, most paraphrases used are obtained manually.', 'a recent study using synonyms from wordnet highlights the fact that these are not effective in software engineering tasks due to domain specificity  #AUTHOR_TAG.', 'therefore, an automatic way to derive technical paraphrases specific to software engineering is desired.', 'paraphrases can be extracted from non - parallel corpora using contextual similarity  #AUTHOR_TAG.', 'they can also be obtained from parallel corpora if such data is available  #TAUTHOR_TAG.', 'recently, there are also a number of studies that extract paraphrases from multilingual corpora ( bannard and callison  #AUTHOR_TAG.', 'the approach in  #TAUTHOR_TAG does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.', 'due to this reason, we build our technique on top of  #TAUTHOR_TAG.', 'the following provides a summary of  #TAUTHOR_TAG.', 'two types of paraphrase patterns are defined : ( 1 ) syntactic patterns which consist of the pos tags of the phrases.', 'for example, the paraphrases "" a vga monitor "" and "" a monitor "" are represented as "" dt 1 jj nn 2 "" ↔ "" dt 1 nn 2 "", where the subscripts denote common words.', '( 2 ) contextual patterns which consist of the pos tags before and after the phrases.', 'for example, the contexts "" in the middle of "" and "" in middle of "" in table 1 ( bottom ) are represented as ""', 'during pre - processing, the parallel corpus is aligned to give a list of parallel sentence pairs.', 'the sentences are then processed by a pos tagger and a chunker.', ' #TAUTHOR_TAG first used identical words and phrases as seeds to find and score contextual patterns.', 'the patterns are scored based on the following formula : ( n + ) / n, in which, n + refers to the number of positively labeled paraphrases satisfying the patterns and n refers to the number of all paraphrases satisfying the patterns.', 'only patterns with scores above a threshold are considered.', 'more paraphrases are identified using these contextual patterns, and more patterns are then found and scored using the newly - discovered paraphrases.', 'this co - training algorithm is employed in an iterative fashion to find more patterns and positively labeled paraphrases']",0
"['is available  #TAUTHOR_TAG.', 'recently, there']","['is available  #TAUTHOR_TAG.', 'recently, there']","['also be obtained from parallel corpora if such data is available  #TAUTHOR_TAG.', 'recently, there are also a number of studies that extract parap']","['the area of text mining for software engineering, paraphrases have been used in many tasks, e. g.,  #AUTHOR_TAG.', 'however, most paraphrases used are obtained manually.', 'a recent study using synonyms from wordnet highlights the fact that these are not effective in software engineering tasks due to domain specificity  #AUTHOR_TAG.', 'therefore, an automatic way to derive technical paraphrases specific to software engineering is desired.', 'paraphrases can be extracted from non - parallel corpora using contextual similarity  #AUTHOR_TAG.', 'they can also be obtained from parallel corpora if such data is available  #TAUTHOR_TAG.', 'recently, there are also a number of studies that extract paraphrases from multilingual corpora ( bannard and callison  #AUTHOR_TAG.', 'the approach in  #TAUTHOR_TAG does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.', 'due to this reason, we build our technique on top of  #TAUTHOR_TAG.', 'the following provides a summary of  #TAUTHOR_TAG.', 'two types of paraphrase patterns are defined : ( 1 ) syntactic patterns which consist of the pos tags of the phrases.', 'for example, the paraphrases "" a vga monitor "" and "" a monitor "" are represented as "" dt 1 jj nn 2 "" ↔ "" dt 1 nn 2 "", where the subscripts denote common words.', '( 2 ) contextual patterns which consist of the pos tags before and after the phrases.', 'for example, the contexts "" in the middle of "" and "" in middle of "" in table 1 ( bottom ) are represented as ""', 'during pre - processing, the parallel corpus is aligned to give a list of parallel sentence pairs.', 'the sentences are then processed by a pos tagger and a chunker.', ' #TAUTHOR_TAG first used identical words and phrases as seeds to find and score contextual patterns.', 'the patterns are scored based on the following formula : ( n + ) / n, in which, n + refers to the number of positively labeled paraphrases satisfying the patterns and n refers to the number of all paraphrases satisfying the patterns.', 'only patterns with scores above a threshold are considered.', 'more paraphrases are identified using these contextual patterns, and more patterns are then found and scored using the newly - discovered paraphrases.', 'this co - training algorithm is employed in an iterative fashion to find more patterns and positively labeled paraphrases']",0
"['', 'on  #TAUTHOR_TAG']","['the set of patterns with affixed pattern scores based', 'on  #TAUTHOR_TAG']","['of patterns with affixed pattern scores based', 'on  #TAUTHOR_TAG']",[' #TAUTHOR_TAG'],0
['publishing code alongside their papers  #TAUTHOR_TAG but other papers have not released code  #AUTHOR_TAG. in'],"['publishing code alongside their papers  #TAUTHOR_TAG but other papers have not released code  #AUTHOR_TAG. in some', 'cases,']",['publishing code alongside their papers  #TAUTHOR_TAG but other papers have not released code  #AUTHOR_TAG. in'],"['0 international licence. licence details : http : / / creativecommons. org / licenses /', 'by / 4. 0 / 1 we follow the definitions in antske fokkens\'guest blog post "" replication ( obtaining the same results using the same experiment ) as well as reproduction ( reach the same conclusion through different means ) "" from http : / / coling2018. org', '/ slowly - growing - offspring - zigglebottom - anno - 2017 - guest - post / are evaluated on the semeval dataset  #AUTHOR_TAG but not all. datasets can vary by domain ( e. g. product ), type', '( social media, review ), or medium ( written or spoken ), and to date there has been no comparative evaluation of methods from these multiple classes. our primary and secondary contributions therefore', ', are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of tdsa methods. in terms of reproducibility via code release, recent tdsa papers have generally been very good with regards to publishing code alongside their papers  #TAUTHOR_TAG but other papers have not released code  #AUTHOR_TAG. in some', 'cases, the code was initially made available, then removed, and is now back online  #AUTHOR_TAG a ). unfortunately, in some cases even when code has', 'been published, different results have been obtained relative to the original paper. this can be seen when  #AUTHOR_TAG used the code and embeddings in  #AUTHOR_TAG b ) they observe different results. similarly, when', 'others  #AUTHOR_TAG attempt to replicate the experiments of  #AUTHOR_TAG a ) they also produce different results to the original authors. our observations within this one sub -', '']",0
[' #TAUTHOR_TAG extended'],[' #TAUTHOR_TAG extended'],[' #TAUTHOR_TAG extended'],"['', '( gru ) which they called recurrent attention on memory ( ram ), and they found this method to allow models to better understand', 'more complex sentiment for each comparison. used neural pooling features e. g. max, min, etc of the word embeddings of the left and right context of the target', 'word, the target itself, and the whole tweet. they inputted the features into a linear svm, and showed the importance of using the left and right context for the first time. they found in their study that using a combination of word2vec embeddings and sentiment embeddings performed best alongside', 'using sentiment lexicons to filter the embedding space. other studies have adopted more linguistic approaches.  #TAUTHOR_TAG extended the work of by using the dependency linked words from the target.  #AUTHOR_TAG used the dependency tree to create a recursive neural network ( recnn ) inspired by  #AUTHOR_TAG but compared to  #AUTHOR_TAG they also utilised the', '']",0
[' #TAUTHOR_TAG extended'],[' #TAUTHOR_TAG extended'],[' #TAUTHOR_TAG extended'],"['', '( gru ) which they called recurrent attention on memory ( ram ), and they found this method to allow models to better understand', 'more complex sentiment for each comparison. used neural pooling features e. g. max, min, etc of the word embeddings of the left and right context of the target', 'word, the target itself, and the whole tweet. they inputted the features into a linear svm, and showed the importance of using the left and right context for the first time. they found in their study that using a combination of word2vec embeddings and sentiment embeddings performed best alongside', 'using sentiment lexicons to filter the embedding space. other studies have adopted more linguistic approaches.  #TAUTHOR_TAG extended the work of by using the dependency linked words from the target.  #AUTHOR_TAG used the dependency tree to create a recursive neural network ( recnn ) inspired by  #AUTHOR_TAG but compared to  #AUTHOR_TAG they also utilised the', '']",0
"['target multiple appearances issue as originally raised by  #TAUTHOR_TAG.', 'as']","['target multiple appearances issue as originally raised by  #TAUTHOR_TAG.', 'as']","['multiple appearances issue as originally raised by  #TAUTHOR_TAG.', 'as the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.', 'we therefore took the approach of  #TAUTHOR_TAG and']","['', 'target - dep uses both features of target - ind and target - dep -, and 4.', 'target - dep + uses the features of target - dep and adds two additional contexts left and right sentiment ( ls & rs ) contexts where only the words within a specified lexicon are kept and the rest of the words are zero vectors.', 'all of their experiments are performed on  #AUTHOR_TAG twitter data set.', 'for each of the experiments below we used the following configurations unless otherwise stated : we performed 5 fold stratified cross validation, features are scaled using max min scaling before inputting into the svm, and used the respective c - values for the svm stated in the paper for each of the models.', 'one major difficulty with the description of the method in the paper and re - implementation is handling the same target multiple appearances issue as originally raised by  #TAUTHOR_TAG.', 'as the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.', 'we therefore took the approach of  #TAUTHOR_TAG and found all of the features for each appearance and performed median pooling over features.', 'this change could explain the subtle differences between the results we report and those of the original paper']",0
['extended the np work of and instead of using the full tweet / sentence / text contexts they used the full dependency'],['extended the np work of and instead of using the full tweet / sentence / text contexts they used the full dependency'],"['extended the np work of and instead of using the full tweet / sentence / text contexts they used the full dependency graph of the target word.', '']","['extended the np work of and instead of using the full tweet / sentence / text contexts they used the full dependency graph of the target word.', 'thus, they created three different methods : 1. tdparse - uses only the full dependency graph context, 2.', 'tdparse the feature of tdparse - and the left and right contexts, and 3.', 'tdparse + the features of tdparse and ls and rs contexts.', 'the experiments are performed on the  #AUTHOR_TAG and  #TAUTHOR_TAG twitter datasets where we train and test on the previously specified train and test splits.', 'we also scale our features using max min scaling before inputting into the svm.', 'we used all three sentiment lexicons as in the original paper, and we found the c - value by performing 5 fold stratified cross validation on the training datasets.', 'the results of these experiments can be seen in figure 3 10.', 'as found with the results of replication, scaling is very important but is typically overlooked when reporting.', ' #AUTHOR_TAG a ) was the first to use lstms specifically for tdsa.', 'they created three different models : 1. lstm a standard lstm that runs over the length of the sentence and takes no target information into account, 2.', '']",0
['publishing code alongside their papers  #TAUTHOR_TAG but other papers have not released code  #AUTHOR_TAG. in'],"['publishing code alongside their papers  #TAUTHOR_TAG but other papers have not released code  #AUTHOR_TAG. in some', 'cases,']",['publishing code alongside their papers  #TAUTHOR_TAG but other papers have not released code  #AUTHOR_TAG. in'],"['0 international licence. licence details : http : / / creativecommons. org / licenses /', 'by / 4. 0 / 1 we follow the definitions in antske fokkens\'guest blog post "" replication ( obtaining the same results using the same experiment ) as well as reproduction ( reach the same conclusion through different means ) "" from http : / / coling2018. org', '/ slowly - growing - offspring - zigglebottom - anno - 2017 - guest - post / are evaluated on the semeval dataset  #AUTHOR_TAG but not all. datasets can vary by domain ( e. g. product ), type', '( social media, review ), or medium ( written or spoken ), and to date there has been no comparative evaluation of methods from these multiple classes. our primary and secondary contributions therefore', ', are to carry out the first study that reports results across all three different dataset classes, and to release a open source code framework implementing three complementary groups of tdsa methods. in terms of reproducibility via code release, recent tdsa papers have generally been very good with regards to publishing code alongside their papers  #TAUTHOR_TAG but other papers have not released code  #AUTHOR_TAG. in some', 'cases, the code was initially made available, then removed, and is now back online  #AUTHOR_TAG a ). unfortunately, in some cases even when code has', 'been published, different results have been obtained relative to the original paper. this can be seen when  #AUTHOR_TAG used the code and embeddings in  #AUTHOR_TAG b ) they observe different results. similarly, when', 'others  #AUTHOR_TAG attempt to replicate the experiments of  #AUTHOR_TAG a ) they also produce different results to the original authors. our observations within this one sub -', '']",1
"['have used, we have only had issue with parsing  #TAUTHOR_TAG where the annotations for']","['have used, we have only had issue with parsing  #TAUTHOR_TAG where the annotations for']","['contain three distinct sentiments (  #AUTHOR_TAG only has two ).', 'from the datasets we have used, we have only had issue with parsing  #TAUTHOR_TAG where the annotations for the first set of the data contains the target span but the second set does not.', 'thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.', 'an as example of this : "" got rid of bureaucrats\'and we put that money, into 9000 more doctors']","['are evaluating our models over six different english datasets deliberately chosen to represent a range of domains, types and mediums.', 'as highlighted above, previous papers tend to only carry out evaluations on one or two datasets which limits the generalisability of their results.', 'in this paper, we do not consider the quality or inter - annotator agreement levels of these datasets but it has been noted that some datasets may have issues here.', 'for example,  #AUTHOR_TAG point out that the  #AUTHOR_TAG dataset does not state their inter - annotator agreement scores nor do they have aspect terms that express neutral opinion.', 'we only use a subset of the english datasets available.', 'for two reasons.', 'first, the time it takes to write parsers and run the models.', 'second, we only used datasets that contain three distinct sentiments (  #AUTHOR_TAG only has two ).', 'from the datasets we have used, we have only had issue with parsing  #TAUTHOR_TAG where the annotations for the first set of the data contains the target span but the second set does not.', 'thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.', 'an as example of this : "" got rid of bureaucrats\'and we put that money, into 9000 more doctors and nurses \'... to turn the doctors into bureaucrats # battlefornumber10 "" in that tweet\'bureaucrats\'was annotated as negative but it does not state if it was the first or second instance of\'bureaucrats\'since it does not use target spans.', 'as we can see from table 2, generally the social media datasets ( twitter and youtube ) contain more targets per sentence with the exception of  #AUTHOR_TAG and  #AUTHOR_TAG.', 'the only dataset that has a small difference between the number of unique sentiments per sentence is the']",7
"['have used, we have only had issue with parsing  #TAUTHOR_TAG where the annotations for']","['have used, we have only had issue with parsing  #TAUTHOR_TAG where the annotations for']","['contain three distinct sentiments (  #AUTHOR_TAG only has two ).', 'from the datasets we have used, we have only had issue with parsing  #TAUTHOR_TAG where the annotations for the first set of the data contains the target span but the second set does not.', 'thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.', 'an as example of this : "" got rid of bureaucrats\'and we put that money, into 9000 more doctors']","['are evaluating our models over six different english datasets deliberately chosen to represent a range of domains, types and mediums.', 'as highlighted above, previous papers tend to only carry out evaluations on one or two datasets which limits the generalisability of their results.', 'in this paper, we do not consider the quality or inter - annotator agreement levels of these datasets but it has been noted that some datasets may have issues here.', 'for example,  #AUTHOR_TAG point out that the  #AUTHOR_TAG dataset does not state their inter - annotator agreement scores nor do they have aspect terms that express neutral opinion.', 'we only use a subset of the english datasets available.', 'for two reasons.', 'first, the time it takes to write parsers and run the models.', 'second, we only used datasets that contain three distinct sentiments (  #AUTHOR_TAG only has two ).', 'from the datasets we have used, we have only had issue with parsing  #TAUTHOR_TAG where the annotations for the first set of the data contains the target span but the second set does not.', 'thus making it impossible to use the second set of annotation and forcing us to only use a subset of the dataset.', 'an as example of this : "" got rid of bureaucrats\'and we put that money, into 9000 more doctors and nurses \'... to turn the doctors into bureaucrats # battlefornumber10 "" in that tweet\'bureaucrats\'was annotated as negative but it does not state if it was the first or second instance of\'bureaucrats\'since it does not use target spans.', 'as we can see from table 2, generally the social media datasets ( twitter and youtube ) contain more targets per sentence with the exception of  #AUTHOR_TAG and  #AUTHOR_TAG.', 'the only dataset that has a small difference between the number of unique sentiments per sentence is the']",7
"['the link to embeddings no longer works 8.', 'however, the embeddings were released through  #TAUTHOR_TAG code base 9 following requesting of the code from.', 'figure 1 shows the results of the different word embeddings across the different methods.', 'the main finding we see is that sswe by']","['the link to embeddings no longer works 8.', 'however, the embeddings were released through  #TAUTHOR_TAG code base 9 following requesting of the code from.', 'figure 1 shows the results of the different word embeddings across the different methods.', 'the main finding we see is that sswe by']","['the link to embeddings no longer works 8.', 'however, the embeddings were released through  #TAUTHOR_TAG code base 9 following requesting of the code from.', 'figure 1 shows the results of the different word embeddings across the different methods.', 'the main finding we see is that sswe by themselves are not as informative as w2v vectors which is different to the findings of.', 'however we agree that combining the two vectors is beneficial and that the rank of methods is the same in our observations']","['original authors tested their methods using three different word vectors : 1. word2vec trained by on 5 million tweets containing emoticons ( w2v ), 2.', 'sentiment specific word embedding ( sswe ) from, and 3.', 'w2v and sswe combined.', 'neither of these word embeddings are available from the original authors as never released the embeddings and the link to embeddings no longer works 8.', 'however, the embeddings were released through  #TAUTHOR_TAG code base 9 following requesting of the code from.', 'figure 1 shows the results of the different word embeddings across the different methods.', 'the main finding we see is that sswe by themselves are not as informative as w2v vectors which is different to the findings of.', 'however we agree that combining the two vectors is beneficial and that the rank of methods is the same in our observations']",7
"['target multiple appearances issue as originally raised by  #TAUTHOR_TAG.', 'as']","['target multiple appearances issue as originally raised by  #TAUTHOR_TAG.', 'as']","['multiple appearances issue as originally raised by  #TAUTHOR_TAG.', 'as the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.', 'we therefore took the approach of  #TAUTHOR_TAG and']","['', 'target - dep uses both features of target - ind and target - dep -, and 4.', 'target - dep + uses the features of target - dep and adds two additional contexts left and right sentiment ( ls & rs ) contexts where only the words within a specified lexicon are kept and the rest of the words are zero vectors.', 'all of their experiments are performed on  #AUTHOR_TAG twitter data set.', 'for each of the experiments below we used the following configurations unless otherwise stated : we performed 5 fold stratified cross validation, features are scaled using max min scaling before inputting into the svm, and used the respective c - values for the svm stated in the paper for each of the models.', 'one major difficulty with the description of the method in the paper and re - implementation is handling the same target multiple appearances issue as originally raised by  #TAUTHOR_TAG.', 'as the method requires context with regards to the target word, if there is more than one appearance of the target word then the method does not specify which to use.', 'we therefore took the approach of  #TAUTHOR_TAG and found all of the features for each appearance and performed median pooling over features.', 'this change could explain the subtle differences between the results we report and those of the original paper']",5
['extended the np work of and instead of using the full tweet / sentence / text contexts they used the full dependency'],['extended the np work of and instead of using the full tweet / sentence / text contexts they used the full dependency'],"['extended the np work of and instead of using the full tweet / sentence / text contexts they used the full dependency graph of the target word.', '']","['extended the np work of and instead of using the full tweet / sentence / text contexts they used the full dependency graph of the target word.', 'thus, they created three different methods : 1. tdparse - uses only the full dependency graph context, 2.', 'tdparse the feature of tdparse - and the left and right contexts, and 3.', 'tdparse + the features of tdparse and ls and rs contexts.', 'the experiments are performed on the  #AUTHOR_TAG and  #TAUTHOR_TAG twitter datasets where we train and test on the previously specified train and test splits.', 'we also scale our features using max min scaling before inputting into the svm.', 'we used all three sentiment lexicons as in the original paper, and we found the c - value by performing 5 fold stratified cross validation on the training datasets.', 'the results of these experiments can be seen in figure 3 10.', 'as found with the results of replication, scaling is very important but is typically overlooked when reporting.', ' #AUTHOR_TAG a ) was the first to use lstms specifically for tdsa.', 'they created three different models : 1. lstm a standard lstm that runs over the length of the sentence and takes no target information into account, 2.', '']",5
[' #TAUTHOR_TAG has shown that textual information'],[' #TAUTHOR_TAG has shown that textual information'],"['the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.', 'the combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.', 'recent work  #TAUTHOR_TAG has shown that textual information can be used to predict emojis associated to text.', 'in this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in']","['the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.', 'the combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.', 'recent work  #TAUTHOR_TAG has shown that textual information can be used to predict emojis associated to text.', 'in this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models.', 'we explore the use of emojis in the social media platform instagram.', 'we put forward a multimodal approach to predict the emojis associated to an instagram post, given its picture and text 1.', 'our task and experimental framework are similar to  #TAUTHOR_TAG, however, we use different data ( instagram instead of twitter ) and, in addition, we rely on images to improve the selection of the most likely emojis to associate to a post.', 'we show that a multimodal approach ( textual and visual content of the posts ) increases the emoji prediction accuracy compared to the one that only uses textual information.', 'this suggests that textual and visual content embed different but complementary features of the use of emojis.', 'in general, an effective approach to predict the emoji to be associated to a piece of content may help to improve natural language processing tasks  #AUTHOR_TAG, such as information retrieval, generation of emoji - enriched social media content, suggestion of emojis when writing text messages or sharing pictures online.', 'given that emojis may also mislead humans  #AUTHOR_TAG, the automated prediction of emojis may help to achieve better language understanding.', 'as a consequence, by modeling the semantics of emojis, we can improve highly - subjective tasks like sentiment analysis, emotion recognition and irony detection  #AUTHOR_TAG']",0
[' #TAUTHOR_TAG has shown that textual information'],[' #TAUTHOR_TAG has shown that textual information'],"['the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.', 'the combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.', 'recent work  #TAUTHOR_TAG has shown that textual information can be used to predict emojis associated to text.', 'in this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in']","['the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.', 'the combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.', 'recent work  #TAUTHOR_TAG has shown that textual information can be used to predict emojis associated to text.', 'in this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models.', 'we explore the use of emojis in the social media platform instagram.', 'we put forward a multimodal approach to predict the emojis associated to an instagram post, given its picture and text 1.', 'our task and experimental framework are similar to  #TAUTHOR_TAG, however, we use different data ( instagram instead of twitter ) and, in addition, we rely on images to improve the selection of the most likely emojis to associate to a post.', 'we show that a multimodal approach ( textual and visual content of the posts ) increases the emoji prediction accuracy compared to the one that only uses textual information.', 'this suggests that textual and visual content embed different but complementary features of the use of emojis.', 'in general, an effective approach to predict the emoji to be associated to a piece of content may help to improve natural language processing tasks  #AUTHOR_TAG, such as information retrieval, generation of emoji - enriched social media content, suggestion of emojis when writing text messages or sharing pictures online.', 'given that emojis may also mislead humans  #AUTHOR_TAG, the automated prediction of emojis may help to achieve better language understanding.', 'as a consequence, by modeling the semantics of emojis, we can improve highly - subjective tasks like sentiment analysis, emotion recognition and irony detection  #AUTHOR_TAG']",1
[' #TAUTHOR_TAG has shown that textual information'],[' #TAUTHOR_TAG has shown that textual information'],"['the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.', 'the combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.', 'recent work  #TAUTHOR_TAG has shown that textual information can be used to predict emojis associated to text.', 'in this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in']","['the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.', 'the combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.', 'recent work  #TAUTHOR_TAG has shown that textual information can be used to predict emojis associated to text.', 'in this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models.', 'we explore the use of emojis in the social media platform instagram.', 'we put forward a multimodal approach to predict the emojis associated to an instagram post, given its picture and text 1.', 'our task and experimental framework are similar to  #TAUTHOR_TAG, however, we use different data ( instagram instead of twitter ) and, in addition, we rely on images to improve the selection of the most likely emojis to associate to a post.', 'we show that a multimodal approach ( textual and visual content of the posts ) increases the emoji prediction accuracy compared to the one that only uses textual information.', 'this suggests that textual and visual content embed different but complementary features of the use of emojis.', 'in general, an effective approach to predict the emoji to be associated to a piece of content may help to improve natural language processing tasks  #AUTHOR_TAG, such as information retrieval, generation of emoji - enriched social media content, suggestion of emojis when writing text messages or sharing pictures online.', 'given that emojis may also mislead humans  #AUTHOR_TAG, the automated prediction of emojis may help to achieve better language understanding.', 'as a consequence, by modeling the semantics of emojis, we can improve highly - subjective tasks like sentiment analysis, emotion recognition and irony detection  #AUTHOR_TAG']",4
"['4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","['4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","['least 4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","[': we gathered instagram posts published between july 2016 and october 2016, and geolocalized in the united states of america.', 'we considered only posts that contained a photo together with the related user description of at least 4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG, we considered only the posts which include one and only one of the 20 most frequent emojis ( the most frequent emojis are shown in table 3 ).', 'our dataset is composed of 299, 809 posts, each containing a picture, the text associated to it and only one emoji.', 'in the experiments we also considered the subsets of the 10 ( 238, 646 posts ) and 5 most frequent emojis ( 184, 044 posts ) ( similarly to the approach followed by  #TAUTHOR_TAG']",4
"['extend the experimental scheme of  #TAUTHOR_TAG, by considering also visual information when modeling posts.', 'we cast the emoji prediction problem as a classification task : given']","['extend the experimental scheme of  #TAUTHOR_TAG, by considering also visual information when modeling posts.', 'we cast the emoji prediction problem as a classification task : given']","['extend the experimental scheme of  #TAUTHOR_TAG, by considering also visual information when modeling posts.', 'we cast the emoji prediction problem as a classification task : given an image']","['extend the experimental scheme of  #TAUTHOR_TAG, by considering also visual information when modeling posts.', 'we cast the emoji prediction problem as a classification task : given an image or a text ( or both inputs in the multimodal scenario ) we select the most likely emoji that could be added to ( thus used to label ) such contents.', 'the task for our machine learning models is, given the visual and textual content of a post, to predict the single emoji that appears in the input comment']",4
[' #TAUTHOR_TAG has shown that textual information'],[' #TAUTHOR_TAG has shown that textual information'],"['the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.', 'the combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.', 'recent work  #TAUTHOR_TAG has shown that textual information can be used to predict emojis associated to text.', 'in this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in']","['the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.', 'the combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.', 'recent work  #TAUTHOR_TAG has shown that textual information can be used to predict emojis associated to text.', 'in this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models.', 'we explore the use of emojis in the social media platform instagram.', 'we put forward a multimodal approach to predict the emojis associated to an instagram post, given its picture and text 1.', 'our task and experimental framework are similar to  #TAUTHOR_TAG, however, we use different data ( instagram instead of twitter ) and, in addition, we rely on images to improve the selection of the most likely emojis to associate to a post.', 'we show that a multimodal approach ( textual and visual content of the posts ) increases the emoji prediction accuracy compared to the one that only uses textual information.', 'this suggests that textual and visual content embed different but complementary features of the use of emojis.', 'in general, an effective approach to predict the emoji to be associated to a piece of content may help to improve natural language processing tasks  #AUTHOR_TAG, such as information retrieval, generation of emoji - enriched social media content, suggestion of emojis when writing text messages or sharing pictures online.', 'given that emojis may also mislead humans  #AUTHOR_TAG, the automated prediction of emojis may help to achieve better language understanding.', 'as a consequence, by modeling the semantics of emojis, we can improve highly - subjective tasks like sentiment analysis, emotion recognition and irony detection  #AUTHOR_TAG']",6
"['4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","['4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","['least 4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","[': we gathered instagram posts published between july 2016 and october 2016, and geolocalized in the united states of america.', 'we considered only posts that contained a photo together with the related user description of at least 4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG, we considered only the posts which include one and only one of the 20 most frequent emojis ( the most frequent emojis are shown in table 3 ).', 'our dataset is composed of 299, 809 posts, each containing a picture, the text associated to it and only one emoji.', 'in the experiments we also considered the subsets of the 10 ( 238, 646 posts ) and 5 most frequent emojis ( 184, 044 posts ) ( similarly to the approach followed by  #TAUTHOR_TAG']",6
"['extend the experimental scheme of  #TAUTHOR_TAG, by considering also visual information when modeling posts.', 'we cast the emoji prediction problem as a classification task : given']","['extend the experimental scheme of  #TAUTHOR_TAG, by considering also visual information when modeling posts.', 'we cast the emoji prediction problem as a classification task : given']","['extend the experimental scheme of  #TAUTHOR_TAG, by considering also visual information when modeling posts.', 'we cast the emoji prediction problem as a classification task : given an image']","['extend the experimental scheme of  #TAUTHOR_TAG, by considering also visual information when modeling posts.', 'we cast the emoji prediction problem as a classification task : given an image or a text ( or both inputs in the multimodal scenario ) we select the most likely emoji that could be added to ( thus used to label ) such contents.', 'the task for our machine learning models is, given the visual and textual content of a post, to predict the single emoji that appears in the input comment']",6
"['4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","['4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","['least 4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","[': we gathered instagram posts published between july 2016 and october 2016, and geolocalized in the united states of america.', 'we considered only posts that contained a photo together with the related user description of at least 4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG, we considered only the posts which include one and only one of the 20 most frequent emojis ( the most frequent emojis are shown in table 3 ).', 'our dataset is composed of 299, 809 posts, each containing a picture, the text associated to it and only one emoji.', 'in the experiments we also considered the subsets of the 10 ( 238, 646 posts ) and 5 most frequent emojis ( 184, 044 posts ) ( similarly to the approach followed by  #TAUTHOR_TAG']",3
"['on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on']","['on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on']","['order to study the relation between instagram posts and emojis, we performed two different experiments.', 'in the first experiment ( section 4. 2 ) we compare the fasttext model with the state of the art on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on the emoji prediction task.', 'moreover, we evaluate a multimodal combination of both models respectively based on visual and  #TAUTHOR_TAG, using the same twitter dataset.', 'textual inputs.', 'finally we discuss the contribution of each modality to the prediction task.', 'we use 80 % of our dataset ( introduced in section 2 ) for training, 10 % to tune our models, and 10']","['order to study the relation between instagram posts and emojis, we performed two different experiments.', 'in the first experiment ( section 4. 2 ) we compare the fasttext model with the state of the art on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on the emoji prediction task.', 'moreover, we evaluate a multimodal combination of both models respectively based on visual and  #TAUTHOR_TAG, using the same twitter dataset.', 'textual inputs.', 'finally we discuss the contribution of each modality to the prediction task.', 'we use 80 % of our dataset ( introduced in section 2 ) for training, 10 % to tune our models, and 10 % for testing ( selecting the sets randomly )']",3
"['on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on']","['on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on']","['order to study the relation between instagram posts and emojis, we performed two different experiments.', 'in the first experiment ( section 4. 2 ) we compare the fasttext model with the state of the art on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on the emoji prediction task.', 'moreover, we evaluate a multimodal combination of both models respectively based on visual and  #TAUTHOR_TAG, using the same twitter dataset.', 'textual inputs.', 'finally we discuss the contribution of each modality to the prediction task.', 'we use 80 % of our dataset ( introduced in section 2 ) for training, 10 % to tune our models, and 10']","['order to study the relation between instagram posts and emojis, we performed two different experiments.', 'in the first experiment ( section 4. 2 ) we compare the fasttext model with the state of the art on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on the emoji prediction task.', 'moreover, we evaluate a multimodal combination of both models respectively based on visual and  #TAUTHOR_TAG, using the same twitter dataset.', 'textual inputs.', 'finally we discuss the contribution of each modality to the prediction task.', 'we use 80 % of our dataset ( introduced in section 2 ) for training, 10 % to tune our models, and 10 % for testing ( selecting the sets randomly )']",3
"['compare the fasttext model with the word and character based b - lstms presented by  #TAUTHOR_TAG, we']","['compare the fasttext model with the word and character based b - lstms presented by  #TAUTHOR_TAG, we']","['compare the fasttext model with the word and character based b - lstms presented by  #TAUTHOR_TAG, we consider the same three emoji prediction tasks they proposed :']","['compare the fasttext model with the word and character based b - lstms presented by  #TAUTHOR_TAG, we consider the same three emoji prediction tasks they proposed : top - 5, top - 10 and top - 20 emojis most frequently used in their tweet datasets.', 'in this comparison we used the same twitter datasets.', 'as we can see in table 1 fasttext model is competitive, and it is also able to outperform the character based b - lstm in one of the emoji prediction tasks ( top - 20 emojis table 2 : prediction results of top - 5, top - 10 and top - 20 most frequent emojis in the instagram dataset : precision ( p ), recall ( r ), f - measure ( f1 ).', 'experimental settings : majority baseline, weighted random, visual, textual and multimodal systems.', 'in the last line we report the percentage improvement of the multimodal over the textual system']",3
['to emoji prediction has been spotted by  #TAUTHOR_TAG : relying on twitter textual data'],['to emoji prediction has been spotted by  #TAUTHOR_TAG : relying on twitter textual data'],['to emoji prediction has been spotted by  #TAUTHOR_TAG : relying on twitter textual data they showed that the emoji was hard to predict'],"['', 'another relevant confusion scenario related to emoji prediction has been spotted by  #TAUTHOR_TAG : relying on twitter textual data they showed that the emoji was hard to predict as it was used similarly to.', 'instead when we consider instagram data, the emoji is easier to predict ( 0. 23 ), even if it is often confused with.', 'when we rely on visual contents ( instagram picture ), the emojis which are easily predicted are the ones in which the associated photos are similar.', 'for instance, most of the pictures associated to are dog / pet pictures.', 'similarly, is predicted along with very bright pictures taken outside.', 'is correctly predicted along with pictures related to gym and fitness.', 'the accuracy of is also high since most posts including this emoji are related to fitness ( and the pictures are simply either selfies at the gym, weight lifting images, or protein food ).', 'employing a multimodal approach improves performance.', 'this means that the two modalities are somehow complementary, and adding visual information helps to solve potential ambiguities that arise when relying only on textual content.', 'in figure 1 we report the confusion matrix of the multimodal model.', 'the emojis are plotted from the most frequent to the least, and we can see that the model tends to mispredict emojis selecting more frequent emojis ( the left part of the matrix is brighter )']",3
"['4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","['4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","['least 4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG,']","[': we gathered instagram posts published between july 2016 and october 2016, and geolocalized in the united states of america.', 'we considered only posts that contained a photo together with the related user description of at least 4 words and exactly one emoji.', 'moreover, as done by  #TAUTHOR_TAG, we considered only the posts which include one and only one of the 20 most frequent emojis ( the most frequent emojis are shown in table 3 ).', 'our dataset is composed of 299, 809 posts, each containing a picture, the text associated to it and only one emoji.', 'in the experiments we also considered the subsets of the 10 ( 238, 646 posts ) and 5 most frequent emojis ( 184, 044 posts ) ( similarly to the approach followed by  #TAUTHOR_TAG']",5
"['on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on']","['on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on']","['order to study the relation between instagram posts and emojis, we performed two different experiments.', 'in the first experiment ( section 4. 2 ) we compare the fasttext model with the state of the art on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on the emoji prediction task.', 'moreover, we evaluate a multimodal combination of both models respectively based on visual and  #TAUTHOR_TAG, using the same twitter dataset.', 'textual inputs.', 'finally we discuss the contribution of each modality to the prediction task.', 'we use 80 % of our dataset ( introduced in section 2 ) for training, 10 % to tune our models, and 10']","['order to study the relation between instagram posts and emojis, we performed two different experiments.', 'in the first experiment ( section 4. 2 ) we compare the fasttext model with the state of the art on emoji classification ( b - lstm ) by  #TAUTHOR_TAG.', 'our second experiment ( section 4. 3 ) evaluates the visual ( resnet ) and textual ( fasttext ) models on the emoji prediction task.', 'moreover, we evaluate a multimodal combination of both models respectively based on visual and  #TAUTHOR_TAG, using the same twitter dataset.', 'textual inputs.', 'finally we discuss the contribution of each modality to the prediction task.', 'we use 80 % of our dataset ( introduced in section 2 ) for training, 10 % to tune our models, and 10 % for testing ( selecting the sets randomly )']",5
"['compare the fasttext model with the word and character based b - lstms presented by  #TAUTHOR_TAG, we']","['compare the fasttext model with the word and character based b - lstms presented by  #TAUTHOR_TAG, we']","['compare the fasttext model with the word and character based b - lstms presented by  #TAUTHOR_TAG, we consider the same three emoji prediction tasks they proposed :']","['compare the fasttext model with the word and character based b - lstms presented by  #TAUTHOR_TAG, we consider the same three emoji prediction tasks they proposed : top - 5, top - 10 and top - 20 emojis most frequently used in their tweet datasets.', 'in this comparison we used the same twitter datasets.', 'as we can see in table 1 fasttext model is competitive, and it is also able to outperform the character based b - lstm in one of the emoji prediction tasks ( top - 20 emojis table 2 : prediction results of top - 5, top - 10 and top - 20 most frequent emojis in the instagram dataset : precision ( p ), recall ( r ), f - measure ( f1 ).', 'experimental settings : majority baseline, weighted random, visual, textual and multimodal systems.', 'in the last line we report the percentage improvement of the multimodal over the textual system']",5
['revision  #TAUTHOR_TAG'],['revision  #TAUTHOR_TAG'],"['revision  #TAUTHOR_TAG.', 'existing works on']","['natural language processing into systems that provide writing assistance beyond grammar is an area of increasing research and commercial interest ( e. g.,  #AUTHOR_TAG ).', ""as one example, the automatic recognition of the purpose of each of an author's revisions allows writing assistance systems to provide better rewriting suggestions."", 'in this paper, we propose contextbased methods to improve the automatic identification of revision purposes in student argumentative writing.', 'argumentation plays an important role in analyzing many types of writing such as persuasive essays, scientific papers  #AUTHOR_TAG and law documents  #AUTHOR_TAG.', 'in student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision  #TAUTHOR_TAG.', '']",0
['revision  #TAUTHOR_TAG'],['revision  #TAUTHOR_TAG'],"['revision  #TAUTHOR_TAG.', 'existing works on']","['natural language processing into systems that provide writing assistance beyond grammar is an area of increasing research and commercial interest ( e. g.,  #AUTHOR_TAG ).', ""as one example, the automatic recognition of the purpose of each of an author's revisions allows writing assistance systems to provide better rewriting suggestions."", 'in this paper, we propose contextbased methods to improve the automatic identification of revision purposes in student argumentative writing.', 'argumentation plays an important role in analyzing many types of writing such as persuasive essays, scientific papers  #AUTHOR_TAG and law documents  #AUTHOR_TAG.', 'in student papers, identifying revision purposes with respect to argument structure has been used to predict the grade improvement in the paper after revision  #TAUTHOR_TAG.', '']",0
"['of revisions  #TAUTHOR_TAG.', 'while different classification tasks were explored, similar approaches were']","['of revisions  #TAUTHOR_TAG.', 'while different classification tasks were explored, similar approaches were']","['are multiple works on the classification of revisions  #TAUTHOR_TAG.', 'while different classification tasks were explored, similar approaches were taken by extracting features ( location, text, meta - data, language ) from']","['are multiple works on the classification of revisions  #TAUTHOR_TAG.', 'while different classification tasks were explored, similar approaches were taken by extracting features ( location, text, meta - data, language ) from the revised text to train a classification model ( svm, random forest, etc. ) on the annotated data.', 'one problem with prior works is that the contextual features used were typically shallow ( location ), while we cap - and sentence 2 acts as the warrant for the claim.', 'sentence 1 in draft 1 is modified to sentence 1 ( also acts as the claim ) of draft 2.', 'sentence 2 in draft 1 is deleted in draft 2.', '']",0
[' #TAUTHOR_TAG used'],[' #TAUTHOR_TAG used'],['previous work  #TAUTHOR_TAG used three types of features primarily from prior work  #AUTHOR_TAG'],"['previous work  #TAUTHOR_TAG used three types of features primarily from prior work  #AUTHOR_TAG for argumentative revision classification.', ""location features encode the location of the sentence in the paragraph and the location of the sentence's paragraph in the essay."", 'textual features encode revision operation, sentence length, edit distance between aligned sentences and the difference in sentence length and punctuation numbers.', 'language features encode part of speech ( pos ) unigrams and difference in pos tag counts.', 'we implement this feature set as the baseline as our tasks are similar, then propose two new types of contextual features.', 'the first type ( ext ) extends prior work by extracting the baseline features from not only the aligned sentence pair representing the revision in question, but also for the sentence pairs before and after the revision.', 'the second type ( coh ) measures the cohesion and coherence changes in a 2 - sentence block around the revision 2.', 'utilizing the cohesion and coherence difference.', 'inspired by  #AUTHOR_TAG vaughan and mc  #AUTHOR_TAG, we hypothesize that different revisions can have different impacts on the cohesion and coherence of the essay.', 'we propose to extract features for both impact on cohesion ( lexical ) and impact on coherence ( semantic ).', 'inspired by  #AUTHOR_TAG, sequences of blocks are created for sentences 2 in this paper we consider the most adjacent sentence only.', 'each sentence block after stop - word filtering and stemming.', 'jaccard similarity is used for the calculation of lexical similarity between sentence blocks.', 'word embedding vectors  #AUTHOR_TAG are used for the calculation of semantic similarity.', 'a vector is calculated for each sentence block by summing up the embedding vectors of words that are not stop - words 5.', 'afterwards the similarity is calculated as the cosine similarity between the block vectors.', 'this approach has been taken by multiple groups in the semeval - 2015 semantic similarity task ( semeval - 2015 task 1 )  #AUTHOR_TAG']",0
"['schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the']","['schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the']","['purposes.', 'to label our data, we adapt the schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the surface subcategories into one surface category.', 'as  #TAUTHOR_TAG reported that both rebuttals and multiple labels for a single revision were rare, we merge re']","['purposes.', 'to label our data, we adapt the schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the surface subcategories into one surface category.', 'as  #TAUTHOR_TAG reported that both rebuttals and multiple labels for a single revision were rare, we merge rebuttal and warrant into one warrant category 1 and allow only a single ( primary ) label per revision.', 'corpora.', 'our experiments use two corpora consisting of drafts 1 and 2 of papers written by high school students taking ap - english courses ; papers were revised after receiving and generating peer feedback.', 'corpus a was collected in our earlier pa - per  #TAUTHOR_TAG, although the original annotations were modified as described above.', ""it contains 47 paper draft pairs about placing contemporaries in dante's inferno."", 'corpus b was collected in the same manor as a with agreement kappa 0. 69.', 'it contains 63 paper draft pairs explaining the rhetorical strategies used by the speaker / author of a previously read lecture / essay.', 'both corpora were double coded and gold standard labels were created upon agreement of two annotators.', 'two example annotated revisions from corpus b are shown in table 1, while the distribution of annotated revision purposes for both corpora are shown in table 2.', '4 utilizing']",6
"['schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the']","['schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the']","['purposes.', 'to label our data, we adapt the schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the surface subcategories into one surface category.', 'as  #TAUTHOR_TAG reported that both rebuttals and multiple labels for a single revision were rare, we merge re']","['purposes.', 'to label our data, we adapt the schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the surface subcategories into one surface category.', 'as  #TAUTHOR_TAG reported that both rebuttals and multiple labels for a single revision were rare, we merge rebuttal and warrant into one warrant category 1 and allow only a single ( primary ) label per revision.', 'corpora.', 'our experiments use two corpora consisting of drafts 1 and 2 of papers written by high school students taking ap - english courses ; papers were revised after receiving and generating peer feedback.', 'corpus a was collected in our earlier pa - per  #TAUTHOR_TAG, although the original annotations were modified as described above.', ""it contains 47 paper draft pairs about placing contemporaries in dante's inferno."", 'corpus b was collected in the same manor as a with agreement kappa 0. 69.', 'it contains 63 paper draft pairs explaining the rhetorical strategies used by the speaker / author of a previously read lecture / essay.', 'both corpora were double coded and gold standard labels were created upon agreement of two annotators.', 'two example annotated revisions from corpus b are shown in table 1, while the distribution of annotated revision purposes for both corpora are shown in table 2.', '4 utilizing']",1
"['schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the']","['schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the']","['purposes.', 'to label our data, we adapt the schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the surface subcategories into one surface category.', 'as  #TAUTHOR_TAG reported that both rebuttals and multiple labels for a single revision were rare, we merge re']","['purposes.', 'to label our data, we adapt the schema defined in  #TAUTHOR_TAG as it can be reliably annotated and is argument -  #AUTHOR_TAG.', 'as we focus on argumentative changes, we merge all the surface subcategories into one surface category.', 'as  #TAUTHOR_TAG reported that both rebuttals and multiple labels for a single revision were rare, we merge rebuttal and warrant into one warrant category 1 and allow only a single ( primary ) label per revision.', 'corpora.', 'our experiments use two corpora consisting of drafts 1 and 2 of papers written by high school students taking ap - english courses ; papers were revised after receiving and generating peer feedback.', 'corpus a was collected in our earlier pa - per  #TAUTHOR_TAG, although the original annotations were modified as described above.', ""it contains 47 paper draft pairs about placing contemporaries in dante's inferno."", 'corpus b was collected in the same manor as a with agreement kappa 0. 69.', 'it contains 63 paper draft pairs explaining the rhetorical strategies used by the speaker / author of a previously read lecture / essay.', 'both corpora were double coded and gold standard labels were created upon agreement of two annotators.', 'two example annotated revisions from corpus b are shown in table 1, while the distribution of annotated revision purposes for both corpora are shown in table 2.', '4 utilizing']",5
,,,,3
,,,,6
['features as were used in  #TAUTHOR_TAG'],['features as were used in  #TAUTHOR_TAG'],"['mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model']","['trained an image - caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input.', 'as improvements over previous work we used a 3 - layer gru and employed importance sampling, cyclic learning rates, ensembling and vectorial self - attention.', 'our results on both mbn and mfcc features are significantly higher than the previous state - of - the - art.', 'the largest improvement comes from using the learned mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model learns to recognise these words in the input.', 'the system is not explicitly optimised to do so, but our results show that the lower layers learn to recognise this form related information from the input.', 'after layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task - specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi - modal embedding space.', '']",6
,,,,0
"['13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work']","['speech - based models described in [ 13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for']","['13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for the image encoder we use a single - layer linear projection on top of the pretrained image recognition model, and nor - malise the result to have unit l2 norm.', 'the image encoder has 2048 input units and 2048 output units.', 'our caption encoder consists of three main components.', 'first we apply']","['multimodal encoder maps images and their corresponding captions to a common embedding space.', 'the idea is to make matching images and captions lie close together and mismatched images and captions lie far apart in the embedding space.', 'our model consists of two parts ; an image encoder and a sentence encoder as depicted in figure 1.', 'the approach is based on our own text - based model described in [ 8 ] and on the speech - based models described in [ 13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for the image encoder we use a single - layer linear projection on top of the pretrained image recognition model, and nor - malise the result to have unit l2 norm.', 'the image encoder has 2048 input units and 2048 output units.', 'our caption encoder consists of three main components.', 'first we apply a 1 - dimensional convolutional layer to the acoustic input features.', 'the convolution has a stride of size 2, kernel size 6 and 64 output channels.', 'this is the only layer where the model differs from the text - based model, which features a character embedding layer instead of a convolutional layer.', '']",0
"['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['- processing to the acoustic features and the intermediate layer outputs to ensure that our word detection inputs are all of the same size. as the intermediate gru layers produce 2048', 'features for each time step in the signal, we use average - pooling along the temporal dimension to create a single input vector', 'and normalise the result to have unit l2 norm. the acoustic features consist of 30 ( mbn', ') or 39 ( mfcc ) features for each time step, so we apply the convolutional layer followed by an untrained gr', '##u layer to the input features, use average - pooling and normalise the result to have unit l2 norm. the word detection networks', 'are trained for 32 epochs using adam [ 25 ] with a constant learning rate of 0. 001. we use the same data split that was used for training the multi - modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder.', 'table 1 shows the performance of our models on the imagecaption retrieval task. the caption embeddings are ranked by cosine distance', 'to the image and vice versa where r @ n is the percentage of test items for which the correct', 'image or caption was in the top n results. we compare our', 'models to [ 12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison. [ 12 ] is a', 'convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character', '- based model is similar to the model we use here and was trained on the original flickr', '##8k text captions the results of the word presence detection task are shown in figure 2 and table 2. figure 2 shows the f1 score for all the classifiers at 20 equally spaced', ""detection thresholds ( i. e. a word is classified as'present'if the word detection output is above"", 'this threshold ). table 2 displays the area under the curve for the receiver operating characteristic.', 'even though the mbn model outperforms the mfcc model for all layers we see the same pattern emerging from both the f1 score and the auc', "". the performance on the feature level is not much better than random. predicting'not present'for every word would be the best random guess as this is a heavy majority class in this task. inspection of the predictions shows that"", '']",0
"['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['- processing to the acoustic features and the intermediate layer outputs to ensure that our word detection inputs are all of the same size. as the intermediate gru layers produce 2048', 'features for each time step in the signal, we use average - pooling along the temporal dimension to create a single input vector', 'and normalise the result to have unit l2 norm. the acoustic features consist of 30 ( mbn', ') or 39 ( mfcc ) features for each time step, so we apply the convolutional layer followed by an untrained gr', '##u layer to the input features, use average - pooling and normalise the result to have unit l2 norm. the word detection networks', 'are trained for 32 epochs using adam [ 25 ] with a constant learning rate of 0. 001. we use the same data split that was used for training the multi - modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder.', 'table 1 shows the performance of our models on the imagecaption retrieval task. the caption embeddings are ranked by cosine distance', 'to the image and vice versa where r @ n is the percentage of test items for which the correct', 'image or caption was in the top n results. we compare our', 'models to [ 12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison. [ 12 ] is a', 'convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character', '- based model is similar to the model we use here and was trained on the original flickr', '##8k text captions the results of the word presence detection task are shown in figure 2 and table 2. figure 2 shows the f1 score for all the classifiers at 20 equally spaced', ""detection thresholds ( i. e. a word is classified as'present'if the word detection output is above"", 'this threshold ). table 2 displays the area under the curve for the receiver operating characteristic.', 'even though the mbn model outperforms the mfcc model for all layers we see the same pattern emerging from both the f1 score and the auc', "". the performance on the feature level is not much better than random. predicting'not present'for every word would be the best random guess as this is a heavy majority class in this task. inspection of the predictions shows that"", '']",0
"['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['- processing to the acoustic features and the intermediate layer outputs to ensure that our word detection inputs are all of the same size. as the intermediate gru layers produce 2048', 'features for each time step in the signal, we use average - pooling along the temporal dimension to create a single input vector', 'and normalise the result to have unit l2 norm. the acoustic features consist of 30 ( mbn', ') or 39 ( mfcc ) features for each time step, so we apply the convolutional layer followed by an untrained gr', '##u layer to the input features, use average - pooling and normalise the result to have unit l2 norm. the word detection networks', 'are trained for 32 epochs using adam [ 25 ] with a constant learning rate of 0. 001. we use the same data split that was used for training the multi - modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder.', 'table 1 shows the performance of our models on the imagecaption retrieval task. the caption embeddings are ranked by cosine distance', 'to the image and vice versa where r @ n is the percentage of test items for which the correct', 'image or caption was in the top n results. we compare our', 'models to [ 12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison. [ 12 ] is a', 'convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character', '- based model is similar to the model we use here and was trained on the original flickr', '##8k text captions the results of the word presence detection task are shown in figure 2 and table 2. figure 2 shows the f1 score for all the classifiers at 20 equally spaced', ""detection thresholds ( i. e. a word is classified as'present'if the word detection output is above"", 'this threshold ). table 2 displays the area under the curve for the receiver operating characteristic.', 'even though the mbn model outperforms the mfcc model for all layers we see the same pattern emerging from both the f1 score and the auc', "". the performance on the feature level is not much better than random. predicting'not present'for every word would be the best random guess as this is a heavy majority class in this task. inspection of the predictions shows that"", '']",0
['features as were used in  #TAUTHOR_TAG'],['features as were used in  #TAUTHOR_TAG'],"['mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model']","['trained an image - caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input.', 'as improvements over previous work we used a 3 - layer gru and employed importance sampling, cyclic learning rates, ensembling and vectorial self - attention.', 'our results on both mbn and mfcc features are significantly higher than the previous state - of - the - art.', 'the largest improvement comes from using the learned mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model learns to recognise these words in the input.', 'the system is not explicitly optimised to do so, but our results show that the lower layers learn to recognise this form related information from the input.', 'after layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task - specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi - modal embedding space.', '']",0
"['13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work']","['speech - based models described in [ 13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for']","['13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for the image encoder we use a single - layer linear projection on top of the pretrained image recognition model, and nor - malise the result to have unit l2 norm.', 'the image encoder has 2048 input units and 2048 output units.', 'our caption encoder consists of three main components.', 'first we apply']","['multimodal encoder maps images and their corresponding captions to a common embedding space.', 'the idea is to make matching images and captions lie close together and mismatched images and captions lie far apart in the embedding space.', 'our model consists of two parts ; an image encoder and a sentence encoder as depicted in figure 1.', 'the approach is based on our own text - based model described in [ 8 ] and on the speech - based models described in [ 13,  #TAUTHOR_TAG and we refer to those studies for more details.', 'here, we focus on the differences with previous work.', 'for the image encoder we use a single - layer linear projection on top of the pretrained image recognition model, and nor - malise the result to have unit l2 norm.', 'the image encoder has 2048 input units and 2048 output units.', 'our caption encoder consists of three main components.', 'first we apply a 1 - dimensional convolutional layer to the acoustic input features.', 'the convolution has a stride of size 2, kernel size 6 and 64 output channels.', 'this is the only layer where the model differs from the text - based model, which features a character embedding layer instead of a convolutional layer.', '']",5
"['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison.']","['- processing to the acoustic features and the intermediate layer outputs to ensure that our word detection inputs are all of the same size. as the intermediate gru layers produce 2048', 'features for each time step in the signal, we use average - pooling along the temporal dimension to create a single input vector', 'and normalise the result to have unit l2 norm. the acoustic features consist of 30 ( mbn', ') or 39 ( mfcc ) features for each time step, so we apply the convolutional layer followed by an untrained gr', '##u layer to the input features, use average - pooling and normalise the result to have unit l2 norm. the word detection networks', 'are trained for 32 epochs using adam [ 25 ] with a constant learning rate of 0. 001. we use the same data split that was used for training the multi - modal encoder, so that we test word presence detection on data that was not seen by either the encoder or the decoder.', 'table 1 shows the performance of our models on the imagecaption retrieval task. the caption embeddings are ranked by cosine distance', 'to the image and vice versa where r @ n is the percentage of test items for which the correct', 'image or caption was in the top n results. we compare our', 'models to [ 12 ] and  #TAUTHOR_TAG, and include our own character - based model for comparison. [ 12 ] is a', 'convolutional approach, whereas  #TAUTHOR_TAG is an approach using recurrent highway networks with scalar attention. the character', '- based model is similar to the model we use here and was trained on the original flickr', '##8k text captions the results of the word presence detection task are shown in figure 2 and table 2. figure 2 shows the f1 score for all the classifiers at 20 equally spaced', ""detection thresholds ( i. e. a word is classified as'present'if the word detection output is above"", 'this threshold ). table 2 displays the area under the curve for the receiver operating characteristic.', 'even though the mbn model outperforms the mfcc model for all layers we see the same pattern emerging from both the f1 score and the auc', "". the performance on the feature level is not much better than random. predicting'not present'for every word would be the best random guess as this is a heavy majority class in this task. inspection of the predictions shows that"", '']",5
['features as were used in  #TAUTHOR_TAG'],['features as were used in  #TAUTHOR_TAG'],"['mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model']","['trained an image - caption retrieval model on spoken input and investigated whether it learns to recognise linguistic units in the input.', 'as improvements over previous work we used a 3 - layer gru and employed importance sampling, cyclic learning rates, ensembling and vectorial self - attention.', 'our results on both mbn and mfcc features are significantly higher than the previous state - of - the - art.', 'the largest improvement comes from using the learned mbn features but our approach also improves results for mfccs, which are the same features as were used in  #TAUTHOR_TAG.', 'the learned mbn features provide better performance whereas the mfccs are more cognitively plausible input features.', 'the probing task shows that the model learns to recognise these words in the input.', 'the system is not explicitly optimised to do so, but our results show that the lower layers learn to recognise this form related information from the input.', 'after layer 2, the performance starts to decrease slightly which might indicate that these layers learn a more task - specific representation and it is to be expected that the final attention layer specialises in mapping from audio features to the multi - modal embedding space.', '']",5
"['8,  #TAUTHOR_TAG,']","['as mismatched pairs ( as done in [ 8,  #TAUTHOR_TAG,']","['8,  #TAUTHOR_TAG, we calculate the loss using only the hardest examples (']","['[ 8 ], the model is trained to embed the images and captions such that the cosine similarity between image and caption pairs is larger ( by a certain margin ) than the similarity between mismatching pairs.', 'this so called hinge loss l as a function of the network parameters θ is given by :', 'b is a minibatch of correct captionimage pairs ( c, i ), where the other caption - image pairs in the batch serve to create mismatched pairs ( c, i ′ ) and ( c ′, i ).', 'we take the cosine similarity cos ( x, y ) and subtract the similarity of the mismatched pairs from the matching pairs such that the loss is only zero when the matching pair is more similar than the mismatched pairs by a margin α.', 'we use importance sampling to select the mismatched pairs ; rather than using all the other samples in the mini - batch as mismatched pairs ( as done in [ 8,  #TAUTHOR_TAG, we calculate the loss using only the hardest examples ( i. e. mismatched pairs with high cosine similarity ).', 'while [ 10 ] used only the single hardest example in the batch for text - captions, we found that this did not work for the spoken captions.', 'instead we found that using the hardest 25 percent worked well.', 'the networks are trained using adam [ 25 ] with a cyclic learning rate schedule based on [ 26 ].', 'the learning rate schedule varies the learning rate smoothly between a minimum and maximum bound which were set to 10 −6 and 2 × 10 −4 respectively.', 'the learning rate schedule causes the network to visit several local minima during training, allowing us to use snapshot ensembling [ 27 ].', 'by saving the network parameters at each local minimum, we can ensemble the embeddings of multiple networks at no extra cost.', 'we use a margin α = 0. 2 for the loss function.', 'we train the networks for 32 epochs and take a snapshot for ensembling at every fourth epoch.', 'for ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings.', 'the main differences with the approaches described in [ 13,  #TAUTHOR_TAG are the use of multi - layered grus, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention']",4
"['8,  #TAUTHOR_TAG,']","['as mismatched pairs ( as done in [ 8,  #TAUTHOR_TAG,']","['8,  #TAUTHOR_TAG, we calculate the loss using only the hardest examples (']","['[ 8 ], the model is trained to embed the images and captions such that the cosine similarity between image and caption pairs is larger ( by a certain margin ) than the similarity between mismatching pairs.', 'this so called hinge loss l as a function of the network parameters θ is given by :', 'b is a minibatch of correct captionimage pairs ( c, i ), where the other caption - image pairs in the batch serve to create mismatched pairs ( c, i ′ ) and ( c ′, i ).', 'we take the cosine similarity cos ( x, y ) and subtract the similarity of the mismatched pairs from the matching pairs such that the loss is only zero when the matching pair is more similar than the mismatched pairs by a margin α.', 'we use importance sampling to select the mismatched pairs ; rather than using all the other samples in the mini - batch as mismatched pairs ( as done in [ 8,  #TAUTHOR_TAG, we calculate the loss using only the hardest examples ( i. e. mismatched pairs with high cosine similarity ).', 'while [ 10 ] used only the single hardest example in the batch for text - captions, we found that this did not work for the spoken captions.', 'instead we found that using the hardest 25 percent worked well.', 'the networks are trained using adam [ 25 ] with a cyclic learning rate schedule based on [ 26 ].', 'the learning rate schedule varies the learning rate smoothly between a minimum and maximum bound which were set to 10 −6 and 2 × 10 −4 respectively.', 'the learning rate schedule causes the network to visit several local minima during training, allowing us to use snapshot ensembling [ 27 ].', 'by saving the network parameters at each local minimum, we can ensemble the embeddings of multiple networks at no extra cost.', 'we use a margin α = 0. 2 for the loss function.', 'we train the networks for 32 epochs and take a snapshot for ensembling at every fourth epoch.', 'for ensembling we use the two snapshots with the highest performance on the development data and simply sum their embeddings.', 'the main differences with the approaches described in [ 13,  #TAUTHOR_TAG are the use of multi - layered grus, importance sampling, the cyclic learning rate, snapshot ensembling and the use of vectorial rather than scalar attention']",4
"[' #TAUTHOR_TAG a ), sentiment']","[' #TAUTHOR_TAG a ), sentiment classification, recommendation system,']","[', sentence generation  #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG a ), sentiment classification, recommendation']","['tutorial introduces the advances in deep bayesian learning with abundant applications for natural language understanding ranging from speech recognition  #AUTHOR_TAG to document summarization  #AUTHOR_TAG, text classification  #AUTHOR_TAG, text segmentation  #AUTHOR_TAG, information extraction  #AUTHOR_TAG, image caption generation  #AUTHOR_TAG, sentence generation  #AUTHOR_TAG b ), dialogue control  #TAUTHOR_TAG a ), sentiment classification, recommendation system, question answering  #AUTHOR_TAG and machine translation, to name a few.', 'traditionally, "" deep learning "" is taken to be a learning process where the inference or optimization is based on the real - valued deterministic model.', 'the "" semantic structure "" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.', 'the "" distribution function "" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.', 'this tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced bayesian models and deep models including hierarchical dirichlet process, chinese restaurant process  #AUTHOR_TAG, hierarchical pitman - yor process  #AUTHOR_TAG, indian buffet process  #AUTHOR_TAG, recurrent neural network  #AUTHOR_TAG, long short - term memory  #AUTHOR_TAG sequence - to - sequence model  #AUTHOR_TAG, variational auto - encoder  #AUTHOR_TAG, generative adversarial network  #AUTHOR_TAG, attention mechanism  #AUTHOR_TAG, memory - augmented neural network  #AUTHOR_TAG, stochastic neural network  #AUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient  #AUTHOR_TAG and reinforcement learning  #AUTHOR_TAG.', 'we present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language.', 'the variational inference and sampling method are formulated to tackle the optimization for complicated models  #AUTHOR_TAG.', 'the word and sentence embeddings, clustering and co - clustering are merged with linguistic and semantic constraints.', 'a series of case studies are presented to tackle different issues in deep bayesian learning and understanding.', '']",0
"['with given answer, the best matched option will be our answer.', 'for answer generation we used s - net  #TAUTHOR_TAG model trained on squad and to evaluate our model we used large - scale race ( reading comprehension']","['with given answer, the best matched option will be our answer.', 'for answer generation we used s - net  #TAUTHOR_TAG model trained on squad and to evaluate our model we used large - scale race ( reading comprehension']","['with given answer, the best matched option will be our answer.', 'for answer generation we used s - net  #TAUTHOR_TAG model trained on squad and to evaluate our model we used large - scale race ( reading comprehension dataset from examinations )  #AUTHOR_TAG']","['- choice machine reading comprehension is difficult task as its required machines to select the correct option from a set of candidate or possible options using the given passage and question.', 'reading comprehension with multiple choice questions task, required a human ( or machine ) to read a given passage, question pair and select the best one option from n given options.', 'there are two different ways to select the correct answer from the given passage.', 'either by selecting the best match answer to by eliminating the worst match answer.', 'here we proposed gennet model, a neural network - based model.', 'in this model first we will generate the answer of the question from the passage and then will matched the generated answer with given answer, the best matched option will be our answer.', 'for answer generation we used s - net  #TAUTHOR_TAG model trained on squad and to evaluate our model we used large - scale race ( reading comprehension dataset from examinations )  #AUTHOR_TAG']",5
['generation using state - of - art s - net model  #TAUTHOR_TAG which extract'],['generation using state - of - art s - net model  #TAUTHOR_TAG which extract'],"['tries to retain portions of the passage which are only relevant to the question ).', 'then we use answer generation using state - of - art s - net model  #TAUTHOR_TAG which extract']","['', 'we first compute a question - aware representation of the passage ( which essentially tries to retain portions of the passage which are only relevant to the question ).', 'then we use answer generation using state - of - art s - net model  #TAUTHOR_TAG which extract and generate answer figure 2.', 'after we have answer generated from the passage now we weight every given candidate option and select the best matched option.', '']",5
['generation model  #TAUTHOR_TAG the produced'],['generation model  #TAUTHOR_TAG the produced'],[': answer synthesis / generation model  #TAUTHOR_TAG the produced'],"['', 'at each decoding time step t, the gru reads the previous word embedding w', 't−1 and previous context vector c t−1 and finally produced answer. figure 4 : answer synthesis / generation model  #TAUTHOR_TAG the produced answer will', 'be stored in answer vector. a n = [ a 1, a 2, a 3,... a a ] where a is length', 'of the answer. figure 3 shows the overview of selection module. the selection module will take the refined answer representation a t and computes its bi - linear similarity with each option representation. score ( i ) = a t w att z ti (', '3 ) where i is the number of option, a t is generated answer vector, z ti is option vector and w att is a', '']",5
['generation model  #TAUTHOR_TAG the produced'],['generation model  #TAUTHOR_TAG the produced'],[': answer synthesis / generation model  #TAUTHOR_TAG the produced'],"['', 'at each decoding time step t, the gru reads the previous word embedding w', 't−1 and previous context vector c t−1 and finally produced answer. figure 4 : answer synthesis / generation model  #TAUTHOR_TAG the produced answer will', 'be stored in answer vector. a n = [ a 1, a 2, a 3,... a a ] where a is length', 'of the answer. figure 3 shows the overview of selection module. the selection module will take the refined answer representation a t and computes its bi - linear similarity with each option representation. score ( i ) = a t w att z ti (', '3 ) where i is the number of option, a t is generated answer vector, z ti is option vector and w att is a', '']",5
[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG'],"['', 'obtained tree into amr graph by an eight - action transition', 'system. their later works have investigated a richer feature set including co - reference, semantic role labeling, word', 'cluster [ 17 ] ; rich name entity tag, and isi verbalization list [ 16 ]. neuralamr  #TAUTHOR_TAG has succeeded at', 'both amr parsing and sentence generation as the result of a bootstrapping', 'training strategy on a 20 - million -', 'sentence unsupervised dataset. an efficient adaptation of machine translation to amr parsing by barzd', '##ins et al [ 2 ] indicates that character - based features are better than [ 12 ]. the work of ballesteros et al has combined recurrent neural network and transition system into a deep transition model [ 1 ]. among those methods, the information is encoded in lstm hidden', 'state using embedding vector and syntactic features instead of gathering a large number of features which are introduced in the conventional transition method. although recent studies have utilized long short - term memory ( lstm ) in amr parsing  #TAUTHOR_TAG 1 ],', 'there are several disadvantages of employing lstm compared to cnn. first, lstm models long dependency, which might be', 'noise to generate a linearized graph, whereas cnn provides a shorter dependency which is advantageous to generate graph', 'traversal. secondly, lstm requires a chronologically computing process', 'that restrains the ability of parallelization ; on the contrary, cnn enables simultaneous parsing', '. in this paper, we present the first success of applying convolutional seq2seq in amr parsing', '. the main contributions of this research are : • an outstanding performance with 5 points', 'smatch score improvement resulted from the proposed amr parsing model using depth - first - search graph linearization and convolutional seq2seq network. • a', 'new public amr test 1 set of legal document. • the first study of amr parsing in the legal domain']",0
[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG'],"['', 'obtained tree into amr graph by an eight - action transition', 'system. their later works have investigated a richer feature set including co - reference, semantic role labeling, word', 'cluster [ 17 ] ; rich name entity tag, and isi verbalization list [ 16 ]. neuralamr  #TAUTHOR_TAG has succeeded at', 'both amr parsing and sentence generation as the result of a bootstrapping', 'training strategy on a 20 - million -', 'sentence unsupervised dataset. an efficient adaptation of machine translation to amr parsing by barzd', '##ins et al [ 2 ] indicates that character - based features are better than [ 12 ]. the work of ballesteros et al has combined recurrent neural network and transition system into a deep transition model [ 1 ]. among those methods, the information is encoded in lstm hidden', 'state using embedding vector and syntactic features instead of gathering a large number of features which are introduced in the conventional transition method. although recent studies have utilized long short - term memory ( lstm ) in amr parsing  #TAUTHOR_TAG 1 ],', 'there are several disadvantages of employing lstm compared to cnn. first, lstm models long dependency, which might be', 'noise to generate a linearized graph, whereas cnn provides a shorter dependency which is advantageous to generate graph', 'traversal. secondly, lstm requires a chronologically computing process', 'that restrains the ability of parallelization ; on the contrary, cnn enables simultaneous parsing', '. in this paper, we present the first success of applying convolutional seq2seq in amr parsing', '. the main contributions of this research are : • an outstanding performance with 5 points', 'smatch score improvement resulted from the proposed amr parsing model using depth - first - search graph linearization and convolutional seq2seq network. • a', 'new public amr test 1 set of legal document. • the first study of amr parsing in the legal domain']",0
[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG'],"['', 'obtained tree into amr graph by an eight - action transition', 'system. their later works have investigated a richer feature set including co - reference, semantic role labeling, word', 'cluster [ 17 ] ; rich name entity tag, and isi verbalization list [ 16 ]. neuralamr  #TAUTHOR_TAG has succeeded at', 'both amr parsing and sentence generation as the result of a bootstrapping', 'training strategy on a 20 - million -', 'sentence unsupervised dataset. an efficient adaptation of machine translation to amr parsing by barzd', '##ins et al [ 2 ] indicates that character - based features are better than [ 12 ]. the work of ballesteros et al has combined recurrent neural network and transition system into a deep transition model [ 1 ]. among those methods, the information is encoded in lstm hidden', 'state using embedding vector and syntactic features instead of gathering a large number of features which are introduced in the conventional transition method. although recent studies have utilized long short - term memory ( lstm ) in amr parsing  #TAUTHOR_TAG 1 ],', 'there are several disadvantages of employing lstm compared to cnn. first, lstm models long dependency, which might be', 'noise to generate a linearized graph, whereas cnn provides a shorter dependency which is advantageous to generate graph', 'traversal. secondly, lstm requires a chronologically computing process', 'that restrains the ability of parallelization ; on the contrary, cnn enables simultaneous parsing', '. in this paper, we present the first success of applying convolutional seq2seq in amr parsing', '. the main contributions of this research are : • an outstanding performance with 5 points', 'smatch score improvement resulted from the proposed amr parsing model using depth - first - search graph linearization and convolutional seq2seq network. • a', 'new public amr test 1 set of legal document. • the first study of amr parsing in the legal domain']",0
[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG'],"['', 'obtained tree into amr graph by an eight - action transition', 'system. their later works have investigated a richer feature set including co - reference, semantic role labeling, word', 'cluster [ 17 ] ; rich name entity tag, and isi verbalization list [ 16 ]. neuralamr  #TAUTHOR_TAG has succeeded at', 'both amr parsing and sentence generation as the result of a bootstrapping', 'training strategy on a 20 - million -', 'sentence unsupervised dataset. an efficient adaptation of machine translation to amr parsing by barzd', '##ins et al [ 2 ] indicates that character - based features are better than [ 12 ]. the work of ballesteros et al has combined recurrent neural network and transition system into a deep transition model [ 1 ]. among those methods, the information is encoded in lstm hidden', 'state using embedding vector and syntactic features instead of gathering a large number of features which are introduced in the conventional transition method. although recent studies have utilized long short - term memory ( lstm ) in amr parsing  #TAUTHOR_TAG 1 ],', 'there are several disadvantages of employing lstm compared to cnn. first, lstm models long dependency, which might be', 'noise to generate a linearized graph, whereas cnn provides a shorter dependency which is advantageous to generate graph', 'traversal. secondly, lstm requires a chronologically computing process', 'that restrains the ability of parallelization ; on the contrary, cnn enables simultaneous parsing', '. in this paper, we present the first success of applying convolutional seq2seq in amr parsing', '. the main contributions of this research are : • an outstanding performance with 5 points', 'smatch score improvement resulted from the proposed amr parsing model using depth - first - search graph linearization and convolutional seq2seq network. • a', 'new public amr test 1 set of legal document. • the first study of amr parsing in the legal domain']",1
[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG has'],[' #TAUTHOR_TAG'],"['', 'obtained tree into amr graph by an eight - action transition', 'system. their later works have investigated a richer feature set including co - reference, semantic role labeling, word', 'cluster [ 17 ] ; rich name entity tag, and isi verbalization list [ 16 ]. neuralamr  #TAUTHOR_TAG has succeeded at', 'both amr parsing and sentence generation as the result of a bootstrapping', 'training strategy on a 20 - million -', 'sentence unsupervised dataset. an efficient adaptation of machine translation to amr parsing by barzd', '##ins et al [ 2 ] indicates that character - based features are better than [ 12 ]. the work of ballesteros et al has combined recurrent neural network and transition system into a deep transition model [ 1 ]. among those methods, the information is encoded in lstm hidden', 'state using embedding vector and syntactic features instead of gathering a large number of features which are introduced in the conventional transition method. although recent studies have utilized long short - term memory ( lstm ) in amr parsing  #TAUTHOR_TAG 1 ],', 'there are several disadvantages of employing lstm compared to cnn. first, lstm models long dependency, which might be', 'noise to generate a linearized graph, whereas cnn provides a shorter dependency which is advantageous to generate graph', 'traversal. secondly, lstm requires a chronologically computing process', 'that restrains the ability of parallelization ; on the contrary, cnn enables simultaneous parsing', '. in this paper, we present the first success of applying convolutional seq2seq in amr parsing', '. the main contributions of this research are : • an outstanding performance with 5 points', 'smatch score improvement resulted from the proposed amr parsing model using depth - first - search graph linearization and convolutional seq2seq network. • a', 'new public amr test 1 set of legal document. • the first study of amr parsing in the legal domain']",1
"['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian should be modified by a term that introduces the relations as a lagrange multiplier. this alters the dynamics and the equilibrium state, depending', 'on a parameter that measures how strongly enforced the relations are. from the point of view of coding theory discussed here, it seems more reasonable to modify this dynamical system, so that it can be better described as a dynamics in the space of code parameters. it is natural therefore to consider a similar setting, where we assign a given set l of', 'languages to the vertices of a complete graph g l, with assigned energies j e = j, at the edges e ∈ e ( g l ) with ∂e = {', ', }. we denote by x ( ) = ( x j ( ) ) n j = 1 the vector of binary variables that lists the n syntactic features of the language. we consider these as maps x : l → { 0, 1 } n, or equivalently as points x', '∈ { 0, 1 } n l. consider an', 'energy functional of the form for j, = j e > 0, where d h ( x ( ), x ( ) ) is the hamming distance, the corresponding partition function is given by at low temperature ( large β ), the partition function is concentrated around the minimum', 'of h ( x ), that is, were all d h ( x ( ), x ( ) ) = 0, hence where all the vectors x ( ) ∈ { 0, 1 } n agree. given an initial condition x 0 ∈ { 0, 1 }', 'n l and the datum ( j e ) e∈e ( g l ) of the strengths of the interaction energies along the', 'edges, the same method used in  #TAUTHOR_TAG, based on the standard metropolis - hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior. in the space of code parameters, given the code point ( δ 0, r', '0 ) = ( δ ( c ( x 0 ) ), r ( c ( x 0 ) ) ) associated to the initial condition x 0, the dynamics moves the code point along the line with constant r = r 0. as the', 'dynamics approaches the minimum of the action, the code point enters the region below the gilbert - varshamov', 'bound, as it moves towards smaller values of δ', '']",0
"['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian should be modified by a term that introduces the relations as a lagrange multiplier. this alters the dynamics and the equilibrium state, depending', 'on a parameter that measures how strongly enforced the relations are. from the point of view of coding theory discussed here, it seems more reasonable to modify this dynamical system, so that it can be better described as a dynamics in the space of code parameters. it is natural therefore to consider a similar setting, where we assign a given set l of', 'languages to the vertices of a complete graph g l, with assigned energies j e = j, at the edges e ∈ e ( g l ) with ∂e = {', ', }. we denote by x ( ) = ( x j ( ) ) n j = 1 the vector of binary variables that lists the n syntactic features of the language. we consider these as maps x : l → { 0, 1 } n, or equivalently as points x', '∈ { 0, 1 } n l. consider an', 'energy functional of the form for j, = j e > 0, where d h ( x ( ), x ( ) ) is the hamming distance, the corresponding partition function is given by at low temperature ( large β ), the partition function is concentrated around the minimum', 'of h ( x ), that is, were all d h ( x ( ), x ( ) ) = 0, hence where all the vectors x ( ) ∈ { 0, 1 } n agree. given an initial condition x 0 ∈ { 0, 1 }', 'n l and the datum ( j e ) e∈e ( g l ) of the strengths of the interaction energies along the', 'edges, the same method used in  #TAUTHOR_TAG, based on the standard metropolis - hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior. in the space of code parameters, given the code point ( δ 0, r', '0 ) = ( δ ( c ( x 0 ) ), r ( c ( x 0 ) ) ) associated to the initial condition x 0, the dynamics moves the code point along the line with constant r = r 0. as the', 'dynamics approaches the minimum of the action, the code point enters the region below the gilbert - varshamov', 'bound, as it moves towards smaller values of δ', '']",0
"['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian should be modified by a term that introduces the relations as a lagrange multiplier. this alters the dynamics and the equilibrium state, depending', 'on a parameter that measures how strongly enforced the relations are. from the point of view of coding theory discussed here, it seems more reasonable to modify this dynamical system, so that it can be better described as a dynamics in the space of code parameters. it is natural therefore to consider a similar setting, where we assign a given set l of', 'languages to the vertices of a complete graph g l, with assigned energies j e = j, at the edges e ∈ e ( g l ) with ∂e = {', ', }. we denote by x ( ) = ( x j ( ) ) n j = 1 the vector of binary variables that lists the n syntactic features of the language. we consider these as maps x : l → { 0, 1 } n, or equivalently as points x', '∈ { 0, 1 } n l. consider an', 'energy functional of the form for j, = j e > 0, where d h ( x ( ), x ( ) ) is the hamming distance, the corresponding partition function is given by at low temperature ( large β ), the partition function is concentrated around the minimum', 'of h ( x ), that is, were all d h ( x ( ), x ( ) ) = 0, hence where all the vectors x ( ) ∈ { 0, 1 } n agree. given an initial condition x 0 ∈ { 0, 1 }', 'n l and the datum ( j e ) e∈e ( g l ) of the strengths of the interaction energies along the', 'edges, the same method used in  #TAUTHOR_TAG, based on the standard metropolis - hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior. in the space of code parameters, given the code point ( δ 0, r', '0 ) = ( δ ( c ( x 0 ) ), r ( c ( x 0 ) ) ) associated to the initial condition x 0, the dynamics moves the code point along the line with constant r = r 0. as the', 'dynamics approaches the minimum of the action, the code point enters the region below the gilbert - varshamov', 'bound, as it moves towards smaller values of δ', '']",0
"['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian should be modified by a term that introduces the relations as a lagrange multiplier. this alters the dynamics and the equilibrium state, depending', 'on a parameter that measures how strongly enforced the relations are. from the point of view of coding theory discussed here, it seems more reasonable to modify this dynamical system, so that it can be better described as a dynamics in the space of code parameters. it is natural therefore to consider a similar setting, where we assign a given set l of', 'languages to the vertices of a complete graph g l, with assigned energies j e = j, at the edges e ∈ e ( g l ) with ∂e = {', ', }. we denote by x ( ) = ( x j ( ) ) n j = 1 the vector of binary variables that lists the n syntactic features of the language. we consider these as maps x : l → { 0, 1 } n, or equivalently as points x', '∈ { 0, 1 } n l. consider an', 'energy functional of the form for j, = j e > 0, where d h ( x ( ), x ( ) ) is the hamming distance, the corresponding partition function is given by at low temperature ( large β ), the partition function is concentrated around the minimum', 'of h ( x ), that is, were all d h ( x ( ), x ( ) ) = 0, hence where all the vectors x ( ) ∈ { 0, 1 } n agree. given an initial condition x 0 ∈ { 0, 1 }', 'n l and the datum ( j e ) e∈e ( g l ) of the strengths of the interaction energies along the', 'edges, the same method used in  #TAUTHOR_TAG, based on the standard metropolis - hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior. in the space of code parameters, given the code point ( δ 0, r', '0 ) = ( δ ( c ( x 0 ) ), r ( c ( x 0 ) ) ) associated to the initial condition x 0, the dynamics moves the code point along the line with constant r = r 0. as the', 'dynamics approaches the minimum of the action, the code point enters the region below the gilbert - varshamov', 'bound, as it moves towards smaller values of δ', '']",0
"['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian should be modified by a term that introduces the relations as a lagrange multiplier. this alters the dynamics and the equilibrium state, depending', 'on a parameter that measures how strongly enforced the relations are. from the point of view of coding theory discussed here, it seems more reasonable to modify this dynamical system, so that it can be better described as a dynamics in the space of code parameters. it is natural therefore to consider a similar setting, where we assign a given set l of', 'languages to the vertices of a complete graph g l, with assigned energies j e = j, at the edges e ∈ e ( g l ) with ∂e = {', ', }. we denote by x ( ) = ( x j ( ) ) n j = 1 the vector of binary variables that lists the n syntactic features of the language. we consider these as maps x : l → { 0, 1 } n, or equivalently as points x', '∈ { 0, 1 } n l. consider an', 'energy functional of the form for j, = j e > 0, where d h ( x ( ), x ( ) ) is the hamming distance, the corresponding partition function is given by at low temperature ( large β ), the partition function is concentrated around the minimum', 'of h ( x ), that is, were all d h ( x ( ), x ( ) ) = 0, hence where all the vectors x ( ) ∈ { 0, 1 } n agree. given an initial condition x 0 ∈ { 0, 1 }', 'n l and the datum ( j e ) e∈e ( g l ) of the strengths of the interaction energies along the', 'edges, the same method used in  #TAUTHOR_TAG, based on the standard metropolis - hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior. in the space of code parameters, given the code point ( δ 0, r', '0 ) = ( δ ( c ( x 0 ) ), r ( c ( x 0 ) ) ) associated to the initial condition x 0, the dynamics moves the code point along the line with constant r = r 0. as the', 'dynamics approaches the minimum of the action, the code point enters the region below the gilbert - varshamov', 'bound, as it moves towards smaller values of δ', '']",0
['0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail'],"['0, log 3 ( 2 ) ) ) already lies on the vertical line δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail']",['δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail'],"['{ english, welsh, russian,', 'bulgarian } and the two syntactic parameters are { x 1, x 2 } = { strongdeixis, stronganaphoric', '##ity }. since we have an entailment relation, the possible values of the variables x i are now ternary, x i ( )', '∈ { 0, −1, + 1 }, that is, we consider here codes c ⊂ f n 3. in this example n = 2. the initial condition x 0 is given by note that, since we have two identical code words x 0 ( 1 ) = x 0 ( 3 ) in this initial condition, the parameter d ( c l ) = 0, so the code point ( δ ( c l ), r ( c', 'l ) ) = ( 0, log 3 ( 2 ) ) ) already lies on the vertical line δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entailment, which is a modification', 'of the ising model to a coupling of an ising', 'and a potts model with q = 3 at the vertices of the graph. this dynamics, which depends on the temperature parameter t = 1 / β an on an auxiliary parameter e, the', '']",0
['0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail'],"['0, log 3 ( 2 ) ) ) already lies on the vertical line δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail']",['δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail'],"['{ english, welsh, russian,', 'bulgarian } and the two syntactic parameters are { x 1, x 2 } = { strongdeixis, stronganaphoric', '##ity }. since we have an entailment relation, the possible values of the variables x i are now ternary, x i ( )', '∈ { 0, −1, + 1 }, that is, we consider here codes c ⊂ f n 3. in this example n = 2. the initial condition x 0 is given by note that, since we have two identical code words x 0 ( 1 ) = x 0 ( 3 ) in this initial condition, the parameter d ( c l ) = 0, so the code point ( δ ( c l ), r ( c', 'l ) ) = ( 0, log 3 ( 2 ) ) ) already lies on the vertical line δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entailment, which is a modification', 'of the ising model to a coupling of an ising', 'and a potts model with q = 3 at the vertices of the graph. this dynamics, which depends on the temperature parameter t = 1 / β an on an auxiliary parameter e, the', '']",0
['0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail'],"['0, log 3 ( 2 ) ) ) already lies on the vertical line δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail']",['δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail'],"['{ english, welsh, russian,', 'bulgarian } and the two syntactic parameters are { x 1, x 2 } = { strongdeixis, stronganaphoric', '##ity }. since we have an entailment relation, the possible values of the variables x i are now ternary, x i ( )', '∈ { 0, −1, + 1 }, that is, we consider here codes c ⊂ f n 3. in this example n = 2. the initial condition x 0 is given by note that, since we have two identical code words x 0 ( 1 ) = x 0 ( 3 ) in this initial condition, the parameter d ( c l ) = 0, so the code point ( δ ( c l ), r ( c', 'l ) ) = ( 0, log 3 ( 2 ) ) ) already lies on the vertical line δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entailment, which is a modification', 'of the ising model to a coupling of an ising', 'and a potts model with q = 3 at the vertices of the graph. this dynamics, which depends on the temperature parameter t = 1 / β an on an auxiliary parameter e, the', '']",0
"['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian should be modified by a term that introduces the relations as a lagrange multiplier. this alters the dynamics and the equilibrium state, depending', 'on a parameter that measures how strongly enforced the relations are. from the point of view of coding theory discussed here, it seems more reasonable to modify this dynamical system, so that it can be better described as a dynamics in the space of code parameters. it is natural therefore to consider a similar setting, where we assign a given set l of', 'languages to the vertices of a complete graph g l, with assigned energies j e = j, at the edges e ∈ e ( g l ) with ∂e = {', ', }. we denote by x ( ) = ( x j ( ) ) n j = 1 the vector of binary variables that lists the n syntactic features of the language. we consider these as maps x : l → { 0, 1 } n, or equivalently as points x', '∈ { 0, 1 } n l. consider an', 'energy functional of the form for j, = j e > 0, where d h ( x ( ), x ( ) ) is the hamming distance, the corresponding partition function is given by at low temperature ( large β ), the partition function is concentrated around the minimum', 'of h ( x ), that is, were all d h ( x ( ), x ( ) ) = 0, hence where all the vectors x ( ) ∈ { 0, 1 } n agree. given an initial condition x 0 ∈ { 0, 1 }', 'n l and the datum ( j e ) e∈e ( g l ) of the strengths of the interaction energies along the', 'edges, the same method used in  #TAUTHOR_TAG, based on the standard metropolis - hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior. in the space of code parameters, given the code point ( δ 0, r', '0 ) = ( δ ( c ( x 0 ) ), r ( c ( x 0 ) ) ) associated to the initial condition x 0, the dynamics moves the code point along the line with constant r = r 0. as the', 'dynamics approaches the minimum of the action, the code point enters the region below the gilbert - varshamov', 'bound, as it moves towards smaller values of δ', '']",5
"['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian']","['where each syntactic variables runs as an independent ising model, the minimum is achieved where, that is, when all', 'the spins align. in the presence of entailment relations between different syntactic variables, it was shown in  #TAUTHOR_TAG that the hamiltonian should be modified by a term that introduces the relations as a lagrange multiplier. this alters the dynamics and the equilibrium state, depending', 'on a parameter that measures how strongly enforced the relations are. from the point of view of coding theory discussed here, it seems more reasonable to modify this dynamical system, so that it can be better described as a dynamics in the space of code parameters. it is natural therefore to consider a similar setting, where we assign a given set l of', 'languages to the vertices of a complete graph g l, with assigned energies j e = j, at the edges e ∈ e ( g l ) with ∂e = {', ', }. we denote by x ( ) = ( x j ( ) ) n j = 1 the vector of binary variables that lists the n syntactic features of the language. we consider these as maps x : l → { 0, 1 } n, or equivalently as points x', '∈ { 0, 1 } n l. consider an', 'energy functional of the form for j, = j e > 0, where d h ( x ( ), x ( ) ) is the hamming distance, the corresponding partition function is given by at low temperature ( large β ), the partition function is concentrated around the minimum', 'of h ( x ), that is, were all d h ( x ( ), x ( ) ) = 0, hence where all the vectors x ( ) ∈ { 0, 1 } n agree. given an initial condition x 0 ∈ { 0, 1 }', 'n l and the datum ( j e ) e∈e ( g l ) of the strengths of the interaction energies along the', 'edges, the same method used in  #TAUTHOR_TAG, based on the standard metropolis - hastings algorithm, can be used to study the dynamics in this setting, with a similar behavior. in the space of code parameters, given the code point ( δ 0, r', '0 ) = ( δ ( c ( x 0 ) ), r ( c ( x 0 ) ) ) associated to the initial condition x 0, the dynamics moves the code point along the line with constant r = r 0. as the', 'dynamics approaches the minimum of the action, the code point enters the region below the gilbert - varshamov', 'bound, as it moves towards smaller values of δ', '']",5
['0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail'],"['0, log 3 ( 2 ) ) ) already lies on the vertical line δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail']",['δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entail'],"['{ english, welsh, russian,', 'bulgarian } and the two syntactic parameters are { x 1, x 2 } = { strongdeixis, stronganaphoric', '##ity }. since we have an entailment relation, the possible values of the variables x i are now ternary, x i ( )', '∈ { 0, −1, + 1 }, that is, we consider here codes c ⊂ f n 3. in this example n = 2. the initial condition x 0 is given by note that, since we have two identical code words x 0 ( 1 ) = x 0 ( 3 ) in this initial condition, the parameter d ( c l ) = 0, so the code point ( δ ( c l ), r ( c', 'l ) ) = ( 0, log 3 ( 2 ) ) ) already lies on the vertical line δ = 0. we consider in this case the same dynamical system used in  #TAUTHOR_TAG to model the case with entailment, which is a modification', 'of the ising model to a coupling of an ising', 'and a potts model with q = 3 at the vertices of the graph. this dynamics, which depends on the temperature parameter t = 1 / β an on an auxiliary parameter e, the', '']",5
"['interaction energies along the edges as in  #TAUTHOR_TAG, taken from the']","['interaction energies along the edges as in  #TAUTHOR_TAG, taken from the']","['7 ], with all the entailment relations taken into account, and the same interaction energies along the edges as in  #TAUTHOR_TAG, taken from the data']","['.', 'the example mentioned above is too simple and artificial to be significant, but we can analyze a more general situation, where we consider the full syntactic data of [ 6 ], [ 7 ], with all the entailment relations taken into account, and the same interaction energies along the edges as in  #TAUTHOR_TAG, taken from the data of [ 16 ], which can be regarded as roughly proportional to a measure of the amount of bilingualism.', 'when we work with the full set of data from [ 6 ], [ 7 ], involving 63 parameters for 28 languages ( from which we exclude those that do not occur in the [ 16 ] data ), we see that the large size of the graph and the presence of many entailment relations render the dynamics figure 8.', 'dynamics in the space of code parameters : average distance.', 'a lot more complicated than the simple examples discussed in  #TAUTHOR_TAG.', 'indeed for such a large graph the convergence of the dynamics becomes extremely slow, even in the low temperature case and even when entailment relations are switched off, as shown in the graph of the average magnetization in figure 6.', 'such a large system becomes computationally too heavy, and it is difficult to handle a sufficiently large iterations to get to see any convergence effect.', 'however, when one considers codes obtained by extracting arbitrary subsets of three languages from this set and follows them along the dynamics, computing the corresponding position in the space of code parameters, one sees that, in the case without entailment ( e = 0 ) the average distance drops notably after enough iteration, as shown in figure 8 indicating that the simulation might in fact converge, even though at the same state in the number of iteration the average magnetization is not settling yet.', 'in the case with entailment relations one should expect the convergence process to be even slower.', 'moreover, as in the small example discussed above, the δ parameter may settle on a limit value different than zero, so the data of the simulation are less informative']",5
"['interaction energies along the edges as in  #TAUTHOR_TAG, taken from the']","['interaction energies along the edges as in  #TAUTHOR_TAG, taken from the']","['7 ], with all the entailment relations taken into account, and the same interaction energies along the edges as in  #TAUTHOR_TAG, taken from the data']","['.', 'the example mentioned above is too simple and artificial to be significant, but we can analyze a more general situation, where we consider the full syntactic data of [ 6 ], [ 7 ], with all the entailment relations taken into account, and the same interaction energies along the edges as in  #TAUTHOR_TAG, taken from the data of [ 16 ], which can be regarded as roughly proportional to a measure of the amount of bilingualism.', 'when we work with the full set of data from [ 6 ], [ 7 ], involving 63 parameters for 28 languages ( from which we exclude those that do not occur in the [ 16 ] data ), we see that the large size of the graph and the presence of many entailment relations render the dynamics figure 8.', 'dynamics in the space of code parameters : average distance.', 'a lot more complicated than the simple examples discussed in  #TAUTHOR_TAG.', 'indeed for such a large graph the convergence of the dynamics becomes extremely slow, even in the low temperature case and even when entailment relations are switched off, as shown in the graph of the average magnetization in figure 6.', 'such a large system becomes computationally too heavy, and it is difficult to handle a sufficiently large iterations to get to see any convergence effect.', 'however, when one considers codes obtained by extracting arbitrary subsets of three languages from this set and follows them along the dynamics, computing the corresponding position in the space of code parameters, one sees that, in the case without entailment ( e = 0 ) the average distance drops notably after enough iteration, as shown in figure 8 indicating that the simulation might in fact converge, even though at the same state in the number of iteration the average magnetization is not settling yet.', 'in the case with entailment relations one should expect the convergence process to be even slower.', 'moreover, as in the small example discussed above, the δ parameter may settle on a limit value different than zero, so the data of the simulation are less informative']",4
"['mtnt ) dataset  #TAUTHOR_TAG to enhance the robustness of mt systems by emulating naturally occurring noise in otherwise clean data.', 'synthesizing noise in this manner we are ultimately able to']","['of noisy text ( mtnt ) dataset  #TAUTHOR_TAG to enhance the robustness of mt systems by emulating naturally occurring noise in otherwise clean data.', 'synthesizing noise in this manner we are ultimately able to']","['mtnt ) dataset  #TAUTHOR_TAG to enhance the robustness of mt systems by emulating naturally occurring noise in otherwise clean data.', 'synthesizing noise in this manner we are ultimately able to']","['machine translation ( mt ) systems perform consistently well on clean, in - domain text.', 'however human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of output translation.', 'in this paper we leverage the machine translation of noisy text ( mtnt ) dataset  #TAUTHOR_TAG to enhance the robustness of mt systems by emulating naturally occurring noise in otherwise clean data.', 'synthesizing noise in this manner we are ultimately able to make a vanilla mt system resilient to naturally occurring noise and partially mitigate loss in accuracy resulting therefrom']",5
"['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the']","['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the']","['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the robustness of mt systems to naturally occurring noise presents an important and interesting task.', 'recent work on mt robustness  #AUTHOR_TAG has further demonstrated the need']","['translation ( mt ) systems have been shown to exhibit severely degraded performance when presented with translation of out - of - domain or noisy data  #AUTHOR_TAG.', 'this is particularly pronounced in systems trained on clean, formalized parallel data such as europarl  #AUTHOR_TAG, are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the robustness of mt systems to naturally occurring noise presents an important and interesting task.', 'recent work on mt robustness  #AUTHOR_TAG has further demonstrated the need to build or adapt systems that are resilient to such noise.', '']",5
"['this method, we inject artificial noise in the clean data according to the distribution of types of noise in mtnt specified in  #TAUTHOR_TAG.', 'for every token we choose to introduce the different types of noise with some probability on both french and english sides in 100k sentences of ep.', 'specifically, we fix']","['this method, we inject artificial noise in the clean data according to the distribution of types of noise in mtnt specified in  #TAUTHOR_TAG.', 'for every token we choose to introduce the different types of noise with some probability on both french and english sides in 100k sentences of ep.', 'specifically, we fix']","['this method, we inject artificial noise in the clean data according to the distribution of types of noise in mtnt specified in  #TAUTHOR_TAG.', 'for every token we choose to introduce the different types of noise with some probability on both french and english sides in 100k sentences of ep.', 'specifically, we fix the probabilities']","['this method, we inject artificial noise in the clean data according to the distribution of types of noise in mtnt specified in  #TAUTHOR_TAG.', 'for every token we choose to introduce the different types of noise with some probability on both french and english sides in 100k sentences of ep.', 'specifically, we fix the probabilities of error types as follows : spelling ( 0. 04 ), profanity ( 0. 007 ), grammar ( 0. 015 ) and emoticons ( 0. 002 ).', 'to simulate spelling error, we randomly add or drop a character in a given word.', 'for grammar error and profanity, we randomly select and insert a stop word or an expletive and its translation on either side.', 'similarly for emoticons, we randomly select an emoticon and insert it on both sides.', 'algorithm 1 elaborates on this procedure']",5
"['mtnt ) dataset  #TAUTHOR_TAG to enhance the robustness of mt systems by emulating naturally occurring noise in otherwise clean data.', 'synthesizing noise in this manner we are ultimately able to']","['of noisy text ( mtnt ) dataset  #TAUTHOR_TAG to enhance the robustness of mt systems by emulating naturally occurring noise in otherwise clean data.', 'synthesizing noise in this manner we are ultimately able to']","['mtnt ) dataset  #TAUTHOR_TAG to enhance the robustness of mt systems by emulating naturally occurring noise in otherwise clean data.', 'synthesizing noise in this manner we are ultimately able to']","['machine translation ( mt ) systems perform consistently well on clean, in - domain text.', 'however human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of output translation.', 'in this paper we leverage the machine translation of noisy text ( mtnt ) dataset  #TAUTHOR_TAG to enhance the robustness of mt systems by emulating naturally occurring noise in otherwise clean data.', 'synthesizing noise in this manner we are ultimately able to make a vanilla mt system resilient to naturally occurring noise and partially mitigate loss in accuracy resulting therefrom']",3
"['have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['baseline mt model architecture consists of a bidirectional long short - term memory ( lstm ) network encoder - decoder model with two layers.', 'the hidden and embedding sizes are set to 256 and 512, respectively.', 'we also employ weighttying  #AUTHOR_TAG between the embedding layer and projection layer of the decoder.', 'for expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]",3
"['this method, we inject artificial noise in the clean data according to the distribution of types of noise in mtnt specified in  #TAUTHOR_TAG.', 'for every token we choose to introduce the different types of noise with some probability on both french and english sides in 100k sentences of ep.', 'specifically, we fix']","['this method, we inject artificial noise in the clean data according to the distribution of types of noise in mtnt specified in  #TAUTHOR_TAG.', 'for every token we choose to introduce the different types of noise with some probability on both french and english sides in 100k sentences of ep.', 'specifically, we fix']","['this method, we inject artificial noise in the clean data according to the distribution of types of noise in mtnt specified in  #TAUTHOR_TAG.', 'for every token we choose to introduce the different types of noise with some probability on both french and english sides in 100k sentences of ep.', 'specifically, we fix the probabilities']","['this method, we inject artificial noise in the clean data according to the distribution of types of noise in mtnt specified in  #TAUTHOR_TAG.', 'for every token we choose to introduce the different types of noise with some probability on both french and english sides in 100k sentences of ep.', 'specifically, we fix the probabilities of error types as follows : spelling ( 0. 04 ), profanity ( 0. 007 ), grammar ( 0. 015 ) and emoticons ( 0. 002 ).', 'to simulate spelling error, we randomly add or drop a character in a given word.', 'for grammar error and profanity, we randomly select and insert a stop word or an expletive and its translation on either side.', 'similarly for emoticons, we randomly select an emoticon and insert it on both sides.', 'algorithm 1 elaborates on this procedure']",3
"['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the']","['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the']","['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the robustness of mt systems to naturally occurring noise presents an important and interesting task.', 'recent work on mt robustness  #AUTHOR_TAG has further demonstrated the need']","['translation ( mt ) systems have been shown to exhibit severely degraded performance when presented with translation of out - of - domain or noisy data  #AUTHOR_TAG.', 'this is particularly pronounced in systems trained on clean, formalized parallel data such as europarl  #AUTHOR_TAG, are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the robustness of mt systems to naturally occurring noise presents an important and interesting task.', 'recent work on mt robustness  #AUTHOR_TAG has further demonstrated the need to build or adapt systems that are resilient to such noise.', '']",0
"['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the']","['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the']","['accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the robustness of mt systems to naturally occurring noise presents an important and interesting task.', 'recent work on mt robustness  #AUTHOR_TAG has further demonstrated the need']","['translation ( mt ) systems have been shown to exhibit severely degraded performance when presented with translation of out - of - domain or noisy data  #AUTHOR_TAG.', 'this is particularly pronounced in systems trained on clean, formalized parallel data such as europarl  #AUTHOR_TAG, are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance  #TAUTHOR_TAG.', 'improving the robustness of mt systems to naturally occurring noise presents an important and interesting task.', 'recent work on mt robustness  #AUTHOR_TAG has further demonstrated the need to build or adapt systems that are resilient to such noise.', '']",0
"['of natural noise  #AUTHOR_TAG which causes pronounced problems for mt  #TAUTHOR_TAG.', 'robustness to noise in mt can be treated as a domain adaptation problem  #AUTHOR_TAG and several attempts have been made to handle noise from this perspective']","['of natural noise  #AUTHOR_TAG which causes pronounced problems for mt  #TAUTHOR_TAG.', 'robustness to noise in mt can be treated as a domain adaptation problem  #AUTHOR_TAG and several attempts have been made to handle noise from this perspective.', 'notable approaches']","['generated text on the internet and social media are a particularly rich source of natural noise  #AUTHOR_TAG which causes pronounced problems for mt  #TAUTHOR_TAG.', 'robustness to noise in mt can be treated as a domain adaptation problem  #AUTHOR_TAG and several attempts have been made to handle noise from this perspective.', 'notable approaches include training on varying amounts of data from the target domain  #AUTHOR_TAG,  #AUTHOR_TAG suggest the use of fine - tuning on varying amounts of target domain data, and  #AUTHOR_TAG note']","['generated text on the internet and social media are a particularly rich source of natural noise  #AUTHOR_TAG which causes pronounced problems for mt  #TAUTHOR_TAG.', 'robustness to noise in mt can be treated as a domain adaptation problem  #AUTHOR_TAG and several attempts have been made to handle noise from this perspective.', 'notable approaches include training on varying amounts of data from the target domain  #AUTHOR_TAG,  #AUTHOR_TAG suggest the use of fine - tuning on varying amounts of target domain data, and  #AUTHOR_TAG note a logarithmic relationship between the amount of data used in fine - tuning and the relative success of mt models.', 'other approaches to domain adaptation include weighting of domains in the system objective function  #AUTHOR_TAG and specifically curated datasets for adaptation  #AUTHOR_TAG.', ' #AUTHOR_TAG introduce a method of domain tagging to assist neural models in differentiating domains.', 'whilst the above approaches have shown success in specifically adapting across domains, we contend that adaptation to noise is a nuanced task and treating the problem as a domain adaptation task may fail to fully account for the varied types of noise that can occur in internet and social media text.', 'experiments that specifically handle noise include text normalization approaches  #AUTHOR_TAG and ( most relevant to our work ) the artificial induction of noise in otherwise clean data  #AUTHOR_TAG']",0
"['have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['baseline mt model architecture consists of a bidirectional long short - term memory ( lstm ) network encoder - decoder model with two layers.', 'the hidden and embedding sizes are set to 256 and 512, respectively.', 'we also employ weighttying  #AUTHOR_TAG between the embedding layer and projection layer of the decoder.', 'for expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]",4
"['have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]","['baseline mt model architecture consists of a bidirectional long short - term memory ( lstm ) network encoder - decoder model with two layers.', 'the hidden and embedding sizes are set to 256 and 512, respectively.', 'we also employ weighttying  #AUTHOR_TAG between the embedding layer and projection layer of the decoder.', 'for expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in  #TAUTHOR_TAG, which allows us to provide comparative results across a variety of settings.', 'other model parameters reflect the implementation outlined in  #TAUTHOR_TAG.', ""in all experimental settings we employ byte - pair encoding ( bpe )  #AUTHOR_TAG b ) using google's sentencepiece 2""]",6
"['of parameters comparable to  #TAUTHOR_TAG. choi', 'et']","['of parameters comparable to  #TAUTHOR_TAG. choi', 'et']","['number of parameters comparable to  #TAUTHOR_TAG. choi', 'et al. build on this']","['to as grouped dsconv ( gdsconv ). our key contributions are : • we propose a neural', 'network architecture tuned towards energy efficiency in microcontrollers grounded on the observation that memory access is costly,', 'while computation is cheap [ 9 ]. • our keyword - spotting network classifies on raw audio employing sincconvs while at the same time reducing the number of', 'parameters using ( g ) dsconvs. [', '3 ], while keeping the number of parameters comparable to  #TAUTHOR_TAG. choi', 'et al. build on this work as they also use a resnet - inspired architecture. instead of using 2d convolution over a time - frequency representation of the data they convolve along the time dimension and treat', 'the frequency dimension as channels [ 4 ]. this bears similarities with our approach as we are using 1d convolution along the time dimension as well. however, all the', 'approaches mentioned classify from mfccs or similar preprocessed features. our architecture works directly on raw audio signals. there is a recent trend towards using cnn', '##s on raw audio data directly [ 6, 10, 11, 12 ]. ravanelli et al. present an effective method of processing raw audio with cnns, called sincnet. kernels of', 'the first convolutional layer are restricted to only learn shapes of parametrized sinc functions. this method was first introduced', '']",0
"['of parameters comparable to  #TAUTHOR_TAG. choi', 'et']","['of parameters comparable to  #TAUTHOR_TAG. choi', 'et']","['number of parameters comparable to  #TAUTHOR_TAG. choi', 'et al. build on this']","['to as grouped dsconv ( gdsconv ). our key contributions are : • we propose a neural', 'network architecture tuned towards energy efficiency in microcontrollers grounded on the observation that memory access is costly,', 'while computation is cheap [ 9 ]. • our keyword - spotting network classifies on raw audio employing sincconvs while at the same time reducing the number of', 'parameters using ( g ) dsconvs. [', '3 ], while keeping the number of parameters comparable to  #TAUTHOR_TAG. choi', 'et al. build on this work as they also use a resnet - inspired architecture. instead of using 2d convolution over a time - frequency representation of the data they convolve along the time dimension and treat', 'the frequency dimension as channels [ 4 ]. this bears similarities with our approach as we are using 1d convolution along the time dimension as well. however, all the', 'approaches mentioned classify from mfccs or similar preprocessed features. our architecture works directly on raw audio signals. there is a recent trend towards using cnn', '##s on raw audio data directly [ 6, 10, 11, 12 ]. ravanelli et al. present an effective method of processing raw audio with cnns, called sincnet. kernels of', 'the first convolutional layer are restricted to only learn shapes of parametrized sinc functions. this method was first introduced', '']",0
"['[ 8, 13 ], neural translation [ 7 ] and kws  #TAUTHOR_TAG.', 'fig. 3 provides an overview of the steps']","['[ 8, 13 ], neural translation [ 7 ] and kws  #TAUTHOR_TAG.', 'fig. 3 provides an overview of the steps']","['of computer vision [ 8, 13 ], neural translation [ 7 ] and kws  #TAUTHOR_TAG.', 'fig. 3 provides an overview of the steps']","['##onv have been successfully applied to the domain of computer vision [ 8, 13 ], neural translation [ 7 ] and kws  #TAUTHOR_TAG.', 'fig. 3 provides an overview of the steps from a regular convolution to the gdsconv.', 'the number of parameters of one dsconv layer amounts to n dsconv = k · c in + c in · c out with the kernel size k and the number of input and output channels c in and c out respectively ; the first summand is determined by the depthwise convolution, the second summand by the pointwise convolution [ 7 ].', 'in our model configuration, the depthwise convolution only accounts for roughly 5 % of parameters in this layer, the pointwise for 95 %.', 'we therefore reduced the parameters of the pointwise convolution using grouping by a factor g to n gdsconv = k · c in + cin · cout g, rather than the parameters in the depthwise convolution.', 'to allow information exchange between groups we alternate the number of groups per layer, namely 2 and 3, as proposed in [ 7 ]']",0
"['of parameters comparable to  #TAUTHOR_TAG. choi', 'et']","['of parameters comparable to  #TAUTHOR_TAG. choi', 'et']","['number of parameters comparable to  #TAUTHOR_TAG. choi', 'et al. build on this']","['to as grouped dsconv ( gdsconv ). our key contributions are : • we propose a neural', 'network architecture tuned towards energy efficiency in microcontrollers grounded on the observation that memory access is costly,', 'while computation is cheap [ 9 ]. • our keyword - spotting network classifies on raw audio employing sincconvs while at the same time reducing the number of', 'parameters using ( g ) dsconvs. [', '3 ], while keeping the number of parameters comparable to  #TAUTHOR_TAG. choi', 'et al. build on this work as they also use a resnet - inspired architecture. instead of using 2d convolution over a time - frequency representation of the data they convolve along the time dimension and treat', 'the frequency dimension as channels [ 4 ]. this bears similarities with our approach as we are using 1d convolution along the time dimension as well. however, all the', 'approaches mentioned classify from mfccs or similar preprocessed features. our architecture works directly on raw audio signals. there is a recent trend towards using cnn', '##s on raw audio data directly [ 6, 10, 11, 12 ]. ravanelli et al. present an effective method of processing raw audio with cnns, called sincnet. kernels of', 'the first convolutional layer are restricted to only learn shapes of parametrized sinc functions. this method was first introduced', '']",3
"['second  #TAUTHOR_TAG.', 'we assume']","['second  #TAUTHOR_TAG.', 'we assume']","['second  #TAUTHOR_TAG.', 'we assume']","['application scenarios for smart devices imply that the device is powered by a battery, and possesses restricted hardware resources.', 'the requirements for a kws system in these scenarios are ( 1 ) very low power consumption to maximize battery life, ( 2 ) real - time or near real - time capability, ( 3 ) low memory footprint and ( 4 ) high accuracy to avoid random activations and to ensure responsiveness.', 'regarding real - time capability, our model is designed to operate on a single - core microcontroller capable of 50 mops per second  #TAUTHOR_TAG.', 'we assume that in microcontrollers the memory consumption of a kws neural network is associated with its power consumption : reading memory values contributes most to power consumption which makes re - use of weights favorable.', 'while in general large memory modules leak more power than small memory modules, one read operation from ram costs far more energy than the corresponding multiply - and - accumulate computation [ 16, 9 ].', 'in addition to the parameter - reducing approach in this work, further steps may be employed to reduce power consumption such as quantization, model compression or optimization strategies regarding dataflows that depend on the utilized hardware platform [ 16, 9, 17, 18 ]']",7
"['network in  #TAUTHOR_TAG, our network is more efficient in']","['work.', 'compared to the dsconv network in  #TAUTHOR_TAG, our network is more efficient in']","['', 'compared to the dsconv network in  #TAUTHOR_TAG, our network is more efficient in']","['base model composed of dsconv layers without grouping achieves the state - of - the - art accuracy of 96. 6 % on the speech commands test set.', 'the low - parameter model with gdsconv achieves almost the same accuracy of 96. 4 % with only about half the parameters.', 'this validates the effectiveness of gdsconv for model size reduction.', 'table 1 lists these results in comparison with related work.', 'compared to the dsconv network in  #TAUTHOR_TAG, our network is more efficient in terms of accuracy for a given parameter count.', 'their biggest model has a 1. 2 % lower accuracy than our base model while having about 4 times the parameters.', 'choi et al. [ 4 ] has the most competitive results while we are still able to improve upon their accuracy for a given number of parameters.', 'they are using 1d convolution along the time dimension as well which may be evidence that this yields better performance for audio processing or at least kws']",4
"['##ed features  #TAUTHOR_TAG, recent results indicate that']","['on handcrafted features  #TAUTHOR_TAG, recent results indicate that']","['##ed features  #TAUTHOR_TAG, recent results indicate that']","['essay scoring ( aes ) is the task of assigning grades to essays written in an educational setting, using a computer - based system with natural language processing capabilities.', 'the aim of designing such systems is to reduce the involvement of human graders as far as possible.', 'aes is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse  #AUTHOR_TAG.', 'although traditional aes methods typically rely on handcrafted features  #TAUTHOR_TAG, recent results indicate that state - of - the - art deep learning methods reach better performance  #AUTHOR_TAG, perhaps because these methods are able to capture subtle and complex information that is relevant to the task  #AUTHOR_TAG.', 'in this paper, we propose to combine string kernels ( low - level character n - gram features ) and word embeddings ( high - level semantic features ) to obtain state - of - the - art aes results.', 'since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification  #AUTHOR_TAG and sentiment analysis ( gimenez - perez et al., 2017 ; to native language identification  #AUTHOR_TAG dialect identification, we believe that string kernels can reach equally good results in aes.', 'to the best of our knowledge, string kernels have never been used for this task.', 'as string kernels are a simple approach that relies solely on character n - grams as features, it is fairly obvious that such an approach will not to cover several aspects ( e. g. : semantics, discourse ) required for the aes task.', 'to solve this problem, we propose to combine string kernels with a recent approach based on word embeddings, namely the bag - of - super - wordembeddings ( boswe ).', 'to our knowledge, this is the first successful attempt to combine string kernels and word embeddings.', 'we evaluate our approach on the automated student assessment prize data set, in both in - domain and cross - domain settings.', '']",0
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",0
"['##ed features  #TAUTHOR_TAG, recent results indicate that']","['on handcrafted features  #TAUTHOR_TAG, recent results indicate that']","['##ed features  #TAUTHOR_TAG, recent results indicate that']","['essay scoring ( aes ) is the task of assigning grades to essays written in an educational setting, using a computer - based system with natural language processing capabilities.', 'the aim of designing such systems is to reduce the involvement of human graders as far as possible.', 'aes is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse  #AUTHOR_TAG.', 'although traditional aes methods typically rely on handcrafted features  #TAUTHOR_TAG, recent results indicate that state - of - the - art deep learning methods reach better performance  #AUTHOR_TAG, perhaps because these methods are able to capture subtle and complex information that is relevant to the task  #AUTHOR_TAG.', 'in this paper, we propose to combine string kernels ( low - level character n - gram features ) and word embeddings ( high - level semantic features ) to obtain state - of - the - art aes results.', 'since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification  #AUTHOR_TAG and sentiment analysis ( gimenez - perez et al., 2017 ; to native language identification  #AUTHOR_TAG dialect identification, we believe that string kernels can reach equally good results in aes.', 'to the best of our knowledge, string kernels have never been used for this task.', 'as string kernels are a simple approach that relies solely on character n - grams as features, it is fairly obvious that such an approach will not to cover several aspects ( e. g. : semantics, discourse ) required for the aes task.', 'to solve this problem, we propose to combine string kernels with a recent approach based on word embeddings, namely the bag - of - super - wordembeddings ( boswe ).', 'to our knowledge, this is the first successful attempt to combine string kernels and word embeddings.', 'we evaluate our approach on the automated student assessment prize data set, in both in - domain and cross - domain settings.', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",4
"['approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the']","['state - of - the - art approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels,']","[' #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels,']","['this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring.', 'we compared our approach on the automated student assessment prize data set, in both in - domain and crossdomain settings, with several state - of - the - art approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.', 'using a shallow approach, we report better results compared to recent deep learning approaches  #AUTHOR_TAG']",4
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",1
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",3
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",3
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",3
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",3
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",3
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",5
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",5
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",5
"['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['##art methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores']","['task are', 'presented in table 3. for each and every source→target pair, we report better results than both state - of -', 'theart methods  #TAUTHOR_TAG. we observe that the difference between our best q', '##wk scores and the  #TAUTHOR_TAG are sometimes much higher in the cross - domain setting than in the', 'in - domain setting. we particularly notice that the difference from  #TAUTHOR_TAG when n t = 0 is always higher than 10 %. our highest improvement ( more than 54 %, from 0. 187 to 0. 728 ) over  #TAUTHOR_TAG is recorded for the pair 5→6, when n t', '= 0.', 'our score in this case ( 0. 728 ) is even higher', 'than both scores of  #TAUTHOR_TAG and  #AUTHOR_TAG when they use n t', '= 50. different from the in - domain setting, we note that the combination', 'of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples ( n t ) added into', 'the training set is less or equal to 25. discussion. it is worth noting', 'that in a set of preliminary experiments ( not included in the paper ), we actually considered another approach based on word embeddings. we tried to obtain a document embedding by averaging the word', '']",5
['a language model based on the matrix factorization interpretation of language modeling introduced by  #TAUTHOR_TAG'],['a language model based on the matrix factorization interpretation of language modeling introduced by  #TAUTHOR_TAG'],"['a language model based on the matrix factorization interpretation of language modeling introduced by  #TAUTHOR_TAG.', 'the']","['paper proposes a state - of - the - art recurrent neural network ( rnn ) language model that combines probability distributions computed not only from a final rnn layer but also from middle layers.', 'our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by  #TAUTHOR_TAG.', 'the proposed method improves the current state - of - the - art language model and achieves the best score on the penn treebank and wikitext - 2, which are the standard benchmark datasets.', 'moreover, we indicate our proposed method contributes to two application tasks : machine translation and headline generation.', 'our code is publicly available at : https : / / github. com / nttcslabnlp / doc lm']",5
"['in  #TAUTHOR_TAG as a baseline.', '‡ represents the perplex']","['in  #TAUTHOR_TAG as a baseline.', '‡ represents the perplexity']","['in  #TAUTHOR_TAG as a baseline.', '‡ represents the perplexity']","['implementation is based on the averaged stochastic gradient descent weight - dropped lstm ( awd - lstm ) 5 proposed by  #AUTHOR_TAG table 3 : perplexities of awd - lstm with doc on the ptb dataset.', 'we varied the number of probability distributions from each layer in situation j = 20 except for the top row.', 'the top row ( † ) represents mos scores reported in  #TAUTHOR_TAG as a baseline.', '‡ represents the perplexity obtained by the implementation of  #AUTHOR_TAG 6 with identical hyperparameters except for i 3.', 'dropout rate for vector k j, ct and the non - monotone interval.', 'since we found that the dropout rate for vector k j, ct greatly influences β in equation 13, we varied it from 0. 3 to 0. 6 with 0. 1 intervals.', 'we selected 0. 6 because this value achieved the best score on the ptb validation dataset.', '']",5
"['in  #TAUTHOR_TAG as a baseline.', '‡ represents the perplex']","['in  #TAUTHOR_TAG as a baseline.', '‡ represents the perplexity']","['in  #TAUTHOR_TAG as a baseline.', '‡ represents the perplexity']","['implementation is based on the averaged stochastic gradient descent weight - dropped lstm ( awd - lstm ) 5 proposed by  #AUTHOR_TAG table 3 : perplexities of awd - lstm with doc on the ptb dataset.', 'we varied the number of probability distributions from each layer in situation j = 20 except for the top row.', 'the top row ( † ) represents mos scores reported in  #TAUTHOR_TAG as a baseline.', '‡ represents the perplexity obtained by the implementation of  #AUTHOR_TAG 6 with identical hyperparameters except for i 3.', 'dropout rate for vector k j, ct and the non - monotone interval.', 'since we found that the dropout rate for vector k j, ct greatly influences β in equation 13, we varied it from 0. 3 to 0. 6 with 0. 1 intervals.', 'we selected 0. 6 because this value achieved the best score on the ptb validation dataset.', '']",5
"['lstmmos  #TAUTHOR_TAG.', 'we trained']","['with awd - lstm  #AUTHOR_TAG and awd - lstmmos  #TAUTHOR_TAG.', 'we trained']","['lstmmos  #TAUTHOR_TAG.', 'we trained each model with the same hyperparameters from our language modeling experiments ( section 5 ).', 'we selected the model that achieved the best perplexity on the validation set during the training.', '']","['compare awd - lstm - doc with awd - lstm  #AUTHOR_TAG and awd - lstmmos  #TAUTHOR_TAG.', 'we trained each model with the same hyperparameters from our language modeling experiments ( section 5 ).', 'we selected the model that achieved the best perplexity on the validation set during the training.', 'state - of - the - art results  #AUTHOR_TAG 91. 7 93. 3  #AUTHOR_TAG ( ensemble ) 92. 72 94. 25  #AUTHOR_TAG ( ensemble ) 92. 74 94. 32  #AUTHOR_TAG 95. 13 - moreover, awd - lstm - doc outperformed awd - lstm and awd - lstm - mos. these results correspond to the performance on the language modeling task ( section 5. 3 ).', '']",5
"['), a generalization method of mos introduced by  #TAUTHOR_TAG.', 'doc raises the expressive']","['connection ( doc ), a generalization method of mos introduced by  #TAUTHOR_TAG.', 'doc raises the expressive']","['), a generalization method of mos introduced by  #TAUTHOR_TAG.', 'doc raises the expressive power of rnn language models and improves quality of the model.', 'doc']","['proposed direct output connection ( doc ), a generalization method of mos introduced by  #TAUTHOR_TAG.', 'doc raises the expressive power of rnn language models and improves quality of the model.', 'doc outperformed mos and achieved the best perplexities on the standard benchmark datasets of language modeling : ptb and wikitext - 2.', 'moreover, we investigated its effectiveness on machine translation and headline generation.', 'our results show that doc also improved the performance of encdec and using a middle layer positively affected such application tasks']",5
"['', 'however,  #TAUTHOR_TAG proved that existing rn']","['', 'however,  #TAUTHOR_TAG proved that existing rnn language models have']","['high performance by using several regularizations and selecting appropriate hyperparameters  #AUTHOR_TAG.', 'however,  #TAUTHOR_TAG proved that existing rnn language models have low expressive power']","['', 'to compute the probability distribution, rnn language models encode sequence w 1 : t into a fixed - length vector and apply a transformation matrix and the softmax function.', 'previous researches demonstrated that rnn language models achieve high performance by using several regularizations and selecting appropriate hyperparameters  #AUTHOR_TAG.', 'however,  #TAUTHOR_TAG proved that existing rnn language models have low expressive power due to the softmax bottleneck, which means the output matrix of rnn language models is low rank when we interpret the training of rnn language models as a matrix factorization problem.', 'to solve the softmax bottleneck,  #TAUTHOR_TAG proposed mixture of softmaxes ( mos ), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed - length vector.', 'in this study, we propose direct output connection ( doc ) as a generalization of mos. for stacked rnns, doc computes the probability distributions from the middle layers including input embeddings.', 'in addition to raising the rank, the proposed method helps weaken the vanishing gradient problem in backpropagation because doc provides a shortcut connection to the output.', 'we conduct experiments']",1
"['', 'however,  #TAUTHOR_TAG proved that existing rn']","['', 'however,  #TAUTHOR_TAG proved that existing rnn language models have']","['high performance by using several regularizations and selecting appropriate hyperparameters  #AUTHOR_TAG.', 'however,  #TAUTHOR_TAG proved that existing rnn language models have low expressive power']","['', 'to compute the probability distribution, rnn language models encode sequence w 1 : t into a fixed - length vector and apply a transformation matrix and the softmax function.', 'previous researches demonstrated that rnn language models achieve high performance by using several regularizations and selecting appropriate hyperparameters  #AUTHOR_TAG.', 'however,  #TAUTHOR_TAG proved that existing rnn language models have low expressive power due to the softmax bottleneck, which means the output matrix of rnn language models is low rank when we interpret the training of rnn language models as a matrix factorization problem.', 'to solve the softmax bottleneck,  #TAUTHOR_TAG proposed mixture of softmaxes ( mos ), which increases the rank of the matrix by combining multiple probability distributions computed from the encoded fixed - length vector.', 'in this study, we propose direct output connection ( doc ) as a generalization of mos. for stacked rnns, doc computes the probability distributions from the middle layers including input embeddings.', 'in addition to raising the rank, the proposed method helps weaken the vanishing gradient problem in backpropagation because doc provides a shortcut connection to the output.', 'we conduct experiments']",6
[' #TAUTHOR_TAG also argued that rank'],[' #TAUTHOR_TAG also argued'],[').  #TAUTHOR_TAG also argued that rank'],"['', 'be interpreted as', 'a matrix factorization problem. in most cases, the', 'rank of matrix hw is d h n because d h n', 'is smaller than v and u in common rnn language models. thus, an rnn language model cannot express true distributions if d h n is much smaller than rank ( a ).  #TAUTHOR_TAG also argued that rank ( a ) is as high as vocabulary size v based on the following two assumptions : 1. natural language is highly context - dependent. in addition, since we can imagine many kinds of contexts, it is difficult to assume a basis', 'that represents a conditional probability distribution for any contexts. in other words, compressing u is difficult. 2. since we also have many kinds of semantic meanings, it is difficult to assume basic meanings that can create all other semantic', 'meanings by such simple operations as addition and subtraction ; compressing v is difficult. in summary,  #TAUTHOR_TAG indicated that d h n is much smaller than rank ( a ) because its scale is usually 10 2 and vocabulary size v', 'is at least 10 4']",6
"['construct a high - rank matrix,  #TAUTHOR_TAG proposed mixture of softmaxes ( mos ).', 'mos computes multiple probability distributions from']","['construct a high - rank matrix,  #TAUTHOR_TAG proposed mixture of softmaxes ( mos ).', 'mos computes multiple probability distributions from']","['construct a high - rank matrix,  #TAUTHOR_TAG proposed mixture of softmaxes ( mos ).', 'mos computes multiple probability distributions from']","['construct a high - rank matrix,  #TAUTHOR_TAG proposed mixture of softmaxes ( mos ).', 'mos computes multiple probability distributions from the hidden state of final rnn layer h n and regards the weighted average of the probability distributions as the final distribution.', '']",6
"['3,  #TAUTHOR_TAG interpreted training language']","['3,  #TAUTHOR_TAG interpreted training language']","['trained language models.', 'as described in section 3,  #TAUTHOR_TAG interpreted training language modeling as matrix factorization and improved']","['', ' #AUTHOR_TAG used lstm  #AUTHOR_TAG instead of a simple rnn for language modeling and significantly improved an rnn language model by applying dropout  #AUTHOR_TAG to all the connections except for the recurrent connections.', 'to regularize the recurrent connections,  #AUTHOR_TAG proposed variational inference - based dropout.', 'their method uses the same dropout mask at each timestep.', ' #AUTHOR_TAG proposed fraternal dropout, which minimizes the differences between outputs from different dropout masks to be invariant to the dropout mask.', ' #AUTHOR_TAG used black - box optimization to find appropriate hyperparameters for rnn language models and demonstrated that the standard lstm with proper regularizations can outperform other architectures.', 'apart from dropout techniques,  #AUTHOR_TAG and  #AUTHOR_TAG proposed the word tying method ( wt ), which unifies word embeddings ( e in equation 4 ) with the weight matrix to compute probability distributions ( w in equation 2 ).', 'in addition to quantitative evaluation,  #AUTHOR_TAG provided a theoretical justification for wt and proposed the augmented loss technique ( al ), which computes an objective probability based on word embeddings.', 'in addition to these regularization techniques,  #AUTHOR_TAG used dropconnect  #AUTHOR_TAG and averaged sgd  #AUTHOR_TAG for an lstm language model.', 'their awd - lstm achieved lower perplexity than  #AUTHOR_TAG on ptb and wikitext - 2.', 'previous studies also explored superior architecture for language modeling.', ' #AUTHOR_TAG proposed recurrent highway networks that use highway layers  #AUTHOR_TAG to deepen recurrent connections.', ' #AUTHOR_TAG adopted reinforcement learning to construct the best rnn structure.', 'however, as mentioned,  #AUTHOR_TAG established that the standard lstm is superior to these architectures.', 'apart from rnn architecture, proposed the input - tooutput gate ( iog ), which boosts the performance of trained language models.', 'as described in section 3,  #TAUTHOR_TAG interpreted training language modeling as matrix factorization and improved performance by computing multiple probability distributions.', 'in this study, we generalized their approach to use the middle layers of rnns.', 'finally, our proposed method, doc, achieved the state - of - the - art score on the']",6
"['), a generalization method of mos introduced by  #TAUTHOR_TAG.', 'doc raises the expressive']","['connection ( doc ), a generalization method of mos introduced by  #TAUTHOR_TAG.', 'doc raises the expressive']","['), a generalization method of mos introduced by  #TAUTHOR_TAG.', 'doc raises the expressive power of rnn language models and improves quality of the model.', 'doc']","['proposed direct output connection ( doc ), a generalization method of mos introduced by  #TAUTHOR_TAG.', 'doc raises the expressive power of rnn language models and improves quality of the model.', 'doc outperformed mos and achieved the best perplexities on the standard benchmark datasets of language modeling : ptb and wikitext - 2.', 'moreover, we investigated its effectiveness on machine translation and headline generation.', 'our results show that doc also improved the performance of encdec and using a middle layer positively affected such application tasks']",6
['- mos  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],3
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['work  #TAUTHOR_TAG,']","['abuse detection models. in the case', 'of online social networks, the great variety of users, including very different language registers', ', spelling mistakes, as well as intentional users obfuscation, makes it almost impossible to have', 'models robust enough to be applied in all cases.  #AUTHOR_TAG have then shown that it is very easy', 'to bypass automatic toxic comment detection systems by making the abusive content difficult to detect ( intentional spelling mistakes, uncommon negatives... ). because the reactions', ""of other users to an abuse case are completely beyond the abuser's control, some authors consider the content of messages occurring around the targeted message, instead of focusing only"", 'on the targeted message itself. for instance,  #AUTHOR_TAG use features derived from the sentences neighboring a', 'given message to detect harassment on the web.  #AUTHOR_TAG take advantage of user features such as the gender, the number of in - game friends or the', 'number of daily logins to detect abuse in the community of an online game. in our previous work  #TAUTHOR_TAG, we proposed a radically different', 'method that completely ignores the textual content of the messages, and relies only on a graph', '- based modeling of the conversation. this is the only graph - based approach ignoring the linguistic content', 'proposed in the context of abusive messages detection. our conversational network extraction process is inspired from other works leveraging such graphs', 'for other purposes : chat logs  #AUTHOR_TAG or online forums  #AUTHOR_TAG interaction modeling, user group detection  #AUTHOR_TAG. additional references on abusive message detection and conversation', '##al network modeling can be found in  #TAUTHOR_TAG. in this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose', 'a new method to perform abuse detection while leveraging both sources. for this', 'purpose, we take advantage of the content -  #AUTHOR_TAG b ) and graph - based  #TAUTHOR_TAG methods that we previously developed. we', 'propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a french', 'multiplayer online game. we then perform a feature study, finding the most informative ones and discussing their role. our contribution is twofold', ': the exploration of fusion methods, and more importantly the identification of discriminative features for this problem. the rest of this article is organized as follows. in section 2, we describe the methods and strategies used in this work. in section 3 we present our dataset', '']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['work  #TAUTHOR_TAG,']","['abuse detection models. in the case', 'of online social networks, the great variety of users, including very different language registers', ', spelling mistakes, as well as intentional users obfuscation, makes it almost impossible to have', 'models robust enough to be applied in all cases.  #AUTHOR_TAG have then shown that it is very easy', 'to bypass automatic toxic comment detection systems by making the abusive content difficult to detect ( intentional spelling mistakes, uncommon negatives... ). because the reactions', ""of other users to an abuse case are completely beyond the abuser's control, some authors consider the content of messages occurring around the targeted message, instead of focusing only"", 'on the targeted message itself. for instance,  #AUTHOR_TAG use features derived from the sentences neighboring a', 'given message to detect harassment on the web.  #AUTHOR_TAG take advantage of user features such as the gender, the number of in - game friends or the', 'number of daily logins to detect abuse in the community of an online game. in our previous work  #TAUTHOR_TAG, we proposed a radically different', 'method that completely ignores the textual content of the messages, and relies only on a graph', '- based modeling of the conversation. this is the only graph - based approach ignoring the linguistic content', 'proposed in the context of abusive messages detection. our conversational network extraction process is inspired from other works leveraging such graphs', 'for other purposes : chat logs  #AUTHOR_TAG or online forums  #AUTHOR_TAG interaction modeling, user group detection  #AUTHOR_TAG. additional references on abusive message detection and conversation', '##al network modeling can be found in  #TAUTHOR_TAG. in this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose', 'a new method to perform abuse detection while leveraging both sources. for this', 'purpose, we take advantage of the content -  #AUTHOR_TAG b ) and graph - based  #TAUTHOR_TAG methods that we previously developed. we', 'propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a french', 'multiplayer online game. we then perform a feature study, finding the most informative ones and discussing their role. our contribution is twofold', ': the exploration of fusion methods, and more importantly the identification of discriminative features for this problem. the rest of this article is organized as follows. in section 2, we describe the methods and strategies used in this work. in section 3 we present our dataset', '']",0
['based method from  #TAUTHOR_TAG ('],"['method from  #AUTHOR_TAG b ) ( section 2. 1 ) and the graph - based method from  #TAUTHOR_TAG ( section 2. 2 ).', 'we then present']","['from  #AUTHOR_TAG b ) ( section 2. 1 ) and the graph - based method from  #TAUTHOR_TAG ( section 2. 2 ).', 'we then present the fusion method proposed in this paper, aiming at taking advantage of both sources of information ( section 2. 3 ).', 'figure 1 shows']","['this section, we summarize the content - based method from  #AUTHOR_TAG b ) ( section 2. 1 ) and the graph - based method from  #TAUTHOR_TAG ( section 2. 2 ).', 'we then present the fusion method proposed in this paper, aiming at taking advantage of both sources of information ( section 2. 3 ).', 'figure 1 shows the whole process, and is discussed through this section.', 'figure 1.', 'representation of our processing pipeline.', 'existing methods refers to our previous work described in  #AUTHOR_TAG b ) ( content - based method ) and  #TAUTHOR_TAG ( graph - based method ), whereas the contribution presented in this article appears on the right side ( fusion strategies )']",0
"['', 'the graph - based tf are discussed in depth in our previous article  #TAUTHOR_TAG.', 'to summarize, the most important features']","['caused by abusive message tending to be shouted, and therefore written in capitals.', 'the graph - based tf are discussed in depth in our previous article  #TAUTHOR_TAG.', 'to summarize, the most important features']","['', 'the graph - based tf are discussed in depth in our previous article  #TAUTHOR_TAG.', 'to summarize, the most important features help detecting changes in']","['', 'indeed, as explained in section 2. 2, most of these topological measures can handle / ignore edge weights and / or edge directions, can be vertex - or graph - focused, and can be computed for each of the three types of networks ( before, after and full ).', 'there are three content - based tf.', 'the first is the naive bayes prediction, which is not surprising as it comes from a fully fledged classifier processing bows.', 'the second is the tf - idf score computed over the abuse class, which shows that considering term frequencies indeed improve the classification performance.', 'the third is the capital ratio ( proportion of capital letters in the comment ), which is likely to be caused by abusive message tending to be shouted, and therefore written in capitals.', 'the graph - based tf are discussed in depth in our previous article  #TAUTHOR_TAG.', 'to summarize, the most important features help detecting changes in the direct neighborhood of the targeted author ( coreness, strength ), in the average node centrality at the level of the whole graph in terms of distance ( closeness ), and in the general reciprocity of exchanges between']",0
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['work  #TAUTHOR_TAG,']","['abuse detection models. in the case', 'of online social networks, the great variety of users, including very different language registers', ', spelling mistakes, as well as intentional users obfuscation, makes it almost impossible to have', 'models robust enough to be applied in all cases.  #AUTHOR_TAG have then shown that it is very easy', 'to bypass automatic toxic comment detection systems by making the abusive content difficult to detect ( intentional spelling mistakes, uncommon negatives... ). because the reactions', ""of other users to an abuse case are completely beyond the abuser's control, some authors consider the content of messages occurring around the targeted message, instead of focusing only"", 'on the targeted message itself. for instance,  #AUTHOR_TAG use features derived from the sentences neighboring a', 'given message to detect harassment on the web.  #AUTHOR_TAG take advantage of user features such as the gender, the number of in - game friends or the', 'number of daily logins to detect abuse in the community of an online game. in our previous work  #TAUTHOR_TAG, we proposed a radically different', 'method that completely ignores the textual content of the messages, and relies only on a graph', '- based modeling of the conversation. this is the only graph - based approach ignoring the linguistic content', 'proposed in the context of abusive messages detection. our conversational network extraction process is inspired from other works leveraging such graphs', 'for other purposes : chat logs  #AUTHOR_TAG or online forums  #AUTHOR_TAG interaction modeling, user group detection  #AUTHOR_TAG. additional references on abusive message detection and conversation', '##al network modeling can be found in  #TAUTHOR_TAG. in this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose', 'a new method to perform abuse detection while leveraging both sources. for this', 'purpose, we take advantage of the content -  #AUTHOR_TAG b ) and graph - based  #TAUTHOR_TAG methods that we previously developed. we', 'propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a french', 'multiplayer online game. we then perform a feature study, finding the most informative ones and discussing their role. our contribution is twofold', ': the exploration of fusion methods, and more importantly the identification of discriminative features for this problem. the rest of this article is organized as follows. in section 2, we describe the methods and strategies used in this work. in section 3 we present our dataset', '']",6
"['leverage message content  #AUTHOR_TAG a ) and interactions between users  #TAUTHOR_TAG, and create a new method using both types of information simultaneously.', 'we show that the features']","['leverage message content  #AUTHOR_TAG a ) and interactions between users  #TAUTHOR_TAG, and create a new method using both types of information simultaneously.', 'we show that the features']","['leverage message content  #AUTHOR_TAG a ) and interactions between users  #TAUTHOR_TAG, and create a new method using both types of information simultaneously.', 'we show that the features']","['this article, we tackle the problem of automatic abuse detection in online communities.', 'we take advantage of the methods that we previously developed to leverage message content  #AUTHOR_TAG a ) and interactions between users  #TAUTHOR_TAG, and create a new method using both types of information simultaneously.', 'we show that the features extracted from our content - and graph - based approaches are complementary, and that combining them allows to sensibly improve the results up to 93. 26 ( f - measure ).', 'one limitation of our method is the computational time required to extract certain features.', 'however, we show that using only a small subset of relevant features allows to dramatically reduce the processing time ( down to 3 % ) while keeping more than 97 % of the original performance.', 'another limitation of our work is the small size of our dataset.', 'we must find some other corpora to test our methods at a much higher scale.', 'however, all the available datasets are composed of isolated messages, when we need threads to make the most of our approach.', 'a solution could be to start from datasets such as the wikipedia - based corpus proposed by  #AUTHOR_TAG, and complete them by reconstructing the original conversations containing the annotated messages.', 'this could also be the opportunity to test our methods on an other language than french.', 'our content - based method may be impacted by this change, but this should not be the case for the graph - based method, as it is independent from the content ( and therefore the language ).', 'besides language, a different online community is likely to behave differently from the one we studied before.', 'in particular, its members could react differently differently to abuse.', 'the wikipedia dataset would therefore allow assessing how such cultural differences affect our classifiers, and identifying which observations made for space origin still apply to wikipedia']",6
"[' #TAUTHOR_TAG,']","[' #TAUTHOR_TAG,']","['work  #TAUTHOR_TAG,']","['abuse detection models. in the case', 'of online social networks, the great variety of users, including very different language registers', ', spelling mistakes, as well as intentional users obfuscation, makes it almost impossible to have', 'models robust enough to be applied in all cases.  #AUTHOR_TAG have then shown that it is very easy', 'to bypass automatic toxic comment detection systems by making the abusive content difficult to detect ( intentional spelling mistakes, uncommon negatives... ). because the reactions', ""of other users to an abuse case are completely beyond the abuser's control, some authors consider the content of messages occurring around the targeted message, instead of focusing only"", 'on the targeted message itself. for instance,  #AUTHOR_TAG use features derived from the sentences neighboring a', 'given message to detect harassment on the web.  #AUTHOR_TAG take advantage of user features such as the gender, the number of in - game friends or the', 'number of daily logins to detect abuse in the community of an online game. in our previous work  #TAUTHOR_TAG, we proposed a radically different', 'method that completely ignores the textual content of the messages, and relies only on a graph', '- based modeling of the conversation. this is the only graph - based approach ignoring the linguistic content', 'proposed in the context of abusive messages detection. our conversational network extraction process is inspired from other works leveraging such graphs', 'for other purposes : chat logs  #AUTHOR_TAG or online forums  #AUTHOR_TAG interaction modeling, user group detection  #AUTHOR_TAG. additional references on abusive message detection and conversation', '##al network modeling can be found in  #TAUTHOR_TAG. in this paper, based on the assumption that the interactions between users and the content of the exchanged messages convey different information, we propose', 'a new method to perform abuse detection while leveraging both sources. for this', 'purpose, we take advantage of the content -  #AUTHOR_TAG b ) and graph - based  #TAUTHOR_TAG methods that we previously developed. we', 'propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a french', 'multiplayer online game. we then perform a feature study, finding the most informative ones and discussing their role. our contribution is twofold', ': the exploration of fusion methods, and more importantly the identification of discriminative features for this problem. the rest of this article is organized as follows. in section 2, we describe the methods and strategies used in this work. in section 3 we present our dataset', '']",5
['based method from  #TAUTHOR_TAG ('],"['method from  #AUTHOR_TAG b ) ( section 2. 1 ) and the graph - based method from  #TAUTHOR_TAG ( section 2. 2 ).', 'we then present']","['from  #AUTHOR_TAG b ) ( section 2. 1 ) and the graph - based method from  #TAUTHOR_TAG ( section 2. 2 ).', 'we then present the fusion method proposed in this paper, aiming at taking advantage of both sources of information ( section 2. 3 ).', 'figure 1 shows']","['this section, we summarize the content - based method from  #AUTHOR_TAG b ) ( section 2. 1 ) and the graph - based method from  #TAUTHOR_TAG ( section 2. 2 ).', 'we then present the fusion method proposed in this paper, aiming at taking advantage of both sources of information ( section 2. 3 ).', 'figure 1 shows the whole process, and is discussed through this section.', 'figure 1.', 'representation of our processing pipeline.', 'existing methods refers to our previous work described in  #AUTHOR_TAG b ) ( content - based method ) and  #TAUTHOR_TAG ( graph - based method ), whereas the contribution presented in this article appears on the right side ( fusion strategies )']",5
['same measures as in  #TAUTHOR_TAG'],['same measures as in  #TAUTHOR_TAG'],['same measures as in  #TAUTHOR_TAG'],"['', 'example of such networks obtained for a message of the corpus described', 'in section 3. 1. once the conversational networks have been extracted, they must be described through numeric values in order to feed the svm', 'classifier. this is done through a selection of standard topological measures allowing to describe a graph in a number of distinct ways, focusing on different scales and scopes. the scale denotes the nature of the characterized entity. in this', 'work, the individual vertex and the whole graph are considered. when considering a single vertex', ', the measure focuses on the targeted author ( i. e. the author of the targeted message ). the scope can be either micro -, meso - or macroscopic : it corresponds to the amount of information considered by the measure. for instance', ', the graph density is microscopic, the modularity is mesoscopic, and the diameter is macroscopic. all these', 'measures are computed for each graph, and allow describing the conversation surrounding the message of interest. the svm is then trained using', 'these values as features. in this work, we use exactly the same measures as in  #TAUTHOR_TAG']",5
"['publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary']","['publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary']","['as in our previous publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary database containing 4, 029, 343 messages in french, exchanged on the in - game chat of spaceorigin 1, a massively multiplayer online role - playing game ( mmorp']","['dataset is the same as in our previous publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary database containing 4, 029, 343 messages in french, exchanged on the in - game chat of spaceorigin 1, a massively multiplayer online role - playing game ( mmorpg ).', 'among them, 779 have been flagged as being abusive by at least one user in the game, and confirmed as such by a human moderator.', '']",5
"['publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary']","['publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary']","['as in our previous publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary database containing 4, 029, 343 messages in french, exchanged on the in - game chat of spaceorigin 1, a massively multiplayer online role - playing game ( mmorp']","['dataset is the same as in our previous publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary database containing 4, 029, 343 messages in french, exchanged on the in - game chat of spaceorigin 1, a massively multiplayer online role - playing game ( mmorpg ).', 'among them, 779 have been flagged as being abusive by at least one user in the game, and confirmed as such by a human moderator.', '']",5
"['publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary']","['publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary']","['as in our previous publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary database containing 4, 029, 343 messages in french, exchanged on the in - game chat of spaceorigin 1, a massively multiplayer online role - playing game ( mmorp']","['dataset is the same as in our previous publications  #AUTHOR_TAG b  #TAUTHOR_TAG.', 'it is a proprietary database containing 4, 029, 343 messages in french, exchanged on the in - game chat of spaceorigin 1, a massively multiplayer online role - playing game ( mmorpg ).', 'among them, 779 have been flagged as being abusive by at least one user in the game, and confirmed as such by a human moderator.', '']",5
['same measures as in  #TAUTHOR_TAG'],['same measures as in  #TAUTHOR_TAG'],['same measures as in  #TAUTHOR_TAG'],"['', 'example of such networks obtained for a message of the corpus described', 'in section 3. 1. once the conversational networks have been extracted, they must be described through numeric values in order to feed the svm', 'classifier. this is done through a selection of standard topological measures allowing to describe a graph in a number of distinct ways, focusing on different scales and scopes. the scale denotes the nature of the characterized entity. in this', 'work, the individual vertex and the whole graph are considered. when considering a single vertex', ', the measure focuses on the targeted author ( i. e. the author of the targeted message ). the scope can be either micro -, meso - or macroscopic : it corresponds to the amount of information considered by the measure. for instance', ', the graph density is microscopic, the modularity is mesoscopic, and the diameter is macroscopic. all these', 'measures are computed for each graph, and allow describing the conversation surrounding the message of interest. the svm is then trained using', 'these values as features. in this work, we use exactly the same measures as in  #TAUTHOR_TAG']",1
['over  #TAUTHOR_TAG'],['significant gains over  #TAUTHOR_TAG'],"['over  #TAUTHOR_TAG for all translation', 'tasks in']","['cross - lingual mapping in a projected latent space and add more constraints to guide the unsupervised mapping in this space. we accomplish this by proposing a novel adversarial autoencoder framework  #AUTHOR_TAG, where adversarial mapping is done at the', '( latent ) code space as opposed to the original embedding space ( figure 1 ). this gives the model the flexibility to automatically induce the required geometric structures in its latent code space that could potentially', 'yield better mappings. søgaard et al. ( 2018 ) recently find that the isomorphic assumption made by most existing methods does not hold in general even for two closely related languages like english and german. in their words', '"" approaches based on this assumption have important limitations "". by mapping the latent vectors through', 'adversarial training, our approach therefore departs from the isomorphic assumption. in our adversarial training, not only the mapper but also the target encoder is trained to fool the discriminator. this forces the discriminator to', 'improve its discrimination skills, which in turn pushes the mapper to generate indistinguishable translation. to guide the mapping, we include two additional constraints. our first constraint enforces cycle consistency so that', 'code vectors after being translated from one language to another, and then translated back to their source space remain close to the', 'original vectors. the second constraint ensures reconstruction of the original input word embeddings', 'from the back - translated codes. this grounding', 'step forces the model to retain word semantics during the mapping process. we conduct a series of experiments with six different', 'language pairs ( in both directions ) comprising european, non - european, and low - resource languages from two different datasets. our results show that our model is more robust and yields significant gains over  #TAUTHOR_TAG for all translation', 'tasks in all evaluation measures. our method also gives better initial mapping compared to other existing methods  #AUTHOR_TAG b ). we also perform an', 'extensive ablation study to understand the contribution of different components of our model. the study reveals that cycle consistency contributes the most, while advers', '##arial training of the target encoder and post - cycle reconstruction also have significant effect. we have released our source code at https : / / ntunlps', '##g. github. io / project / unsup - word - translation / the remainder of this paper is organized as follows. after discussing related work in section 2, we present our unsupervised word', '']",0
['over  #TAUTHOR_TAG'],['significant gains over  #TAUTHOR_TAG'],"['over  #TAUTHOR_TAG for all translation', 'tasks in']","['cross - lingual mapping in a projected latent space and add more constraints to guide the unsupervised mapping in this space. we accomplish this by proposing a novel adversarial autoencoder framework  #AUTHOR_TAG, where adversarial mapping is done at the', '( latent ) code space as opposed to the original embedding space ( figure 1 ). this gives the model the flexibility to automatically induce the required geometric structures in its latent code space that could potentially', 'yield better mappings. søgaard et al. ( 2018 ) recently find that the isomorphic assumption made by most existing methods does not hold in general even for two closely related languages like english and german. in their words', '"" approaches based on this assumption have important limitations "". by mapping the latent vectors through', 'adversarial training, our approach therefore departs from the isomorphic assumption. in our adversarial training, not only the mapper but also the target encoder is trained to fool the discriminator. this forces the discriminator to', 'improve its discrimination skills, which in turn pushes the mapper to generate indistinguishable translation. to guide the mapping, we include two additional constraints. our first constraint enforces cycle consistency so that', 'code vectors after being translated from one language to another, and then translated back to their source space remain close to the', 'original vectors. the second constraint ensures reconstruction of the original input word embeddings', 'from the back - translated codes. this grounding', 'step forces the model to retain word semantics during the mapping process. we conduct a series of experiments with six different', 'language pairs ( in both directions ) comprising european, non - european, and low - resource languages from two different datasets. our results show that our model is more robust and yields significant gains over  #TAUTHOR_TAG for all translation', 'tasks in all evaluation measures. our method also gives better initial mapping compared to other existing methods  #AUTHOR_TAG b ). we also perform an', 'extensive ablation study to understand the contribution of different components of our model. the study reveals that cycle consistency contributes the most, while advers', '##arial training of the target encoder and post - cycle reconstruction also have significant effect. we have released our source code at https : / / ntunlps', '##g. github. io / project / unsup - word - translation / the remainder of this paper is organized as follows. after discussing related work in section 2, we present our unsupervised word', '']",0
"['initially used an adversarial', 'network similar to  #TAUTHOR_TAG, and found that the mapp']","['initially used an adversarial', 'network similar to  #TAUTHOR_TAG, and found that the mapp']","['initially used an adversarial', 'network similar to  #TAUTHOR_TAG, and found that the mapper ( which is also the encoder ) translates everything to a single embedding,', 'known']","['eliminate the seed dictionary totally and learn the map - ping in a purely unsupervised way. this was', 'first proposed by  #AUTHOR_TAG, who initially used an adversarial', 'network similar to  #TAUTHOR_TAG, and found that the mapper ( which is also the encoder ) translates everything to a single embedding,', 'known commonly as the mode collapse issue  #AUTHOR_TAG. to preserve diversity in mapping, he used a decoder to reconstruct the source embedding from the mapped embedding, extending', 'the framework to an adversarial autoencoder. his preliminary qualitative analysis shows encouraging results but not competitive with methods using bilingual seeds. he suspected', 'issues with training and with the isomorphic assumption. in our work, we successfully address these issues with an improved model', 'that also relaxes the isomorphic assumption. our model uses two separate autoencoders, one for each language, which allows us to put more constraints to guide the mapping. we also', '']",0
"['initially used an adversarial', 'network similar to  #TAUTHOR_TAG, and found that the mapp']","['initially used an adversarial', 'network similar to  #TAUTHOR_TAG, and found that the mapp']","['initially used an adversarial', 'network similar to  #TAUTHOR_TAG, and found that the mapper ( which is also the encoder ) translates everything to a single embedding,', 'known']","['eliminate the seed dictionary totally and learn the map - ping in a purely unsupervised way. this was', 'first proposed by  #AUTHOR_TAG, who initially used an adversarial', 'network similar to  #TAUTHOR_TAG, and found that the mapper ( which is also the encoder ) translates everything to a single embedding,', 'known commonly as the mode collapse issue  #AUTHOR_TAG. to preserve diversity in mapping, he used a decoder to reconstruct the source embedding from the mapped embedding, extending', 'the framework to an adversarial autoencoder. his preliminary qualitative analysis shows encouraging results but not competitive with methods using bilingual seeds. he suspected', 'issues with training and with the isomorphic assumption. in our work, we successfully address these issues with an improved model', 'that also relaxes the isomorphic assumption. our model uses two separate autoencoders, one for each language, which allows us to put more constraints to guide the mapping. we also', '']",0
"['following  #TAUTHOR_TAG with β = 0. 01.', 'our training setting is similar to  #TAUTHOR_TAG, and we']","['mappers following  #TAUTHOR_TAG with β = 0. 01.', 'our training setting is similar to  #TAUTHOR_TAG, and we']","['the orthogonalization update to the mappers following  #TAUTHOR_TAG with β = 0. 01.', 'our training setting is similar to  #TAUTHOR_TAG, and we apply the same pre - and postprocessing steps.', 'we use']","['', 'we also apply the orthogonalization update to the mappers following  #TAUTHOR_TAG with β = 0. 01.', 'our training setting is similar to  #TAUTHOR_TAG, and we apply the same pre - and postprocessing steps.', 'we use stochastic gradient descent ( sgd ) with a batch size of 32, a learning rate of 0. 1, and a decay of 0. 98.', 'for selecting the best model, we use the unsupervised validation criterion proposed by  #TAUTHOR_TAG, which correlates highly with the mapping quality.', '']",0
"['following  #TAUTHOR_TAG with β = 0. 01.', 'our training setting is similar to  #TAUTHOR_TAG, and we']","['mappers following  #TAUTHOR_TAG with β = 0. 01.', 'our training setting is similar to  #TAUTHOR_TAG, and we']","['the orthogonalization update to the mappers following  #TAUTHOR_TAG with β = 0. 01.', 'our training setting is similar to  #TAUTHOR_TAG, and we apply the same pre - and postprocessing steps.', 'we use']","['', 'we also apply the orthogonalization update to the mappers following  #TAUTHOR_TAG with β = 0. 01.', 'our training setting is similar to  #TAUTHOR_TAG, and we apply the same pre - and postprocessing steps.', 'we use stochastic gradient descent ( sgd ) with a batch size of 32, a learning rate of 0. 1, and a decay of 0. 98.', 'for selecting the best model, we use the unsupervised validation criterion proposed by  #TAUTHOR_TAG, which correlates highly with the mapping quality.', '']",0
['of  #TAUTHOR_TAG. to note that in'],"['adversarial autoencoder model on the dataset of  #TAUTHOR_TAG. to note that in contrast', '']","['of  #TAUTHOR_TAG. to note that in contrast', 'to  #TAUTHOR_TAG, our mapping is']","['', 'presents the results of  #AUTHOR_TAG that uses adversarial training to map the word embeddings. the next row shows the', 'results of our full model. the subsequent rows incrementally detach one component from our model. for example, - enc. adv denotes the variant of our model where the target encoder is not trained on', 'the adversarial loss ( θ e x in eq. 4 ) ; - - recon excludes the post - cycle reconstruction loss from - enc. adv, and - - - cycle excludes the cycle consistency from - - recon. thus, -', '- - cycle is a variant of our model that uses only adversarial loss to learn the mapping. however, it is important table', '5 : ablation study of our adversarial autoencoder model on the dataset of  #TAUTHOR_TAG. to note that in contrast', 'to  #TAUTHOR_TAG, our mapping is performed at the code space. as we compare our full model with', 'the model of  #TAUTHOR_TAG in the without fine - tuning setting', '']",0
['over  #TAUTHOR_TAG'],['significant gains over  #TAUTHOR_TAG'],"['over  #TAUTHOR_TAG for all translation', 'tasks in']","['cross - lingual mapping in a projected latent space and add more constraints to guide the unsupervised mapping in this space. we accomplish this by proposing a novel adversarial autoencoder framework  #AUTHOR_TAG, where adversarial mapping is done at the', '( latent ) code space as opposed to the original embedding space ( figure 1 ). this gives the model the flexibility to automatically induce the required geometric structures in its latent code space that could potentially', 'yield better mappings. søgaard et al. ( 2018 ) recently find that the isomorphic assumption made by most existing methods does not hold in general even for two closely related languages like english and german. in their words', '"" approaches based on this assumption have important limitations "". by mapping the latent vectors through', 'adversarial training, our approach therefore departs from the isomorphic assumption. in our adversarial training, not only the mapper but also the target encoder is trained to fool the discriminator. this forces the discriminator to', 'improve its discrimination skills, which in turn pushes the mapper to generate indistinguishable translation. to guide the mapping, we include two additional constraints. our first constraint enforces cycle consistency so that', 'code vectors after being translated from one language to another, and then translated back to their source space remain close to the', 'original vectors. the second constraint ensures reconstruction of the original input word embeddings', 'from the back - translated codes. this grounding', 'step forces the model to retain word semantics during the mapping process. we conduct a series of experiments with six different', 'language pairs ( in both directions ) comprising european, non - european, and low - resource languages from two different datasets. our results show that our model is more robust and yields significant gains over  #TAUTHOR_TAG for all translation', 'tasks in all evaluation measures. our method also gives better initial mapping compared to other existing methods  #AUTHOR_TAG b ). we also perform an', 'extensive ablation study to understand the contribution of different components of our model. the study reveals that cycle consistency contributes the most, while advers', '##arial training of the target encoder and post - cycle reconstruction also have significant effect. we have released our source code at https : / / ntunlps', '##g. github. io / project / unsup - word - translation / the remainder of this paper is organized as follows. after discussing related work in section 2, we present our unsupervised word', '']",1
"['present our results on european languages on the datasets of  #TAUTHOR_TAG and  #AUTHOR_TAG in tables 1 and 3, while the results on non - european languages are shown in table 2.', 'through experiments, our goal is to assess :', '1']","['present our results on european languages on the datasets of  #TAUTHOR_TAG and  #AUTHOR_TAG in tables 1 and 3, while the results on non - european languages are shown in table 2.', 'through experiments, our goal is to assess :', '1.']","['present our results on european languages on the datasets of  #TAUTHOR_TAG and  #AUTHOR_TAG in tables 1 and 3, while the results on non - european languages are shown in table 2.', 'through experiments, our goal is to assess :', '1']","['present our results on european languages on the datasets of  #TAUTHOR_TAG and  #AUTHOR_TAG in tables 1 and 3, while the results on non - european languages are shown in table 2.', 'through experiments, our goal is to assess :', '1. does the unsupervised mapping method based on our proposed adversarial autoencoder model improve over the best existing adversarial method of  #TAUTHOR_TAG in terms of mapping accuracy and convergence ( section 5. 1 )?', '2. how does our unsupervised mapping method compare with other unsupervised and supervised approaches ( section 5. 2 )? 3.', 'which components of our adversarial autoencoder model attribute to improvements ( section 5. 3 )']",1
['over  #TAUTHOR_TAG'],['significant gains over  #TAUTHOR_TAG'],"['over  #TAUTHOR_TAG for all translation', 'tasks in']","['cross - lingual mapping in a projected latent space and add more constraints to guide the unsupervised mapping in this space. we accomplish this by proposing a novel adversarial autoencoder framework  #AUTHOR_TAG, where adversarial mapping is done at the', '( latent ) code space as opposed to the original embedding space ( figure 1 ). this gives the model the flexibility to automatically induce the required geometric structures in its latent code space that could potentially', 'yield better mappings. søgaard et al. ( 2018 ) recently find that the isomorphic assumption made by most existing methods does not hold in general even for two closely related languages like english and german. in their words', '"" approaches based on this assumption have important limitations "". by mapping the latent vectors through', 'adversarial training, our approach therefore departs from the isomorphic assumption. in our adversarial training, not only the mapper but also the target encoder is trained to fool the discriminator. this forces the discriminator to', 'improve its discrimination skills, which in turn pushes the mapper to generate indistinguishable translation. to guide the mapping, we include two additional constraints. our first constraint enforces cycle consistency so that', 'code vectors after being translated from one language to another, and then translated back to their source space remain close to the', 'original vectors. the second constraint ensures reconstruction of the original input word embeddings', 'from the back - translated codes. this grounding', 'step forces the model to retain word semantics during the mapping process. we conduct a series of experiments with six different', 'language pairs ( in both directions ) comprising european, non - european, and low - resource languages from two different datasets. our results show that our model is more robust and yields significant gains over  #TAUTHOR_TAG for all translation', 'tasks in all evaluation measures. our method also gives better initial mapping compared to other existing methods  #AUTHOR_TAG b ). we also perform an', 'extensive ablation study to understand the contribution of different components of our model. the study reveals that cycle consistency contributes the most, while advers', '##arial training of the target encoder and post - cycle reconstruction also have significant effect. we have released our source code at https : / / ntunlps', '##g. github. io / project / unsup - word - translation / the remainder of this paper is organized as follows. after discussing related work in section 2, we present our unsupervised word', '']",6
,,,,3
"[' #TAUTHOR_TAG,  #AUTHOR_TAG b ), alvarez -  #AUTHOR_TAG,  #AUTHOR_TAG a ), and  #AUTHOR_TAG.', 'to evaluate how']","[' #TAUTHOR_TAG,  #AUTHOR_TAG b ), alvarez -  #AUTHOR_TAG,  #AUTHOR_TAG a ), and  #AUTHOR_TAG.', 'to evaluate how']","[' #TAUTHOR_TAG,  #AUTHOR_TAG b ), alvarez -  #AUTHOR_TAG,  #AUTHOR_TAG a ), and  #AUTHOR_TAG.', 'to evaluate how']","['compare our method with the unsupervised models of  #TAUTHOR_TAG,  #AUTHOR_TAG b ), alvarez -  #AUTHOR_TAG,  #AUTHOR_TAG a ), and  #AUTHOR_TAG.', 'to evaluate how our unsupervised method compares with methods that rely on a bilingual seed dictionary, we follow  #TAUTHOR_TAG, and compute a supervised baseline that uses the procrustes solution directly on the seed dictionary ( 5000 pairs ) to learn the mapping function, and then uses csls to do the nearest neighbor search.', 'we also compare with the supervised approaches of  #AUTHOR_TAG artetxe et al. (, 2018a, which to our knowledge are the state - of - the - art supervised systems.', 'for some of the  #TAUTHOR_TAG, results are reported from their papers, while for the rest we report results by running the publicly available codes on our machine.', 'for training our model on european languages, the weight for cycle consistency ( λ 1 ) in eq. 7 was always set to 5, and the weight for post - cycle reconstruction ( λ 2 ) was set to 1.', 'for non - european languages, we use different values of λ 1 and λ 2 for different language pairs.', '4 the dimension of the code vectors in our model was set to 350']",3
"['steps as  #TAUTHOR_TAG.', 'in the tables, we present the numbers']","['steps as  #TAUTHOR_TAG.', 'in the tables, we present the numbers']","['as  #TAUTHOR_TAG.', 'in the tables, we present the numbers']","['our approach follows the same steps as  #TAUTHOR_TAG.', 'in the tables, we present the numbers that they reported in their paper (  #AUTHOR_TAG ( paper ) ) as well as the results that we get by running their code on our machine (  #AUTHOR_TAG ( code ) ).', 'for a fair comparison with respect to the quality of the learned mappings ( or induced seed dictionary ), here we only consider the results of our approach that use the refinement procedure of  #TAUTHOR_TAG.', 'in table 1, we see that our adversarial autoencoder +  #TAUTHOR_TAG in all the six translation tasks involving european language pairs, yielding gains in the range 0. 3 - 1. 3 %.', 'our method is also superior to  #TAUTHOR_TAG for the non - european and low - resource language pairs in table 2.', '']",3
"['steps as  #TAUTHOR_TAG.', 'in the tables, we present the numbers']","['steps as  #TAUTHOR_TAG.', 'in the tables, we present the numbers']","['as  #TAUTHOR_TAG.', 'in the tables, we present the numbers']","['our approach follows the same steps as  #TAUTHOR_TAG.', 'in the tables, we present the numbers that they reported in their paper (  #AUTHOR_TAG ( paper ) ) as well as the results that we get by running their code on our machine (  #AUTHOR_TAG ( code ) ).', 'for a fair comparison with respect to the quality of the learned mappings ( or induced seed dictionary ), here we only consider the results of our approach that use the refinement procedure of  #TAUTHOR_TAG.', 'in table 1, we see that our adversarial autoencoder +  #TAUTHOR_TAG in all the six translation tasks involving european language pairs, yielding gains in the range 0. 3 - 1. 3 %.', 'our method is also superior to  #TAUTHOR_TAG for the non - european and low - resource language pairs in table 2.', '']",3
['of  #TAUTHOR_TAG. to note that in'],"['adversarial autoencoder model on the dataset of  #TAUTHOR_TAG. to note that in contrast', '']","['of  #TAUTHOR_TAG. to note that in contrast', 'to  #TAUTHOR_TAG, our mapping is']","['', 'presents the results of  #AUTHOR_TAG that uses adversarial training to map the word embeddings. the next row shows the', 'results of our full model. the subsequent rows incrementally detach one component from our model. for example, - enc. adv denotes the variant of our model where the target encoder is not trained on', 'the adversarial loss ( θ e x in eq. 4 ) ; - - recon excludes the post - cycle reconstruction loss from - enc. adv, and - - - cycle excludes the cycle consistency from - - recon. thus, -', '- - cycle is a variant of our model that uses only adversarial loss to learn the mapping. however, it is important table', '5 : ablation study of our adversarial autoencoder model on the dataset of  #TAUTHOR_TAG. to note that in contrast', 'to  #TAUTHOR_TAG, our mapping is performed at the code space. as we compare our full model with', 'the model of  #TAUTHOR_TAG in the without fine - tuning setting', '']",3
"['4, 11, 23, 42, 44,  #TAUTHOR_TAG, 5, 14, 45, 2 ].', 'in']","['commonly used evaluation criterion [ 4, 11, 23, 42, 44,  #TAUTHOR_TAG, 5, 14, 45, 2 ].', 'in']","['4, 11, 23, 42, 44,  #TAUTHOR_TAG, 5, 14, 45, 2 ].', 'in the multiple - choice setting, where only one answer is']","['its introduction, the task of visual question answering ( vqa ) [ 4 ] has received considerable attention in the vision and language community.', 'the task is straightforward : given an image and a question in natural language, models are asked to output the correct answer.', 'this is usually treated as a classification problem, where answers are categories that are inferred using features from imagequestion pairs.', 'traditionally, two main versions of the tasks * shailza and sandro share the first authorship.', 'have been proposed : one, multiple - choice, requires models to pick up the correct answer among a limited set of options ; the other, open - ended, challenges systems to guess the correct answer from the whole vocabulary.', 'several metrics have been proposed recently for evaluating vqa systems ( see section 2 ), but accuracy is still the most commonly used evaluation criterion [ 4, 11, 23, 42, 44,  #TAUTHOR_TAG, 5, 14, 45, 2 ].', '']",0
"['', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['which computes the similarity between two words in terms of their longest common subsequence in the taxonomy tree. in practice, the predicted answer is considered as correct when its similarity with the ground truth exceeds a threshold, which in [ 24 ] is set to either 0. 9 ( strict ) or 0. 0 ( tolerant ). this metric has been extended by [ 25 ] to account for settings where more than one ground - truth answer is available. two versions were proposed : in one, wups - acm, the overall score comes from the average of all pairwise similarities and thus considers inter - annotator agreement ; in the other, wups - mcm, the pair with the highest similarity is taken as representative of the pattern. as observed by [ 19 ], the measure of similarity embedded in wups has some shortcomings.', 'in particular, it is shown to produce high scores even for answers which are semantically very different, leading to', 'significantly higher accuracies in both [ 24', '] and [ 30 ]. moreover, it only works with rigid semantic concepts, making it not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for vqa by [ 13 ]. based on the characteristics of', '']",0
"['', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['which computes the similarity between two words in terms of their longest common subsequence in the taxonomy tree. in practice, the predicted answer is considered as correct when its similarity with the ground truth exceeds a threshold, which in [ 24 ] is set to either 0. 9 ( strict ) or 0. 0 ( tolerant ). this metric has been extended by [ 25 ] to account for settings where more than one ground - truth answer is available. two versions were proposed : in one, wups - acm, the overall score comes from the average of all pairwise similarities and thus considers inter - annotator agreement ; in the other, wups - mcm, the pair with the highest similarity is taken as representative of the pattern. as observed by [ 19 ], the measure of similarity embedded in wups has some shortcomings.', 'in particular, it is shown to produce high scores even for answers which are semantically very different, leading to', 'significantly higher accuracies in both [ 24', '] and [ 30 ]. moreover, it only works with rigid semantic concepts, making it not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for vqa by [ 13 ]. based on the characteristics of', '']",0
"['', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for']","['which computes the similarity between two words in terms of their longest common subsequence in the taxonomy tree. in practice, the predicted answer is considered as correct when its similarity with the ground truth exceeds a threshold, which in [ 24 ] is set to either 0. 9 ( strict ) or 0. 0 ( tolerant ). this metric has been extended by [ 25 ] to account for settings where more than one ground - truth answer is available. two versions were proposed : in one, wups - acm, the overall score comes from the average of all pairwise similarities and thus considers inter - annotator agreement ; in the other, wups - mcm, the pair with the highest similarity is taken as representative of the pattern. as observed by [ 19 ], the measure of similarity embedded in wups has some shortcomings.', 'in particular, it is shown to produce high scores even for answers which are semantically very different, leading to', 'significantly higher accuracies in both [ 24', '] and [ 30 ]. moreover, it only works with rigid semantic concepts, making it not suitable for phrasal or sentence answers that can be found in [', '4,  #TAUTHOR_TAG, 16, 47, 14 ]. visual turing test has been proposed as a human - based evaluation metric for vqa by [ 13 ]. based on the characteristics of', '']",0
"[' #TAUTHOR_TAG ], and only']","['- abstract [  #TAUTHOR_TAG ], and only 3 % in viz', '##wi']","[' #TAUTHOR_TAG ], and only']","['', 's is 0. 33. the method used', 'for computing s is shown in ( 2 ) : where the formula represents the standard way for computing wd, u, v are two different', 'probability distributions, and γ ( u, v ) is the set of ( probability ) distributions. the value of s is further normalized to range from 0 to', '1. introducing such component allows us to take into account the subjectivity of a sample', '( and a dataset ). this is crucial since, as shown in figure 3, in current', 'datasets the proportion of samples with a perfect', 'inter - annotator agreement ( i. e., 1 unique answer ) is relatively low : 35 % in vqa 1. 0 [ 4 ], 33 % in vqa 2. 0 [ 14 ], 43 % in vqa - abstract [  #TAUTHOR_TAG ], and only 3 % in viz', '##wiz [ 16 ]. moreover, we compute this score independently from the predictions of the models, thus providing a self - standing measure for the analysis of any vqa dataset. as clearly depicted in figure 3, subjectivity is indeed a property of the datasets : in vizwiz, only 30', '% of samples display 3 or less unique answers, whereas this percentage exceeds 70 % in the other datasets. the', 'motivation behind proposing this component is loosely similar to [ 15 ], who tackle the task of predicting the degree of agreement between annotators, and very close to [', '43 ], who model subjectivity of samples in terms of the entropy', 'of the response pattern ( ranging from 0 to 3. 32 ). compared to [ 43 ], we', 'believe ours to be an essentially equivalent measure, though simpler and more intuitive. finally, subjectivity is indirectly taken into account in wups - acm, where the score', 'is given by the average of the pairwise distances between the elements. however, this measure mixes quantitative ( frequency ) and qualitative ( semantic similarity ) information, while s specifically focuses on the former']",0
"['- abstract [  #TAUTHOR_TAG ], and vizwiz']","['2. 0 [ 14 ], vqa - abstract [  #TAUTHOR_TAG ], and vizwiz [ 16 ].', 'to']","['- abstract [  #TAUTHOR_TAG ], and vizwiz [ 16 ].', 'to']","['tested the validity of our metric by experimenting with four vqa datasets : vqa 1. 0 [ 4 ], vqa 2. 0 [ 14 ], vqa - abstract [  #TAUTHOR_TAG ], and vizwiz [ 16 ].', 'to enable a fair comparison across the datasets, for each dataset we followed the same pipeline : the standard vqa model used in [  #TAUTHOR_TAG ] was trained on the training split and tested on the validation split.', 'model predictions were evaluated by means of three metrics : vqa3 + [ 4 ] ( using the evaluation tools ), wups [ 25 ], and our masses.', 'wups was tested in both its consensus versions, i. e. acm and mcm with a threshold of 0. 9.', 'as for masses, we computed its overall score as well as the scores provided by each of its components.', ""the impact of'tuning'semantic similarity is evaluated by exploring two thresholds : a strict 0. 9 and a more tolerant 0. 7""]",5
"['- abstract [  #TAUTHOR_TAG ], and vizwiz']","['2. 0 [ 14 ], vqa - abstract [  #TAUTHOR_TAG ], and vizwiz [ 16 ].', 'to']","['- abstract [  #TAUTHOR_TAG ], and vizwiz [ 16 ].', 'to']","['tested the validity of our metric by experimenting with four vqa datasets : vqa 1. 0 [ 4 ], vqa 2. 0 [ 14 ], vqa - abstract [  #TAUTHOR_TAG ], and vizwiz [ 16 ].', 'to enable a fair comparison across the datasets, for each dataset we followed the same pipeline : the standard vqa model used in [  #TAUTHOR_TAG ] was trained on the training split and tested on the validation split.', 'model predictions were evaluated by means of three metrics : vqa3 + [ 4 ] ( using the evaluation tools ), wups [ 25 ], and our masses.', 'wups was tested in both its consensus versions, i. e. acm and mcm with a threshold of 0. 9.', 'as for masses, we computed its overall score as well as the scores provided by each of its components.', ""the impact of'tuning'semantic similarity is evaluated by exploring two thresholds : a strict 0. 9 and a more tolerant 0. 7""]",5
['representations with self - supervised objectives  #TAUTHOR_TAG have further pushed'],['representations with self - supervised objectives  #TAUTHOR_TAG have further pushed'],"['language representations with self - supervised objectives  #TAUTHOR_TAG have further pushed forward the state - of - the - art on many english tasks.', 'while these sorts of deep models']","['to notable advances in deep learning and representation learning, important progress has been achieved on text classification, reading comprehension, and other nlp tasks.', 'recently, pretrained language representations with self - supervised objectives  #TAUTHOR_TAG have further pushed forward the state - of - the - art on many english tasks.', '']",6
"['- english validation set.', 'for the encoder, we invoke the multilingual bert model  #TAUTHOR_TAG, which supports 104 languages 1.', '']","['based on each non - english validation set.', 'for the encoder, we invoke the multilingual bert model  #TAUTHOR_TAG, which supports 104 languages 1.', '']","['based on each non - english validation set.', 'for the encoder, we invoke the multilingual bert model  #TAUTHOR_TAG, which supports 104 languages 1.', 'it relies on a shared 110k wordpiece vocabulary across all languages and yields sentence representations in a common multilingual space.', 'most model hyperparameters are']","['', 'model details.', 'we tune the hyper - parameters for our neural network architecture based on each non - english validation set.', 'for the encoder, we invoke the multilingual bert model  #TAUTHOR_TAG, which supports 104 languages 1.', 'it relies on a shared 110k wordpiece vocabulary across all languages and yields sentence representations in a common multilingual space.', 'most model hyperparameters are the same as in pretraining, with the exception of the batch size, max.', 'sequence length, and number of training epochs.', 'the batch size, max.', 'sequence length and number of training epochs used for the mldoc task are 128, 32, and 4, respectively, while they are 128, 96, and 3 for sentiment classification.', 'another hyper - parameter involved in self - learning is k t, which is 40 for mldoc and 100 for sentiment classification.', 'we rely on early stopping as a termination criterion']",5
"['evaluation metric first described in  #TAUTHOR_TAG.', '']","['evaluation metric first described in  #TAUTHOR_TAG.', '']","['##er is a machine translation evaluation metric first described in  #TAUTHOR_TAG.', 'it is designed to have the advantages of bleu  #AUTHOR_TAG, such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment.', 'according to the paper just cited : "" it can be thought of as a weighted combination of']","['##er is a machine translation evaluation metric first described in  #TAUTHOR_TAG.', 'it is designed to have the advantages of bleu  #AUTHOR_TAG, such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment.', 'according to the paper just cited : "" it can be thought of as a weighted combination of dozens of computationally cheap features based on word surface forms for evaluating mt quality "".', 'many recently defined machine translation metrics seek to exploit deeper sources of knowledge than are available to bleu, such as external lexical and syntactic resources.', 'unlike these and like bleu, amber relies entirely on matching surface forms in tokens in the hypothesis and reference, thus sacrificing depth of knowledge for simplicity and speed.', 'in this paper, we describe two improvements to amber.', 'the first is a new ordering penalty called "" v "" developed in  #AUTHOR_TAG.', ""the second remedies a weakness in the 2011 version of amber by carrying out automatic, rather than manual, tuning of this metric's free parameters ; we now use the simplex algorithm to do the tuning""]",0
"['weighted product of several different penalties ( equation 3 ).', 'our original amber paper  #TAUTHOR_TAG describes the ten penalties used at that time ; two of these penalties,']","['weighted product of several different penalties ( equation 3 ).', 'our original amber paper  #TAUTHOR_TAG describes the ten penalties used at that time ; two of these penalties,']","[').', 'the penalty part is a weighted product of several different penalties ( equation 3 ).', ""our original amber paper  #TAUTHOR_TAG describes the ten penalties used at that time ; two of these penalties, the normalized spearman's correlation penalty and the normalized kendall's correlation penalty, model word reordering."", 'in addition to the more complex score and penalty factors, amber differs from ble']","['', 'the penalty part is a weighted product of several different penalties ( equation 3 ).', ""our original amber paper  #TAUTHOR_TAG describes the ten penalties used at that time ; two of these penalties, the normalized spearman's correlation penalty and the normalized kendall's correlation penalty, model word reordering."", 'in addition to the more complex score and penalty factors, amber differs from bleu in two other ways :', '• not only fixed n - grams, but three different kinds of flexible n - grams, are used in computing scores and penalties']",0
"[', stemming, word splitting, etc.', '8 types were tried in  #TAUTHOR_TAG.', 'when using more than one type, the final score is']","['amber score can be computed with different types of text preprocessing, i. e. different combinations of several text preprocessing techniques : lowercasing, tokenization, stemming, word splitting, etc.', '8 types were tried in  #TAUTHOR_TAG.', 'when using more than one type, the final score is']","['amber score can be computed with different types of text preprocessing, i. e. different combinations of several text preprocessing techniques : lowercasing, tokenization, stemming, word splitting, etc.', '8 types were tried in  #TAUTHOR_TAG.', 'when using more than one type, the final score is computed as an average']","['amber score can be computed with different types of text preprocessing, i. e. different combinations of several text preprocessing techniques : lowercasing, tokenization, stemming, word splitting, etc.', '8 types were tried in  #TAUTHOR_TAG.', 'when using more than one type, the final score is computed as an average over runs, one run per type.', 'in the experiments reported below, we averaged over two types of preprocessing']",0
"[' #TAUTHOR_TAG, we manually set the 17 free parameters of amber ( see section 3. 2 of that paper ).', 'in the experiments reported below, we tuned the 18 free parameters - the original 17 plus the ordering metric v described in the previous']","[' #TAUTHOR_TAG, we manually set the 17 free parameters of amber ( see section 3. 2 of that paper ).', 'in the experiments reported below, we tuned the 18 free parameters - the original 17 plus the ordering metric v described in the previous']","[' #TAUTHOR_TAG, we manually set the 17 free parameters of amber ( see section 3. 2 of that paper ).', 'in the experiments reported below, we tuned the 18 free parameters - the original 17 plus the ordering metric v described in the previous section - automatically, using the downhill simplex method of  #AUTHOR_TAG as described in  #AUTHOR_TAG.', 'this is a multidimensional optimization technique inspired by geometrical considerations that has shown good performance in a variety of applications']","[' #TAUTHOR_TAG, we manually set the 17 free parameters of amber ( see section 3. 2 of that paper ).', 'in the experiments reported below, we tuned the 18 free parameters - the original 17 plus the ordering metric v described in the previous section - automatically, using the downhill simplex method of  #AUTHOR_TAG as described in  #AUTHOR_TAG.', 'this is a multidimensional optimization technique inspired by geometrical considerations that has shown good performance in a variety of applications']",4
"['as described in  #TAUTHOR_TAG.', 'specifically,']","['follows, "" amber1 "" will denote a variant of amber as described in  #TAUTHOR_TAG.', 'specifically,']","['as described in  #TAUTHOR_TAG.', 'specifically, it is']","['experiments are carried out on wmt metric task data : specifically, the wmt 2008, wmt 2009, wmt 2010, wmt 2011 all - to - english, and english - to - all submissions.', 'the languages "" all "" ( "" xx "" in table 1 ) we used 2008 and 2011 data as dev sets, 2009 and 2010 data as test sets.', ""spearman's rank correlation coefficient ρ was employed to measure correlation of the metric with system - level human judgments of translation."", 'the human judgment score was based on the "" rank "" only, i. e., how often the translations of the system were rated as better than those from other systems ( callison  #AUTHOR_TAG.', 'thus, bleu and the new version of amber were evaluated on how well their rankings correlated with the human ones.', ""for the segment level, we followed ( callison -  #AUTHOR_TAG in using kendall's rank correlation coefficient τ."", 'in what follows, "" amber1 "" will denote a variant of amber as described in  #TAUTHOR_TAG.', 'specifically, it is the variant amber ( 1, 4 ) - that is, the variant in which results are averaged over two runs with the following preprocessing :', '']",7
"[', a metric described in  #TAUTHOR_TAG.', 'in our experiments, the new version of amb']","['two changes to amber, a metric described in  #TAUTHOR_TAG.', 'in our experiments, the new version of amber was shown to be an improvement on the original version in']","[', a metric described in  #TAUTHOR_TAG.', 'in our experiments, the new version of amb']","['have made two changes to amber, a metric described in  #TAUTHOR_TAG.', 'in our experiments, the new version of amber was shown to be an improvement on the original version in terms of correlation with human judgment.', '']",6
['of  #TAUTHOR_TAG and explore how such models would far'],['of  #TAUTHOR_TAG and explore how such models would fare in this'],"[', we follow the approach of  #TAUTHOR_TAG and explore how such models would fare in this task']","['models  #AUTHOR_TAG trained on large - scale language modeling datasets have recently proved to be a very effective means of representing the meaning of a sentence, being put to effective use in fine - tuning both sentence - level tasks, such as the glue benchmark  #AUTHOR_TAG and token - level tasks, such as named entity recognition  #AUTHOR_TAG.', 'recent work has also found them to produce linguistically valid representations  #AUTHOR_TAG, as well as to display excellent performance across multiple downstream nlp tasks ( e. g., houlsby et al. 2019 ).', 'in this work, we explore how such models perform in the task of grammatical error correction ( gec ).', 'while there is a substantial amount of work on statistical  #AUTHOR_TAG junczys -  #AUTHOR_TAG and neural  #AUTHOR_TAG machine translation methods for gec, we follow the approach of  #TAUTHOR_TAG and explore how such models would fare in this task when treated as simple language models.', 'more specifically,  #TAUTHOR_TAG train a 5 - gram language model on the one billion word benchmark  #AUTHOR_TAG dataset and find that it produces competitive baseline results without any supervised training.', 'in our work, we extend  #TAUTHOR_TAG by substituting the n - gram model for several publicly available implementations of state - of - the - art transformer language models trained on large linguistic corpora and assess their performance on gec without any supervised training.', 'we find that transformer language models produce results on par with supervised approaches providing a solid baseline system.', 'this finding is of particular importance in gec, where data collection and annotation requires substantial manual effort']",5
"['not generate novel sequences, we follow  #TAUTHOR_TAG and use simple heuristics to generate a confusion set of sentences that our language models score.', 'for prep']","['not generate novel sequences, we follow  #TAUTHOR_TAG and use simple heuristics to generate a confusion set of sentences that our language models score.', 'for prepositions']","['our systems do not generate novel sequences, we follow  #TAUTHOR_TAG and use simple heuristics to generate a confusion set of sentences that our language models score.', 'for prep']","['our systems do not generate novel sequences, we follow  #TAUTHOR_TAG and use simple heuristics to generate a confusion set of sentences that our language models score.', 'for prepositions and determiners, the confusion set includes the set of all prepositions and determiners plus an empty string o to remove unnecessary additions.', 'for morphological errors ( e. g., past tense or pluralization ), we use the automatically generated inflection database ( agid ) which contains different morphological forms for each word.', '2 however, we notice that due to the automatic generation, agid contains errors that might propagate into our scoring.', ""the problem with introducing new errors and non - words is that they would be interpreted as unknown words ( henceforth [UNK] s ) from the model's perspective."", 'an unknown word in some context might give higher probabilities to an erroneous sentence and cause the model not to select the correct alternative.', 'to remedy this issue, we generate a vocabulary from all the training sets and make sure that any proposed words which do not exist in the vocabulary are replaced by [UNK] s.', 'note that there is no reason to re - use the vocabulary of the training sets as any large english wordlist would achieve a similar effect.', 'finally, for spelling mistakes, we, again, follow  #TAUTHOR_TAG and use cyhunspell 3 to generate alternatives for non - words']",5
"['not generate novel sequences, we follow  #TAUTHOR_TAG and use simple heuristics to generate a confusion set of sentences that our language models score.', 'for prep']","['not generate novel sequences, we follow  #TAUTHOR_TAG and use simple heuristics to generate a confusion set of sentences that our language models score.', 'for prepositions']","['our systems do not generate novel sequences, we follow  #TAUTHOR_TAG and use simple heuristics to generate a confusion set of sentences that our language models score.', 'for prep']","['our systems do not generate novel sequences, we follow  #TAUTHOR_TAG and use simple heuristics to generate a confusion set of sentences that our language models score.', 'for prepositions and determiners, the confusion set includes the set of all prepositions and determiners plus an empty string o to remove unnecessary additions.', 'for morphological errors ( e. g., past tense or pluralization ), we use the automatically generated inflection database ( agid ) which contains different morphological forms for each word.', '2 however, we notice that due to the automatic generation, agid contains errors that might propagate into our scoring.', ""the problem with introducing new errors and non - words is that they would be interpreted as unknown words ( henceforth [UNK] s ) from the model's perspective."", 'an unknown word in some context might give higher probabilities to an erroneous sentence and cause the model not to select the correct alternative.', 'to remedy this issue, we generate a vocabulary from all the training sets and make sure that any proposed words which do not exist in the vocabulary are replaced by [UNK] s.', 'note that there is no reason to re - use the vocabulary of the training sets as any large english wordlist would achieve a similar effect.', 'finally, for spelling mistakes, we, again, follow  #TAUTHOR_TAG and use cyhunspell 3 to generate alternatives for non - words']",5
['similar approaches  #TAUTHOR_TAG and'],"['similar approaches  #TAUTHOR_TAG and state - of - the - art on grammatical error correction.', '']",['similar approaches  #TAUTHOR_TAG and'],"['', 'concretely, let p ( s c ) be the probability of the candidate sentence and p ( s o ) the probability of the table 2 : results of our transformer - language model approach against similar approaches  #TAUTHOR_TAG and state - of - the - art on grammatical error correction.', 'for each of the datasets, we use the corresponding test set, and we do not train our models on the corpora.', 'as bert, we report the best performing bert model ( 12 layers, retaining uppercase characters ).', 'in the top part of each dataset, we report the scores of supervised methods and in the bottom the unsupervised ones.', '† denotes this system won the shared task competition.', 'original sentence, then we accept the candidate if p ( s c ) > p ( s o ) + τ, where τ is some threshold parameter which we fit on each development set.', 'note that, practically, this parameter controls the trade - off between precision and recall as higher τ values would mean that there is less chance of changing the original sentence ( i. e., higher precision ) and vice versa.', 'we explore different values for τ ∈ { 0, 2, 4, 6, 8 } by, as above, fitting them on the corresponding development set.', '']",5
['use the development sets used by  #TAUTHOR_TAG'],['use the development sets used by  #TAUTHOR_TAG'],"['use of the training sets commonly used with these datasets.', 'however, we use the development sets used by  #TAUTHOR_TAG']","['evaluate our method and report results on two standard publicly available datasets.', 'our evalua - 4 note that the probability of each sentence is in log space.', 'tion is aimed to stay as true to  #AUTHOR_TAG  #AUTHOR_TAG.', 'unfortunately, due to licensing issues, we were unable to obtain permission to use the jfleg  #AUTHOR_TAG corpus for evaluation.', 'note that in our method, we do not make use of the training sets commonly used with these datasets.', 'however, we use the development sets used by  #TAUTHOR_TAG to tune the hyperparameter τ.', 'the number of sentences and tokens for the datasets we used can be found in table 1.', 'similar to  #TAUTHOR_TAG, we report results on three metrics.', 'we use the maxmatch ( m 2 ) precision, recall and f 0. 5  #AUTHOR_TAG b ) and errant precision, recall and f 0. 5  #AUTHOR_TAG.', 'source it will start by a speech from the director of the conference, followed by a meal']",5
"[' #TAUTHOR_TAG.', 'table 3 shows some qualitative examples on']","[' #TAUTHOR_TAG.', 'table 3 shows some qualitative examples on']","[' #TAUTHOR_TAG.', 'table 3 shows some qualitative examples on']","['', 'gpt - 2 they all know where the conference is and when.', 'table 3 : source sentences along with the gold edits and the proposed candidates from each of our models.', 'table 2 presents the results of our method comparing them against recent state - of - the - art supervised models and the simple n - gram language model used by  #TAUTHOR_TAG.', 'table 3 shows some qualitative examples on how each model corrects two sentences pulled from the fce along with the gold annotations.', 'the reported results come from the best performing hyperparameter τ on each dataset.', 'for bert, we also explored different sizes ( 12 vs. 24 layers ) and whether retaining uppercase characters helps in performance.', 'the best performing τ values were τ = 4 for conll14 for all models ; for the fce dataset : bert τ = 4, gpt τ = 8, and gpt - 2 τ = 6.', ""the best'version,'of bert was the large, cased ( i. e., retaining the lower - / uppercase distinction )""]",5
"['', 'however,  #TAUTHOR_TAG recently revived the idea, achieving competitive']","['', 'however,  #TAUTHOR_TAG recently revived the idea, achieving competitive']","['##s -  #AUTHOR_TAG.', 'however,  #TAUTHOR_TAG recently revived the idea, achieving competitive performance with']","['idea of using language models is quite fundamental to the task of grammatical error correction, which has fed a substantial body of work over the years.', 'more recently, with the availability of web - scale data powering the advances in language modeling, among most of the other advances in nlp, a plethora of language - modeling based approaches have been proposed for the gec task.', ' #AUTHOR_TAG ;  #AUTHOR_TAG and  #AUTHOR_TAG were some of the early works to successfully leverage language models trained on large amounts of web - scale data into a gec system, reinforcing the idea that simple models and a lot of data trump more elaborate models based on annotated data  #AUTHOR_TAG.', 'since then, multiple works based on languagemodels have been proposed for the gec task  #AUTHOR_TAG a ), either relying entirely on lms or using them for fine - tuning their systems.', 'many of the topranked systems in the conll - 2013 gec shared tasks  #AUTHOR_TAG were either based on language models or had them as integral parts of their systems  #AUTHOR_TAG junczys -  #AUTHOR_TAG.', 'lm - only approaches though took a backseat and were only sporadically used after the shared tasks, as neural machine translationbased approaches took over, but lms remained an integral part of the gec systems ( junczys  #AUTHOR_TAG junczys -  #AUTHOR_TAG.', 'however,  #TAUTHOR_TAG recently revived the idea, achieving competitive performance with the state - ofthe - art, demonstrating the effectiveness of the approaches to the task without using any annotated data for training']",0
['use the development sets used by  #TAUTHOR_TAG'],['use the development sets used by  #TAUTHOR_TAG'],"['use of the training sets commonly used with these datasets.', 'however, we use the development sets used by  #TAUTHOR_TAG']","['evaluate our method and report results on two standard publicly available datasets.', 'our evalua - 4 note that the probability of each sentence is in log space.', 'tion is aimed to stay as true to  #AUTHOR_TAG  #AUTHOR_TAG.', 'unfortunately, due to licensing issues, we were unable to obtain permission to use the jfleg  #AUTHOR_TAG corpus for evaluation.', 'note that in our method, we do not make use of the training sets commonly used with these datasets.', 'however, we use the development sets used by  #TAUTHOR_TAG to tune the hyperparameter τ.', 'the number of sentences and tokens for the datasets we used can be found in table 1.', 'similar to  #TAUTHOR_TAG, we report results on three metrics.', 'we use the maxmatch ( m 2 ) precision, recall and f 0. 5  #AUTHOR_TAG b ) and errant precision, recall and f 0. 5  #AUTHOR_TAG.', 'source it will start by a speech from the director of the conference, followed by a meal']",4
['use the development sets used by  #TAUTHOR_TAG'],['use the development sets used by  #TAUTHOR_TAG'],"['use of the training sets commonly used with these datasets.', 'however, we use the development sets used by  #TAUTHOR_TAG']","['evaluate our method and report results on two standard publicly available datasets.', 'our evalua - 4 note that the probability of each sentence is in log space.', 'tion is aimed to stay as true to  #AUTHOR_TAG  #AUTHOR_TAG.', 'unfortunately, due to licensing issues, we were unable to obtain permission to use the jfleg  #AUTHOR_TAG corpus for evaluation.', 'note that in our method, we do not make use of the training sets commonly used with these datasets.', 'however, we use the development sets used by  #TAUTHOR_TAG to tune the hyperparameter τ.', 'the number of sentences and tokens for the datasets we used can be found in table 1.', 'similar to  #TAUTHOR_TAG, we report results on three metrics.', 'we use the maxmatch ( m 2 ) precision, recall and f 0. 5  #AUTHOR_TAG b ) and errant precision, recall and f 0. 5  #AUTHOR_TAG.', 'source it will start by a speech from the director of the conference, followed by a meal']",3
"['unsupervised approaches of  #TAUTHOR_TAG, respectively']","['unsupervised approaches of  #TAUTHOR_TAG, respectively']","['unsupervised approaches of  #TAUTHOR_TAG, respectively']","['- noun idiomatic combinations ( vnics ) are idioms consisting of a verb with a noun in its direct object position.', 'usages of these expressions can be ambiguous between an idiomatic usage and a literal combination.', 'in this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of vnics.', 'our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of  #TAUTHOR_TAG, respectively']",4
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",4
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",4
"[' #TAUTHOR_TAG, the supervised approach was evaluated using a']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', '']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and the classifier is trained on the remaining instances.', 'the trained model is then used to classify the held out instance.', 'this is table 1 for a variety of settings of window size and number of dimensions for the word embeddings.', 'the results reveal the general trend that smaller window sizes, and more dimensions, tend to give higher accuracy, although the overall amount of variation is relatively small.', '']",4
"[' #TAUTHOR_TAG, the supervised approach was evaluated using a']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', '']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and the classifier is trained on the remaining instances.', 'the trained model is then used to classify the held out instance.', 'this is table 1 for a variety of settings of window size and number of dimensions for the word embeddings.', 'the results reveal the general trend that smaller window sizes, and more dimensions, tend to give higher accuracy, although the overall amount of variation is relatively small.', '']",4
"['', 'moreover, the unsupervised cform method of  #TAUTHOR_TAG gives substantially higher']","['leave - one - token - out setup.', 'moreover, the unsupervised cform method of  #TAUTHOR_TAG gives substantially higher']","['the previously - considered leave - one - token - out setup.', 'moreover, the unsupervised cform method of  #TAUTHOR_TAG gives substantially higher accuracies than this supervised approach.', 'the limited ability of this model']","['', 'moreover, the unsupervised cform method of  #TAUTHOR_TAG gives substantially higher accuracies than this supervised approach.', '']",4
"["" #TAUTHOR_TAG.'s cform approach""]","["" #TAUTHOR_TAG.'s cform approach""]","["" #TAUTHOR_TAG.'s cform approach""]","['k - means clustering for the unsupervised approach is repeated 100 times with randomlyselected initial centroids, for several values of k. the average accuracy and standard deviation of the unsupervised approach over these 100 runs are shown in the left panel of table 2.', ""for k = 4 and 5 on test, this approach surpasses the unsupervised cform method of  #TAUTHOR_TAG.'s cform approach for any of the val - ues of k considered."", 'analyzing the results on individual expressions indicates that the unsupervised approach gives especially low accuracy for hit roof - which is in dev - as compared to the cform method of  #TAUTHOR_TAG, which could contribute to the overall lower accuracy of the unsupervised approach on this dataset.', 'we now consider the upperbound of an unsupervised approach that selects a single label for each cluster of usages.', 'in the right panel of table 2 we show results for an oracle approach that always selects the best label for each cluster.', 'in this case, as the number of clusters increases, so too will the accuracy.', '9 nevertheless, these results show that, even for relatively small values of k, there is scope for improving the proposed unsupervised method through improved methods for selecting the label for each cluster, and that the performance of such a method could potentially come close to that of the supervised approach.', ""a word's predominant sense is known to be a powerful baseline in word - sense disambiguation, and prior work has addressed automatically identifying predominant word senses ( mc  #AUTHOR_TAG."", 'the findings here suggest that methods for determining whether a set of usages of a vnic are predominantly literal or idiomatic could be leveraged to give further improvements in unsupervised vnic identification']",4
"["" #TAUTHOR_TAG.'s cform approach""]","["" #TAUTHOR_TAG.'s cform approach""]","["" #TAUTHOR_TAG.'s cform approach""]","['k - means clustering for the unsupervised approach is repeated 100 times with randomlyselected initial centroids, for several values of k. the average accuracy and standard deviation of the unsupervised approach over these 100 runs are shown in the left panel of table 2.', ""for k = 4 and 5 on test, this approach surpasses the unsupervised cform method of  #TAUTHOR_TAG.'s cform approach for any of the val - ues of k considered."", 'analyzing the results on individual expressions indicates that the unsupervised approach gives especially low accuracy for hit roof - which is in dev - as compared to the cform method of  #TAUTHOR_TAG, which could contribute to the overall lower accuracy of the unsupervised approach on this dataset.', 'we now consider the upperbound of an unsupervised approach that selects a single label for each cluster of usages.', 'in the right panel of table 2 we show results for an oracle approach that always selects the best label for each cluster.', 'in this case, as the number of clusters increases, so too will the accuracy.', '9 nevertheless, these results show that, even for relatively small values of k, there is scope for improving the proposed unsupervised method through improved methods for selecting the label for each cluster, and that the performance of such a method could potentially come close to that of the supervised approach.', ""a word's predominant sense is known to be a powerful baseline in word - sense disambiguation, and prior work has addressed automatically identifying predominant word senses ( mc  #AUTHOR_TAG."", 'the findings here suggest that methods for determining whether a set of usages of a vnic are predominantly literal or idiomatic could be leveraged to give further improvements in unsupervised vnic identification']",4
"['unsupervised cform approach, of  #TAUTHOR_TAG, respectively.', 'in future']","['unsupervised cform approach, of  #TAUTHOR_TAG, respectively.', 'in future']","['unsupervised cform approach, of  #TAUTHOR_TAG, respectively.', 'in future work we intend to consider methods']","['this paper we proposed supervised and unsupervised approaches, based on word embeddings, to identifying token instances of vnics that performed better than the supervised approach, and unsupervised cform approach, of  #TAUTHOR_TAG, respectively.', 'in future work we intend to consider methods for determining the predominant "" sense "" ( i. e., idiomatic or literal ) of a set of usages of a vnic, in an effort to further improve unsupervised vnic identification']",4
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",0
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",0
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",0
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",0
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",0
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",0
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",0
"[' #TAUTHOR_TAG, the supervised approach was evaluated using a']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', '']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and the classifier is trained on the remaining instances.', 'the trained model is then used to classify the held out instance.', 'this is table 1 for a variety of settings of window size and number of dimensions for the word embeddings.', 'the results reveal the general trend that smaller window sizes, and more dimensions, tend to give higher accuracy, although the overall amount of variation is relatively small.', '']",0
"['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['.,  #TAUTHOR_TAG.', 'in this study we focus on english']","['mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a']","['such as machine translation that must be able to distinguish mwes from literal combinations in context. some recent work has focused on token - level identification of a wide range of types of mwes and other multiword units ( e. g.,  #AUTHOR_TAG. many studies, however, have taken a word sense disambiguation - inspired approach to mwe identification ( e. g.', ',  #AUTHOR_TAG, treating literal combinations and mwes as different word senses,', 'and have exploited linguistic knowledge of mwes ( e. g.,  #TAUTHOR_TAG.', 'in this study we focus on english verb - noun idiomatic combinations ( vnics', '). vnics are formed from a verb with a noun in its direct object position.', 'they are a common and productive type of english idiom, and occur cross - lingually  #TAUTHOR_TAG. vn', '##ics tend to be relatively lexico - syntactically fixed, e. g., whereas hit the', 'roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages.  #TAUTHOR_TAG exploit this property in their unsupervised approach, referred to as cform.  #TAUTHOR_TAG define lexico', '']",5
"['by  #TAUTHOR_TAG.', 'the feature vectors are then used']","['by  #TAUTHOR_TAG.', 'the feature vectors are then used']","['by  #TAUTHOR_TAG.', 'the feature vectors are then used']","['', 'after computing c verb and c noun these vectors are averaged to form c. figure 1 shows the process for forming c for an example sentence.', 'finally, to form the feature vector representing a vnic instance, we subtract e from c, and append to this vector a single binary feature representing whether the vnic instance occurs in its canonical form, as determined by  #TAUTHOR_TAG.', 'the feature vectors are then used to train a supervised classifier ; in our experiments we use the linear svm implementation from  #AUTHOR_TAG.', 'the motivation for the subtraction is to capture the difference between the context in which a vnic instance occurs ( c ) and a type - level representation of that expression ( e ), to potentially represent vnic instances such that the classifier is able to generalize across expressions ( i. e., to generalize to mwe types that are unseen during training ).', 'the canonical form feature is included because it is known to be highly informative as to whether an instance is idiomatic or literal']",5
"['unsupervised cform method of  #TAUTHOR_TAG.', 'in this approach, we first']","['unsupervised cform method of  #TAUTHOR_TAG.', 'in this approach, we first']","['( without relying on training a supervised classifier, of course ) with the unsupervised cform method of  #TAUTHOR_TAG.', 'in this approach, we first']","['unsupervised approach combines the word embedding - based representation used in the supervised approach ( without relying on training a supervised classifier, of course ) with the unsupervised cform method of  #TAUTHOR_TAG.', 'in this approach, we first represent each token instance of a given vnic type as a feature vector, using the same representation as in section 2. 1.', '3 we then apply k - means clustering to form k clusters of the token instances.', ""4 all instances in each cluster are then assigned a single class, idiomatic or literal, depending on whether the majority of token instances in a cluster are in that vnic's canonical form or not, respectively."", 'in the case of ties the method backs off to a most - frequent class ( idiomatic ) baseline.', 'this method is unsupervised in that it does not rely on any gold standard labels']",5
"[' #TAUTHOR_TAG, the supervised approach was evaluated using a']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', '']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and the classifier is trained on the remaining instances.', 'the trained model is then used to classify the held out instance.', 'this is table 1 for a variety of settings of window size and number of dimensions for the word embeddings.', 'the results reveal the general trend that smaller window sizes, and more dimensions, tend to give higher accuracy, although the overall amount of variation is relatively small.', '']",5
"[' #TAUTHOR_TAG, the supervised approach was evaluated using a']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', '']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and']","[' #TAUTHOR_TAG, the supervised approach was evaluated using a leave - one - token - out strategy.', 'that is, for each mwe, a single token instance is held out, and the classifier is trained on the remaining instances.', 'the trained model is then used to classify the held out instance.', 'this is table 1 for a variety of settings of window size and number of dimensions for the word embeddings.', 'the results reveal the general trend that smaller window sizes, and more dimensions, tend to give higher accuracy, although the overall amount of variation is relatively small.', '']",3
"['as forced alignment  #TAUTHOR_TAG, reordering and language models']","['as forced alignment  #TAUTHOR_TAG, reordering and language models']","['as forced alignment  #TAUTHOR_TAG, reordering and language models']","['phrase - based smt, the phrase pairs in the translation model are traditionally trained by applying a heuristic extraction method  #AUTHOR_TAG which extracts phrase pairs based on consistency of word alignments from a word - aligned bilingual training data.', 'the probabilities of the translation model are then calculated based on the relative frequencies of the extracted phrase pairs.', 'a notable shortcoming of this approach is that the translation model probabilities thus calculated from the training bitext can be unintuitive and unreliable  #AUTHOR_TAG as they reflect only the distribution over the phrase pairs observed in the training data.', 'however, from an smt perspective it is important that the models reflect probability distributions which are preferred by the decoding process, i. e., phrase translations which are likely to be used frequently to achieve better translations should get higher scores and phrases which are less likely to be used should get low scores.', 'in addition, the heuristic extraction algorithm generates all possible, consistent phrases including overlapping phrases.', 'this means that translation probabilities are distributed over a very large number of phrase translation candidates most of which never lead to the best possible translation of a sentence.', 'in this paper, we propose a novel solution which is to re - estimate the models from the best bleu translation of each source sentence in the bitext.', 'an important contribution of our approach is that unlike previous approaches such as forced alignment  #TAUTHOR_TAG, reordering and language models can also be re - estimated']",4
"['- estimation of the translation models from the n - best translation of the bitext could re - enforce the probabilities of the low frequency phrase pairs in the re - estimated models leading to over - fitting.', 'within forced decoding,  #TAUTHOR_TAG address this problem by using a']","['- estimation of the translation models from the n - best translation of the bitext could re - enforce the probabilities of the low frequency phrase pairs in the re - estimated models leading to over - fitting.', 'within forced decoding,  #TAUTHOR_TAG address this problem by using a']","['- estimation of the translation models from the n - best translation of the bitext could re - enforce the probabilities of the low frequency phrase pairs in the re - estimated models leading to over - fitting.', 'within forced decoding,  #TAUTHOR_TAG address this problem by using a leave - one - out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence.', 'however, in our']","['- estimation of the translation models from the n - best translation of the bitext could re - enforce the probabilities of the low frequency phrase pairs in the re - estimated models leading to over - fitting.', 'within forced decoding,  #TAUTHOR_TAG address this problem by using a leave - one - out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence.', 'however, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest bleu translations which may be very different from the references.', 'thus it is not strictly necessary to apply leave - one - out in our approach as a solution to over - fitting.', 'instead, we handle the problem by simply removing all the phrase pairs below a threshold count which in our case is 2,', 'therefore removing phrase pairs with high probability but low frequency']",4
"['- oneout', ' #TAUTHOR_TAG by evaluating both on a concatenation of 5 test sets ( mt03, mt']","['- oneout', ' #TAUTHOR_TAG by evaluating both on a concatenation of 5 test sets ( mt03, mt']","['- oneout', ' #TAUTHOR_TAG by evaluating both on a concatenation of 5 test sets ( mt03, mt05 - mt09 ). as shown in']","['', ' #TAUTHOR_TAG by evaluating both on a concatenation of 5 test sets ( mt03, mt05 - mt09 ). as shown in table 3, even with leaveone - out', '']",4
['forced alignment technique of  #TAUTHOR_TAG forms the main'],['forced alignment technique of  #TAUTHOR_TAG forms the main'],['forced alignment technique of  #TAUTHOR_TAG forms the main motivation'],"['forced alignment technique of  #TAUTHOR_TAG forms the main motivation for our work.', 'in forced alignment, given a sentence pair ( f, e ), a decoder determines the best phrase segmentation and alignment which will result in a translation of f into e. the best segmentation is defined as the one which maximizes the probability of translating the source sentence into the given target sentence.', 'at the end, the phrase table is re - estimated using the phrase pair segmentations obtained from forced decoding.', 'thus forced alignment is a reestimation technique where translation probabilities are calculated based on their frequency in best - scoring hypotheses instead of the frequencies of all possible phrase pairs in the bitext.', 'however, one limitation of forced alignment is that only the phrase translation model can be re - estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions.', 'a similar line of work is proposed by and who use a self - enhancing strategy to utilize additional mono - lingual source language data by aligning it to its target language translation obtained by using an smt system to rank sentence translation probabilities.', 'however, the main focus of their work is translation model adaptation by augmenting the bitext with additional training data and not the reestimation of the translation models trained on the parallel data.', 'in this work, we propose that aligning source sentences to their oracle bleu translations provides a more realistic estimate of the models from the decoding perspective instead of aligning them to high quality human translations as in forced decoding.', 'another relevant line of research relates tuning ( weight optimisation ), where our work lies between forced decoding  #TAUTHOR_TAG and the bold updating approach of  #AUTHOR_TAG.', 'however, our approach specifically proposes a novel method for training models using oracle bleu translations']",1
['forced alignment technique of  #TAUTHOR_TAG forms the main'],['forced alignment technique of  #TAUTHOR_TAG forms the main'],['forced alignment technique of  #TAUTHOR_TAG forms the main motivation'],"['forced alignment technique of  #TAUTHOR_TAG forms the main motivation for our work.', 'in forced alignment, given a sentence pair ( f, e ), a decoder determines the best phrase segmentation and alignment which will result in a translation of f into e. the best segmentation is defined as the one which maximizes the probability of translating the source sentence into the given target sentence.', 'at the end, the phrase table is re - estimated using the phrase pair segmentations obtained from forced decoding.', 'thus forced alignment is a reestimation technique where translation probabilities are calculated based on their frequency in best - scoring hypotheses instead of the frequencies of all possible phrase pairs in the bitext.', 'however, one limitation of forced alignment is that only the phrase translation model can be re - estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions.', 'a similar line of work is proposed by and who use a self - enhancing strategy to utilize additional mono - lingual source language data by aligning it to its target language translation obtained by using an smt system to rank sentence translation probabilities.', 'however, the main focus of their work is translation model adaptation by augmenting the bitext with additional training data and not the reestimation of the translation models trained on the parallel data.', 'in this work, we propose that aligning source sentences to their oracle bleu translations provides a more realistic estimate of the models from the decoding perspective instead of aligning them to high quality human translations as in forced decoding.', 'another relevant line of research relates tuning ( weight optimisation ), where our work lies between forced decoding  #TAUTHOR_TAG and the bold updating approach of  #AUTHOR_TAG.', 'however, our approach specifically proposes a novel method for training models using oracle bleu translations']",3
"['- estimation of the translation models from the n - best translation of the bitext could re - enforce the probabilities of the low frequency phrase pairs in the re - estimated models leading to over - fitting.', 'within forced decoding,  #TAUTHOR_TAG address this problem by using a']","['- estimation of the translation models from the n - best translation of the bitext could re - enforce the probabilities of the low frequency phrase pairs in the re - estimated models leading to over - fitting.', 'within forced decoding,  #TAUTHOR_TAG address this problem by using a']","['- estimation of the translation models from the n - best translation of the bitext could re - enforce the probabilities of the low frequency phrase pairs in the re - estimated models leading to over - fitting.', 'within forced decoding,  #TAUTHOR_TAG address this problem by using a leave - one - out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence.', 'however, in our']","['- estimation of the translation models from the n - best translation of the bitext could re - enforce the probabilities of the low frequency phrase pairs in the re - estimated models leading to over - fitting.', 'within forced decoding,  #TAUTHOR_TAG address this problem by using a leave - one - out approach where they modify the phrase translation probabilities for each sentence pair by removing the counts of all phrases that were extracted from that particular sentence.', 'however, in our approach, we do not impose a constraint to produce the exact translation, instead we use the highest bleu translations which may be very different from the references.', 'thus it is not strictly necessary to apply leave - one - out in our approach as a solution to over - fitting.', 'instead, we handle the problem by simply removing all the phrase pairs below a threshold count which in our case is 2,', 'therefore removing phrase pairs with high probability but low frequency']",0
"['- oneout', ' #TAUTHOR_TAG by evaluating both on a concatenation of 5 test sets ( mt03, mt']","['- oneout', ' #TAUTHOR_TAG by evaluating both on a concatenation of 5 test sets ( mt03, mt']","['- oneout', ' #TAUTHOR_TAG by evaluating both on a concatenation of 5 test sets ( mt03, mt05 - mt09 ). as shown in']","['', ' #TAUTHOR_TAG by evaluating both on a concatenation of 5 test sets ( mt03, mt05 - mt09 ). as shown in table 3, even with leaveone - out', '']",5
"['turn response selection.', 'this task aims to select the best - matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances  #TAUTHOR_TAG.', 'an example of this task is illustrated in table 1']","['of multi - turn response selection.', 'this task aims to select the best - matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances  #TAUTHOR_TAG.', 'an example of this task is illustrated in table 1']","['turn response selection.', 'this task aims to select the best - matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances  #TAUTHOR_TAG.', 'an example of this task is illustrated in table 1']","['##bots aim to engage users in open - domain human - computer conversations and are currently receiving increasing attention.', 'the existing work on building chatbots includes generation - based methods and retrieval - based methods.', 'the first type of methods synthesize a response with a natural language generation model  #AUTHOR_TAG.', 'in this paper, we focus on the second type and study the problem of multi - turn response selection.', 'this task aims to select the best - matched response from a set of candidates, given the context of a conversation which is composed of multiple utterances  #TAUTHOR_TAG.', 'an example of this task is illustrated in table 1']",0
['by means of response selection algorithms  #TAUTHOR_TAG ea ea ea ea'],['by means of response selection algorithms  #TAUTHOR_TAG ea ea ea ea'],['they select a proper response for the current conversation from a repository by means of response selection algorithms  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],0
"['##1  #TAUTHOR_TAG, ubunt']","['public multi - turn response selection datasets, ubuntu dialogue corpus v1  #TAUTHOR_TAG, ubuntu dialogue']","['returns a score to denote the matching degree of this context - response pair.', 'we tested sa - bert on five public multi - turn response selection datasets, ubuntu dialogue corpus v1  #TAUTHOR_TAG, ubunt']","['first token of each concatenated sequence is the [CLS] token, with its embedding being used as the aggregated representation for a context - response pair classification.', 'this embedding captures the matching information between a context - response pair, which is sent into a classifier with a sigmoid output layer.', 'parameters of this classifier need to be estimated during the fine - tuning process.', 'finally, the classifier returns a score to denote the matching degree of this context - response pair.', 'we tested sa - bert on five public multi - turn response selection datasets, ubuntu dialogue corpus v1  #TAUTHOR_TAG, ubuntu dialogue corpus v2  #AUTHOR_TAG, douban conversation corpus  #AUTHOR_TAG, e - commerce dialogue corpus  #AUTHOR_TAG b ) and dstc 8 - track 2 - subtask 2 corpus  #AUTHOR_TAG.', 'the first four datasets have been disentangled in advance and our proposed speaker - aware disentanglement strategy has been applied to only the last dstc 8 - track 2 - subtask 2 corpus.', 'ubuntu dialogue corpus v1, v2 and dstc 8 - track 2 - subtask 2 corpus contain multi - turn dialogues about ubuntu system troubleshooting in english.', 'here, we adopted the version of ubuntu dialogue corpus v1 shared in, in which numbers, paths and urls were replaced by placeholders.', 'compared with ubuntu dialogue corpus v1, the training, validation and test dialogues in the v2 dataset were generated in different periods without overlap.', 'in the dstc 8 - track 2 - subtask 2 corpus, the candidate pool may not contain the correct response, so we need to choose a threshold.', 'when the probability of positive labels was smaller than the threshold, we predicted that candidate pool did not contain the correct response.', 'the threshold was selected among [ 0. 6, 0. 65,.., 0. 95 ] based on the validation set.', 'in all of the ubuntu corpora, the positive responses are true responses from humans, and the negative responses are randomly sampled.', 'the douban conversation corpus was crawled from a chinese social network on open - domain topics.', 'it was constructed in a similar way to the ubuntu corpus.', 'the douban conversation corpus collected responses via a small inverted - index system, and labels were manually annotated.', 'the douban conversation corpus is different from the other three datasets in that it includes multiple correct candidates for a context in the test set, which leads to low r n @ k, e. g., if there are 3 correct responses, the maximum r 10 @ 1 is']",0
['by means of response selection algorithms  #TAUTHOR_TAG ea ea ea ea'],['by means of response selection algorithms  #TAUTHOR_TAG ea ea ea ea'],['they select a proper response for the current conversation from a repository by means of response selection algorithms  #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],3
"['##1  #TAUTHOR_TAG, ubunt']","['public multi - turn response selection datasets, ubuntu dialogue corpus v1  #TAUTHOR_TAG, ubuntu dialogue']","['returns a score to denote the matching degree of this context - response pair.', 'we tested sa - bert on five public multi - turn response selection datasets, ubuntu dialogue corpus v1  #TAUTHOR_TAG, ubunt']","['first token of each concatenated sequence is the [CLS] token, with its embedding being used as the aggregated representation for a context - response pair classification.', 'this embedding captures the matching information between a context - response pair, which is sent into a classifier with a sigmoid output layer.', 'parameters of this classifier need to be estimated during the fine - tuning process.', 'finally, the classifier returns a score to denote the matching degree of this context - response pair.', 'we tested sa - bert on five public multi - turn response selection datasets, ubuntu dialogue corpus v1  #TAUTHOR_TAG, ubuntu dialogue corpus v2  #AUTHOR_TAG, douban conversation corpus  #AUTHOR_TAG, e - commerce dialogue corpus  #AUTHOR_TAG b ) and dstc 8 - track 2 - subtask 2 corpus  #AUTHOR_TAG.', 'the first four datasets have been disentangled in advance and our proposed speaker - aware disentanglement strategy has been applied to only the last dstc 8 - track 2 - subtask 2 corpus.', 'ubuntu dialogue corpus v1, v2 and dstc 8 - track 2 - subtask 2 corpus contain multi - turn dialogues about ubuntu system troubleshooting in english.', 'here, we adopted the version of ubuntu dialogue corpus v1 shared in, in which numbers, paths and urls were replaced by placeholders.', 'compared with ubuntu dialogue corpus v1, the training, validation and test dialogues in the v2 dataset were generated in different periods without overlap.', 'in the dstc 8 - track 2 - subtask 2 corpus, the candidate pool may not contain the correct response, so we need to choose a threshold.', 'when the probability of positive labels was smaller than the threshold, we predicted that candidate pool did not contain the correct response.', 'the threshold was selected among [ 0. 6, 0. 65,.., 0. 95 ] based on the validation set.', 'in all of the ubuntu corpora, the positive responses are true responses from humans, and the negative responses are randomly sampled.', 'the douban conversation corpus was crawled from a chinese social network on open - domain topics.', 'it was constructed in a similar way to the ubuntu corpus.', 'the douban conversation corpus collected responses via a small inverted - index system, and labels were manually annotated.', 'the douban conversation corpus is different from the other three datasets in that it includes multiple correct candidates for a context in the test set, which leads to low r n @ k, e. g., if there are 3 correct responses, the maximum r 10 @ 1 is']",5
"[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']","[' #TAUTHOR_TAG b ;  #AUTHOR_TAG.', '']","['used in previous work  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'each model was tasked with selecting the k best - matched responses from n available candidates for the']","['used the same evaluation metrics as those used in previous work  #TAUTHOR_TAG b ;  #AUTHOR_TAG.', 'each model was tasked with selecting the k best - matched responses from n available candidates for the given conversation context c, and we calculated the recall of the true positive replies among the k selected responses, denoted as r n @ k, as the main evaluation metric.', 'in addition to r n @ k, we considered the mean average precision ( map ) ( baezayates and ribeiro -  #AUTHOR_TAG, mean reciprocal rank ( mrr )  #AUTHOR_TAG and precision - at - one ( p @ 1 ), especially for the douban corpus, following the settings of previous work']",5
"['', 'recently,  #TAUTHOR_TAG proposed the transformer architecture']","['network.', 'recently,  #TAUTHOR_TAG proposed the transformer architecture']","['', 'recently,  #TAUTHOR_TAG proposed the transformer architecture']","['', 'recently,  #TAUTHOR_TAG proposed the transformer architecture for machine translation.', 'it relies only on attention mechanisms, instead of making use of either recurrent or convolutional * authors contributed equally to this work.', 'neural networks.', 'this architecture contains layers called self - attention ( or intra - attention ) which allow each word in the sequence to pay attention to other words in the sequence, independently of their positions.', 'we modified this architecture, resulting in the following contributions :', '• a novel architecture for text classification called self - attention network ( sanet ) that models the interactions between all input word pairs.', 'it is sequence length - agnostic, thanks to a global max pooling layer.', '• a study on the impact of this self - attention mechanism on large scale datasets.', '']",0
"['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is']","['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","['in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","[', keys k with associated values v. in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows. hence, w qk and w v are learned parameters.', 'our network ( depicted in figure 1 ) first encodes each word to its embedding. pre - trained embeddings', ', like glove  #AUTHOR_TAG, may be used and fine - tuned during the learning process. next, to inject information about the order of the words, the positional', 'encoding layer adds location information to each word. we use the positional encoding vectors that were defined by  #TAUTHOR_TAG as follows. a linear layer then performs dimensionality reduction / augmentation of the embedding space to a', 'vector space of dimension d, which is kept constant throughout the network. it', '']",0
"['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is']","['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","['in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","[', keys k with associated values v. in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows. hence, w qk and w v are learned parameters.', 'our network ( depicted in figure 1 ) first encodes each word to its embedding. pre - trained embeddings', ', like glove  #AUTHOR_TAG, may be used and fine - tuned during the learning process. next, to inject information about the order of the words, the positional', 'encoding layer adds location information to each word. we use the positional encoding vectors that were defined by  #TAUTHOR_TAG as follows. a linear layer then performs dimensionality reduction / augmentation of the embedding space to a', 'vector space of dimension d, which is kept constant throughout the network. it', '']",0
"['', 'recently,  #TAUTHOR_TAG proposed the transformer architecture']","['network.', 'recently,  #TAUTHOR_TAG proposed the transformer architecture']","['', 'recently,  #TAUTHOR_TAG proposed the transformer architecture']","['', 'recently,  #TAUTHOR_TAG proposed the transformer architecture for machine translation.', 'it relies only on attention mechanisms, instead of making use of either recurrent or convolutional * authors contributed equally to this work.', 'neural networks.', 'this architecture contains layers called self - attention ( or intra - attention ) which allow each word in the sequence to pay attention to other words in the sequence, independently of their positions.', 'we modified this architecture, resulting in the following contributions :', '• a novel architecture for text classification called self - attention network ( sanet ) that models the interactions between all input word pairs.', 'it is sequence length - agnostic, thanks to a global max pooling layer.', '• a study on the impact of this self - attention mechanism on large scale datasets.', '']",1
"['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is']","['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","['in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","[', keys k with associated values v. in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows. hence, w qk and w v are learned parameters.', 'our network ( depicted in figure 1 ) first encodes each word to its embedding. pre - trained embeddings', ', like glove  #AUTHOR_TAG, may be used and fine - tuned during the learning process. next, to inject information about the order of the words, the positional', 'encoding layer adds location information to each word. we use the positional encoding vectors that were defined by  #TAUTHOR_TAG as follows. a linear layer then performs dimensionality reduction / augmentation of the embedding space to a', 'vector space of dimension d, which is kept constant throughout the network. it', '']",3
"['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is']","['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","['in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","[', keys k with associated values v. in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows. hence, w qk and w v are learned parameters.', 'our network ( depicted in figure 1 ) first encodes each word to its embedding. pre - trained embeddings', ', like glove  #AUTHOR_TAG, may be used and fine - tuned during the learning process. next, to inject information about the order of the words, the positional', 'encoding layer adds location information to each word. we use the positional encoding vectors that were defined by  #TAUTHOR_TAG as follows. a linear layer then performs dimensionality reduction / augmentation of the embedding space to a', 'vector space of dimension d, which is kept constant throughout the network. it', '']",3
"['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is']","['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","['in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","[', keys k with associated values v. in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows. hence, w qk and w v are learned parameters.', 'our network ( depicted in figure 1 ) first encodes each word to its embedding. pre - trained embeddings', ', like glove  #AUTHOR_TAG, may be used and fine - tuned during the learning process. next, to inject information about the order of the words, the positional', 'encoding layer adds location information to each word. we use the positional encoding vectors that were defined by  #TAUTHOR_TAG as follows. a linear layer then performs dimensionality reduction / augmentation of the embedding space to a', 'vector space of dimension d, which is kept constant throughout the network. it', '']",4
"['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is']","['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","['in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","[', keys k with associated values v. in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows. hence, w qk and w v are learned parameters.', 'our network ( depicted in figure 1 ) first encodes each word to its embedding. pre - trained embeddings', ', like glove  #AUTHOR_TAG, may be used and fine - tuned during the learning process. next, to inject information about the order of the words, the positional', 'encoding layer adds location information to each word. we use the positional encoding vectors that were defined by  #TAUTHOR_TAG as follows. a linear layer then performs dimensionality reduction / augmentation of the embedding space to a', 'vector space of dimension d, which is kept constant throughout the network. it', '']",4
"['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is']","['thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","['in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows.']","[', keys k with associated values v. in the case of self - attention, q, k and v are linear projections of x', '. thus, we define the dot - product  #TAUTHOR_TAG. the self - attention block is repeated n times. self - attention mechanism as', 'follows. hence, w qk and w v are learned parameters.', 'our network ( depicted in figure 1 ) first encodes each word to its embedding. pre - trained embeddings', ', like glove  #AUTHOR_TAG, may be used and fine - tuned during the learning process. next, to inject information about the order of the words, the positional', 'encoding layer adds location information to each word. we use the positional encoding vectors that were defined by  #TAUTHOR_TAG as follows. a linear layer then performs dimensionality reduction / augmentation of the embedding space to a', 'vector space of dimension d, which is kept constant throughout the network. it', '']",4
"['operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', '']","['operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', '']","['sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', 'whether a combination of modeling with minimal translation units and using phrasal information during decoding']","['', 'context internal to the phrase. in this work, we extend the n', '- gram model, based on operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', 'whether a combination of modeling with minimal translation units and using phrasal information during decoding helps to solve the above - mentioned problems. the remainder of this paper is organized as follows. the next two sections review phrase - based and n - gram - based sm', '##t. section 2 provides a comparison of phrase - based and', 'n - gram - based smt. section 3 summarizes the operation sequence model ( osm ), the main baseline for this work', '. section 4 analyzes the search problem when decoding with minimal units. section 5 discusses how information available in phrases can be used to improve search performance. section', '6 presents the results of this work. we conducted experiments on the german - toenglish and french - to - english translation tasks and found that using phrases in decoding improves both search accuracy and bleu scores. finally we compare our system with two', 'state - of - the - art phrasebased systems ( moses and phrasal ) and two stateof - the - art n - gram - based systems', '( ncode and osm ) on standard translation tasks']",0
"['of long distance reorderings.  #TAUTHOR_TAG recently addressed', 'these problems by proposing an operation sequence ngram model']","['case of long distance reorderings.  #TAUTHOR_TAG recently addressed', 'these problems by proposing an operation sequence ngram model']","['long distance reorderings.  #TAUTHOR_TAG recently addressed', 'these problems by proposing an operation sequence ngram model which strongly couples translation and reordering, hypothesizes all possible re', '']","['', 'prp nn vb → vb in prp nn "". the pos - based rewrite rules serve to precompute', 'the orderings that are hypothesized during decoding. coupling reordering and', 'search allows the n - gram model to arrange hypotheses in 2 m stacks ( for an m word source sentence ), each containing hypotheses that cover exactly', 'the same foreign words', '. this removes the need for futurecost estimation 3. secondly, memorizing pos - based rules enables phrase - based like reordering, however without lexical selection. there are three drawbacks of this approach. firstly,', 'lexical generation and reordering are decoupled. search is only performed on a small number of reorderings, pre - calculated using the source side and completely ignoring the targetside. and lastly, the pos - based rules face data sparsity problems especially in', 'the case of long distance reorderings.  #TAUTHOR_TAG recently addressed', 'these problems by proposing an operation sequence ngram model which strongly couples translation and reordering, hypothesizes all possible re', '##orderings and does not require pos - based rules.', 'representing bilingual sentences as a sequence of operations enables them to memorize phrases and lexical reordering triggers like pbsmt. however, using minimal units during decoding and searching over all possible', 'reorderings means that hypotheses can no longer be arranged in 2 m stacks. the problem of inaccurate future - cost estimates resurfaces resulting in more search errors. a higher beam size of 500 is therefore used to produce translation units in comparison to', 'phrase - based systems. this, however, still does not eliminate all search errors. this paper shows that using', 'phrases instead of cepts in de - coding improves the search accuracy and translation quality', '. it also shows that using some phrasal information in cept - based decoding captures some of these improvements']",0
"['', '( refer to algorithm 1 in  #TAUTHOR_TAG.', 'the probability of an operation depends on the n − 1 previous operations']","['position of the translator, the last foreign word generated etc.', '( refer to algorithm 1 in  #TAUTHOR_TAG.', 'the probability of an operation depends on the n − 1 previous operations.', 'the model backs - off to the smaller n - grams of operations if the full history is unknown.', 'we use kneser - ney smoothing to handle back - off 5']","['', '( refer to algorithm 1 in  #TAUTHOR_TAG.', 'the probability of an operation depends on the n − 1 previous operations.', 'the model backs - off to the smaller n - grams of operations if the full history is unknown.', 'we use kneser - ney smoothing to handle back - off 5']","['decoding framework used in the operation sequence model is based on pharaoh  #AUTHOR_TAG a ).', 'the decoder uses beam search to build up the translation from left to right.', 'the hypotheses are arranged in m stacks such that stack i maintains hypotheses that have already translated i many foreign words.', 'the ultimate goal is to find the best scoring hypothesis, that has translated all the words in the foreign sentence.', 'the overall process can be roughly divided into the following steps : i ) extraction of translation units ii ) future - cost estimation, iii ) hypothesis extension iv ) recombination and pruning.', 'during the hypothesis extension each extracted phrase is translated into a sequence of operations.', 'the reordering operations ( gaps and jumps ) are generated by looking at the position of the translator, the last foreign word generated etc.', '( refer to algorithm 1 in  #TAUTHOR_TAG.', 'the probability of an operation depends on the n − 1 previous operations.', 'the model backs - off to the smaller n - grams of operations if the full history is unknown.', 'we use kneser - ney smoothing to handle back - off 5']",0
"['operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', '']","['operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', '']","['sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', 'whether a combination of modeling with minimal translation units and using phrasal information during decoding']","['', 'context internal to the phrase. in this work, we extend the n', '- gram model, based on operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', 'whether a combination of modeling with minimal translation units and using phrasal information during decoding helps to solve the above - mentioned problems. the remainder of this paper is organized as follows. the next two sections review phrase - based and n - gram - based sm', '##t. section 2 provides a comparison of phrase - based and', 'n - gram - based smt. section 3 summarizes the operation sequence model ( osm ), the main baseline for this work', '. section 4 analyzes the search problem when decoding with minimal units. section 5 discusses how information available in phrases can be used to improve search performance. section', '6 presents the results of this work. we conducted experiments on the german - toenglish and french - to - english translation tasks and found that using phrases in decoding improves both search accuracy and bleu scores. finally we compare our system with two', 'state - of - the - art phrasebased systems ( moses and phrasal ) and two stateof - the - art n - gram - based systems', '( ncode and osm ) on standard translation tasks']",5
"['as gap - width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log - linear framework  #TAUTHOR_TAG']","['as gap - width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log - linear framework  #TAUTHOR_TAG']","['as gap - width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log - linear framework  #TAUTHOR_TAG']","['n - gram model with integrated reordering models a sequence of operations obtained through the transformation of a bilingual sentence pair.', 'an operation can either be to i ) generate a sequence of source and target words, ii ) to insert a gap as a placeholder for skipped words, iii ) or to jump forward and backward in a sentence to translate words discontinuously.', 'the translate operation generate ( x, y ) encapsulates the translation tuple ( x, y ).', 'it generates source and target translations simultaneously 4.', 'this is similar to n - gram - based smt except that the tuples in the n - gram - based model are generated monotonically, whereas in this case lexical generation and reordering information is strongly coupled in an operation sequence.', 'consider the phrase pair :', 'the model memorizes it through the sequence :', '.., o j−1 be a sequence of operations as hypothesized by the translator to generate the bilingual sentence pair f, e with an alignment function a. the translation model is defined as :', 'where n indicates the amount of context used.', 'the translation model is implemented as an n - gram model of operations using srilm - toolkit  #AUTHOR_TAG with kneser - ney smoothing.', 'a 9 - gram model is used.', 'several count - based features such as gap and open gap penalties and distance - based features such as gap - width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log - linear framework  #TAUTHOR_TAG']",5
"['', '( refer to algorithm 1 in  #TAUTHOR_TAG.', 'the probability of an operation depends on the n − 1 previous operations']","['position of the translator, the last foreign word generated etc.', '( refer to algorithm 1 in  #TAUTHOR_TAG.', 'the probability of an operation depends on the n − 1 previous operations.', 'the model backs - off to the smaller n - grams of operations if the full history is unknown.', 'we use kneser - ney smoothing to handle back - off 5']","['', '( refer to algorithm 1 in  #TAUTHOR_TAG.', 'the probability of an operation depends on the n − 1 previous operations.', 'the model backs - off to the smaller n - grams of operations if the full history is unknown.', 'we use kneser - ney smoothing to handle back - off 5']","['decoding framework used in the operation sequence model is based on pharaoh  #AUTHOR_TAG a ).', 'the decoder uses beam search to build up the translation from left to right.', 'the hypotheses are arranged in m stacks such that stack i maintains hypotheses that have already translated i many foreign words.', 'the ultimate goal is to find the best scoring hypothesis, that has translated all the words in the foreign sentence.', 'the overall process can be roughly divided into the following steps : i ) extraction of translation units ii ) future - cost estimation, iii ) hypothesis extension iv ) recombination and pruning.', 'during the hypothesis extension each extracted phrase is translated into a sequence of operations.', 'the reordering operations ( gaps and jumps ) are generated by looking at the position of the translator, the last foreign word generated etc.', '( refer to algorithm 1 in  #TAUTHOR_TAG.', 'the probability of an operation depends on the n − 1 previous operations.', 'the model backs - off to the smaller n - grams of operations if the full history is unknown.', 'we use kneser - ney smoothing to handle back - off 5']",5
"['by running giza + +  #AUTHOR_TAG with the grow - diag - final - and  #AUTHOR_TAG symmetrization heuristic.', 'we follow the training steps described in  #TAUTHOR_TAG, consisting of i ) post - processing the alignments to remove discontinuous and una']","['by running giza + +  #AUTHOR_TAG with the grow - diag - final - and  #AUTHOR_TAG symmetrization heuristic.', 'we follow the training steps described in  #TAUTHOR_TAG, consisting of i ) post - processing the alignments to remove discontinuous and unaligned target cepts, ii ) conversion of bilingual alignments']","['of the translation model and 2m sentences from the monolingual corpus ( news commentary ) which also contains the english part of the bilingual corpus.', 'word alignments are obtained by running giza + +  #AUTHOR_TAG with the grow - diag - final - and  #AUTHOR_TAG symmetrization heuristic.', 'we follow the training steps described in  #TAUTHOR_TAG, consisting of i ) post - processing the alignments to remove discontinuous and una']","['initially experimented with two language pairs : german - to - english ( g - e ) and french - to - english ( f - e ).', 'we trained our system and the baseline systems on most of the data 6 made available for the translation task of the fourth workshop on statistical machine translation.', '7 we used 1m bilingual sentences, for the estimation of the translation model and 2m sentences from the monolingual corpus ( news commentary ) which also contains the english part of the bilingual corpus.', 'word alignments are obtained by running giza + +  #AUTHOR_TAG with the grow - diag - final - and  #AUTHOR_TAG symmetrization heuristic.', 'we follow the training steps described in  #TAUTHOR_TAG, consisting of i ) post - processing the alignments to remove discontinuous and unaligned target cepts, ii ) conversion of bilingual alignments into operation sequences, iii ) estimation of the n - gram language models']",5
"['operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', '']","['operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', '']","['sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', 'whether a combination of modeling with minimal translation units and using phrasal information during decoding']","['', 'context internal to the phrase. in this work, we extend the n', '- gram model, based on operation sequences  #TAUTHOR_TAG, to use phrases during decoding. the main idea is to study', 'whether a combination of modeling with minimal translation units and using phrasal information during decoding helps to solve the above - mentioned problems. the remainder of this paper is organized as follows. the next two sections review phrase - based and n - gram - based sm', '##t. section 2 provides a comparison of phrase - based and', 'n - gram - based smt. section 3 summarizes the operation sequence model ( osm ), the main baseline for this work', '. section 4 analyzes the search problem when decoding with minimal units. section 5 discusses how information available in phrases can be used to improve search performance. section', '6 presents the results of this work. we conducted experiments on the german - toenglish and french - to - english translation tasks and found that using phrases in decoding improves both search accuracy and bleu scores. finally we compare our system with two', 'state - of - the - art phrasebased systems ( moses and phrasal ) and two stateof - the - art n - gram - based systems', '( ncode and osm ) on standard translation tasks']",6
"['extended the training steps in  #TAUTHOR_TAG to extract a phrase lexicon from the parallel data.', 'we extract all phrase pairs of length 6 and below, that are consistent  #AUTHOR_TAG with the word alignments.', 'only continuous phrases as used in']","['extended the training steps in  #TAUTHOR_TAG to extract a phrase lexicon from the parallel data.', 'we extract all phrase pairs of length 6 and below, that are consistent  #AUTHOR_TAG with the word alignments.', 'only continuous phrases as used in']","['extended the training steps in  #TAUTHOR_TAG to extract a phrase lexicon from the parallel data.', 'we extract all phrase pairs of length 6 and below, that are consistent  #AUTHOR_TAG with the word alignments.', 'only continuous phrases as used in a traditional phrase - based system are extracted thus allowing only inside - out  #AUTHOR_TAG type of alignments.', 'the future cost']","['extended the training steps in  #TAUTHOR_TAG to extract a phrase lexicon from the parallel data.', 'we extract all phrase pairs of length 6 and below, that are consistent  #AUTHOR_TAG with the word alignments.', 'only continuous phrases as used in a traditional phrase - based system are extracted thus allowing only inside - out  #AUTHOR_TAG type of alignments.', 'the future cost of each feature component used in the log - linear model is calculated.', 'the operation sequence required to hypothesize each phrase is generated and its future cost is calculated.', 'the future costs of other features such as language models, lexicalized probability features, etc. are also estimated.', 'the estimates of the countbased reordering penalties ( gap penalty and open gap penalty ) and the distance - based features ( gapwidth and reordering distance ) could not be estimated previously with cepts but are available when using phrases']",6
"['extended the decoder developed by  #TAUTHOR_TAG and tried three ideas.', 'in our primary experiments we enabled the decoder to use phrases instead of cepts.', 'this allows the decoder']","['extended the decoder developed by  #TAUTHOR_TAG and tried three ideas.', 'in our primary experiments we enabled the decoder to use phrases instead of cepts.', 'this allows the decoder']","['extended the decoder developed by  #TAUTHOR_TAG and tried three ideas.', 'in our primary experiments we enabled the decoder to use phrases instead of cepts.', 'this allows the decoder']","['extended the decoder developed by  #TAUTHOR_TAG and tried three ideas.', 'in our primary experiments we enabled the decoder to use phrases instead of cepts.', 'this allows the decoder to i ) use phraseinternal context when computing the future - cost estimates, ii ) hypothesize translation options not available to the cept - based decoder iii ) cover multiple source words in a single step subsequently improving translation coverage and search.', 'note that using phrases instead of cepts during decoding, does not reintroduce the spurious phrasal segmentation problem as is present in the phrase - based system, because the model is built on minimal units which avoids segmentation ambiguity.', 'different compositions of the same phrasal unit lead to exactly the same model score.', 'we therefore do not create any alternative compositions of the same phrasal unit during decoding.', 'this option is not available in phrase - based decoding, because an alternative composition may lead towards a better model score.', 'in our secondary set of experiments, we used cept - based decoding but modified the decoder to use information available from the phrases extracted for the test sentences.', 'firstly, we used future - cost estimates from the extracted phrases ( see system cept. 500. fc in table1 ).', 'this however, leads to inconsistency in the cases where the future cost is estimated from some phrasal unit that cannot be generated through the available cept translations.', 'for example, say the best cost to cover the sequence "" wie heißen sie "" is given by the phrase "" what is your name "".', 'the 20 - best translation options in ceptbased system, however, do not have tuples "" wiewhat "" and "" heißen - name "".', 'to remove this discrepancy, we add all such tuples that are used in the extracted phrases, to the list of extracted cepts ( system cept. 500. fc. t ).', 'we also studied how much gain we obtain by only adding tuples from phrases and using cept - based future - cost estimates ( system cept. 500. t )']",6
['a result as sig - 8 discontinuous source - side units did not lead to any improvements in  #TAUTHOR_TAG and increased'],['a result as sig - 8 discontinuous source - side units did not lead to any improvements in  #TAUTHOR_TAG and increased'],"['bleu scores of the two experiments.', 'using phrases during decoding in the g - e experiments resulted in a statistically significant 12 0. 69 bleu points gain comparing our best system phrase. 200 with the baseline system cept. 500.', 'we mark a result as sig - 8 discontinuous source - side units did not lead to any improvements in  #TAUTHOR_TAG and increased the decoding times by multiple folds.', 'we also']","['', 'we first tuned the baseline on dev 11 to obtain an optimized weight vector.', 'we then ran the baseline and our decoders as discussed in section 5. 2 on the dev - test.', 'then we repeated this experiment by tuning the weights with our phrase - based decoder ( using a stack size of 100 ) and ran all the decoders again using the new weights.', 'table 1 shows the average search accuracies and bleu scores of the two experiments.', 'using phrases during decoding in the g - e experiments resulted in a statistically significant 12 0. 69 bleu points gain comparing our best system phrase. 200 with the baseline system cept. 500.', 'we mark a result as sig - 8 discontinuous source - side units did not lead to any improvements in  #TAUTHOR_TAG and increased the decoding times by multiple folds.', 'we also found these to be less useful.', '9 imposing a hard reordering limit significantly reduced the decoding time and also slightly increased the bleu scores.', '10 higher stack sizes leads to improvement in model scores for both german - english and french - english and slight improvement of bleu in the case of the former.', '11 we used news - dev2009a as dev and news - dev2009b as devtest and tuned the weights with z - mert  #AUTHOR_TAG.', '12 we use bootstrap resampling  #AUTHOR_TAG b ) nificant if the improvement shown by our decoder']",4
"['##s has become very popular [ 5, 6, 7,  #TAUTHOR_TAG.', 'low power consumption requirement']","['( nn ) based kws has become very popular [ 5, 6, 7,  #TAUTHOR_TAG.', 'low power consumption requirement']","['##s has become very popular [ 5, 6, 7,  #TAUTHOR_TAG.', 'low power consumption requirement']","['', 'on the other hand, the kws system should detect the keywords with high accuracy and low latency, for best user experience.', 'these conflicting system requirements make kws an active area of research ever since its inception over 50 years ago [ 4 ].', 'recently, with the renaissance of artificial neural networks in the form of deep learning algorithms, neural network ( nn ) based kws has become very popular [ 5, 6, 7,  #TAUTHOR_TAG.', 'low power consumption requirement for keyword spotting systems make microcontrollers an obvious choice for deploying kws in an always - on system.', 'microcontrollers are low - cost energy - efficient processors that are ubiquitous in our everyday life with their presence in a variety of devices ranging from home appliances, automobiles and consumer electronics to wearables.', 'however, deployment of neural network based kws on microcontrollers comes with following challenges :', 'limited memory footprint : typical microcontroller systems have only tens to few hundred kb of memory available.', 'the entire neural network model, including input / output, weights and activations, has to fit within this small memory budget']",0
"['##s model with long short - term', 'memory ( lstm ) is proposed in  #TAUTHOR_TAG, which achieves better']","['training kws model with long short - term', 'memory ( lstm ) is proposed in  #TAUTHOR_TAG, which achieves better']","['##s model with long short - term', 'memory ( lstm ) is proposed in  #TAUTHOR_TAG, which achieves better']","['', 'functions are introduced in [ 5 ], which outperforms the hmm models with a very small detection latency. furthermore, low - rank approximation techniques are used to compress the dnn model', 'weights achieving similar accuracy with less hardware resources [ 15, 16 ]. the main drawback of dnns is', 'that they ignore the local temporal and spectral correlation in the input speech features. in order to exploit these correlations, different variants of convolutional neural network ( cnn ) based kws are explored in [ 6 ], which demonstrate higher', 'accuracy than dnns. the drawback of cnns in modeling time varying signals ( e. g. speech ) is that they ignore', 'long term temporal dependencies. combining the strengths of cnns and rnns, convolutional recurrent neural network', 'based kws is investigated in [ 7 ] and demonstrate the robustness of the model to noise. while all the prior kws neural networks are trained with cross', 'entropy loss function, a max - pooling based loss function for training kws model with long short - term', 'memory ( lstm ) is proposed in  #TAUTHOR_TAG, which achieves better accuracy than the dnns and lstms trained with cross entropy loss. although many neural network models for kws are presented in literature, it is difficult to make a', 'fair comparison between them as they are all trained and evaluated on different proprietary datasets ( e. g. "" talktype "" dataset in [ 7 ]', ', "" alexa "" dataset in  #TAUTHOR_TAG, etc. ) with different input speech features and audio duration. also,', 'the primary focus of prior research has been to maximize the accuracy with a small memory footprint model, without explicit constraints of underlying hardware, such as limits on number of operations per', 'inference. in contrast, this work is more hardware - centric and targeted towards neural network architectures that maximize accuracy on microcontroller devices.', 'the constraints on memory and compute significantly limit the neural network parameters and the number of operations']",0
"['##s model with long short - term', 'memory ( lstm ) is proposed in  #TAUTHOR_TAG, which achieves better']","['training kws model with long short - term', 'memory ( lstm ) is proposed in  #TAUTHOR_TAG, which achieves better']","['##s model with long short - term', 'memory ( lstm ) is proposed in  #TAUTHOR_TAG, which achieves better']","['', 'functions are introduced in [ 5 ], which outperforms the hmm models with a very small detection latency. furthermore, low - rank approximation techniques are used to compress the dnn model', 'weights achieving similar accuracy with less hardware resources [ 15, 16 ]. the main drawback of dnns is', 'that they ignore the local temporal and spectral correlation in the input speech features. in order to exploit these correlations, different variants of convolutional neural network ( cnn ) based kws are explored in [ 6 ], which demonstrate higher', 'accuracy than dnns. the drawback of cnns in modeling time varying signals ( e. g. speech ) is that they ignore', 'long term temporal dependencies. combining the strengths of cnns and rnns, convolutional recurrent neural network', 'based kws is investigated in [ 7 ] and demonstrate the robustness of the model to noise. while all the prior kws neural networks are trained with cross', 'entropy loss function, a max - pooling based loss function for training kws model with long short - term', 'memory ( lstm ) is proposed in  #TAUTHOR_TAG, which achieves better accuracy than the dnns and lstms trained with cross entropy loss. although many neural network models for kws are presented in literature, it is difficult to make a', 'fair comparison between them as they are all trained and evaluated on different proprietary datasets ( e. g. "" talktype "" dataset in [ 7 ]', ', "" alexa "" dataset in  #TAUTHOR_TAG, etc. ) with different input speech features and audio duration. also,', 'the primary focus of prior research has been to maximize the accuracy with a small memory footprint model, without explicit constraints of underlying hardware, such as limits on number of operations per', 'inference. in contrast, this work is more hardware - centric and targeted towards neural network architectures that maximize accuracy on microcontroller devices.', 'the constraints on memory and compute significantly limit the neural network parameters and the number of operations']",0
"['activations.', 'figure 5 shows the number of operations per inference, memory requirement and test accuracy of neural network models from prior work [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from']","['inferences per second and 8 - bit weights / activations.', 'figure 5 shows the number of operations per inference, memory requirement and test accuracy of neural network models from prior work [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from']","['inferences per second and 8 - bit weights / activations.', 'figure 5 shows the number of operations per inference, memory requirement and test accuracy of neural network models from prior work [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from section 4']","['discussed in section 2. 2, memory footprint and execution time are the two important considerations in being able to run keyword spotting on microcontrollers.', 'these should be considered when designing and optimizing neural networks for running keyword spotting.', 'based on typical microcontroller system configurations ( as described in table 1 ), we derive three sets of constraints for the neural networks in table 3, targeting small, medium and large microcontroller systems.', 'both memory and compute limit are derived with assumptions that some amount of resources will be allocated for running other tasks such as os, i / o, network communication, etc.', 'the operations per inference limit assumes that the system is running 10 inferences per second.', 'table 3 : neural network ( nn ) classes for kws models considered in this work, assuming 10 inferences per second and 8 - bit weights / activations.', 'figure 5 shows the number of operations per inference, memory requirement and test accuracy of neural network models from prior work [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset overlayed with the memory and compute bounding boxes for the neural network classes from section 4']",0
"['.', '[ 5, 6, 7,  #TAUTHOR_TAG trained on the speech commands dataset']","['still achieve high accuracy.', '[ 5, 6, 7,  #TAUTHOR_TAG trained on the speech commands dataset [ 9 ]']","['still achieve high accuracy.', '[ 5, 6, 7,  #TAUTHOR_TAG trained on the speech commands dataset [ 9 ].', 'as shown in']","['', '[ 5, 6, 7,  #TAUTHOR_TAG trained on the speech commands dataset [ 9 ].', 'as shown in fig. 1, from each input speech signal, t × f features are extracted and the number of these features impact the model size, number of operations and accuracy.', '']",0
"['5, 6, 7,  #TAUTHOR_TAG on google speech commands dataset']","['as follows :', '• we first train the popular kws neural net models from the literature [ 5, 6, 7,  #TAUTHOR_TAG on google speech commands dataset [ 9 ] and compare']","['5, 6, 7,  #TAUTHOR_TAG on google speech commands dataset']","['kws is always - on, the real - time requirement limits the total number of operations per neural network inference.', 'these microcontroller resource constraints in conjunction with the high accuracy and low latency requirements of kws call for a resource - constrained neural network architecture exploration to find lean neural network structures suitable for kws, which is the primary focus of our work.', 'the main contributions of this work are as follows :', '• we first train the popular kws neural net models from the literature [ 5, 6, 7,  #TAUTHOR_TAG on google speech commands dataset [ 9 ] and compare them in terms of accuracy, memory footprint and number of operations per inference.', '• in addition, we implement a new kws model using depth - wise separable convolutions and point - wise convolutions, inspired by the success of resource - efficient mobilenet [ 10 ] in computer vision.', 'this model outperforms the other prior models in all aspects of accuracy, model size and number of operations.', '• finally, we perform resource - constrained neural network architecture exploration and present comprehensive comparison of different network architectures within a set of compute and memory constraints of typical microcontrollers.', 'the code, model definitions and pretrained models are available at https : / / github. com / arm - software / ml - kws - for - mcu']",3
"[', memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset']","['test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset [ 9 ].', 'for all the models, we use 40 mfcc features extracted from a speech frame of length 40ms with a stride of 20ms, which gives 1960 ( 49×40 ) features for 1 second of audio.', 'the']","['on the test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset [ 9 ].', 'for all the models, we use 40 mfcc features extracted from a speech frame of length 40ms with a stride of 20ms, which gives 1960 ( 49×40 ) features for 1 second of audio.', 'the accuracy shown in the table is']","['', 'the training data is augmented with background noise and random time shift of up to 100ms.', 'the trained models are evaluated based on the classification accuracy on the test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset [ 9 ].', 'for all the models, we use 40 mfcc features extracted from a speech frame of length 40ms with a stride of 20ms, which gives 1960 ( 49×40 ) features for 1 second of audio.', 'the accuracy shown in the table is the accuracy on test set.', 'the memory shown in the table assumes 8 - bit weights and activations, which is sufficient to achieve same accuracy as that from a full - precision network']",3
"['.', '[ 5, 6, 7,  #TAUTHOR_TAG trained on the speech commands dataset']","['still achieve high accuracy.', '[ 5, 6, 7,  #TAUTHOR_TAG trained on the speech commands dataset [ 9 ]']","['still achieve high accuracy.', '[ 5, 6, 7,  #TAUTHOR_TAG trained on the speech commands dataset [ 9 ].', 'as shown in']","['', '[ 5, 6, 7,  #TAUTHOR_TAG trained on the speech commands dataset [ 9 ].', 'as shown in fig. 1, from each input speech signal, t × f features are extracted and the number of these features impact the model size, number of operations and accuracy.', '']",3
"['5, 6, 7,  #TAUTHOR_TAG on google speech commands dataset']","['as follows :', '• we first train the popular kws neural net models from the literature [ 5, 6, 7,  #TAUTHOR_TAG on google speech commands dataset [ 9 ] and compare']","['5, 6, 7,  #TAUTHOR_TAG on google speech commands dataset']","['kws is always - on, the real - time requirement limits the total number of operations per neural network inference.', 'these microcontroller resource constraints in conjunction with the high accuracy and low latency requirements of kws call for a resource - constrained neural network architecture exploration to find lean neural network structures suitable for kws, which is the primary focus of our work.', 'the main contributions of this work are as follows :', '• we first train the popular kws neural net models from the literature [ 5, 6, 7,  #TAUTHOR_TAG on google speech commands dataset [ 9 ] and compare them in terms of accuracy, memory footprint and number of operations per inference.', '• in addition, we implement a new kws model using depth - wise separable convolutions and point - wise convolutions, inspired by the success of resource - efficient mobilenet [ 10 ] in computer vision.', 'this model outperforms the other prior models in all aspects of accuracy, model size and number of operations.', '• finally, we perform resource - constrained neural network architecture exploration and present comprehensive comparison of different network architectures within a set of compute and memory constraints of typical microcontrollers.', 'the code, model definitions and pretrained models are available at https : / / github. com / arm - software / ml - kws - for - mcu']",5
"[', memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset']","['test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset [ 9 ].', 'for all the models, we use 40 mfcc features extracted from a speech frame of length 40ms with a stride of 20ms, which gives 1960 ( 49×40 ) features for 1 second of audio.', 'the']","['on the test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset [ 9 ].', 'for all the models, we use 40 mfcc features extracted from a speech frame of length 40ms with a stride of 20ms, which gives 1960 ( 49×40 ) features for 1 second of audio.', 'the accuracy shown in the table is']","['', 'the training data is augmented with background noise and random time shift of up to 100ms.', 'the trained models are evaluated based on the classification accuracy on the test set.', 'table 2 summarizes the accuracy, memory requirement and operations per inference for the network architectures for kws from literature [ 5, 6, 7,  #TAUTHOR_TAG trained on google speech commands dataset [ 9 ].', 'for all the models, we use 40 mfcc features extracted from a speech frame of length 40ms with a stride of 20ms, which gives 1960 ( 49×40 ) features for 1 second of audio.', 'the accuracy shown in the table is the accuracy on test set.', 'the memory shown in the table assumes 8 - bit weights and activations, which is sufficient to achieve same accuracy as that from a full - precision network']",5
"['neighboring words and parts of speech  #TAUTHOR_TAG.', 'however, while structure']","['neighboring words and parts of speech  #TAUTHOR_TAG.', 'however, while structure']","['neighboring words and parts of speech  #TAUTHOR_TAG.', 'however, while structure does provide valuable information,  #AUTHOR_TAG have shown that gains provided']","['morphological analysis ( ma ) takes an unsegmented string of japanese text as input, and outputs a string of morphemes annotated with parts of speech ( poss ).', 'as ma is the first step in japanese nlp, its accuracy directly affects the accuracy of nlp systems as a whole.', 'in addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out - of - domain data  #AUTHOR_TAG.', 'previous approaches have used structured predictors such as hidden markov models ( hmms ) or conditional random fields ( crfs ), which consider the interactions between neighboring words and parts of speech  #TAUTHOR_TAG.', 'however, while structure does provide valuable information,  #AUTHOR_TAG have shown that gains provided by structured prediction can be largely recovered by using a richer feature set.', '']",0
"['##gmented text  #TAUTHOR_TAG.', 'in this section we describe an existing joint sequence - based method for japanese ma, as well as']","['single joint process of finding a morpheme / pos string from unsegmented text  #TAUTHOR_TAG.', 'in this section we describe an existing joint sequence - based method for japanese ma, as well as']","['##gmented text  #TAUTHOR_TAG.', 'in this section we describe an existing joint sequence - based method for japanese ma, as well as']","['ma takes an unsegmented string of characters x i 1 as input, segments it into morphemes w j 1, and annotates each morpheme with a part of speech t j 1.', 'this can be formulated as a two - step process of first segmenting words, then estimating poss  #AUTHOR_TAG, or as a single joint process of finding a morpheme / pos string from unsegmented text  #TAUTHOR_TAG.', 'in this section we describe an existing joint sequence - based method for japanese ma, as well as our proposed two - step pointwise method']",0
"['1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are']","['1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are']","['strings tire sentences as in figure 1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are trained over segmentation lattices, which allows']","['strings tire sentences as in figure 1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are trained over segmentation lattices, which allows for the handling of variable length sequences that occur due to multiple segmentations.', 'the model is able to take into account arbitrary features, as well as the context between neighboring tags.', 'we follow  #TAUTHOR_TAG in defining our feature set, as summarized in table 1 1.', 'lexical features were trained for the top 5000 most frequent words in the corpus.', 'it should be noted that these are wordbased features, and information about transitions between pos tags is included.', 'when creating training data, the use of word - based features indicates that word boundaries must be annotated, while the use of pos transition information further indicates that all of these words must be annotated with poss.', '1 more fine - grained pos tags have provided small boosts in accuracy in previous research  #TAUTHOR_TAG, but these increase the annotation burden, which is contrary to our goal.', 'feature strings']",0
"['1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are']","['1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are']","['strings tire sentences as in figure 1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are trained over segmentation lattices, which allows']","['strings tire sentences as in figure 1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are trained over segmentation lattices, which allows for the handling of variable length sequences that occur due to multiple segmentations.', 'the model is able to take into account arbitrary features, as well as the context between neighboring tags.', 'we follow  #TAUTHOR_TAG in defining our feature set, as summarized in table 1 1.', 'lexical features were trained for the top 5000 most frequent words in the corpus.', 'it should be noted that these are wordbased features, and information about transitions between pos tags is included.', 'when creating training data, the use of word - based features indicates that word boundaries must be annotated, while the use of pos transition information further indicates that all of these words must be annotated with poss.', '1 more fine - grained pos tags have provided small boosts in accuracy in previous research  #TAUTHOR_TAG, but these increase the annotation burden, which is contrary to our goal.', 'feature strings']",0
"['neighboring words and parts of speech  #TAUTHOR_TAG.', 'however, while structure']","['neighboring words and parts of speech  #TAUTHOR_TAG.', 'however, while structure']","['neighboring words and parts of speech  #TAUTHOR_TAG.', 'however, while structure does provide valuable information,  #AUTHOR_TAG have shown that gains provided']","['morphological analysis ( ma ) takes an unsegmented string of japanese text as input, and outputs a string of morphemes annotated with parts of speech ( poss ).', 'as ma is the first step in japanese nlp, its accuracy directly affects the accuracy of nlp systems as a whole.', 'in addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out - of - domain data  #AUTHOR_TAG.', 'previous approaches have used structured predictors such as hidden markov models ( hmms ) or conditional random fields ( crfs ), which consider the interactions between neighboring words and parts of speech  #TAUTHOR_TAG.', 'however, while structure does provide valuable information,  #AUTHOR_TAG have shown that gains provided by structured prediction can be largely recovered by using a richer feature set.', '']",4
"['1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are']","['1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are']","['strings tire sentences as in figure 1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are trained over segmentation lattices, which allows']","['strings tire sentences as in figure 1 ( a ).', 'the crf - based method presented by  #TAUTHOR_TAG is generally accepted as the state - of - the - art in this paradigm.', 'crfs are trained over segmentation lattices, which allows for the handling of variable length sequences that occur due to multiple segmentations.', 'the model is able to take into account arbitrary features, as well as the context between neighboring tags.', 'we follow  #TAUTHOR_TAG in defining our feature set, as summarized in table 1 1.', 'lexical features were trained for the top 5000 most frequent words in the corpus.', 'it should be noted that these are wordbased features, and information about transitions between pos tags is included.', 'when creating training data, the use of word - based features indicates that word boundaries must be annotated, while the use of pos transition information further indicates that all of these words must be annotated with poss.', '1 more fine - grained pos tags have provided small boosts in accuracy in previous research  #TAUTHOR_TAG, but these increase the annotation burden, which is contrary to our goal.', 'feature strings']",5
"[""used mecab  #AUTHOR_TAG, an open source implementation of  #TAUTHOR_TAG's crf - based method ( we will""]","[""used mecab  #AUTHOR_TAG, an open source implementation of  #TAUTHOR_TAG's crf - based method ( we will""]","[""1, we used mecab  #AUTHOR_TAG, an open source implementation of  #TAUTHOR_TAG's crf - based method ( we will call this joint )."", 'for the pointwise two - step method,']","['order to test the effectiveness of pointwise ma, we did an experiment measuring accuracy both on in - domain data, and in a domain - adaptation situation.', 'we used the balanced corpus of contemporary written japanese ( bccwj )  #AUTHOR_TAG, specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus ( table 3 ).', ""as a representative of joint sequence - based ma described in 2. 1, we used mecab  #AUTHOR_TAG, an open source implementation of  #TAUTHOR_TAG's crf - based method ( we will call this joint )."", 'for the pointwise two - step method, we trained logistic regression models with the liblinear toolkit  #AUTHOR_TAG using the features described in section 2. 2 ( 2 - lr ).', 'in addition, we trained a crf - based model with the crfsuite toolkit  #AUTHOR_TAG using the same features and set - up ( for both word segmentation and pos tagging ) to examine the contribution of context information ( 2 - crf ).', 'to create the dictionary, we added all of the words in the corpus, but left out a small portion of singletons to prevent overfitting on the training data 3.', 'as an evaluation measure, we follow  #AUTHOR_TAG and  #TAUTHOR_TAG and use word / pos tag pair fmeasure, so that both word boundaries and pos tags must be correct for a word to be considered correct']",5
"[""used mecab  #AUTHOR_TAG, an open source implementation of  #TAUTHOR_TAG's crf - based method ( we will""]","[""used mecab  #AUTHOR_TAG, an open source implementation of  #TAUTHOR_TAG's crf - based method ( we will""]","[""1, we used mecab  #AUTHOR_TAG, an open source implementation of  #TAUTHOR_TAG's crf - based method ( we will call this joint )."", 'for the pointwise two - step method,']","['order to test the effectiveness of pointwise ma, we did an experiment measuring accuracy both on in - domain data, and in a domain - adaptation situation.', 'we used the balanced corpus of contemporary written japanese ( bccwj )  #AUTHOR_TAG, specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus ( table 3 ).', ""as a representative of joint sequence - based ma described in 2. 1, we used mecab  #AUTHOR_TAG, an open source implementation of  #TAUTHOR_TAG's crf - based method ( we will call this joint )."", 'for the pointwise two - step method, we trained logistic regression models with the liblinear toolkit  #AUTHOR_TAG using the features described in section 2. 2 ( 2 - lr ).', 'in addition, we trained a crf - based model with the crfsuite toolkit  #AUTHOR_TAG using the same features and set - up ( for both word segmentation and pos tagging ) to examine the contribution of context information ( 2 - crf ).', 'to create the dictionary, we added all of the words in the corpus, but left out a small portion of singletons to prevent overfitting on the training data 3.', 'as an evaluation measure, we follow  #AUTHOR_TAG and  #TAUTHOR_TAG and use word / pos tag pair fmeasure, so that both word boundaries and pos tags must be correct for a word to be considered correct']",7
"['of complex - simple texts  #TAUTHOR_TAG, and learning word embeddings from a large']","['of complex - simple texts  #TAUTHOR_TAG, and learning word embeddings from a large']","['- aligned parallel corpora of complex - simple texts  #TAUTHOR_TAG, and learning word embeddings from a large corpora']","['simplification is the task of automatically rewriting a text by substituting words or phrases with simpler variants, while retaining its meaning and grammaticality.', 'the goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines.', 'approaches to lexical simplification generally follow a standard pipeline consisting of two main steps : generation and ranking.', 'in the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as wordnet  #AUTHOR_TAG, learning substitution rules from sentence - aligned parallel corpora of complex - simple texts  #TAUTHOR_TAG, and learning word embeddings from a large corpora to obtain similar words of the complex word ( glavas andstajner, 2015 ;  #AUTHOR_TAG a, 2017 ).', 'in the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness.', '* this research was conducted while the first author was a post doctoral fellow at the city university of hong kong.', 'the ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features.', 'state - of - the - art approaches to ranking in lexical simplification exploit supervised machine learning - based methods that rely mostly on surface features, such as word frequency, word length and n - gram probability, for training the model  #AUTHOR_TAG bingel and søgaard, 2016 ;  #AUTHOR_TAG a, 2017 ).', 'moreover, deep architectures are not explored in these models.', 'surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates.', 'in this paper, we propose to use a deep structured similarity model ( dssm )  #AUTHOR_TAG to rank substitution candidates.', 'the dssm exploits a deep architecture by using a deep neural network ( dnn ), that can effectively capture contextual features to perform semantic matching between two sentences.', 'it has been successfully applied to several natural language processing ( nlp ) tasks, such as machine translation, web search ranking  #AUTHOR_TAG, question answering, and image captioning  #AUTHOR_TAG.', 'to the best of our knowledge, this is the first time this model is applied to lexical simplification.', 'we adapt the original dssm architecture and objective function to our specific task.', 'our evaluation on two standard datasets for lexical simplification shows that this method outperforms state - of - the - art approaches that use supervised machine learning - based methods']",0
"[' #TAUTHOR_TAG, which was recently shown to']","[' #TAUTHOR_TAG, which was recently shown to']","[' #TAUTHOR_TAG, which was recently shown to']","['focus on the ranking step of the standard lexical simplification pipeline.', 'given a dataset of tar - get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching.', 'for generating substitution candidates, we utilize the method proposed by  #TAUTHOR_TAG, which was recently shown to be the state - of - art method for generating substitution candidates.', 'they exploit a hybrid substitution generation approach where candidates are first extracted from 550, 644 simple - complex aligned sentences from the newsela corpus  #AUTHOR_TAG.', 'then, these candidates are complemented with candidates generated with a retrofitted word embedding model.', ""the word embedding model is retrofitted over wordnet's synonym pairs ( for details, please refer to  #TAUTHOR_TAG."", 'for ranking substitution candidates, we use a dssm, which we elaborate in the next section']",0
"[' #TAUTHOR_TAG, which was recently shown to']","[' #TAUTHOR_TAG, which was recently shown to']","[' #TAUTHOR_TAG, which was recently shown to']","['focus on the ranking step of the standard lexical simplification pipeline.', 'given a dataset of tar - get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching.', 'for generating substitution candidates, we utilize the method proposed by  #TAUTHOR_TAG, which was recently shown to be the state - of - art method for generating substitution candidates.', 'they exploit a hybrid substitution generation approach where candidates are first extracted from 550, 644 simple - complex aligned sentences from the newsela corpus  #AUTHOR_TAG.', 'then, these candidates are complemented with candidates generated with a retrofitted word embedding model.', ""the word embedding model is retrofitted over wordnet's synonym pairs ( for details, please refer to  #TAUTHOR_TAG."", 'for ranking substitution candidates, we use a dssm, which we elaborate in the next section']",5
"[' #TAUTHOR_TAG, which was recently shown to']","[' #TAUTHOR_TAG, which was recently shown to']","[' #TAUTHOR_TAG, which was recently shown to']","['focus on the ranking step of the standard lexical simplification pipeline.', 'given a dataset of tar - get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching.', 'for generating substitution candidates, we utilize the method proposed by  #TAUTHOR_TAG, which was recently shown to be the state - of - art method for generating substitution candidates.', 'they exploit a hybrid substitution generation approach where candidates are first extracted from 550, 644 simple - complex aligned sentences from the newsela corpus  #AUTHOR_TAG.', 'then, these candidates are complemented with candidates generated with a retrofitted word embedding model.', ""the word embedding model is retrofitted over wordnet's synonym pairs ( for details, please refer to  #TAUTHOR_TAG."", 'for ranking substitution candidates, we use a dssm, which we elaborate in the next section']",5
"['n - gram probability features as in  #TAUTHOR_TAG, who also employ a neural']","['n - gram probability features as in  #TAUTHOR_TAG, who also employ a neural']","['baseline features, we use the same n - gram probability features as in  #TAUTHOR_TAG, who also employ a neural network']","['baseline features, we use the same n - gram probability features as in  #TAUTHOR_TAG, who also employ a neural network to rank substitution candidates.', 'as in  #TAUTHOR_TAG, the features were extracted using the subimdb corpus  #AUTHOR_TAG.', 'we also experiment with additional features that have been reported as useful in this task.', 'for each target word and a substitution candidate word we also compute : cosine similarity, word length, and alignment probability in the sentence - aligned normal - simple wikipedia corpus  #AUTHOR_TAG.', 'the cosine similarity feature is computed using the subimdb corpus']",5
"['n - gram probability features as in  #TAUTHOR_TAG, who also employ a neural']","['n - gram probability features as in  #TAUTHOR_TAG, who also employ a neural']","['baseline features, we use the same n - gram probability features as in  #TAUTHOR_TAG, who also employ a neural network']","['baseline features, we use the same n - gram probability features as in  #TAUTHOR_TAG, who also employ a neural network to rank substitution candidates.', 'as in  #TAUTHOR_TAG, the features were extracted using the subimdb corpus  #AUTHOR_TAG.', 'we also experiment with additional features that have been reported as useful in this task.', 'for each target word and a substitution candidate word we also compute : cosine similarity, word length, and alignment probability in the sentence - aligned normal - simple wikipedia corpus  #AUTHOR_TAG.', 'the cosine similarity feature is computed using the subimdb corpus']",5
"['ranking in lexical simplification  #TAUTHOR_TAG.', 'in order to learn the parameters w t and w s ( figure 1 ) of the']","['ranking in lexical simplification  #TAUTHOR_TAG.', 'in order to learn the parameters w t and w s ( figure 1 ) of the dssm, we use the standard']","['ranking in lexical simplification  #TAUTHOR_TAG.', 'in order to learn the parameters w t and w s ( figure 1 ) of the dssm, we use the standard backpropagation algorithm  #AUTHOR_TAG.', 'the objective used in this paper follows the pair - wise learning - to - rank paradigm outlined in  #AUTHOR_TAG.', 'given']","['previous works that used supervised machine learning for ranking in lexical simplification  #TAUTHOR_TAG.', 'in order to learn the parameters w t and w s ( figure 1 ) of the dssm, we use the standard backpropagation algorithm  #AUTHOR_TAG.', 'the objective used in this paper follows the pair - wise learning - to - rank paradigm outlined in  #AUTHOR_TAG.', 'given a target word and its sentential context t, we obtain a list of candidates l. we set different positive values to the candidates based on their simplicity rankings.', 'e. g., if the list of the candidates is ordered by simplificity as, l = { a + > b + > c + }, the labels are first constructed as l = { y a + = 3, y b + = 2, y c + = 1 }. the values are then normalized by dividing by the maximum value in the list : l = { y a + = 1, y b + = 0. 667, y c + = 0. 333 }. if the target word was not originally in l, we add it with label 0.', 'this enables the model to reflect the label information in the similarity scores.', 'we minimize the bayesian expected loss as :', 'note that p ( s l | t ) is computed as :', 'here, γ is a tuning factor.', 'we used 5 - cross validation approach to select hyper - parameters, such as number of hidden nodes.', 'we set the gamma factor as 10 as per  #AUTHOR_TAG.', 'the selected hyperparameters were used to train the model in the whole lexmturk dataset.', 'we employ earlystopping and select the model whose change of the average loss in each epoch was smaller than 1. 0e - 3.', 'since the training data is small ( only 500 samples ) we use a smaller number of hidden nodes, d = 32, in the nonlinear projection layer and adopt a higher dropout rate ( 0. 4 ).', 'the model is optimized using adam  #AUTHOR_TAG with the learning rate fixed at 0. 001, and is trained for 30 epochs.', 'the mini - batch is set to 16 during training']",5
"['simplicity  #TAUTHOR_TAG.', 'since']","['simplicity  #TAUTHOR_TAG.', 'since']","['simplicity  #TAUTHOR_TAG.', 'since']","['evaluate the proposed model, we conduct experiments on two common datasets for lexical simplification : benchls  #AUTHOR_TAG b ), which contains 929 instances, and nnseval  #AUTHOR_TAG a ), which contains 239 instances.', 'each instance is composed of a sentence, a target word, and a set of gold candidates ranked by simplicity  #TAUTHOR_TAG.', 'since both datasets contain instances from the lexmturk dataset  #AUTHOR_TAG, which we use for training the dnn, we remove the overlap instances between training and test datasets 1.', 'we finally obtain 429 remaining instances in the benchls dataset, and 78 instances in the nneval dataset, which are used in our evaluation.', 'we adopt the same evaluation metrics featured in glavas andstajner ( 2015 ) and  #AUTHOR_TAG : 1 ) precision : ratio of correct simplifications out of all the simplifications made by the system ; 2 ) accuracy : ratio of correct simplifications out of all words that should have been simplified ; and 3 ) changed : ratio of target words changed by the system']",5
"['described in  #TAUTHOR_TAG,']","['described in  #TAUTHOR_TAG,']","['nsr ) approach described in  #TAUTHOR_TAG,']","['compared the proposed model ( dssm ranking ) to two state - of - the - art approaches to ranking in lexical simplification that exploit supervised machine learning - based methods.', 'the first baseline is the neural substitution ranking ( nsr ) approach described in  #TAUTHOR_TAG, which employs a multi - layer perceptron neural network.', 'we reimplement their model as part of the lexenstein toolkit  #AUTHOR_TAG.', 'the network has 3 hidden layers with 8 nodes each.', 'unlike the proposed model, they treat ranking in lexical simplification as a standard classification problem.', 'the second baseline is svm rank  #AUTHOR_TAG table 1 : substitution candidates ranking results.', 'n - gram probs.', 'denotes the n - gram probability features described in  #AUTHOR_TAG, and all denotes all features described in section 2. 3.', 'all values marked in bold are significantly higher compared to the best baseline, svm rank, measured by t - test at p - value of 0. 05.', 'with default parameters ) for ranking substitution candidates, similar to the method described in  #AUTHOR_TAG.', 'all the three models employ the n - gram probability features extracted from the subimdb corpus  #AUTHOR_TAG, as described in  #TAUTHOR_TAG, and are trained using the lexmturk dataset']",5
"['described in  #TAUTHOR_TAG,']","['described in  #TAUTHOR_TAG,']","['nsr ) approach described in  #TAUTHOR_TAG,']","['compared the proposed model ( dssm ranking ) to two state - of - the - art approaches to ranking in lexical simplification that exploit supervised machine learning - based methods.', 'the first baseline is the neural substitution ranking ( nsr ) approach described in  #TAUTHOR_TAG, which employs a multi - layer perceptron neural network.', 'we reimplement their model as part of the lexenstein toolkit  #AUTHOR_TAG.', 'the network has 3 hidden layers with 8 nodes each.', 'unlike the proposed model, they treat ranking in lexical simplification as a standard classification problem.', 'the second baseline is svm rank  #AUTHOR_TAG table 1 : substitution candidates ranking results.', 'n - gram probs.', 'denotes the n - gram probability features described in  #AUTHOR_TAG, and all denotes all features described in section 2. 3.', 'all values marked in bold are significantly higher compared to the best baseline, svm rank, measured by t - test at p - value of 0. 05.', 'with default parameters ) for ranking substitution candidates, similar to the method described in  #AUTHOR_TAG.', 'all the three models employ the n - gram probability features extracted from the subimdb corpus  #AUTHOR_TAG, as described in  #TAUTHOR_TAG, and are trained using the lexmturk dataset']",5
"['', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates']","['target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates']","['replace the target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates according to how well they fit the context of the target word, and discard']","['', 'the dssm ranking performs comparably to svm rank when using only n - gram probabilities as features, and consistently leverages all features described in section 2. 3, outperforming all systems in accuracy, precision and changed ratio.', 'we experimented with adding all features described in section 2. 3 to the baselines as well, however, we obtained no improvements compared to using only n - gram probability features.', 'we also tried running all ranking systems on selected candidates that best replace the target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates according to how well they fit the context of the target word, and discards 50 % of the worst ranking candidates.', 'the bottom part of the table 1 ( selection step + substitution candidates ranking ) summarizes the results of all ranking systems after performing the selection step on generated substitution candidates.', '']",5
"['', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates']","['target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates']","['replace the target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates according to how well they fit the context of the target word, and discard']","['', 'the dssm ranking performs comparably to svm rank when using only n - gram probabilities as features, and consistently leverages all features described in section 2. 3, outperforming all systems in accuracy, precision and changed ratio.', 'we experimented with adding all features described in section 2. 3 to the baselines as well, however, we obtained no improvements compared to using only n - gram probability features.', 'we also tried running all ranking systems on selected candidates that best replace the target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates according to how well they fit the context of the target word, and discards 50 % of the worst ranking candidates.', 'the bottom part of the table 1 ( selection step + substitution candidates ranking ) summarizes the results of all ranking systems after performing the selection step on generated substitution candidates.', '']",3
"['', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates']","['target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates']","['replace the target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates according to how well they fit the context of the target word, and discard']","['', 'the dssm ranking performs comparably to svm rank when using only n - gram probabilities as features, and consistently leverages all features described in section 2. 3, outperforming all systems in accuracy, precision and changed ratio.', 'we experimented with adding all features described in section 2. 3 to the baselines as well, however, we obtained no improvements compared to using only n - gram probability features.', 'we also tried running all ranking systems on selected candidates that best replace the target word in the input sentence.', 'we follow the unsupervised boundary ranking substitution selection method described in  #TAUTHOR_TAG, which ranks candidates according to how well they fit the context of the target word, and discards 50 % of the worst ranking candidates.', 'the bottom part of the table 1 ( selection step + substitution candidates ranking ) summarizes the results of all ranking systems after performing the selection step on generated substitution candidates.', '']",4
"['incorrect regular expressions  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG designed']","['incorrect regular expressions  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG designed']","['incorrect regular expressions  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG designed the deep - regex model based on the sequence - to - sequence']","['expressions are an efficient tool to represent structured data with specific rules in a variety of fields such as natural language processing or text classification.', 'however, it is not always an easy task to write an exact regular expression for those who do not have a deep knowledge of regular expressions or when the expression is very complicated, and incorrect or sloppy regular expressions may cause unexpected consequences in practice  #AUTHOR_TAG.', 'indeed, even a single character difference between regular expressions can cause them * now at google. to represent completely different sets of strings.', 'as such, researchers have begun working on a system than generates a regular expression from a natural language description provided by a human while reducing possible errors caused by incorrect regular expressions  #AUTHOR_TAG.', 'recently,  #TAUTHOR_TAG designed the deep - regex model based on the sequence - to - sequence ( seq2seq ) model  #AUTHOR_TAG using minimal domain knowledge during the learning phase while still accurately predicting regular expressions from nls.', ' #AUTHOR_TAG a ) improved the performance by training on not only syntactic content of the expressions ( i. e. the exact textual representation of the expression that was used ), but also the semantic content ( the regular language described by the expression ).', 'however, the reward function in the semregex model  #AUTHOR_TAG a ) that determines if the predicted regular expression is semantically equivalent to the ground truth expression is known to be pspace - complete and is a bottleneck in practice  #AUTHOR_TAG.', '']",0
"['related research.', ' #TAUTHOR_TAG proposed the deep - reg']","['related research.', ' #TAUTHOR_TAG proposed the deep - regex model based on seq2seq']","['related research.', ' #TAUTHOR_TAG proposed the deep - regex model based on seq2seq']","['regular expressions :  #AUTHOR_TAG studied rule - based techniques for the conversion between multi - languages and regular expressions.', ' #AUTHOR_TAG built a parsing model that translates a natural language sentence to a regular expression, and provided a dataset which is now a popular benchmark dataset for related research.', ' #TAUTHOR_TAG proposed the deep - regex model based on seq2seq for generating regular expressions from natural language descriptions together with a dataset of 10, 000 nl - rx pairs.', 'however, due to the limitations of the standard seq2seq model, the deep - regex model can only generate regular expressions similar in shape to the training data.', 'the semregex model improved the deep - regex model by using reinforcement learning based on the determinisitic finite automata ( dfa ) equivalence oracle ( which determines if two regular expressions describe the same language ) as a reward function.', 'this model can generate correct regular expressions that may not resemble the ground truth answer.', 'comparing regular expressions : a regular language can have several syntactically different regular expressions.', 'we say that two regular expressions are equivalent if they both define the same language.', 'the most basic method to deciding whether or not two regular expressions are equivalent is to convert both regular expressions to dfas.', 'however, the time and space complexity of converting regular expressions to dfas are both exponential  #AUTHOR_TAG.', ' #AUTHOR_TAG showed that it is pspace - complete to decide if two regular expressions generate the same set of words.', 'thus, deciding equivalence for two regular expressions is a bottleneck calculation even for small inputs']",0
[':  #TAUTHOR_TAG created a set of nl'],"[':  #TAUTHOR_TAG created a set of nl - rx pair data by arbitrarily creating and combining data in a tree form.', 'we define the depth of a regular expression in this dataset as the depth of the tree that generated the nl - rx pair ( see appendix a ).', 'similar to  #TAUTHOR_TAG, we randomly generate regular']",[':  #TAUTHOR_TAG created a set of nl'],"[':  #TAUTHOR_TAG created a set of nl - rx pair data by arbitrarily creating and combining data in a tree form.', 'we define the depth of a regular expression in this dataset as the depth of the tree that generated the nl - rx pair ( see appendix a ).', 'similar to  #TAUTHOR_TAG, we randomly generate regular expression pairs up to depth three and label the equivalence between each pair.', 'we sample approximately 200, 000 pairs using this method with a ratio of equivalent and non - equivalent pairs of about 2 : 1.', 'we prepare three sets of data having different depths for test data.', '']",0
"['- turk is made from ordinary people by paraphrasing nl descriptions in nl - rx - synth using mechanical turk  #TAUTHOR_TAG.', 'both datasets have']","['generated automatically and nl - rx - turk is made from ordinary people by paraphrasing nl descriptions in nl - rx - synth using mechanical turk  #TAUTHOR_TAG.', 'both datasets have 10, 000 pairs of nl - rx data.', 'we follow the previous']","['- turk is made from ordinary people by paraphrasing nl descriptions in nl - rx - synth using mechanical turk  #TAUTHOR_TAG.', 'both datasets have']","[': we use three public datasets to compare softregex with the - state - of - the - art model, sem - regex.', 'the kb13  #AUTHOR_TAG dataset was constructed by regex experts and is relatively small.', 'on the other hand, nl - rx - synth is data generated automatically and nl - rx - turk is made from ordinary people by paraphrasing nl descriptions in nl - rx - synth using mechanical turk  #TAUTHOR_TAG.', 'both datasets have 10, 000 pairs of nl - rx data.', 'we follow the previous work in splitting the data ( train : 65 %, dev : 10 %, test : 25 % ).', 'model settings : we arrange the softregex architecture based on semregex.', 'we embed the input sequence tokens with dimension size 128, stack two lstm layers, and set the hidden state dimension size to 256.', 'we set the batch size to 32 and learning rate to 0. 05 with the adadelta optimizer  #AUTHOR_TAG.', 'we substitute in the eq reg model as the reward function which gives high reward value 1 if our model generate a regular expression that is equivalent to the ground truth ( figure 2 )']",0
"['since  #TAUTHOR_TAG tried to obtain data from machine - generated sentences.', '']","['exponential time.', ' #AUTHOR_TAG b ) pointed out some problems in the nl - rx datasets.', 'specifically, there are some ambiguities since  #TAUTHOR_TAG tried to obtain data from machine - generated sentences.', 'thus, there are situations that even']","['.', 'though the speedup described in experimental result may appear constant, our softened equivalence approximately decides a pspace - complete problem in linear time to the length of regular expressions, which would otherwise take exponential time.', ' #AUTHOR_TAG b ) pointed out some problems in the nl - rx datasets.', 'specifically, there are some ambiguities since  #TAUTHOR_TAG tried to obtain data from machine - generated sentences.', '']","['', 'in the worst case ( the nl - rx - synth dataset ), we see that our new method is still 3. 6 times faster than that of semregex.', 'though the speedup described in experimental result may appear constant, our softened equivalence approximately decides a pspace - complete problem in linear time to the length of regular expressions, which would otherwise take exponential time.', ' #AUTHOR_TAG b ) pointed out some problems in the nl - rx datasets.', 'specifically, there are some ambiguities since  #TAUTHOR_TAG tried to obtain data from machine - generated sentences.', 'thus, there are situations that even expert humans cannot accurately classify.', 'on the other hand, the nl - rx - turk dataset is unreliable in that it is generated by non - experts who are paraphrasing previously generated data.', 'we investigate all 921 incorrect predictions of softregex in nl - rx - turk and categorize the resulting errors into 4 types ( type - 1 errors, we need to train more nl - rx data where a single nl is paired with several equivalent regex to cope']",0
"['now describe how  #TAUTHOR_TAG generated their synthetic regular expression data.', 'to begin,']","['now describe how  #TAUTHOR_TAG generated their synthetic regular expression data.', 'to begin,']","['now describe how  #TAUTHOR_TAG generated their synthetic regular expression data.', 'to begin,']","['now describe how  #TAUTHOR_TAG generated their synthetic regular expression data.', 'to begin, they manually mapped primitive regular expression operations, such as union and concatenation, to natural language.', 'they then defined a small alphabet on which the operations could be performed.', 'in the end, their system has 15 operations and 6 types of characters in the vocabulary.', 'from this, they were able to build up nl - rx pairs automatically by creating a parse tree of repeated applications of operations to an initial regular expression.', 'the operations ( non - terminals ) and alphabet ( terminals ) are shown in table 3.', 'figure 4 gives an example of how a nl - rx pair is generated by creating a parse tree.', 'it can be seen that it builds from the bottom up by starting with a series of words or characters ( terminals ) and applying some primitive operations ( non - terminals ).', 'simultaneously, natural language descriptions of terminals and non - terminals are composed together to create a semantically identical natural language description.', 'in this example, the nl - rx pair has depth 2.', ' #AUTHOR_TAG']",0
[':  #TAUTHOR_TAG created a set of nl'],"[':  #TAUTHOR_TAG created a set of nl - rx pair data by arbitrarily creating and combining data in a tree form.', 'we define the depth of a regular expression in this dataset as the depth of the tree that generated the nl - rx pair ( see appendix a ).', 'similar to  #TAUTHOR_TAG, we randomly generate regular']",[':  #TAUTHOR_TAG created a set of nl'],"[':  #TAUTHOR_TAG created a set of nl - rx pair data by arbitrarily creating and combining data in a tree form.', 'we define the depth of a regular expression in this dataset as the depth of the tree that generated the nl - rx pair ( see appendix a ).', 'similar to  #TAUTHOR_TAG, we randomly generate regular expression pairs up to depth three and label the equivalence between each pair.', 'we sample approximately 200, 000 pairs using this method with a ratio of equivalent and non - equivalent pairs of about 2 : 1.', 'we prepare three sets of data having different depths for test data.', '']",3
"['18 ].', 'in a more recent attempt  #TAUTHOR_TAG, we']","['recently [ 18 ].', 'in a more recent attempt  #TAUTHOR_TAG, we']","['18 ].', 'in a more recent attempt  #TAUTHOR_TAG, we further proposed a framework of discovering multi - level acoustic patterns with varying model granularity.', 'the different pattern hmm configurations ( number of states per model, number of distinct models ) form a two - dimensional model granularity space.', 'different sets of acoustic patterns with hmm model configurations represented by different points properly distributed over']","['', 'in a more recent attempt  #TAUTHOR_TAG, we further proposed a framework of discovering multi - level acoustic patterns with varying model granularity.', 'the different pattern hmm configurations ( number of states per model, number of distinct models ) form a two - dimensional model granularity space.', 'different sets of acoustic patterns with hmm model configurations represented by different points properly distributed over this two - dimensional space are complementary to one another, thus jointly capture the characteristics of the corpora considered.', 'such a multi - level framework was shown to be very helpful in the task of unsupervised spoken term detection ( std ) with spoken queries, because token matching can be performed with pattern indices on different levels of signal characteristics, and the information integration across multiple model granularities offered the improved performance.', 'in this work, we further propose an enhanced version of the multi - level acoustic patterns with varying model granularity by considering the context consistency for the decoded pattern sequences within each level and across different levels.', 'in other words, the acoustic patterns discovered on different levels are no longer trained completely independently.', 'we try to "" relabel "" the pattern sequence for each utterance in the training corpora considering the context consistency within and across levels.', 'for a certain level, the']",0
"['in fig. 1 corresponds to an acoustic pattern configuration.', 'note that in our previous work  #TAUTHOR_TAG, the effect of the third']","['in fig. 1 corresponds to an acoustic pattern configuration.', 'note that in our previous work  #TAUTHOR_TAG, the effect of the third dimension, the acoustic granularity which is']","['of temporal and phonetic granularities as in fig. 1.', 'any point in this two - dimensional space in fig. 1 corresponds to an acoustic pattern configuration.', 'note that in our previous work  #TAUTHOR_TAG, the effect of the third']","['an unlabeled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set ψ that determines the hmm configuration ( number of states per model and number of distinct models ) [ 20 ].', 'this can be achieved by first finding an initial label ω0 based on a set of assumed patterns for all observations in the corpus χ as in ( 1 ) [ 6 ].', 'then in each iteration t the hmm parameter set θ ψ t can be trained with the label ωt−1 obtained in the previous iteration as in ( 2 ), and the new label ωt can be obtained by pattern decoding with the obtained parameter set θ ψ t as in ( 3 ).', 'the training process can be repeated with enough number of iterations until a converged set of pattern hmms is obtained.', 'the above process can be performed with many different hmm configurations, each characterized by two hyperparameters : the number of states m in each acoustic pattern hmm, and the total number of distinct acoustic patterns n during initialization, ψ = ( m, n ).', 'the transcription of a signal decoded with these patterns can be considered as a temporal segmentation of the signal, so the hmm length ( or number of states in each hmm ) m represents the temporal granularity.', 'the set of all distinct acoustic patterns can be considered as a segmentation of the phonetic space, so the total number n of distinct acoustic patterns represents the phonetic granularity.', 'this gives a two - dimensional representation of the acoustic pattern configurations in terms of temporal and phonetic granularities as in fig. 1.', 'any point in this two - dimensional space in fig. 1 corresponds to an acoustic pattern configuration.', 'note that in our previous work  #TAUTHOR_TAG, the effect of the third dimension, the acoustic granularity which is the number of gaussians in each state, was shown to be negligible, thus here we simply set the number of gaussians in each state to be 4 in all cases.', 'although the selection of the hyperparameters can be arbitrary in this two - dimensional space, here we only select m temporal granularities and n phonetic granularities, forming a two - dimensional array of m × n hyperparameter sets in the granularity space']",0
"['be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['the m × n different pattern sequences based on the m × n different pattern sets can be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['', 'with indices ( d1, d2,..., dd', ') and the query q into a sequence of q patterns with indices ( q1,..., qq ). we thus construct a matching matrix w of size d', '× q for every document - query pair, in which each entry ( i, j ) is the similarity between acoustic patterns with indices di and qj as in ( 8 ) and shown in fig. 4 ( a ) for a simple example of q = 3 and d', '= 6, where s ( i, j ) is defined in ( 7 ), it is possible to consider the n - best pattern sequences rather than the one - best sequences here by considering the posteriorgram vectors based', 'on the n - best sequences for d, q and integrate them in the matrix w. however, previous experiments showed that the extra improvements brought', 'in this way is almost negligible, probably because the m × n different pattern sequences based on the m × n different pattern sets can be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG. for matching the sub -', '']",0
"['be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['the m × n different pattern sequences based on the m × n different pattern sets can be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['', 'with indices ( d1, d2,..., dd', ') and the query q into a sequence of q patterns with indices ( q1,..., qq ). we thus construct a matching matrix w of size d', '× q for every document - query pair, in which each entry ( i, j ) is the similarity between acoustic patterns with indices di and qj as in ( 8 ) and shown in fig. 4 ( a ) for a simple example of q = 3 and d', '= 6, where s ( i, j ) is defined in ( 7 ), it is possible to consider the n - best pattern sequences rather than the one - best sequences here by considering the posteriorgram vectors based', 'on the n - best sequences for d, q and integrate them in the matrix w. however, previous experiments showed that the extra improvements brought', 'in this way is almost negligible, probably because the m × n different pattern sequences based on the m × n different pattern sets can be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG. for matching the sub -', '']",0
"['be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['the m × n different pattern sequences based on the m × n different pattern sets can be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['', 'with indices ( d1, d2,..., dd', ') and the query q into a sequence of q patterns with indices ( q1,..., qq ). we thus construct a matching matrix w of size d', '× q for every document - query pair, in which each entry ( i, j ) is the similarity between acoustic patterns with indices di and qj as in ( 8 ) and shown in fig. 4 ( a ) for a simple example of q = 3 and d', '= 6, where s ( i, j ) is defined in ( 7 ), it is possible to consider the n - best pattern sequences rather than the one - best sequences here by considering the posteriorgram vectors based', 'on the n - best sequences for d, q and integrate them in the matrix w. however, previous experiments showed that the extra improvements brought', 'in this way is almost negligible, probably because the m × n different pattern sequences based on the m × n different pattern sets can be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG. for matching the sub -', '']",7
"['be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['the m × n different pattern sequences based on the m × n different pattern sets can be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG.']","['', 'with indices ( d1, d2,..., dd', ') and the query q into a sequence of q patterns with indices ( q1,..., qq ). we thus construct a matching matrix w of size d', '× q for every document - query pair, in which each entry ( i, j ) is the similarity between acoustic patterns with indices di and qj as in ( 8 ) and shown in fig. 4 ( a ) for a simple example of q = 3 and d', '= 6, where s ( i, j ) is defined in ( 7 ), it is possible to consider the n - best pattern sequences rather than the one - best sequences here by considering the posteriorgram vectors based', 'on the n - best sequences for d, q and integrate them in the matrix w. however, previous experiments showed that the extra improvements brought', 'in this way is almost negligible, probably because the m × n different pattern sequences based on the m × n different pattern sets can be considered as a huge', 'lattice including many one - best paths which will be jointly considered here  #TAUTHOR_TAG. for matching the sub -', '']",7
"['extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social']","['extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social events', '']","['presented a preliminary system that uses tree kernels and support vector machines (', 'svms ) to extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social']","['presented a preliminary system that uses tree kernels and support vector machines (', 'svms ) to extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social events', 'gives us insight into the roles of characters in the story. also, static network analysis has limitations which become apparent from', 'our analysis. we propose the use of dynamic network analysis to overcome these limitations. in agar', '##wal et al. the social event extraction task and show', 'that our system trained on a news corpus using tree kernels and support vector machines beats the baseline', 'systems by a statistically significant margin. we also show that while', '']",0
"['extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social']","['extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social events', '']","['presented a preliminary system that uses tree kernels and support vector machines (', 'svms ) to extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social']","['presented a preliminary system that uses tree kernels and support vector machines (', 'svms ) to extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social events', 'gives us insight into the roles of characters in the story. also, static network analysis has limitations which become apparent from', 'our analysis. we propose the use of dynamic network analysis to overcome these limitations. in agar', '##wal et al. the social event extraction task and show', 'that our system trained on a news corpus using tree kernels and support vector machines beats the baseline', 'systems by a statistically significant margin. we also show that while', '']",0
"['extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social']","['extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social events', '']","['presented a preliminary system that uses tree kernels and support vector machines (', 'svms ) to extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social']","['presented a preliminary system that uses tree kernels and support vector machines (', 'svms ) to extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social events', 'gives us insight into the roles of characters in the story. also, static network analysis has limitations which become apparent from', 'our analysis. we propose the use of dynamic network analysis to overcome these limitations. in agar', '##wal et al. the social event extraction task and show', 'that our system trained on a news corpus using tree kernels and support vector machines beats the baseline', 'systems by a statistically significant margin. we also show that while', '']",0
"['extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social']","['extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social events', '']","['presented a preliminary system that uses tree kernels and support vector machines (', 'svms ) to extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social']","['presented a preliminary system that uses tree kernels and support vector machines (', 'svms ) to extract social events from news articles', '. in  #TAUTHOR_TAG, we presented a case study on a manually extracted network from alice in wonderland, showing that analyzing networks based on these social events', 'gives us insight into the roles of characters in the story. also, static network analysis has limitations which become apparent from', 'our analysis. we propose the use of dynamic network analysis to overcome these limitations. in agar', '##wal et al. the social event extraction task and show', 'that our system trained on a news corpus using tree kernels and support vector machines beats the baseline', 'systems by a statistically significant margin. we also show that while', '']",0
"[' #AUTHOR_TAG ], creation of classical or pop music  #TAUTHOR_TAG ] and automatic images generation [ van den  #AUTHOR_TAG ].', 'whereas there are few researches exploring the possibility of artificial imagination']","['have been conducted to involve ai into poem generation [  #AUTHOR_TAG ], creation of classical or pop music  #TAUTHOR_TAG ] and automatic images generation [ van den  #AUTHOR_TAG ].', 'whereas there are few researches exploring the possibility of artificial imagination']","[' #AUTHOR_TAG ], creation of classical or pop music  #TAUTHOR_TAG ] and automatic images generation [ van den  #AUTHOR_TAG ].', 'whereas there are few researches exploring the possibility of artificial imagination']","[', ai demonstrates stronger potential for art creation.', 'many researches have been conducted to involve ai into poem generation [  #AUTHOR_TAG ], creation of classical or pop music  #TAUTHOR_TAG ] and automatic images generation [ van den  #AUTHOR_TAG ].', 'whereas there are few researches exploring the possibility of artificial imagination for artwork creation.', 'to generate an imaginative and creative mind map, the key problem is automatic topic expansion.', 'there are several challenges : first of all, language is an informative system.', 'thus many linguistic features should be considered in words and relation extension.', ""second, as an artwork, it should reflect artist's mind and individual experience."", 'third, for mind map creation, artists usually pay less attention on defining accurate relation between concepts, instead, they are defined with imagination.', 'so, to build artistic mind map, the system should find a way to break the convention rules for the connection between literal representations']",0
"['increase information variety  #TAUTHOR_TAG during topic expansion.', '']","['increase information variety  #TAUTHOR_TAG during topic expansion.', '']","['increase information variety  #TAUTHOR_TAG during topic expansion.', '']","['imagination is the soul for artistic mind map, mappa mundi employs several features to increase information variety  #TAUTHOR_TAG during topic expansion.', 'it firstly uses word embeddings to find candidates based on semantic similarity [  #AUTHOR_TAG ].', 'to enrich linguistic information of expansions, it also takes the morphological and phonological features into account.', 'words sharing similar characters or morphemes or phonetic syllables are selected as candidates.', ""moreover, we construct a knowledge graph ( kg ) by extracting concepts and relations from artist's original mind map masterpieces."", 'mappa mundi will adopt those concepts and relations once a topic word is covered by the kg.', ""by doing so, it can better imitate artists'mind and thoughts."", ""further, mappa mundi breaks the restriction of domains and conventions and explores more possibilities for words'connections by creating rules following the famous dadaism principle in art movement [  #AUTHOR_TAG ]."", 'finally, with all above mentioned methods, mappa mundi can create a branch of creative and informative word candidates for a given keyword']",0
['chinese and stanford parser  #TAUTHOR_TAG'],['chinese and stanford parser  #TAUTHOR_TAG'],['chinese and stanford parser  #TAUTHOR_TAG'],"['speech recognition ( asr ) engine is designed for interaction.', 'both english and chinese languages are supported.', 'we also performed lexicon and language model adaption to enhance the recognition of words in art domain.', 'after that, we developed a keyword extraction function to extract meaningful words from the converted text for further expansion.', 'we adopted open - source software jieba 2 for chinese and stanford parser  #TAUTHOR_TAG for english pos tagging.', 'only informative words such as noun, verb and adjective words are kept and tfidf weights are calculated for further filtering.', 'the output of this module is the keywords']",5
"['discussions  #TAUTHOR_TAG, and by indicating which utterances contain particular types of']","['discussions  #TAUTHOR_TAG, and by indicating which utterances contain particular types of information,']","['of decision discussions  #TAUTHOR_TAG, and by indicating which utterances contain particular types of']",[' #TAUTHOR_TAG'],0
"[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","['oriented meeting transcripts. only very recent research has', 'specifically investigated the automatic detection of decisions, namely  #AUTHOR_TAG and  #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are "" decision -', '']",0
"[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","['oriented meeting transcripts. only very recent research has', 'specifically investigated the automatic detection of decisions, namely  #AUTHOR_TAG and  #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are "" decision -', '']",0
"[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","['oriented meeting transcripts. only very recent research has', 'specifically investigated the automatic detection of decisions, namely  #AUTHOR_TAG and  #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are "" decision -', '']",0
"['##s.', 'the reader can find a comparison between these annotations and our own manual transcript annotations in  #TAUTHOR_TAG.', 'after obtaining the new off - line and real - time asr transcripts, we transferred the dd']","['manual transcriptions.', 'the reader can find a comparison between these annotations and our own manual transcript annotations in  #TAUTHOR_TAG.', 'after obtaining the new off - line and real - time asr transcripts, we transferred the dda annotations from the manual transcripts.', 'in both sets of asr transcripts,']","['for the four dda classes.', '( 2007 ) are part of the ami corpus, and are for the manual transcriptions.', 'the reader can find a comparison between these annotations and our own manual transcript annotations in  #TAUTHOR_TAG.', 'after obtaining the new off - line and real - time asr transcripts, we transferred the dda annotations from the manual transcripts.', 'in both sets of asr transcripts, each meeting contains on']","[', two individuals used the annotation scheme described above in order to annotate the manual transcripts of 9 and 10 meetings respectively.', 'the annotators overlapped on two meetings, and their kappa inter - annotator agreement ranged from 0. 63 to 0. 73 for the four dda classes.', '( 2007 ) are part of the ami corpus, and are for the manual transcriptions.', 'the reader can find a comparison between these annotations and our own manual transcript annotations in  #TAUTHOR_TAG.', 'after obtaining the new off - line and real - time asr transcripts, we transferred the dda annotations from the manual transcripts.', 'in both sets of asr transcripts, each meeting contains on average around 26 das tagged with one or more of the dda sub - classes in table 1.', 'ddas are thus very sparse, corresponding to only 5. 3 % of utterances in the real - time transcripts, and 6. 0 % in the offline.', '']",0
[' #TAUTHOR_TAG described an'],[' #TAUTHOR_TAG described an'],"['.', '7 conclusion  #TAUTHOR_TAG described an approach to']","['', '7 conclusion  #TAUTHOR_TAG described an approach to decision detection in multi - party meetings and demonstrated how it could work relatively well in an off - line system.', 'the approach has two defining characteristics.', 'the first is its use of an annotation scheme which distinguishes between different utterance types based on the roles which they play in the decision - making process.', 'the second is its use of hierarchical classification, whereby binary sub - classifiers detect instances of each of the decision das ( ddas ), and then based on the sub - classifier hypotheses, a super - classifier determines which regions of dialogue are decision discussions.', 'in this paper then, we have taken the same basic approach to decision detection as  #TAUTHOR_TAG, but changed the way in which it is implemented so that it can work effectively in realtime.', 'our implementation changes include running the detector at regular and frequent intervals during the meeting, and reprocessing recent utterances in case a decision discussion straddles these and brand new utterances.', 'the fact that the detector reprocesses utterances means that on consecutive runs, overlapping and duplicate']",0
[' #TAUTHOR_TAG described an'],[' #TAUTHOR_TAG described an'],"['.', '7 conclusion  #TAUTHOR_TAG described an approach to']","['', '7 conclusion  #TAUTHOR_TAG described an approach to decision detection in multi - party meetings and demonstrated how it could work relatively well in an off - line system.', 'the approach has two defining characteristics.', 'the first is its use of an annotation scheme which distinguishes between different utterance types based on the roles which they play in the decision - making process.', 'the second is its use of hierarchical classification, whereby binary sub - classifiers detect instances of each of the decision das ( ddas ), and then based on the sub - classifier hypotheses, a super - classifier determines which regions of dialogue are decision discussions.', 'in this paper then, we have taken the same basic approach to decision detection as  #TAUTHOR_TAG, but changed the way in which it is implemented so that it can work effectively in realtime.', 'our implementation changes include running the detector at regular and frequent intervals during the meeting, and reprocessing recent utterances in case a decision discussion straddles these and brand new utterances.', 'the fact that the detector reprocesses utterances means that on consecutive runs, overlapping and duplicate']",0
"[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","['oriented meeting transcripts. only very recent research has', 'specifically investigated the automatic detection of decisions, namely  #AUTHOR_TAG and  #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are "" decision -', '']",4
"[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","[' #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are']","['oriented meeting transcripts. only very recent research has', 'specifically investigated the automatic detection of decisions, namely  #AUTHOR_TAG and  #TAUTHOR_TAG.  #AUTHOR_TAG used the ami meeting corpus, and attempted to automatically identify dialogue acts ( das ) in meeting transcripts which are "" decision -', '']",4
"['vs.. 44 ).', 'hence, while  #TAUTHOR_TAG demonstrated that the hierarchical classification']","['(. 54 vs.. 44 ).', 'hence, while  #TAUTHOR_TAG demonstrated that the hierarchical classification']","['vs.. 44 ).', 'hence, while  #TAUTHOR_TAG demonstrated that the hierarchical classification approach could improve off - line decision detection, we have demonstrated here that it can also']","['- line detector, which are shown in table 4.', 'the f1 - scores for the real - time and off - line decision super - classifiers are. 54 and. 55 respectively, and the difference is not statistically significant.', 'this may indicate that the hierarchical classification approach is fairly robust to increasing asr word error rates ( wers ).', 'combining the output from each of the independent sub - classifiers might compensate somewhat for any decreases in their individual accuracy, as there was here for the i and rp sub - classifiers.', ""the hierarchical real - time detector's f1 - score is also 10 points higher than a flat classifier (. 54 vs.. 44 )."", 'hence, while  #TAUTHOR_TAG demonstrated that the hierarchical classification approach could improve off - line decision detection, we have demonstrated here that it can also improve realtime decision detection.', 'table 5 shows the results when an off - line detector is applied to real - time asr transcripts.', 'here, the super - classifier obtains an f1 - score of. 55, one point higher than the real - time detector, but again, the difference is not statistically significant']",4
[' #TAUTHOR_TAG described an'],[' #TAUTHOR_TAG described an'],"['.', '7 conclusion  #TAUTHOR_TAG described an approach to']","['', '7 conclusion  #TAUTHOR_TAG described an approach to decision detection in multi - party meetings and demonstrated how it could work relatively well in an off - line system.', 'the approach has two defining characteristics.', 'the first is its use of an annotation scheme which distinguishes between different utterance types based on the roles which they play in the decision - making process.', 'the second is its use of hierarchical classification, whereby binary sub - classifiers detect instances of each of the decision das ( ddas ), and then based on the sub - classifier hypotheses, a super - classifier determines which regions of dialogue are decision discussions.', 'in this paper then, we have taken the same basic approach to decision detection as  #TAUTHOR_TAG, but changed the way in which it is implemented so that it can work effectively in realtime.', 'our implementation changes include running the detector at regular and frequent intervals during the meeting, and reprocessing recent utterances in case a decision discussion straddles these and brand new utterances.', 'the fact that the detector reprocesses utterances means that on consecutive runs, overlapping and duplicate']",4
['annotation scheme as  #TAUTHOR_TAG in'],['annotation scheme as  #TAUTHOR_TAG in'],['as  #TAUTHOR_TAG in'],"['use the same annotation scheme as  #TAUTHOR_TAG in order to model decision - making dialogue.', 'as stated in section 2, this scheme distinguishes between a small number of dialogue act types based on the role which they perform in the formulation of a decision.', '']",5
"['- match metric as  #TAUTHOR_TAG, which allows a margin of 20 seconds preceding']","['lenient - match metric as  #TAUTHOR_TAG, which allows a margin of 20 seconds preceding']","['- match metric as  #TAUTHOR_TAG, which allows a margin of 20 seconds preceding']","['', ""3. the super - classifier's detection of decision discussion regions."", 'for 1 and 2, we use the same lenient - match metric as  #TAUTHOR_TAG, which allows a margin of 20 seconds preceding and following a hypothesized dda.', 'note that here we only give credit for hypotheses based on a 1 - 1 mapping with the gold - standard labels.', 'for 3, we follow  #TAUTHOR_TAG and use a windowed metric that divides the dialogue into 30 - second windows and evaluates on a per window basis']",5
"['- match metric as  #TAUTHOR_TAG, which allows a margin of 20 seconds preceding']","['lenient - match metric as  #TAUTHOR_TAG, which allows a margin of 20 seconds preceding']","['- match metric as  #TAUTHOR_TAG, which allows a margin of 20 seconds preceding']","['', ""3. the super - classifier's detection of decision discussion regions."", 'for 1 and 2, we use the same lenient - match metric as  #TAUTHOR_TAG, which allows a margin of 20 seconds preceding and following a hypothesized dda.', 'note that here we only give credit for hypotheses based on a 1 - 1 mapping with the gold - standard labels.', 'for 3, we follow  #TAUTHOR_TAG and use a windowed metric that divides the dialogue into 30 - second windows and evaluates on a per window basis']",5
[' #TAUTHOR_TAG was unable to derive any value from them with svms'],[' #TAUTHOR_TAG was unable to derive any value from them with svms'],[' #TAUTHOR_TAG was unable to derive any value from them with svms'],"['classifiers ( sub and super - classifiers ) in all detectors are linear - kernel support vector machines ( svms ), produced using svmlight  #AUTHOR_TAG.', 'for the sub - classifiers, we are obviously restricted to using features which can be computed in a very short period of time, and in the experiments here, we use lexical, utterance and speaker features.', 'these are summarized in table 2.', ""an utterance's lexical features are the words in its transcription, its utterance features are its duration, number of words, and word rate ( number of words divided by duration ), and its speaker features are the speaker's role ( see section 3 ) and id."", 'we also use lexical features for the previous and where available, next utterances : the i, rp and rr sub - classifiers use the lexical features for the previous / next utterance and the a sub - classifier, those from the previous / next 5 utterances.', 'these settings produced the best results in preliminary experiments.', 'we do not use da features because we lack an automatic da tagger, nor do we use prosodic features because  #TAUTHOR_TAG was unable to derive any value from them with svms']",7
"['accuracy for listener models on the color reference task from  #TAUTHOR_TAG, and the effect demonstrated by this']","['for learning.', 'in particular, we show that incorporating pragmatic reasoning at training time yields improved, state - of - the - art accuracy for listener models on the color reference task from  #TAUTHOR_TAG, and the effect demonstrated by this']","['2 ).', 'of the training data.', 'we compare pragmatic and non - pragmatic models at training and at test, while varying conditions on the training data to test hypotheses regarding the utility of pragmatic inference for learning.', 'in particular, we show that incorporating pragmatic reasoning at training time yields improved, state - of - the - art accuracy for listener models on the color reference task from  #TAUTHOR_TAG, and the effect demonstrated by this improvement is especially large under small training data sizes.', 'we further introduce a new color - grid reference task and data set consisting of higher dimensional objects and more complex speaker language ; we find that the effect of pragmatic listener training is even larger in this setting']","['x x x x x "" left dark blue "" "" dark blue "", "" left dark "", "" right black ""...', 'table 1 : speaker utterances describing ( 1 ) colors and ( 2 ) color grids to differentiate them from distractors.', 'a learner might draw inferences about fine - grained linguistic distinctions by explaining the speaker\'s failure to use cheaper alternatives in context ( e. g. they might infer that "" blue "" and "" dark "" apply to some distractors in 1 ).', 'these inferences have the potential to increase in number and in strength as dimensionality of the referents and utterance complexity increase ( as in 2 ).', 'of the training data.', 'we compare pragmatic and non - pragmatic models at training and at test, while varying conditions on the training data to test hypotheses regarding the utility of pragmatic inference for learning.', 'in particular, we show that incorporating pragmatic reasoning at training time yields improved, state - of - the - art accuracy for listener models on the color reference task from  #TAUTHOR_TAG, and the effect demonstrated by this improvement is especially large under small training data sizes.', 'we further introduce a new color - grid reference task and data set consisting of higher dimensional objects and more complex speaker language ; we find that the effect of pragmatic listener training is even larger in this setting']",4
"['in  #TAUTHOR_TAG, table 2 shows that our pragmatically']","['in  #TAUTHOR_TAG, table 2 shows that our pragmatically']","['in  #TAUTHOR_TAG, table 2 shows that our pragmatically']","['accuracies of color target predictions by l 0 and l 1 models under both l 0 and l 1 training are shown in the left columns of table 2.', 'for robustness, average accuracies and standard errors were computed by repeatedly retraining and evaluating with different weight initializations and training data orderings using 4 different random seeds.', 'the results in the top left panel of table 2 show that l 1 pragmatic training coupled with l 1 pragmatic evaluation gives the best average accuracies.', 'the previously studied l 1 pragmatic usage with l 0 non - pragmatic training is next best.', 'these results provide evidence that literal meanings estimated through pragmatic training are better calibrated for pragmatic usage than meanings estimated through non - pragmatic l 0 training.', 'furthermore, relative to state - of - the - art in  #TAUTHOR_TAG, table 2 shows that our pragmatically trained model yields improved accuracy over their best "" blended "" pragmatic l e model which computed predictions based on the product of two separate non - pragmatically trained models.', 'the effect sizes are small for the pragmatic to non - pragmatic comparisons when training on the full color data ( though approaching the ceiling 0. 9108 human accuracy ), but we hypothesized that the effect of pragmatic training would increase for training with smaller data sizes ( as motivated by arguments in the introduction ).', 'to test this, we trained the listener models on smaller subsets of the training data, and evaluated accuracy.', 'as shown by the top left plot of figure 4, pragmatic training results in a larger gain in accuracy when less data is available for training.', 'lastly, we also considered the effect of pragmatic training under the varying close, split, and far data conditions.', 'as shown in the three plots at the right of the top row of figure 4, the effect of l 1 training over l 0 is especially pronounced for inferences on close and split data conditions where the target is more similar to the distractors, and the language is more context - dependent.', 'this makes sense, as these conditions contain examples where the pragmatic, cost - sensitive adjustments to the learned meanings would be the most necessary']",4
"['similar to  #TAUTHOR_TAG, we resolve this issue by taking a small set of samples from']","['the language model is pre - trained over speaker utterances paired with targets, but the support of the distribution encoded by this lstm is too large for the s 1 normalization term within the rsa listener to be computed efficiently. similar to  #TAUTHOR_TAG, we resolve this issue by taking a small set of samples from the pre -', 'trained lstm applied to']","['similar to  #TAUTHOR_TAG, we resolve this issue by taking a small set of samples from the pre -', 'trained lstm applied to each object in a context, to approximate p ( u | o ), each time l 1 is computed during training and evaluation']","['', 'in ( 0, 1 ). this neural net contains all learnable parameters θ of our listeners. utterance prior the utterance prior p ( u | o ) in s 1 is a', 'non - uniform distribution over sequences of english tokens - represented by a pre - trained lstm language model conditioned on an input color or grid ( see figure 2a ).', 'similar to the literal meaning lstm, we apply a linear transformation to the input object to initialize the lstm hidden state. then, each step of the lstm applies to and outputs', 'successive tokens of an utterance. in addition, when operating over grid inputs, we apply a layer of multiplicative attention given by the "" general "" scoring function in  #AUTHOR_TAG between the lstm output and the convolutional grid output before the final softmax.', 'this allows the language model to "" attend "" to individual grid cells when producing output tokens, yielding an improvement in utterance prior sample', 'quality. the language model is pre - trained over speaker utterances paired with targets, but the support of the distribution encoded by this lstm is too large for the s 1 normalization term within the rsa listener to be computed efficiently. similar to  #TAUTHOR_TAG, we resolve this issue by taking a small set of samples from the pre -', 'trained lstm applied to each object in a context, to approximate p ( u | o ), each time l 1 is computed during training and evaluation']",3
"[':', 'sample  #TAUTHOR_TAG.', 'both architectures']","['in a round 28 :', 'sample  #TAUTHOR_TAG.', 'both architectures']","['in a round 28 :', 'sample  #TAUTHOR_TAG.', 'both architectures apply a tanh layer to']","['', 'sample  #TAUTHOR_TAG.', 'both architectures apply a tanh layer to an input object o ( a grid or color ), and use the result as the initial hidden state of an lstm layer.', 'in each case, the lstm operates over embeddings of tokens u 1']",5
['by  #TAUTHOR_TAG from human play on the color reference task through amazon mechanical turk using the'],['by  #TAUTHOR_TAG from human play on the color reference task through amazon mechanical turk using the'],"['by  #TAUTHOR_TAG from human play on the color reference task through amazon mechanical turk using the framework of  #AUTHOR_TAG.', 'each game consists of 50 rounds played by a human speaker and listener.', ""in each round, the speaker describes a target color surrounded by a context of two other distractor colors, and a listener clicks on the targets based on the speaker's description ( see figure 1a )."", 'the resulting data consists of']","['the color reference task, we use the data collected by  #TAUTHOR_TAG from human play on the color reference task through amazon mechanical turk using the framework of  #AUTHOR_TAG.', 'each game consists of 50 rounds played by a human speaker and listener.', ""in each round, the speaker describes a target color surrounded by a context of two other distractor colors, and a listener clicks on the targets based on the speaker's description ( see figure 1a )."", 'the resulting data consists of 46, 994 rounds across 948 games, where the colors of some rounds are sampled to be more likely to require pragmatic reasoning than others.', '']",5
"['most 12 on examples where human listeners picked the correct color.', 'we follow  #TAUTHOR_TAG for language model hyper - parameters, with embedding and lstm layers of size 100.', 'also following this prior']","['most 12 on examples where human listeners picked the correct color.', 'we follow  #TAUTHOR_TAG for language model hyper - parameters, with embedding and lstm layers of size 100.', 'also following this prior work, we use']","['most 12 on examples where human listeners picked the correct color.', 'we follow  #TAUTHOR_TAG for language model hyper - parameters, with embedding and lstm layers of size 100.', 'also following this prior work, we use']","['implement our models in pytorch  #AUTHOR_TAG, and train them using the adam variant of stochastic gradient descent  #AUTHOR_TAG with default parameters ( β 1, β 2 ) = ( 0. 9, 0. 999 ) and = 10 −8.', 'we train with early - stopping based on dev set log - likelihood ( for speaker ) or accuracy ( for listener ) model evaluations.', 'before training our listeners, we pre - train an lstm language model to provide samples for the utterance priors on target colors paired with speaker utterances of length at most 12 on examples where human listeners picked the correct color.', 'we follow  #TAUTHOR_TAG for language model hyper - parameters, with embedding and lstm layers of size 100.', '']",5
"['most 12 on examples where human listeners picked the correct color.', 'we follow  #TAUTHOR_TAG for language model hyper - parameters, with embedding and lstm layers of size 100.', 'also following this prior']","['most 12 on examples where human listeners picked the correct color.', 'we follow  #TAUTHOR_TAG for language model hyper - parameters, with embedding and lstm layers of size 100.', 'also following this prior work, we use']","['most 12 on examples where human listeners picked the correct color.', 'we follow  #TAUTHOR_TAG for language model hyper - parameters, with embedding and lstm layers of size 100.', 'also following this prior work, we use']","['implement our models in pytorch  #AUTHOR_TAG, and train them using the adam variant of stochastic gradient descent  #AUTHOR_TAG with default parameters ( β 1, β 2 ) = ( 0. 9, 0. 999 ) and = 10 −8.', 'we train with early - stopping based on dev set log - likelihood ( for speaker ) or accuracy ( for listener ) model evaluations.', 'before training our listeners, we pre - train an lstm language model to provide samples for the utterance priors on target colors paired with speaker utterances of length at most 12 on examples where human listeners picked the correct color.', 'we follow  #TAUTHOR_TAG for language model hyper - parameters, with embedding and lstm layers of size 100.', '']",5
['did not use syntactic features  #TAUTHOR_TAG'],['did not use syntactic features  #TAUTHOR_TAG'],"['the representation of relations.', 'our supervised base model will be similar to  #AUTHOR_TAG.', 'our initial experiments did not use syntactic features  #TAUTHOR_TAG']","['extraction is typically reduced to a classification problem.', 'a supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities.', 'traditional methods rely on linguistic or semantic features  #AUTHOR_TAG, or kernels based on syntax or sequences  #AUTHOR_TAG a, b ;  #AUTHOR_TAG to represent sentences of relations.', 'more recently, deep neural nets start to show promising results.', 'most rely on convolutional neural nets  #AUTHOR_TAG ( zeng et al.,, 2015  #AUTHOR_TAG 2016 ;  #AUTHOR_TAG or recurrent neural nets  #AUTHOR_TAG to learn the representation of relations.', 'our supervised base model will be similar to  #AUTHOR_TAG.', 'our initial experiments did not use syntactic features  #TAUTHOR_TAG that require additional parsers.', 'in order to further improve the representation learning for relation extraction, tried to transfer knowledge through bilingual representation.', 'they used their multi - task model to train on the bilingual ace05 datasets and obtained improvement when there is less training available ( 10 % - 50 % ).', 'our experiments will show our multitask model can make significant improvement on the full training set.', 'in terms of the regularization to the representation,  #AUTHOR_TAG used l2 regularization between the parameters of the same part of two models in multi - task learning.', 'their method is a kind of soft - parameter sharing, which does not involve sharing any part of the model directly.', ' #AUTHOR_TAG applied domain adversarial networks  #AUTHOR_TAG to relation extraction and obtained improvement on out - of - domain evaluation.', 'inspired by the adversarial training, we attempt to use it as a regularization tool in our multi - task model and find some improvement']",4
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",0
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",0
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",0
"[' #TAUTHOR_TAG set, and']","[' #TAUTHOR_TAG set, and']","[').', 'previous work  #TAUTHOR_TAG set, and the other half of bc, cts']","['apply the multi - task learning, we need at least two datasets.', 'we pick ace05 and ere for our case study.', 'the ace05 dataset provides a cross - domain evaluation setting.', 'it contains 6 domains : broadcast conversation ( bc ), broadcast news ( bn ), telephone conversation ( cts ), newswire ( nw ), usenet ( un ) and weblogs ( wl ).', 'previous work  #TAUTHOR_TAG set, and the other half of bc, cts and wl as the test sets.', 'we followed their split of documents and their split of the relation types for asymmetric relations.', 'the ere dataset has a similar relation schema to ace05, but is different in some annotation guidelines  #AUTHOR_TAG.', 'it also has more data than ace05, which we expect to be helpful in the multi - task learning.', 'it contains documents from newswire and discussion forums.', 'we did not find an existing split of this dataset, so we randomly split the documents into train ( 80 % ), dev ( 10 % ) and test ( 10 % )']",0
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",1
"[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation']","[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation']","[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation learning from scratch first.', 'our experiments focus on whether']","['separately on the two corpora ( row "" supervised "" in table 1 ), we obtain results on ace05 comparable to previous work  #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation learning from scratch first.', 'our experiments focus on whether we can improve the representation with more sources of data.', 'a common way to do so is pre - training.', 'as a baseline, we pre - train the encoder of the supervised model on ere and then fine - tune on ace05, and vice versa ( row "" pretraining "" in table 1 ).', 'we observe improvement on both fine - tuned datasets.', '']",1
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",5
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",5
"[' #TAUTHOR_TAG set, and']","[' #TAUTHOR_TAG set, and']","[').', 'previous work  #TAUTHOR_TAG set, and the other half of bc, cts']","['apply the multi - task learning, we need at least two datasets.', 'we pick ace05 and ere for our case study.', 'the ace05 dataset provides a cross - domain evaluation setting.', 'it contains 6 domains : broadcast conversation ( bc ), broadcast news ( bn ), telephone conversation ( cts ), newswire ( nw ), usenet ( un ) and weblogs ( wl ).', 'previous work  #TAUTHOR_TAG set, and the other half of bc, cts and wl as the test sets.', 'we followed their split of documents and their split of the relation types for asymmetric relations.', 'the ere dataset has a similar relation schema to ace05, but is different in some annotation guidelines  #AUTHOR_TAG.', 'it also has more data than ace05, which we expect to be helpful in the multi - task learning.', 'it contains documents from newswire and discussion forums.', 'we did not find an existing split of this dataset, so we randomly split the documents into train ( 80 % ), dev ( 10 % ) and test ( 10 % )']",5
"['with 300 dimensions from word2vec  #AUTHOR_TAG.', 'we fix the word embeddings during the training.', 'we follow  #TAUTHOR_TAG to set the position and entity type embedding size to be 50.', '']","['use word embedding pre - trained on newswire with 300 dimensions from word2vec  #AUTHOR_TAG.', 'we fix the word embeddings during the training.', 'we follow  #TAUTHOR_TAG to set the position and entity type embedding size to be 50.', '']","['with 300 dimensions from word2vec  #AUTHOR_TAG.', 'we fix the word embeddings during the training.', 'we follow  #TAUTHOR_TAG to set the position and entity type embedding size to be 50.', 'we use 150 dimensions for']","['use word embedding pre - trained on newswire with 300 dimensions from word2vec  #AUTHOR_TAG.', 'we fix the word embeddings during the training.', 'we follow  #TAUTHOR_TAG to set the position and entity type embedding size to be 50.', '']",5
"['path embedding  #TAUTHOR_TAG.', 'similar']","['path embedding  #TAUTHOR_TAG.', 'similar']","['dep path embedding  #TAUTHOR_TAG.', 'similar']","['ace05 has been studied for a long time, numerous features have been found to be effective on this dataset.', ' #AUTHOR_TAG incorporated some of those features into the neural net and beat the state - of - art on the dataset.', 'although representation learning from scratch could be more general across multiple datasets, we compare the effect of multi - task learning with extra features on this specific dataset.', 'we add chunk embedding and on dep path embedding  #TAUTHOR_TAG.', ""similar to entity type embedding, chunk embedding is created according to each token's chunk type, we set the embedding size to 50."", 'on dep path embedding is a vector indicating whether the token is on the dependency path between the two entities.', 'in the multi - task model, the shared encoder is a bidirectional rnn ( birnn ) without attention ( equation ( 1 - 3 ) ).', 'these two embeddings will be concatenated to the output of the birnn to obtain the new h i and then passed to equation ( 4 ).', 'as the results ( table 2 ), our supervised baseline is slightly worse than the previous state - of - the - art neural model with extra features, but the multitask learning can consistently help.', 'the improvement is more obvious with 50 % training data.', 'it is also worth to note that with 50 % training data, the extra features improve the supervised base model, but not the multi - task learning model.', 'it shows the effectiveness of the multi - task model when there is less training data']",5
"[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","[' #TAUTHOR_TAG.', 'we use a similar model as our base model.', '']","['many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position']","['supervised neural model on a single dataset was introduced by  #AUTHOR_TAG and followed by many others  #TAUTHOR_TAG.', 'we use a similar model as our base model.', 'it takes word tokens, position of arguments and their entity types as input.', 'some work  #TAUTHOR_TAG used extra syntax features as input.', '']",3
"[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation']","[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation']","[' #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation learning from scratch first.', 'our experiments focus on whether']","['separately on the two corpora ( row "" supervised "" in table 1 ), we obtain results on ace05 comparable to previous work  #AUTHOR_TAG with substantially fewer features.', 'with syntactic features as  #TAUTHOR_TAG did, it could be further improved.', 'in this paper, however, we want to focus on representation learning from scratch first.', 'our experiments focus on whether we can improve the representation with more sources of data.', 'a common way to do so is pre - training.', 'as a baseline, we pre - train the encoder of the supervised model on ere and then fine - tune on ace05, and vice versa ( row "" pretraining "" in table 1 ).', 'we observe improvement on both fine - tuned datasets.', '']",2
"['parallel meaning bank  #TAUTHOR_TAG, two annotation']","['parallel meaning bank  #TAUTHOR_TAG, two annotation']","['the groningen meaning bank and the parallel meaning bank  #TAUTHOR_TAG, two annotation efforts which use a graphical user interface']","['##y categorial grammar ( ccg ;  #AUTHOR_TAG is a grammar formalism distinguished by its transparent syntax - semantics interface and its elegant handling of coordination.', 'it is a popular tool in semantic parsing, and treebank creation efforts have been made for turkish ( c akıcı, 2005 ), german  #AUTHOR_TAG, english  #AUTHOR_TAG, italian  #AUTHOR_TAG, chinese  #AUTHOR_TAG, arabic  #AUTHOR_TAG, japanese  #AUTHOR_TAG, and hindi  #AUTHOR_TAG.', 'however, all of these treebanks were not directly annotated according to the ccg formalism, but automatically converted from phrase structure or dependency treebanks, which is an error - prone process.', 'direct annotation in ccg has so far mostly been limited to small datasets for seeding or testing semantic parsers ( e. g.,  #AUTHOR_TAG, and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale.', 'the only exceptions we are aware of are the groningen meaning bank and the parallel meaning bank  #TAUTHOR_TAG, two annotation efforts which use a graphical user interface for annotating sentences with ccg derivations and other annotation layers, and which have produced ccg treebanks for english, german, italian, and dutch.', '']",0
"['in the parallel meaning bank  #TAUTHOR_TAG.', 'these do not follow the annotation']","['in the parallel meaning bank  #TAUTHOR_TAG.', 'these do not follow the annotation']","['include in the release the syntactic ccg derivations created so far in the parallel meaning bank  #TAUTHOR_TAG.', 'these do not follow the annotation guidelines in detail']","['', 'table 1 ( upper part ) shows the distribution of disagreement classes, and table 2 shows examples of class ( 3 ).', 'the first author adjudicated all disagreements and updated the annotation manual accordingly.', 'we release the manual and the full adjudicated dataset.', '2 to make the resource more useful ( e. g., for training parsers ), we also include in the release the syntactic ccg derivations created so far in the parallel meaning bank  #TAUTHOR_TAG.', 'these do not follow the annotation guidelines in detail due to their focus on semantics, nor have they been adjudicated, but instead corrected by a single annotator.', 'however, they are much greater in number.', 'for an even greater number, we also release partially corrected derivations, meaning that the annotator made at least one change to the automatically created derivation.', 'lexical label constraints and span constraints, adjudication support, and various conveniences.', 'we have used this tool to create the first published ccg resource that comes with an explicit annotation manual for syntax and has been created by direct annotation, rather than conversion from a non']",7
"[') [ 13, 11,  #TAUTHOR_TAG or']","['of individual weights ( unstructured sparsification ) [ 13, 11,  #TAUTHOR_TAG or']","[') [ 13, 11,  #TAUTHOR_TAG or']",[' #TAUTHOR_TAG'],0
"[""not affect the network's output."", 'in  #TAUTHOR_TAG, sparse']","[""not affect the network's output."", 'in  #TAUTHOR_TAG, sparsevd is adapted to the rnns.', 'training our model.', 'we']","[""m 2 ij / σ 2 ij → 0 and these weights do not affect the network's output."", 'in  #TAUTHOR_TAG, sparse']","['variational dropout.', 'our approach relies on sparse variational dropout [ 10 ] ( sparsevd ).', 'this model treats the weights of the neural network as random variables and comprises a log - uniform prior over the weights : p ( | w ij | ) ∝ 1 | wij | and a fully factorized normal approximate posterior over the weights : q ( w ij ) = n ( w ij | m ij, σ 2 ij ).', 'biases are treated as deterministic parameters.', 'to find the parameters of the approximate posterior distribution and biases, the evidence lower bound ( elbo ) is optimized :', ""because of the log - uniform prior, for the majority of weights, the signal - to - noise ratio m 2 ij / σ 2 ij → 0 and these weights do not affect the network's output."", 'in  #TAUTHOR_TAG, sparsevd is adapted to the rnns.', 'training our model.', 'we work with the group weights z in the same way as with the weights w : we approximate the posterior with the fully factorized normal distribution given the fully factorized log - uniform prior distribution.', 'to estimate the expectation in ( 1 ), we sample weights from the approximate posterior distribution in the same way as in  #TAUTHOR_TAG.', 'with the integral estimated with one monte - carlo sample, the first term in ( 1 ) becomes the usual loss function ( for example, cross - entropy in language modeling ).', 'the second term is a regularizer depending on the parameters [UNK] and σ ( for the exact formula, see [ 10 ] ).', 'after learning, we zero out all the weights and the group weights with the signal - to - noise ratio less than 0. 05.', 'at the testing stage, we use the mean values of all the weights and the group weights']",0
"['this section, we present experimental results for the unstructured bayesian sparsification ( configuration bayes w ).', 'this configuration corresponds to a model of chirkova et al.  #TAUTHOR_TAG.', 'table 2 shows quantitative results, and']","['this section, we present experimental results for the unstructured bayesian sparsification ( configuration bayes w ).', 'this configuration corresponds to a model of chirkova et al.  #TAUTHOR_TAG.', 'table 2 shows quantitative results, and']","['this section, we present experimental results for the unstructured bayesian sparsification ( configuration bayes w ).', 'this configuration corresponds to a model of chirkova et al.  #TAUTHOR_TAG.', 'table 2 shows quantitative results, and']","['this section, we present experimental results for the unstructured bayesian sparsification ( configuration bayes w ).', 'this configuration corresponds to a model of chirkova et al.  #TAUTHOR_TAG.', 'table 2 shows quantitative results, and figure 5 shows the resulting gate structures for the imdb classification task and the second lstm layer of the word - level language modeling task.', 'since bayes w model does not comprise any group weights, the overall compression of the rnn is lower than for bayes w + g + n ( tab. 1 ), so there are more non - constant gates.', 'however, the patterns in gate structures are the same as in bayes w + g + n gate structures ( fig. 1 ) : for the imdb classification, the model has a lot of constant output gates and non - constant information flow, for language modeling, the model has neurons with only non - constant output gates']",0
"[') [ 13, 11,  #TAUTHOR_TAG or']","['of individual weights ( unstructured sparsification ) [ 13, 11,  #TAUTHOR_TAG or']","[') [ 13, 11,  #TAUTHOR_TAG or']",[' #TAUTHOR_TAG'],5
"[') [ 13, 11,  #TAUTHOR_TAG or']","['of individual weights ( unstructured sparsification ) [ 13, 11,  #TAUTHOR_TAG or']","[') [ 13, 11,  #TAUTHOR_TAG or']",[' #TAUTHOR_TAG'],5
"['.', 'bayesian sparsification.', 'we rely on sparse variational dropout [ 10,  #TAUTHOR_TAG to spars']","['to zero.', 'bayesian sparsification.', 'we rely on sparse variational dropout [ 10,  #TAUTHOR_TAG to sparsify individual weights.', 'following [ 5 ], for']","['.', 'bayesian sparsification.', 'we rely on sparse variational dropout [ 10,  #TAUTHOR_TAG to spars']","['incorporate the proposed intermediate level of sparsification into two sparsification frameworks ( more details are given in appendices a and b ) :', 'pruning.', 'we apply lasso to individual weights and group lasso [ 15 ] to five groups of the lstm weights ( four gate groups and one neuron group, see fig. 2 ).', 'we use the same pruning algorithm as in intrinsic sparse structure ( iss ) [ 14 ], a structured pruning approach developed specifically for lstm.', 'in contrast to our approach, they do not sparsify gates, and remove a neuron if all its ingoing and outgoing connections are set to zero.', 'bayesian sparsification.', 'we rely on sparse variational dropout [ 10,  #TAUTHOR_TAG to sparsify individual weights.', 'following [ 5 ], for each neuron, we introduce a group weight which is multiplied by the output of this neuron in the computational graph ( setting to zero this group weight entails removing the neuron ).', 'to sparsify gates, for each gate we introduce a separate group weight which is multiplied by the preactivation of the gate before adding a bias ( setting to zero this group weight makes the gate constant ).', '[ 1 ] with additional group weights for neurons [ 5 ]']",5
"['( dataset ptb, character and word level tasks ) following  #TAUTHOR_TAG.', 'the architecture for the character - level lm is lstm + fc,']","['( dataset ptb, character and word level tasks ) following  #TAUTHOR_TAG.', 'the architecture for the character - level lm is lstm + fc,']","['agnews [ 17 ] ) and language modeling ( dataset ptb, character and word level tasks ) following  #TAUTHOR_TAG.', 'the architecture for the character - level lm is lstm + fc,']","['the pruning framework, we perform experiments on word - level language modeling ( lm ) on a ptb dataset [ 7 ] following iss [ 14 ].', 'we use a standard model of zaremba et al. [ 16 ] of two sizes ( small and large ) with an embedding layer, two lstm layers, and a fully - connected output layer ( emb + 2 lstm + fc ).', 'here regularization is applied only to lstm layers following [ 14 ], and its strength is selected using grid search so that qualities of iss and our model are approximately equal.', 'in the bayesian framework, we perform an evaluation on the text classification ( datasets imdb [ 6 ] and agnews [ 17 ] ) and language modeling ( dataset ptb, character and word level tasks ) following  #TAUTHOR_TAG.', 'the architecture for the character - level lm is lstm + fc, for the text classification is emb + lstm + fc on the last hidden state, for the word level lm is the same as in pruning.', 'here we regularize and sparsify all layers following  #TAUTHOR_TAG.', 'sizes of lstm layers may be found in tab. 1.', 'embedding layers have 300 / 200 / 1500 neurons for classification tasks / small / large word level lm.', 'more experimental details are given in appendix c']",5
"['( dataset ptb, character and word level tasks ) following  #TAUTHOR_TAG.', 'the architecture for the character - level lm is lstm + fc,']","['( dataset ptb, character and word level tasks ) following  #TAUTHOR_TAG.', 'the architecture for the character - level lm is lstm + fc,']","['agnews [ 17 ] ) and language modeling ( dataset ptb, character and word level tasks ) following  #TAUTHOR_TAG.', 'the architecture for the character - level lm is lstm + fc,']","['the pruning framework, we perform experiments on word - level language modeling ( lm ) on a ptb dataset [ 7 ] following iss [ 14 ].', 'we use a standard model of zaremba et al. [ 16 ] of two sizes ( small and large ) with an embedding layer, two lstm layers, and a fully - connected output layer ( emb + 2 lstm + fc ).', 'here regularization is applied only to lstm layers following [ 14 ], and its strength is selected using grid search so that qualities of iss and our model are approximately equal.', 'in the bayesian framework, we perform an evaluation on the text classification ( datasets imdb [ 6 ] and agnews [ 17 ] ) and language modeling ( dataset ptb, character and word level tasks ) following  #TAUTHOR_TAG.', 'the architecture for the character - level lm is lstm + fc, for the text classification is emb + lstm + fc on the last hidden state, for the word level lm is the same as in pruning.', 'here we regularize and sparsify all layers following  #TAUTHOR_TAG.', 'sizes of lstm layers may be found in tab. 1.', 'embedding layers have 300 / 200 / 1500 neurons for classification tasks / small / large word level lm.', 'more experimental details are given in appendix c']",5
"[""not affect the network's output."", 'in  #TAUTHOR_TAG, sparse']","[""not affect the network's output."", 'in  #TAUTHOR_TAG, sparsevd is adapted to the rnns.', 'training our model.', 'we']","[""m 2 ij / σ 2 ij → 0 and these weights do not affect the network's output."", 'in  #TAUTHOR_TAG, sparse']","['variational dropout.', 'our approach relies on sparse variational dropout [ 10 ] ( sparsevd ).', 'this model treats the weights of the neural network as random variables and comprises a log - uniform prior over the weights : p ( | w ij | ) ∝ 1 | wij | and a fully factorized normal approximate posterior over the weights : q ( w ij ) = n ( w ij | m ij, σ 2 ij ).', 'biases are treated as deterministic parameters.', 'to find the parameters of the approximate posterior distribution and biases, the evidence lower bound ( elbo ) is optimized :', ""because of the log - uniform prior, for the majority of weights, the signal - to - noise ratio m 2 ij / σ 2 ij → 0 and these weights do not affect the network's output."", 'in  #TAUTHOR_TAG, sparsevd is adapted to the rnns.', 'training our model.', 'we work with the group weights z in the same way as with the weights w : we approximate the posterior with the fully factorized normal distribution given the fully factorized log - uniform prior distribution.', 'to estimate the expectation in ( 1 ), we sample weights from the approximate posterior distribution in the same way as in  #TAUTHOR_TAG.', 'with the integral estimated with one monte - carlo sample, the first term in ( 1 ) becomes the usual loss function ( for example, cross - entropy in language modeling ).', 'the second term is a regularizer depending on the parameters [UNK] and σ ( for the exact formula, see [ 10 ] ).', 'after learning, we zero out all the weights and the group weights with the signal - to - noise ratio less than 0. 05.', 'at the testing stage, we use the mean values of all the weights and the group weights']",5
"['the input vocabulary following chirkova et al.', ' #TAUTHOR_TAG. for the networks with']","['the input vocabulary following chirkova et al.', ' #TAUTHOR_TAG. for the networks with the embedding layer, in configurations w + n']","['use additional multiplicative weights to sparsify the input vocabulary following chirkova et al.', ' #TAUTHOR_TAG. for the networks with the embedding layer, in configurations']","['learning', 'rate decreases two times during training ( after epochs 18 and 36 ), the learning rate decay is equal to 0. 2 and 0. 1 for two - and three - level sparsification correspondingly.', 'for the two - level sparsification ( w + n ), we use lasso', 'regularization with λ = 1e − 5 and group lasso regularization with λ =', '0. 0015. for the three - level sparsification ( w + g + n ), we use lasso regularization with λ = 1. 5e −', '05 and group lasso regularization with λ = 0. 00125.', 'we use', 'the same threshold 1e − 4 as in the small models. bayesian sparsification.', 'in all the bayesian models, we sparsify the weight matrices of all layers. since in text classification tasks, usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary following chirkova et al.', ' #TAUTHOR_TAG. for the networks with the embedding layer, in configurations w + n and w + g + n, we also sparsify the embedding components ( by introducing group weights z x multiplied by', 'x t. ) we train our networks using adam [ 4 ]. baseline networks overfit for all our tasks, therefore, we present results for them with early stopping. models for the text classification and', 'the character - level lm are trained in the same setting as in  #TAUTHOR_TAG ( we used the code', 'provided by the authors ). for the text classification tasks, we use a learning rate equal to 0. 0005 and train bayesian models for 800 / 150', '']",5
"['the input vocabulary following chirkova et al.', ' #TAUTHOR_TAG. for the networks with']","['the input vocabulary following chirkova et al.', ' #TAUTHOR_TAG. for the networks with the embedding layer, in configurations w + n']","['use additional multiplicative weights to sparsify the input vocabulary following chirkova et al.', ' #TAUTHOR_TAG. for the networks with the embedding layer, in configurations']","['learning', 'rate decreases two times during training ( after epochs 18 and 36 ), the learning rate decay is equal to 0. 2 and 0. 1 for two - and three - level sparsification correspondingly.', 'for the two - level sparsification ( w + n ), we use lasso', 'regularization with λ = 1e − 5 and group lasso regularization with λ =', '0. 0015. for the three - level sparsification ( w + g + n ), we use lasso regularization with λ = 1. 5e −', '05 and group lasso regularization with λ = 0. 00125.', 'we use', 'the same threshold 1e − 4 as in the small models. bayesian sparsification.', 'in all the bayesian models, we sparsify the weight matrices of all layers. since in text classification tasks, usually only a small number of input words are important, we use additional multiplicative weights to sparsify the input vocabulary following chirkova et al.', ' #TAUTHOR_TAG. for the networks with the embedding layer, in configurations w + n and w + g + n, we also sparsify the embedding components ( by introducing group weights z x multiplied by', 'x t. ) we train our networks using adam [ 4 ]. baseline networks overfit for all our tasks, therefore, we present results for them with early stopping. models for the text classification and', 'the character - level lm are trained in the same setting as in  #TAUTHOR_TAG ( we used the code', 'provided by the authors ). for the text classification tasks, we use a learning rate equal to 0. 0005 and train bayesian models for 800 / 150', '']",5
"['this section, we present experimental results for the unstructured bayesian sparsification ( configuration bayes w ).', 'this configuration corresponds to a model of chirkova et al.  #TAUTHOR_TAG.', 'table 2 shows quantitative results, and']","['this section, we present experimental results for the unstructured bayesian sparsification ( configuration bayes w ).', 'this configuration corresponds to a model of chirkova et al.  #TAUTHOR_TAG.', 'table 2 shows quantitative results, and']","['this section, we present experimental results for the unstructured bayesian sparsification ( configuration bayes w ).', 'this configuration corresponds to a model of chirkova et al.  #TAUTHOR_TAG.', 'table 2 shows quantitative results, and']","['this section, we present experimental results for the unstructured bayesian sparsification ( configuration bayes w ).', 'this configuration corresponds to a model of chirkova et al.  #TAUTHOR_TAG.', 'table 2 shows quantitative results, and figure 5 shows the resulting gate structures for the imdb classification task and the second lstm layer of the word - level language modeling task.', 'since bayes w model does not comprise any group weights, the overall compression of the rnn is lower than for bayes w + g + n ( tab. 1 ), so there are more non - constant gates.', 'however, the patterns in gate structures are the same as in bayes w + g + n gate structures ( fig. 1 ) : for the imdb classification, the model has a lot of constant output gates and non - constant information flow, for language modeling, the model has neurons with only non - constant output gates']",5
"[') [ 13, 11,  #TAUTHOR_TAG or']","['of individual weights ( unstructured sparsification ) [ 13, 11,  #TAUTHOR_TAG or']","[') [ 13, 11,  #TAUTHOR_TAG or']",[' #TAUTHOR_TAG'],6
['previous works  #TAUTHOR_TAG have shown that'],['previous works  #TAUTHOR_TAG have shown that'],"['', 'meanwhile, several previous works  #TAUTHOR_TAG have shown that']","['', 'meanwhile, several previous works  #TAUTHOR_TAG have shown that grandchild interactions provide important information for dependency parsing.', '']",0
"['', 'following  #TAUTHOR_TAG,']","['incomplete spans with grandparent indices.', 'following  #TAUTHOR_TAG,']","['##parent indices.', 'following  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],5
"['previous works  #TAUTHOR_TAG,']","['previous works  #TAUTHOR_TAG,']","['previous works  #TAUTHOR_TAG, the fourthorder parser captures not only features associated with corresponding fourth - order grand - trisibling parts, but also the features of']","['previous works  #TAUTHOR_TAG, the fourthorder parser captures not only features associated with corresponding fourth - order grand - trisibling parts, but also the features of relevant lower - order parts that are enclosed in its factorization.', '']",5
"['previous works  #TAUTHOR_TAG,']","['previous works  #TAUTHOR_TAG,']","['previous works  #TAUTHOR_TAG, the fourthorder parser captures not only features associated with corresponding fourth - order grand - trisibling parts, but also the features of']","['previous works  #TAUTHOR_TAG, the fourthorder parser captures not only features associated with corresponding fourth - order grand - trisibling parts, but also the features of relevant lower - order parts that are enclosed in its factorization.', '']",5
"['previous works  #TAUTHOR_TAG,']","['previous works  #TAUTHOR_TAG,']","['previous works  #TAUTHOR_TAG, the fourthorder parser captures not only features associated with corresponding fourth - order grand - trisibling parts, but also the features of']","['previous works  #TAUTHOR_TAG, the fourthorder parser captures not only features associated with corresponding fourth - order grand - trisibling parts, but also the features of relevant lower - order parts that are enclosed in its factorization.', '']",5
"[' #TAUTHOR_TAG.', 'additionally, we compare']","[' #TAUTHOR_TAG.', 'additionally, we compare']","['##s  #TAUTHOR_TAG.', 'additionally, we compare']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG.', 'additionally, we compare']","[' #TAUTHOR_TAG.', 'additionally, we compare']","['##s  #TAUTHOR_TAG.', 'additionally, we compare']",[' #TAUTHOR_TAG'],4
"[' #TAUTHOR_TAG.', 'additionally, we compare']","[' #TAUTHOR_TAG.', 'additionally, we compare']","['##s  #TAUTHOR_TAG.', 'additionally, we compare']",[' #TAUTHOR_TAG'],4
"[', attention mechanism  #TAUTHOR_TAG,']","[', attention mechanism  #TAUTHOR_TAG, memory - augmented']","[', attention mechanism  #TAUTHOR_TAG,']","['tutorial introduces the advances in deep bayesian learning with abundant applications for natural language understanding ranging from speech recognition  #AUTHOR_TAG to document summarization  #AUTHOR_TAG, text classification  #AUTHOR_TAG, text segmentation  #AUTHOR_TAG, information extraction  #AUTHOR_TAG, image caption generation  #AUTHOR_TAG, sentence generation  #AUTHOR_TAG b ), dialogue control  #AUTHOR_TAG a ), sentiment classification, recommendation system, question answering  #AUTHOR_TAG and machine translation, to name a few.', 'traditionally, "" deep learning "" is taken to be a learning process where the inference or optimization is based on the real - valued deterministic model.', 'the "" semantic structure "" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs.', 'the "" distribution function "" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated in model inference.', 'this tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced bayesian models and deep models including hierarchical dirichlet process, chinese restaurant process  #AUTHOR_TAG, hierarchical pitman - yor process  #AUTHOR_TAG, indian buffet process  #AUTHOR_TAG, recurrent neural network  #AUTHOR_TAG, long short - term memory  #AUTHOR_TAG sequence - to - sequence model  #AUTHOR_TAG, variational auto - encoder  #AUTHOR_TAG, generative adversarial network  #AUTHOR_TAG, attention mechanism  #TAUTHOR_TAG, memory - augmented neural network  #AUTHOR_TAG, stochastic neural network  #AUTHOR_TAG, predictive state neural network  #AUTHOR_TAG, policy gradient  #AUTHOR_TAG and reinforcement learning  #AUTHOR_TAG.', '']",0
"['. in another attempt', ',  #TAUTHOR_TAG proposed']","['for co - hyponymy detection. in another attempt', ',  #TAUTHOR_TAG proposed']","['for co - hyponymy detection. in another attempt', ',  #TAUTHOR_TAG proposed various complex network measures which']","['vector operations as features for the classification of hypernymy and co - hyponymy.  #AUTHOR_TAG proposed a supervised method based on a', 'random forest algorithm to learn taxonomical semantic relations and they have shown that the model performs well for co - hyponymy detection. in another attempt', ',  #TAUTHOR_TAG proposed various complex network measures which can be used as features to build a supervised classifier model for co - hyponymy detection, and showed improvements over', 'other baseline approaches.', 'recently, with the emergence of various network representation learning methods  #AUTHOR_TAG, attempts have been made to', 'convert distributional thesauri network into low dimensional vector space.  #AUTHOR_TAG apply distributional thesaurus embedding for synonym extraction and expansion tasks whereas  #AUTHOR_TAG', '']",1
"['. in another attempt', ',  #TAUTHOR_TAG proposed']","['for co - hyponymy detection. in another attempt', ',  #TAUTHOR_TAG proposed']","['for co - hyponymy detection. in another attempt', ',  #TAUTHOR_TAG proposed various complex network measures which']","['vector operations as features for the classification of hypernymy and co - hyponymy.  #AUTHOR_TAG proposed a supervised method based on a', 'random forest algorithm to learn taxonomical semantic relations and they have shown that the model performs well for co - hyponymy detection. in another attempt', ',  #TAUTHOR_TAG proposed various complex network measures which can be used as features to build a supervised classifier model for co - hyponymy detection, and showed improvements over', 'other baseline approaches.', 'recently, with the emergence of various network representation learning methods  #AUTHOR_TAG, attempts have been made to', 'convert distributional thesauri network into low dimensional vector space.  #AUTHOR_TAG apply distributional thesaurus embedding for synonym extraction and expansion tasks whereas  #AUTHOR_TAG', '']",1
['hyper  #AUTHOR_TAG 97. 8 95. 7  #TAUTHOR_TAG for'],"['hyper  #AUTHOR_TAG 97. 8 95. 7  #TAUTHOR_TAG for root9 dataset a set of baseline methodologies, the descriptions of which are presented in table 1. following the']","['hyper  #AUTHOR_TAG 97. 8 95. 7  #TAUTHOR_TAG for root9 dataset a set of baseline methodologies, the descriptions of which are presented in table 1. following the same', 'experimental setup, we report the accuracy measure for ten -']","['hyper  #AUTHOR_TAG 97. 8 95. 7  #TAUTHOR_TAG for root9 dataset a set of baseline methodologies, the descriptions of which are presented in table 1. following the same', 'experimental setup, we report the accuracy measure for ten - fold cross validation and compare our models with the baselines in proposed by  #AUTHOR_TAG. table 2 represents', 'the performance of all the baseline models proposed by  #AUTHOR_TAG. in table 3', 'we show the performance of the best supervised model ( svmdiff ) and the best semi - supervised model ( cosinep ) proposed by  #AUTHOR_TAG along with our models. here, the', 'best model proposed by  #TAUTHOR_TAG uses svm classifier which is fed with structural similarity of the words in the given word pair from the distributional thesaurus network. we see that', '']",4
"['in one of the recent works by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is']","['in one of the recent works by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is']","['co - hyponymy detection in one of the recent works by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is extracted from bless  #AUTHOR_TAG and divided into three small datasets - co - hypo vs hyper, co - hypo vs mero, co - hypo vs random.', '']","['the third experiment we use the dataset specifically build for co - hyponymy detection in one of the recent works by  #TAUTHOR_TAG.', ' #TAUTHOR_TAG is extracted from bless  #AUTHOR_TAG and divided into three small datasets - co - hypo vs hyper, co - hypo vs mero, co - hypo vs random.', 'each of these datasets are balanced, containing 1, 000 co - hyponymy pairs and 1, 000 pairs for the other class.', 'following the same setup, we report accuracy scores for ten - fold cross validation for each of these three datasets of our models along with the best models ( svmss, rfall ) reported by  #TAUTHOR_TAG in table 5.', ' #TAUTHOR_TAG use svm classifier with structural similarity between words in a word pair as feature to obtain svmss and use random forest classifier with five complex network measures computed from distributional thesaurus network as features to obtain rfall.', 'from the results presented in table 5, rf cc proves to be the best among our proposed models which performs at par with the baselines for co - hypo vs random dataset.', 'interestingly, it beats the baselines comprehensively for co - hypo vs mero and co - hypo vs hyper datasets, providing improvements of 9. 88 % and 25. 64 %, respectively']",0
,,,,5
,,,,5
"['inhouse mwe identification system veyn  #TAUTHOR_TAG, based on sequence tagging using recurrent neural']","['inhouse mwe identification system veyn  #TAUTHOR_TAG, based on sequence tagging using recurrent neural networks.', '5']","['inhouse mwe identification system veyn  #TAUTHOR_TAG, based on sequence tagging using recurrent neural networks.', ""5 the system takes as input the concatenation of the embeddings of the words'features ( e. g. lemmas and pos )""]","['use our inhouse mwe identification system veyn  #TAUTHOR_TAG, based on sequence tagging using recurrent neural networks.', ""5 the system takes as input the concatenation of the embeddings of the words'features ( e. g. lemmas and pos )."", ""it uses a crf output layer ( conditional random fields ) to predict valid label sequences, with vmwes encoded using the'biog + cat'format."", ""each token is tagged'b'if it is at the beginning of a vmwe,'i'if it is inside a vmwe,'o'if it does not belong to a vmwe, and'g ', if it does not belong to a vmwe but it is in the gap between two words that are part of a vmwe."", ""the tags'b'and'i'are concatenated with the vmwe categories ( vid, lvc. full, etc. ) present in the corpus."", 'the system is trained on the shared task training corpora, so that the results are comparable with the systems submitted to the closed track.', '6 we use the dev corpus as validation data, training for 25 epochs which 3 epochs of patience for early stopping.', 'we configure it to use two layers of bidirectional gated recurrent units ( gru ) of dimension 128, with all other parameters taking the default values suggested in the veyn documentation.', 'word representations we use two types of word embeddings to represent input surface forms 5 https : / / github. com / zamp13 / veyn 6 http : / / multiword. sourceforge. net / sharedtaskresults2018 and lemmas : word2vec and fasttext.', 'word2vec is a prediction - based distributional model in which a word representation is obtained from a neural network trying to predict a word from its context or vice - versa  #AUTHOR_TAG.', 'fasttext is an adaptation which also takes into account character n - grams, being able to build vectors for oovs from its character n - grams  #AUTHOR_TAG.', 'for each representation, we used the gensim library 7 to train 256 - dimensional vectors for both forms and lemmas on the training corpus of the shared task for 10 epochs.', 'furthermore, all embeddings use the cbow algorithm with the same hyper - parameter values of 5 for the window size ( left / right context of words ) and 1 for min - count ( minimum number of occurrences of words ).', ""for fasttext, we set the size of character n - grams to 1 to combine the whole word's embedding with the embeddings of its characters."", 'we did not use contextual']",5
,,,,3
"['representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in  #TAUTHOR_TAG']","['representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in  #TAUTHOR_TAG']","['tackling various nlp tasks.', 'while common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in  #TAUTHOR_TAG.', 'we argue that although promising results were']","['sentences as numerical vectors while capturing their semantic context is an important and useful intermediate step in natural language processing.', 'representations that are both general and discriminative can serve as a tool for tackling various nlp tasks.', 'while common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in  #TAUTHOR_TAG.', 'we argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto - encoders and by language models.', 'we show that by adding such constraints, superior sentence embeddings can be achieved.', 'we compare our method with the original implementation and show improvements in several tasks']",0
"['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations can be']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations can be achieved via supervised training on a', 'general sentence inference dataset  #AUTHOR_TAG. to this end, the authors use the stanford natural language inference ( snli ) dataset  #AUTHOR_TAG to train different table 1 : sentence embedding results.', 'bilstm refers to the original bilstm followed by maxpooling implementation of  #TAUTHOR_TAG which is the baseline for our work. ae reg and lm reg', 'refers to the auto - encoder and language - model regularization terms described in 2. 1 and combined refers to', 'optimizing with both terms. bi - ae reg and bi - lm reg refers to the bi - directional auto - encoder and bi - directional language - model regularization terms described in 2. 2. as evident from the results, adding simple unsupervised regularization terms improves the results of', 'the model on almost all the evaluated tasks. sentence embedding methods and compare', 'them on various benchmarks. the snli dataset is composed of 570k pairs of sentences with a label depicting the relationship between them, which can', ""be either'neutral ','contradiction'or'entailment '. the authors show that by leveraging the dataset, state - of - the - art representations can be"", '']",0
"['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations can be']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations can be achieved via supervised training on a', 'general sentence inference dataset  #AUTHOR_TAG. to this end, the authors use the stanford natural language inference ( snli ) dataset  #AUTHOR_TAG to train different table 1 : sentence embedding results.', 'bilstm refers to the original bilstm followed by maxpooling implementation of  #TAUTHOR_TAG which is the baseline for our work. ae reg and lm reg', 'refers to the auto - encoder and language - model regularization terms described in 2. 1 and combined refers to', 'optimizing with both terms. bi - ae reg and bi - lm reg refers to the bi - directional auto - encoder and bi - directional language - model regularization terms described in 2. 2. as evident from the results, adding simple unsupervised regularization terms improves the results of', 'the model on almost all the evaluated tasks. sentence embedding methods and compare', 'them on various benchmarks. the snli dataset is composed of 570k pairs of sentences with a label depicting the relationship between them, which can', ""be either'neutral ','contradiction'or'entailment '. the authors show that by leveraging the dataset, state - of - the - art representations can be"", '']",5
"['approach builds upon the previous work of  #TAUTHOR_TAG.', 'specifically, we use their bilstm model with max pooling.', 'more concretely, given a sequence of t words, { w t } t = 1,..., t with given word embedding  #AUTHOR_TAG']","['approach builds upon the previous work of  #TAUTHOR_TAG.', 'specifically, we use their bilstm model with max pooling.', 'more concretely, given a sequence of t words, { w t } t = 1,..., t with given word embedding  #AUTHOR_TAG { v t } t = 1,..., t, a bidirectional lstm computes']","['approach builds upon the previous work of  #TAUTHOR_TAG.', 'specifically, we use their bilstm model with max pooling.', 'more concretely, given a sequence of t words, { w t } t = 1,..., t with given word embedding  #AUTHOR_TAG { v t } t = 1,...,']","['approach builds upon the previous work of  #TAUTHOR_TAG.', 'specifically, we use their bilstm model with max pooling.', 'more concretely, given a sequence of t words, { w t } t = 1,..., t with given word embedding  #AUTHOR_TAG { v t } t = 1,..., t, a bidirectional lstm computes a set of t vectors { h t } t = 1,..., t where each h t is the concatenation of a forward lstm and a backward lstm that read the sentences in two opposite directions.', ""we denote { − → h t } and { ← − h t } as the hidden states of the left and right lstm's respectively, where t = 1,..., t."", 'the final sentence representation is obtained by taking the maximal value of each dimension of the { h t } hidden units ( i. e. : max pooling ).', 'the original model of  #TAUTHOR_TAG was trained on the snli dataset in a supervised fashion - given pairs of sentences s 1 and s 2, denote their representation bys 1 and s 2.', 'during training, the concatenation ofs 1, s 2, | s 1 −s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier']",5
[' #TAUTHOR_TAG we have tested our'],[' #TAUTHOR_TAG we have tested our'],[' #TAUTHOR_TAG we have tested our'],"[' #TAUTHOR_TAG we have tested our approach on a wide array of classification tasks, including sentiment analysis ( mr -  #AUTHOR_TAG, sst -  #AUTHOR_TAG ), question - type ( trec -  #AUTHOR_TAG ), product reviews ( cr -  #AUTHOR_TAG ), subjectivity / objectivity ( subj -  #AUTHOR_TAG ) and opinion polarity ( mpqa -  #AUTHOR_TAG ).', 'we also tested our approach on semantic textual similarity ( sts 14 -  #AUTHOR_TAG ), paraphrase detection ( mrpc -  #AUTHOR_TAG ), entailment and semantic relatedness tasks ( sick - r and sick - e -  #AUTHOR_TAG ), though those tasks are more close in nature to the task of the snli dataset which the model was trained on.', 'in our experiments we have set λ from eq. ( 1 ) and eq. ( 2 ) to be 1 and λ 1, λ 2 from eq. ( 3 ) and eq. ( 4 ) to be 0. 5.', 'all other hyper - parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of  #TAUTHOR_TAG.', 'our results are summarized in table 1.', 'we compared out method against the baseline bil - stm implementation of  #TAUTHOR_TAG and included fastsent  #AUTHOR_TAG and skipthought vectors  #AUTHOR_TAG as a reference.', 'as evident from table 1 in almost all the tasks evaluated, adding the proposed regularization terms improves performance.', 'this serve to show that in a supervised learning setting, additional information on the input sequence can be leveraged and injected to the model by adding simple unsupervised loss criteria']",5
"['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations can be']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations can be achieved via supervised training on a', 'general sentence inference dataset  #AUTHOR_TAG. to this end, the authors use the stanford natural language inference ( snli ) dataset  #AUTHOR_TAG to train different table 1 : sentence embedding results.', 'bilstm refers to the original bilstm followed by maxpooling implementation of  #TAUTHOR_TAG which is the baseline for our work. ae reg and lm reg', 'refers to the auto - encoder and language - model regularization terms described in 2. 1 and combined refers to', 'optimizing with both terms. bi - ae reg and bi - lm reg refers to the bi - directional auto - encoder and bi - directional language - model regularization terms described in 2. 2. as evident from the results, adding simple unsupervised regularization terms improves the results of', 'the model on almost all the evaluated tasks. sentence embedding methods and compare', 'them on various benchmarks. the snli dataset is composed of 570k pairs of sentences with a label depicting the relationship between them, which can', ""be either'neutral ','contradiction'or'entailment '. the authors show that by leveraging the dataset, state - of - the - art representations can be"", '']",6
"['approach builds upon the previous work of  #TAUTHOR_TAG.', 'specifically, we use their bilstm model with max pooling.', 'more concretely, given a sequence of t words, { w t } t = 1,..., t with given word embedding  #AUTHOR_TAG']","['approach builds upon the previous work of  #TAUTHOR_TAG.', 'specifically, we use their bilstm model with max pooling.', 'more concretely, given a sequence of t words, { w t } t = 1,..., t with given word embedding  #AUTHOR_TAG { v t } t = 1,..., t, a bidirectional lstm computes']","['approach builds upon the previous work of  #TAUTHOR_TAG.', 'specifically, we use their bilstm model with max pooling.', 'more concretely, given a sequence of t words, { w t } t = 1,..., t with given word embedding  #AUTHOR_TAG { v t } t = 1,...,']","['approach builds upon the previous work of  #TAUTHOR_TAG.', 'specifically, we use their bilstm model with max pooling.', 'more concretely, given a sequence of t words, { w t } t = 1,..., t with given word embedding  #AUTHOR_TAG { v t } t = 1,..., t, a bidirectional lstm computes a set of t vectors { h t } t = 1,..., t where each h t is the concatenation of a forward lstm and a backward lstm that read the sentences in two opposite directions.', ""we denote { − → h t } and { ← − h t } as the hidden states of the left and right lstm's respectively, where t = 1,..., t."", 'the final sentence representation is obtained by taking the maximal value of each dimension of the { h t } hidden units ( i. e. : max pooling ).', 'the original model of  #TAUTHOR_TAG was trained on the snli dataset in a supervised fashion - given pairs of sentences s 1 and s 2, denote their representation bys 1 and s 2.', 'during training, the concatenation ofs 1, s 2, | s 1 −s 2 | ands 1 * s 2 is fed to a three layer fully connected network followed by a softmax classifier']",6
"['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations can be']","['embeddings in an unsupervised manner, a recent work  #TAUTHOR_TAG argued that better representations can be achieved via supervised training on a', 'general sentence inference dataset  #AUTHOR_TAG. to this end, the authors use the stanford natural language inference ( snli ) dataset  #AUTHOR_TAG to train different table 1 : sentence embedding results.', 'bilstm refers to the original bilstm followed by maxpooling implementation of  #TAUTHOR_TAG which is the baseline for our work. ae reg and lm reg', 'refers to the auto - encoder and language - model regularization terms described in 2. 1 and combined refers to', 'optimizing with both terms. bi - ae reg and bi - lm reg refers to the bi - directional auto - encoder and bi - directional language - model regularization terms described in 2. 2. as evident from the results, adding simple unsupervised regularization terms improves the results of', 'the model on almost all the evaluated tasks. sentence embedding methods and compare', 'them on various benchmarks. the snli dataset is composed of 570k pairs of sentences with a label depicting the relationship between them, which can', ""be either'neutral ','contradiction'or'entailment '. the authors show that by leveraging the dataset, state - of - the - art representations can be"", '']",4
[' #TAUTHOR_TAG we have tested our'],[' #TAUTHOR_TAG we have tested our'],[' #TAUTHOR_TAG we have tested our'],"[' #TAUTHOR_TAG we have tested our approach on a wide array of classification tasks, including sentiment analysis ( mr -  #AUTHOR_TAG, sst -  #AUTHOR_TAG ), question - type ( trec -  #AUTHOR_TAG ), product reviews ( cr -  #AUTHOR_TAG ), subjectivity / objectivity ( subj -  #AUTHOR_TAG ) and opinion polarity ( mpqa -  #AUTHOR_TAG ).', 'we also tested our approach on semantic textual similarity ( sts 14 -  #AUTHOR_TAG ), paraphrase detection ( mrpc -  #AUTHOR_TAG ), entailment and semantic relatedness tasks ( sick - r and sick - e -  #AUTHOR_TAG ), though those tasks are more close in nature to the task of the snli dataset which the model was trained on.', 'in our experiments we have set λ from eq. ( 1 ) and eq. ( 2 ) to be 1 and λ 1, λ 2 from eq. ( 3 ) and eq. ( 4 ) to be 0. 5.', 'all other hyper - parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of  #TAUTHOR_TAG.', 'our results are summarized in table 1.', 'we compared out method against the baseline bil - stm implementation of  #TAUTHOR_TAG and included fastsent  #AUTHOR_TAG and skipthought vectors  #AUTHOR_TAG as a reference.', 'as evident from table 1 in almost all the tasks evaluated, adding the proposed regularization terms improves performance.', 'this serve to show that in a supervised learning setting, additional information on the input sequence can be leveraged and injected to the model by adding simple unsupervised loss criteria']",7
[' #TAUTHOR_TAG we have tested our'],[' #TAUTHOR_TAG we have tested our'],[' #TAUTHOR_TAG we have tested our'],"[' #TAUTHOR_TAG we have tested our approach on a wide array of classification tasks, including sentiment analysis ( mr -  #AUTHOR_TAG, sst -  #AUTHOR_TAG ), question - type ( trec -  #AUTHOR_TAG ), product reviews ( cr -  #AUTHOR_TAG ), subjectivity / objectivity ( subj -  #AUTHOR_TAG ) and opinion polarity ( mpqa -  #AUTHOR_TAG ).', 'we also tested our approach on semantic textual similarity ( sts 14 -  #AUTHOR_TAG ), paraphrase detection ( mrpc -  #AUTHOR_TAG ), entailment and semantic relatedness tasks ( sick - r and sick - e -  #AUTHOR_TAG ), though those tasks are more close in nature to the task of the snli dataset which the model was trained on.', 'in our experiments we have set λ from eq. ( 1 ) and eq. ( 2 ) to be 1 and λ 1, λ 2 from eq. ( 3 ) and eq. ( 4 ) to be 0. 5.', 'all other hyper - parameters and implementation details were left unchanged to provide a fair comparison to the baseline method of  #TAUTHOR_TAG.', 'our results are summarized in table 1.', 'we compared out method against the baseline bil - stm implementation of  #TAUTHOR_TAG and included fastsent  #AUTHOR_TAG and skipthought vectors  #AUTHOR_TAG as a reference.', 'as evident from table 1 in almost all the tasks evaluated, adding the proposed regularization terms improves performance.', 'this serve to show that in a supervised learning setting, additional information on the input sequence can be leveraged and injected to the model by adding simple unsupervised loss criteria']",7
"['sentence representations  #TAUTHOR_TAG.', 'however, every supervised learning tasks is']","['state - of - the - art sentence representations  #TAUTHOR_TAG.', 'however, every supervised learning tasks is']","['our work, we have sought to connect unsupervised and supervised learning in the context of sentence embeddings.', 'leveraging supervision given by some general task aided in obtaining state - of - the - art sentence representations  #TAUTHOR_TAG.', 'however, every supervised learning tasks is']","['our work, we have sought to connect unsupervised and supervised learning in the context of sentence embeddings.', 'leveraging supervision given by some general task aided in obtaining state - of - the - art sentence representations  #TAUTHOR_TAG.', 'however, every supervised learning tasks is prone to overfit.', 'in this context, overfitting to the learning task will result in a model which generalizes less well to new tasks.', ""we alleviate this problem by incorporating unsupervised regularization criteria in the model's loss function which are motivated by autoencoders and language models."", 'we note that the added regularization terms do come at the price of increasing the model size by ld parameters ( where d and l are the dimensions of the word embedding and the lstm hidden state, respectively ) due to the added linear transformation ( see 2. 1 ).', 'however, as evident from our results, this does not hinder the model performance, even though we did not increase the amount of training data.', 'moreover, since those term are unsupervised in nature, it is possible to pre - train the model on unlabeled data and then finetune it on the snli dataset.', 'in conclusion, our experiments show that adding the proposed regularization terms results in a more general model and superior sentence embeddings.', 'this validates our assumption that while the a supervised signal is general enough for learning sentence embeddings, it can be further improved by incorporated a second unsupervised signal']",1
"[' #TAUTHOR_TAG, that we']","[' #TAUTHOR_TAG, that we']","[' #TAUTHOR_TAG, that we']","['and 24 in ace with 7k relation instances for 40k entity mentions ). starting with those', 'projects, the task of re was thought of as a pipeline, where the entities were first detected, resolved to a standard schema, and then the', 're system would determine which of the possible relations was expressed ( if any ) between any given pair of entities. much of the earlier work explored a variety of different features, such as syntactic', 'phrase chunking and constituency parsing  #AUTHOR_TAG, and semantic knowledge like wordnet  #AUTHOR_TAG, although  #AUTHOR_TAG showed that the more complex features might actually hurt the performance of an svm - based re system. the work of  #TAUTHOR_TAG, that we closely follow, is also using both semantic and syntactic features, by combining the dependency', 'paths between entities, with word embedding representations of both the entities and the lemmas in the dependency paths. another related area is relation extraction for open information extraction ( openie ). some of the', 'more representative projects in the area, like reverb  #AUTHOR_TAG and more recently clauseie  #AUTHOR_TAG use syntactic information ( pos tagging / chunking, and dependency parsing respectively ) to extract entity and relation phrases. however, unlike openie, we are interested in normalized entities and relations (', 'i. e. that map to a knowledge base ). in this work, we follow a common way of producing training examples for re is to use distant supervision  #AUTHOR_TAG : the assumption is that if any sentence mentions two entities which we know ( from a kb ) participate', '']",5
"[' #TAUTHOR_TAG, that we']","[' #TAUTHOR_TAG, that we']","[' #TAUTHOR_TAG, that we']","['and 24 in ace with 7k relation instances for 40k entity mentions ). starting with those', 'projects, the task of re was thought of as a pipeline, where the entities were first detected, resolved to a standard schema, and then the', 're system would determine which of the possible relations was expressed ( if any ) between any given pair of entities. much of the earlier work explored a variety of different features, such as syntactic', 'phrase chunking and constituency parsing  #AUTHOR_TAG, and semantic knowledge like wordnet  #AUTHOR_TAG, although  #AUTHOR_TAG showed that the more complex features might actually hurt the performance of an svm - based re system. the work of  #TAUTHOR_TAG, that we closely follow, is also using both semantic and syntactic features, by combining the dependency', 'paths between entities, with word embedding representations of both the entities and the lemmas in the dependency paths. another related area is relation extraction for open information extraction ( openie ). some of the', 'more representative projects in the area, like reverb  #AUTHOR_TAG and more recently clauseie  #AUTHOR_TAG use syntactic information ( pos tagging / chunking, and dependency parsing respectively ) to extract entity and relation phrases. however, unlike openie, we are interested in normalized entities and relations (', 'i. e. that map to a knowledge base ). in this work, we follow a common way of producing training examples for re is to use distant supervision  #AUTHOR_TAG : the assumption is that if any sentence mentions two entities which we know ( from a kb ) participate', '']",5
"['1 presents the distant supervision generation process adapted from  #TAUTHOR_TAG to work with our data.', 'in the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases ( nps ) - these are the candidate entities.', '']","['1 presents the distant supervision generation process adapted from  #TAUTHOR_TAG to work with our data.', 'in the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases ( nps ) - these are the candidate entities.', '']","['of figure 1 presents the distant supervision generation process adapted from  #TAUTHOR_TAG to work with our data.', 'in the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases ( nps ) - these are the candidate entities.', '']","['bottom half of figure 1 presents the distant supervision generation process adapted from  #TAUTHOR_TAG to work with our data.', 'in the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases ( nps ) - these are the candidate entities.', 'they then construct the dependency path between each possible pair of entities.', 'each noun / np pair is checked against the kb for distant supervision.', 'keep only the entities / paths that appear in the list of labelled examples.', 'they also filter out entity pairs that have infrequent paths ( occurring fewer than five times ), and pairs whose path is more than five tokens long.', 'however, as discussed in the beginning of this section, this approach introduces a lot of noise.', 'to avoid this problem, we use the pagespecific gazetteer and a greedy string matching system to scan through the unstructured text and assign kb ids to the longest - matching substring in a sentence.', 'the final step was to generate the annotation labels themselves.', 'to do that, we examine each possible pair of entities to see if they participate in the target relation.', 'for the wikidata kb, we simply checked whether the target relation existed as a property in the data.', '']",5
"[' #TAUTHOR_TAG, and we wanted to establish a basis']","[' #TAUTHOR_TAG, and we wanted to establish a basis']","[' #TAUTHOR_TAG, and we wanted to establish a basis']","['also looked at the role of the dependency path satellite nodes ( words to the left and right of the entities ).', 'this type of features has also been adopted by various systems including  #AUTHOR_TAG and  #TAUTHOR_TAG, and we wanted to establish a basis for its effectiveness across multiple relations.', 'the results, shown in table 4 : effect of using all the supports for each x rel y triple using the fasttext classifier on the alexa kb data ( threshold of 0. 5 ).', 'table 5 : f - score results for the three relations on the alexa kb dataset.', 'the baseline system ( 1 ) is the fasttext classifier using the 5 most frequent supports for each x rel y triple, ( 1 ) - dep refers to the system with both the dependency relation and direction features removed, the last system uses all the ( lowercased ) words in each support as features']",5
"[' #TAUTHOR_TAG, that we']","[' #TAUTHOR_TAG, that we']","[' #TAUTHOR_TAG, that we']","['and 24 in ace with 7k relation instances for 40k entity mentions ). starting with those', 'projects, the task of re was thought of as a pipeline, where the entities were first detected, resolved to a standard schema, and then the', 're system would determine which of the possible relations was expressed ( if any ) between any given pair of entities. much of the earlier work explored a variety of different features, such as syntactic', 'phrase chunking and constituency parsing  #AUTHOR_TAG, and semantic knowledge like wordnet  #AUTHOR_TAG, although  #AUTHOR_TAG showed that the more complex features might actually hurt the performance of an svm - based re system. the work of  #TAUTHOR_TAG, that we closely follow, is also using both semantic and syntactic features, by combining the dependency', 'paths between entities, with word embedding representations of both the entities and the lemmas in the dependency paths. another related area is relation extraction for open information extraction ( openie ). some of the', 'more representative projects in the area, like reverb  #AUTHOR_TAG and more recently clauseie  #AUTHOR_TAG use syntactic information ( pos tagging / chunking, and dependency parsing respectively ) to extract entity and relation phrases. however, unlike openie, we are interested in normalized entities and relations (', 'i. e. that map to a knowledge base ). in this work, we follow a common way of producing training examples for re is to use distant supervision  #AUTHOR_TAG : the assumption is that if any sentence mentions two entities which we know ( from a kb ) participate', '']",3
"[' #TAUTHOR_TAG, and we wanted to establish a basis']","[' #TAUTHOR_TAG, and we wanted to establish a basis']","[' #TAUTHOR_TAG, and we wanted to establish a basis']","['also looked at the role of the dependency path satellite nodes ( words to the left and right of the entities ).', 'this type of features has also been adopted by various systems including  #AUTHOR_TAG and  #TAUTHOR_TAG, and we wanted to establish a basis for its effectiveness across multiple relations.', 'the results, shown in table 4 : effect of using all the supports for each x rel y triple using the fasttext classifier on the alexa kb data ( threshold of 0. 5 ).', 'table 5 : f - score results for the three relations on the alexa kb dataset.', 'the baseline system ( 1 ) is the fasttext classifier using the 5 most frequent supports for each x rel y triple, ( 1 ) - dep refers to the system with both the dependency relation and direction features removed, the last system uses all the ( lowercased ) words in each support as features']",3
['recent paper  #TAUTHOR_TAG proposed hypene'],"['recent paper  #TAUTHOR_TAG proposed hypenet, a new method for']",['recent paper  #TAUTHOR_TAG proposed hypene'],"['recent paper  #TAUTHOR_TAG proposed hypenet, a new method for re that integrated dependency path information with distributional semantic vector representation of the entities.', 'the authors applied this method to extract hyponyms ( i. e. instance of relations ) and also made a new version of their system publicly available.', '2 the training examples used ( entity / relation triples ) come from a number of sources like wordnet  #AUTHOR_TAG, yago  #AUTHOR_TAG, dbpedia and wikidata, and the source of the linguistic features ( part - of - speech tags, dependency paths, noun phrases ) was the 2015 dump of wikipedia, processed using the spacy system 3.', 'their proposed system achieved by far the best results on their dataset.', 'since instance of is one of the most often used relations ( most of the uses are implicit, during inference ), we decided to investigate hypenet as the base of our re system.', 'the training examples used by the authors of hypenet consisted of facts about only one relation.', 'we wanted to build a system that works on multiple relations at a very large scale.', 'hence, in this work we use two different dataset sources : wikidata, a publicly - available large - scale kb to aid reproducibility, as well as the larger alexa kb, built by combining a hand - curated ontology with publicly available data from wikidata, wikipedia, freebase, dbpedia, and other sources']",0
"['1 presents the distant supervision generation process adapted from  #TAUTHOR_TAG to work with our data.', 'in the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases ( nps ) - these are the candidate entities.', '']","['1 presents the distant supervision generation process adapted from  #TAUTHOR_TAG to work with our data.', 'in the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases ( nps ) - these are the candidate entities.', '']","['of figure 1 presents the distant supervision generation process adapted from  #TAUTHOR_TAG to work with our data.', 'in the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases ( nps ) - these are the candidate entities.', '']","['bottom half of figure 1 presents the distant supervision generation process adapted from  #TAUTHOR_TAG to work with our data.', 'in the original work, the text is processed to split and tokenise the sentences, tag the parts of speech and separate the noun phrases ( nps ) - these are the candidate entities.', 'they then construct the dependency path between each possible pair of entities.', 'each noun / np pair is checked against the kb for distant supervision.', 'keep only the entities / paths that appear in the list of labelled examples.', 'they also filter out entity pairs that have infrequent paths ( occurring fewer than five times ), and pairs whose path is more than five tokens long.', 'however, as discussed in the beginning of this section, this approach introduces a lot of noise.', 'to avoid this problem, we use the pagespecific gazetteer and a greedy string matching system to scan through the unstructured text and assign kb ids to the longest - matching substring in a sentence.', 'the final step was to generate the annotation labels themselves.', 'to do that, we examine each possible pair of entities to see if they participate in the target relation.', 'for the wikidata kb, we simply checked whether the target relation existed as a property in the data.', '']",6
"['discover the effectiveness of the approach of  #TAUTHOR_TAG, we wanted to separate hypene']","[""discover the effectiveness of the approach of  #TAUTHOR_TAG, we wanted to separate hypenet's neural architecture from""]","['discover the effectiveness of the approach of  #TAUTHOR_TAG, we wanted to separate hypene']","[""discover the effectiveness of the approach of  #TAUTHOR_TAG, we wanted to separate hypenet's neural architecture from its input features and use those features with different ( and simpler ) classifiers."", ""hypenet's main advantage is that it integrated dependency path features with distributional information about the word lemmas along the path and left and right entities."", 'as our goal was to generate discrete features to be used with more traditional classifiers, we opted for using brown clusters  #AUTHOR_TAG instead of the 50 - dimensional glove vectors  #AUTHOR_TAG used by  #TAUTHOR_TAG.', 'the brown clusters were pre - trained on the reuters corpus vol.', '1  #AUTHOR_TAG using 3, 200 clusters.', 'after evaluating different feature configurations ( see section 6. 7. ), the resulting features were as follows : for each entity pair and for each support, we extracted the dependency path between them and concatenated the lemma, 4 - bit prefix of brown cluster of the lemma, part of speech, dependency relation, and path direction information ; to that we added the strings and 4 - bit brown cluster prefix of the left and right entities.', 'the features from different supports were concatenated into one feature list.', 'for example, given the following sentences containing the entity pair carrie fisher, star wars : "" in 1977, fisher starred in george lucas\'film star wars "", and "" fisher became known for playing princess leia in the star wars film series "".', 'the following is the full list of discrete features extracted, where each space - separated token is a distinct feature, and x and y are used to replace the left and right entities']",1
"['discover the effectiveness of the approach of  #TAUTHOR_TAG, we wanted to separate hypene']","[""discover the effectiveness of the approach of  #TAUTHOR_TAG, we wanted to separate hypenet's neural architecture from""]","['discover the effectiveness of the approach of  #TAUTHOR_TAG, we wanted to separate hypene']","[""discover the effectiveness of the approach of  #TAUTHOR_TAG, we wanted to separate hypenet's neural architecture from its input features and use those features with different ( and simpler ) classifiers."", ""hypenet's main advantage is that it integrated dependency path features with distributional information about the word lemmas along the path and left and right entities."", 'as our goal was to generate discrete features to be used with more traditional classifiers, we opted for using brown clusters  #AUTHOR_TAG instead of the 50 - dimensional glove vectors  #AUTHOR_TAG used by  #TAUTHOR_TAG.', 'the brown clusters were pre - trained on the reuters corpus vol.', '1  #AUTHOR_TAG using 3, 200 clusters.', 'after evaluating different feature configurations ( see section 6. 7. ), the resulting features were as follows : for each entity pair and for each support, we extracted the dependency path between them and concatenated the lemma, 4 - bit prefix of brown cluster of the lemma, part of speech, dependency relation, and path direction information ; to that we added the strings and 4 - bit brown cluster prefix of the left and right entities.', 'the features from different supports were concatenated into one feature list.', 'for example, given the following sentences containing the entity pair carrie fisher, star wars : "" in 1977, fisher starred in george lucas\'film star wars "", and "" fisher became known for playing princess leia in the star wars film series "".', 'the following is the full list of discrete features extracted, where each space - separated token is a distinct feature, and x and y are used to replace the left and right entities']",4
"['distant supervision labels  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG,']","['distant supervision labels  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG,']","['noise in the distant supervision labels  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG, the grouping was performed by the mean pooling layer ; in']","['also wanted to investigate the effect of grouping the supports ( sentences ) for each entity pair.', 'as mentioned earlier ( section 2. ), this had been proposed as a method table 3 : effect of using dependency path satellite nodes for each x rel y triple using the fasttext classifier on the alexa kb data ( threshold of 0. 5 ).', 'to reduce noise in the distant supervision labels  #AUTHOR_TAG.', 'in  #TAUTHOR_TAG, the grouping was performed by the mean pooling layer ; in the case of the fasttext - based system, we simply concatenate the feature tokens from all the supports and feed them into the single hidden layer.', ""for each of the three relations, we ran the fasttext model with and without grouping each entity pair's supports, using exactly the same features in both cases."", 'the table 2 presents the results.', 'interestingly, the effect on instance of is much smaller than on the other two relations.', 'one possible explanation could be that the page - specific gazetteer method is producing fewer false positives for that relation ; more likely, the supports for birthplace of and applies to are more diverse than those of instance of, making their grouping more useful to the classifier']",4
"['by  #TAUTHOR_TAG.', 'the original method']","['by  #TAUTHOR_TAG.', 'the original method']","['by  #TAUTHOR_TAG.', 'the original method']","['goal of the method presented in section 3. 1. was to reduce the number of false positives at the cost of introducing some amount of false negatives ( due to missing entities, missing denotations, or missing kb facts ).', 'in order to quantify the effect of the new method, we manually annotated 1, 000 instance of distant supervision examples produced by our new method and the original method used by  #TAUTHOR_TAG.', '']",7
"['by  #TAUTHOR_TAG.', 'the original method']","['by  #TAUTHOR_TAG.', 'the original method']","['by  #TAUTHOR_TAG.', 'the original method']","['goal of the method presented in section 3. 1. was to reduce the number of false positives at the cost of introducing some amount of false negatives ( due to missing entities, missing denotations, or missing kb facts ).', 'in order to quantify the effect of the new method, we manually annotated 1, 000 instance of distant supervision examples produced by our new method and the original method used by  #TAUTHOR_TAG.', '']",7
"['system of  #TAUTHOR_TAG.', 'table 5 presents the feature ablation results on the alexa kb data using the fasttext classifier.', 'we compare']","['system of  #TAUTHOR_TAG.', 'table 5 presents the feature ablation results on the alexa kb data using the fasttext classifier.', 'we compare']","['the system of  #TAUTHOR_TAG.', 'table 5 presents the feature ablation results on the alexa kb data using the fasttext classifier.', 'we compare']","['a final step in our exploration, we wanted to measure the impact of each of the features used by the system of  #TAUTHOR_TAG.', 'table 5 presents the feature ablation results on the alexa kb data using the fasttext classifier.', 'we compare the full set of features presented in section 4.', 'against feature sets without the brown clusters, word lemmas, pos tags, dependency information, and the x and y entities ( and their brown cluster ).', 'we also show the results of the system using only the x and y entities and just the words of the supporting sentences ( without extracting the dependency path between entities ).', ""the main takeaway is that for the instance of and applies to relations, the structure induced by the dependency parser is critical for the system's performance."", ""one explanation is that these relations are not always lexically defined ( sometimes expressed with just the verb'to be'across long subordinate clauses )."", ""for the birthplace of relation, the system using the full sentence is on par with the best dependencysupported version suggesting that there are strong lexical cues that signify them ( like'born in ', or just the presence of a city name )""]",7
"['speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing']","['speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing']","['estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing the monolingual pidgin english as seen in the bbc pidgin 2, it remains']","['##gin english is one of the the most widely spoken languages in west africa with roughly 75 million speakers estimated in nigeria ; and over 5 million speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing the monolingual pidgin english as seen in the bbc pidgin 2, it remains under - resourced in terms of the available parallel corpus for machine translation.', 'similarly, this low - resource scenario extends to other domains in natural language generation ( nlg ) such as summarization, data - to - text and so on  #AUTHOR_TAG a ; b ;  #AUTHOR_TAG de  #AUTHOR_TAG − where pidgin english generation is largely under - explored.', 'the scarcity is further aggravated when the pipeline language generation system includes other sub - modules that computes semantic textual similarity  #AUTHOR_TAG, which exists solely in english.', 'previous works on unsupervised neural machine translation for pidgin english constructed a monolingual corpus  #TAUTHOR_TAG, and achieved a bleu score of 5. 18 from english to pidgin.', 'however, there is an issue of domain mismatch between down - stream nlg tasks and the trained machine translation system.', '']",0
"['speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing']","['speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing']","['estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing the monolingual pidgin english as seen in the bbc pidgin 2, it remains']","['##gin english is one of the the most widely spoken languages in west africa with roughly 75 million speakers estimated in nigeria ; and over 5 million speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing the monolingual pidgin english as seen in the bbc pidgin 2, it remains under - resourced in terms of the available parallel corpus for machine translation.', 'similarly, this low - resource scenario extends to other domains in natural language generation ( nlg ) such as summarization, data - to - text and so on  #AUTHOR_TAG a ; b ;  #AUTHOR_TAG de  #AUTHOR_TAG − where pidgin english generation is largely under - explored.', 'the scarcity is further aggravated when the pipeline language generation system includes other sub - modules that computes semantic textual similarity  #AUTHOR_TAG, which exists solely in english.', 'previous works on unsupervised neural machine translation for pidgin english constructed a monolingual corpus  #TAUTHOR_TAG, and achieved a bleu score of 5. 18 from english to pidgin.', 'however, there is an issue of domain mismatch between down - stream nlg tasks and the trained machine translation system.', '']",0
"['speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing']","['speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing']","['estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing the monolingual pidgin english as seen in the bbc pidgin 2, it remains']","['##gin english is one of the the most widely spoken languages in west africa with roughly 75 million speakers estimated in nigeria ; and over 5 million speakers estimated in ghana  #TAUTHOR_TAG.', '1 while there have been recent efforts in popularizing the monolingual pidgin english as seen in the bbc pidgin 2, it remains under - resourced in terms of the available parallel corpus for machine translation.', 'similarly, this low - resource scenario extends to other domains in natural language generation ( nlg ) such as summarization, data - to - text and so on  #AUTHOR_TAG a ; b ;  #AUTHOR_TAG de  #AUTHOR_TAG − where pidgin english generation is largely under - explored.', 'the scarcity is further aggravated when the pipeline language generation system includes other sub - modules that computes semantic textual similarity  #AUTHOR_TAG, which exists solely in english.', 'previous works on unsupervised neural machine translation for pidgin english constructed a monolingual corpus  #TAUTHOR_TAG, and achieved a bleu score of 5. 18 from english to pidgin.', 'however, there is an issue of domain mismatch between down - stream nlg tasks and the trained machine translation system.', '']",1
['requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidgin'],"['requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidginunmt ).', 'similar to  #TAUTHOR_TAG, we']",['phase of the approach requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidgin'],"['phase of the approach requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidginunmt ).', 'similar to  #TAUTHOR_TAG, we train the cross - lingual model using fast  #AUTHOR_TAG on the combined pidgin - english corpus.', 'next, we train an unsupervised nmt similar to  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG between them to obtain model unsup.', 'then we further utilize model unsup to construct pseudo parallel corpus by predicting target pidgin text given the english input.', 'we augment this dataset to the existing monolingual corpus.', 'the self - training step involves further updating model unsup on the pseudo parallel corpus and non - parallel monolingual corpus to yield model self']",3
['requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidgin'],"['requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidginunmt ).', 'similar to  #TAUTHOR_TAG, we']",['phase of the approach requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidgin'],"['phase of the approach requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidginunmt ).', 'similar to  #TAUTHOR_TAG, we train the cross - lingual model using fast  #AUTHOR_TAG on the combined pidgin - english corpus.', 'next, we train an unsupervised nmt similar to  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG between them to obtain model unsup.', 'then we further utilize model unsup to construct pseudo parallel corpus by predicting target pidgin text given the english input.', 'we augment this dataset to the existing monolingual corpus.', 'the self - training step involves further updating model unsup on the pseudo parallel corpus and non - parallel monolingual corpus to yield model self']",3
['requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidgin'],"['requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidginunmt ).', 'similar to  #TAUTHOR_TAG, we']",['phase of the approach requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidgin'],"['phase of the approach requires training of an unsupervised nmt system similar to  #TAUTHOR_TAG ( pidginunmt ).', 'similar to  #TAUTHOR_TAG, we train the cross - lingual model using fast  #AUTHOR_TAG on the combined pidgin - english corpus.', 'next, we train an unsupervised nmt similar to  #AUTHOR_TAG ;  #AUTHOR_TAG ;  #TAUTHOR_TAG between them to obtain model unsup.', 'then we further utilize model unsup to construct pseudo parallel corpus by predicting target pidgin text given the english input.', 'we augment this dataset to the existing monolingual corpus.', 'the self - training step involves further updating model unsup on the pseudo parallel corpus and non - parallel monolingual corpus to yield model self']",3
"['extracting bilingual segments from a parallel corpus in a syntax - oriented manner.', 'in  #TAUTHOR_TAG, sitgs were used']","['extracting bilingual segments from a parallel corpus in a syntax - oriented manner.', 'in  #TAUTHOR_TAG, sitgs were used']","['extracting bilingual segments from a parallel corpus in a syntax - oriented manner.', 'in  #TAUTHOR_TAG, sitgs were used']","['machine translation ( smt ) systems have proved in the last years to be an important alternative to rule - based mt systems, being even able of outperforming commercial machine translation systems in the tasks they have been trained on.', 'phrase - based ( pb ) models  #AUTHOR_TAG have proved to provide a very efficient framework for smt.', 'an important issue when training pb models is the algorithm by means of which the bilingual phrases are extracted.', 'hence, a wide variety of methods have been proposed for this purpose, spanning through statistically motivated procedures  #AUTHOR_TAG, heuristic algorithms  #AUTHOR_TAG, and linguistically motivated methods ( sanchez and benedi, 2006a ).', 'in this work, we will be following this last approach, which relies on stochastic inverse transduction grammars ( sitgs )  #AUTHOR_TAG for phrase extraction.', 'sitgs constitute a restricted subset of syntax directed stochastic grammars for translation, and are very related to context - free grammars.', 'these can be used to analyse two strings simultaneously, which makes them specially useful for extracting bilingual segments from a parallel corpus in a syntax - oriented manner.', 'in  #TAUTHOR_TAG, sitgs were used for obtaining word phrases, reporting preliminary results on the europarl corpus.', 'in this work, we extend that work by using bracketed corpora for estimating the stigs.', 'in section 2, we will briefly review the phrase - based smt approach.', 'next, in section 3, we will sum up the grounds of sitgs and the modifications proposed in ( sanchez and benedi, 2006a ).', 'in section 4, we present the translation results on the europarl corpus, obtained when applying one learning iteration on sitgs with several number of nonterminals']",0
['that  #TAUTHOR_TAG reported in the spanish - english'],['that  #TAUTHOR_TAG reported in the spanish - english'],"[', the best result that  #TAUTHOR_TAG reported in the spanish - english task was a bleu score of 23']","[', the best result that  #TAUTHOR_TAG reported in the spanish - english task was a bleu score of 23. 0, which they obtained by combining segments extracted from both the bracketed and the non - bracketed corpus.', 'we have widely exceeded this baseline.', 'on the other hand, the moses toolkit  #AUTHOR_TAG, which is a state of the art statistical machine translation system, obtains in this task a score of 31. 0 bleu.', '']",0
"['extracting bilingual segments from a parallel corpus in a syntax - oriented manner.', 'in  #TAUTHOR_TAG, sitgs were used']","['extracting bilingual segments from a parallel corpus in a syntax - oriented manner.', 'in  #TAUTHOR_TAG, sitgs were used']","['extracting bilingual segments from a parallel corpus in a syntax - oriented manner.', 'in  #TAUTHOR_TAG, sitgs were used']","['machine translation ( smt ) systems have proved in the last years to be an important alternative to rule - based mt systems, being even able of outperforming commercial machine translation systems in the tasks they have been trained on.', 'phrase - based ( pb ) models  #AUTHOR_TAG have proved to provide a very efficient framework for smt.', 'an important issue when training pb models is the algorithm by means of which the bilingual phrases are extracted.', 'hence, a wide variety of methods have been proposed for this purpose, spanning through statistically motivated procedures  #AUTHOR_TAG, heuristic algorithms  #AUTHOR_TAG, and linguistically motivated methods ( sanchez and benedi, 2006a ).', 'in this work, we will be following this last approach, which relies on stochastic inverse transduction grammars ( sitgs )  #AUTHOR_TAG for phrase extraction.', 'sitgs constitute a restricted subset of syntax directed stochastic grammars for translation, and are very related to context - free grammars.', 'these can be used to analyse two strings simultaneously, which makes them specially useful for extracting bilingual segments from a parallel corpus in a syntax - oriented manner.', 'in  #TAUTHOR_TAG, sitgs were used for obtaining word phrases, reporting preliminary results on the europarl corpus.', 'in this work, we extend that work by using bracketed corpora for estimating the stigs.', 'in section 2, we will briefly review the phrase - based smt approach.', 'next, in section 3, we will sum up the grounds of sitgs and the modifications proposed in ( sanchez and benedi, 2006a ).', 'in section 4, we present the translation results on the europarl corpus, obtained when applying one learning iteration on sitgs with several number of nonterminals']",6
"[', we built an initial sitg by following the method described in  #TAUTHOR_TAG']","[', we built an initial sitg by following the method described in  #TAUTHOR_TAG']","[', we built an initial sitg by following the method described in  #TAUTHOR_TAG.', 'then, both source and target languages in the training corpus were bracketed by using freeling  #AUTHOR_TAG, which is an opensource suite of language analysers.', 'this being done,']","[', we built an initial sitg by following the method described in  #TAUTHOR_TAG.', 'then, both source and target languages in the training corpus were bracketed by using freeling  #AUTHOR_TAG, which is an opensource suite of language analysers.', 'this being done, we then used the bracketed corpus to perform one estimation iteration on the initial sitg and obtain improved sitgs.', '']",5
"[', we built an initial sitg by following the method described in  #TAUTHOR_TAG']","[', we built an initial sitg by following the method described in  #TAUTHOR_TAG']","[', we built an initial sitg by following the method described in  #TAUTHOR_TAG.', 'then, both source and target languages in the training corpus were bracketed by using freeling  #AUTHOR_TAG, which is an opensource suite of language analysers.', 'this being done,']","[', we built an initial sitg by following the method described in  #TAUTHOR_TAG.', 'then, both source and target languages in the training corpus were bracketed by using freeling  #AUTHOR_TAG, which is an opensource suite of language analysers.', 'this being done, we then used the bracketed corpus to perform one estimation iteration on the initial sitg and obtain improved sitgs.', '']",5
['that  #TAUTHOR_TAG reported in the spanish - english'],['that  #TAUTHOR_TAG reported in the spanish - english'],"[', the best result that  #TAUTHOR_TAG reported in the spanish - english task was a bleu score of 23']","[', the best result that  #TAUTHOR_TAG reported in the spanish - english task was a bleu score of 23. 0, which they obtained by combining segments extracted from both the bracketed and the non - bracketed corpus.', 'we have widely exceeded this baseline.', 'on the other hand, the moses toolkit  #AUTHOR_TAG, which is a state of the art statistical machine translation system, obtains in this task a score of 31. 0 bleu.', '']",4
[' #TAUTHOR_TAG that collected lexical substitutes'],[' #TAUTHOR_TAG that collected lexical substitutes'],"['- semantic annotation for german  #AUTHOR_TAG.', 'the definition of frames proceeds in a corpusbased fashion, driven by the data  #AUTHOR_TAG.', 'we stand in this tradition by reporting on a recent annotation effort  #TAUTHOR_TAG that collected lexical substitutes']","['', 'this idea of a semantics of understanding, or a frame semantics, has been made concrete in framenet  #AUTHOR_TAG, a large lexical database that lists frames for english words and constructions.', 'at this point, it comprises more than 1, 100 frames covering more than 12, 000 lexical units ( lus ), which are pairs of a term and its frame.', 'researchers working on other languages have adopted the framenet idea.', 'among others, there are now framenet resources for spanish  #AUTHOR_TAG, japanese  #AUTHOR_TAG, italian  #AUTHOR_TAG, as well as frame - semantic annotation for german  #AUTHOR_TAG.', 'the definition of frames proceeds in a corpusbased fashion, driven by the data  #AUTHOR_TAG.', 'we stand in this tradition by reporting on a recent annotation effort  #TAUTHOR_TAG that collected lexical substitutes for content words in part of the masc corpus  #AUTHOR_TAG.', 'if we view substitute sets as indications of the relevant frame, then this data can give us interesting indicators on perceived frames in a naturally occurring text']",7
"['sentences  #TAUTHOR_TAG.', 'in addition, there is']","['sentences  #TAUTHOR_TAG.', 'in addition, there is']","['sentences  #TAUTHOR_TAG.', 'in addition, there is a cross - lingual lexical substitution dataset']","['lexical substitution task was first introduced in the context of semeval 2007 ( mc  #AUTHOR_TAG.', 'for this dataset, annotators are asked to provide substitutes for a selected word ( the target word ) in its sentence context - at least one substitute, but possible more, and ideally a single word, though all the datasets contain some multi - word substitutes.', 'multiple annotators provide substitutes for each target word occurrence.', 'table 1 shows some examples.', 'by now, several lexical substitution datasets exist.', 'some are "" lexical sample "" datasets, that is, only occurrences of some selected lemmas are annotated ( mc  #AUTHOR_TAG, and some are "" all - words "", providing substitutes for all content words in the given sentences  #TAUTHOR_TAG.', 'in addition, there is a cross - lingual lexical substitution dataset ( mc  #AUTHOR_TAG, where annotators provided spanish substitutes for english target words in english sentence context.', 'lexical substitution is a method for characterizing word meaning in context that has several attractive properties.', 'lexical substitution makes it possible to describe word meaning without having to rely on any particular dictionary.', 'in addi - table 2 : analysis of lexical substitution data : relation of the substitute to the target, in percentages by part of speech ( from  #AUTHOR_TAG ) tion, providing substitutes is a task that seems to be well doable by untrained annotators :  #AUTHOR_TAG and our recent annotation  #TAUTHOR_TAG used crowdsourcing to collect the substitutes.', '']",0
"['sentences  #TAUTHOR_TAG.', 'in addition, there is']","['sentences  #TAUTHOR_TAG.', 'in addition, there is']","['sentences  #TAUTHOR_TAG.', 'in addition, there is a cross - lingual lexical substitution dataset']","['lexical substitution task was first introduced in the context of semeval 2007 ( mc  #AUTHOR_TAG.', 'for this dataset, annotators are asked to provide substitutes for a selected word ( the target word ) in its sentence context - at least one substitute, but possible more, and ideally a single word, though all the datasets contain some multi - word substitutes.', 'multiple annotators provide substitutes for each target word occurrence.', 'table 1 shows some examples.', 'by now, several lexical substitution datasets exist.', 'some are "" lexical sample "" datasets, that is, only occurrences of some selected lemmas are annotated ( mc  #AUTHOR_TAG, and some are "" all - words "", providing substitutes for all content words in the given sentences  #TAUTHOR_TAG.', 'in addition, there is a cross - lingual lexical substitution dataset ( mc  #AUTHOR_TAG, where annotators provided spanish substitutes for english target words in english sentence context.', 'lexical substitution is a method for characterizing word meaning in context that has several attractive properties.', 'lexical substitution makes it possible to describe word meaning without having to rely on any particular dictionary.', 'in addi - table 2 : analysis of lexical substitution data : relation of the substitute to the target, in percentages by part of speech ( from  #AUTHOR_TAG ) tion, providing substitutes is a task that seems to be well doable by untrained annotators :  #AUTHOR_TAG and our recent annotation  #TAUTHOR_TAG used crowdsourcing to collect the substitutes.', '']",0
"['a recent lexical substitution annotation effort  #TAUTHOR_TAG, we collected lexical substitution annotation for all nouns, verbs, and adjectives in a mixed news and fiction corpus, using untrained annotators via crowdsour']","['a recent lexical substitution annotation effort  #TAUTHOR_TAG, we collected lexical substitution annotation for all nouns, verbs, and adjectives in a mixed news and fiction corpus, using untrained annotators via crowdsourcing.', 'the data came from masc, a freely available part of']","['a recent lexical substitution annotation effort  #TAUTHOR_TAG, we collected lexical substitution annotation for all nouns, verbs, and adjectives in a mixed news and fiction corpus, using untrained annotators via crowdsour']","['a recent lexical substitution annotation effort  #TAUTHOR_TAG, we collected lexical substitution annotation for all nouns, verbs, and adjectives in a mixed news and fiction corpus, using untrained annotators via crowdsourcing.', 'the data came from masc, a freely available part of the american national corpus that has already been annotated for a number of linguistic phenomena  #AUTHOR_TAG.', 'all in all, more than 15, 000 target tokens were annotated.', 'after the annotation, we performed a number of analyses in order to better understand the nature of lexical substitutes, by linking substitutes to information on wordnet  #AUTHOR_TAG.', 'among other things, we analyzed the relation between targets and substitutes : did substitutes tend to be synonyms, hypernyms, or hyponyms or the targets?', 'to classify substitutes, the shortest route from any synset of the target to any synset of the substitute was used.', 'the results are shown in table 2, for substitutes that are synonyms ( syn ), hypernyms ( direct - hyper, trans - hyper ) and hyponyms ( direct - hypo, trans - hypo ) of the target.', '']",0
['as a speech version of word2vec  #TAUTHOR_TAG'],['as a speech version of word2vec  #TAUTHOR_TAG'],"['as a speech version of word2vec  #TAUTHOR_TAG.', 'its design is based on a rnn encoder - decoder framework, and borrows the methodology of skipgrams or continuous bag - of - words']","['this paper, we propose a novel deep neural network architecture, speech2vec, for learning fixed - length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar.', 'the proposed model can be viewed as a speech version of word2vec  #TAUTHOR_TAG.', 'its design is based on a rnn encoder - decoder framework, and borrows the methodology of skipgrams or continuous bag - of - words for training.', 'learning word embeddings directly from speech enables speech2vec to make use of the semantic information carried by speech that does not exist in plain text.', 'the learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by word2vec from the transcriptions']",6
"['training, extends the textbased word2vec  #TAUTHOR_TAG model to learn word embeddings directly from speech.', 'speech2']","['training, extends the textbased word2vec  #TAUTHOR_TAG model to learn word embeddings directly from speech.', 'speech2vec has access to richer information in the speech signal that']","['training, extends the textbased word2vec  #TAUTHOR_TAG model to learn word embeddings directly from speech.', 'speech2']","['##2vec, which integrates a rnn encoder - decoder framework with skipgrams or cbow for training, extends the textbased word2vec  #TAUTHOR_TAG model to learn word embeddings directly from speech.', 'speech2vec has access to richer information in the speech signal that does not exist in plain text.', 'in our experiments, the learned word embeddings outperform those produced by word2vec from the transcriptions.', 'in the future, we plan to evaluate the word embeddings on speech - related extrinsic tasks such as machine listening comprehension [ 44, 45 ] and speech - based visual question answering [ 46 ] by initializing the embedding layers of the neural network models.', 'finally, in this work, some supervision was incorporated into the learning by using forced alignment segmentations as the basis for audio segments.', 'it would be interesting to explore less supervised segmentations to learn word boundaries [ 47, 48 ]']",6
['word2vec  #TAUTHOR_TAG 2 ]'],['word2vec  #TAUTHOR_TAG 2 ]'],['word2vec  #TAUTHOR_TAG 2 ]'],"['language processing ( nlp ) techniques such as word2vec  #TAUTHOR_TAG 2 ] and glove [ 3 ] transform words into fixed dimensional vectors, or word embeddings.', 'the embeddings are obtained via unsupervised learning from co - occurrence information in text, and contain semantic information about the word which are useful for many nlp tasks [ 4, 5, 6, 7, 8 ].', 'researchers have also explored the concept of learning vector representations from speech [ 9, 10, 11, 12, 13, 14 ].', 'these approaches are based on notions of acoustic - phonetic ( rather than semantic ) similarity, so that different instances of the same underlying word would map to the same point in a latent embedding space.', 'our work, highly inspired by word2vec  #TAUTHOR_TAG, uses a skipgrams or continuous bag - of - words formulation to focus on neighboring acoustic regions, rather than the acoustic segment associated with the word itself.', 'we show that the resulting acoustic embedding space is more semantic in nature.', 'recent research by [ 15, 16, 17 ] has presented a deep neural network model capable of rudimentary spoken language acquisition using raw speech training data paired with contextually relevant images.', 'using this contextual grounding, the model learned a latent semantic audio - visual embedding space.', 'in this paper, we propose a deep neural network architecture capable of learning embeddings of audio segments corresponding to words from raw speech without any other modalities.', 'the proposed model, called speech2vec, integrates an rnn encoderdecoder framework [ 18, 19 ] with the concept of skipgrams or continuous bag - of - words, and can handle arbitrary length audio segments.', 'the resulting word embeddings contain information pertaining to the meaning of the underlying spoken words such that semantically similar words produce vector representations that are nearby in the embedding space.', 'speech2vec can be viewed as a speech version of word2vec.', 'traditionally, when we want to learn word embeddings from speech, we need to first transcribe the speech into text by an asr system, then apply a textual word embedding method on the transcripts.', 'the motivations for this work are that learning word embeddings directly from speech surmounts the recognition errors caused by the process of transcribing.', 'moreover, speech contains richer information than text such as prosody, and a machine should be able to make use of this information in order to learn better semantic representations.', 'in this paper, we build on a preliminary version of the speech2vec model [ 20 ] by introducing additional methodologies and details for training the model, comparing the model with additional baseline approaches, and providing systematic analysis']",0
['word2vec  #TAUTHOR_TAG 2 ]'],['word2vec  #TAUTHOR_TAG 2 ]'],['word2vec  #TAUTHOR_TAG 2 ]'],"['language processing ( nlp ) techniques such as word2vec  #TAUTHOR_TAG 2 ] and glove [ 3 ] transform words into fixed dimensional vectors, or word embeddings.', 'the embeddings are obtained via unsupervised learning from co - occurrence information in text, and contain semantic information about the word which are useful for many nlp tasks [ 4, 5, 6, 7, 8 ].', 'researchers have also explored the concept of learning vector representations from speech [ 9, 10, 11, 12, 13, 14 ].', 'these approaches are based on notions of acoustic - phonetic ( rather than semantic ) similarity, so that different instances of the same underlying word would map to the same point in a latent embedding space.', 'our work, highly inspired by word2vec  #TAUTHOR_TAG, uses a skipgrams or continuous bag - of - words formulation to focus on neighboring acoustic regions, rather than the acoustic segment associated with the word itself.', 'we show that the resulting acoustic embedding space is more semantic in nature.', 'recent research by [ 15, 16, 17 ] has presented a deep neural network model capable of rudimentary spoken language acquisition using raw speech training data paired with contextually relevant images.', 'using this contextual grounding, the model learned a latent semantic audio - visual embedding space.', 'in this paper, we propose a deep neural network architecture capable of learning embeddings of audio segments corresponding to words from raw speech without any other modalities.', 'the proposed model, called speech2vec, integrates an rnn encoderdecoder framework [ 18, 19 ] with the concept of skipgrams or continuous bag - of - words, and can handle arbitrary length audio segments.', 'the resulting word embeddings contain information pertaining to the meaning of the underlying spoken words such that semantically similar words produce vector representations that are nearby in the embedding space.', 'speech2vec can be viewed as a speech version of word2vec.', 'traditionally, when we want to learn word embeddings from speech, we need to first transcribe the speech into text by an asr system, then apply a textual word embedding method on the transcripts.', 'the motivations for this work are that learning word embeddings directly from speech surmounts the recognition errors caused by the process of transcribing.', 'moreover, speech contains richer information than text such as prosody, and a machine should be able to make use of this information in order to learn better semantic representations.', 'in this paper, we build on a preliminary version of the speech2vec model [ 20 ] by introducing additional methodologies and details for training the model, comparing the model with additional baseline approaches, and providing systematic analysis']",1
"['words, and includes the tasks of word similarity and word analogy  #TAUTHOR_TAG.', 'in this paper, we focus on the intrinsic method, especially the word similarity task,']","['words, and includes the tasks of word similarity and word analogy  #TAUTHOR_TAG.', 'in this paper, we focus on the intrinsic method, especially the word similarity task,']","['words, and includes the tasks of word similarity and word analogy  #TAUTHOR_TAG.', 'in this paper, we focus on the intrinsic method, especially the word similarity task,']","['schemes for evaluating methods for word embeddings fall into two major categories : extrinsic and intrinsic [ 29 ].', 'with the extrinsic method, the learned word embeddings are used as input features to a downstream task [ 4, 5, 6, 7, 8 ], and the performance metric varies from task to task.', 'the intrinsic method directly tests for semantic or syntactic relationships between words, and includes the tasks of word similarity and word analogy  #TAUTHOR_TAG.', 'in this paper, we focus on the intrinsic method, especially the word similarity task, for evaluating and analyzing the speech2vec word embeddings.', 'we used 13 benchmarks [ 30 ] [ 40 ], and simverb - 3500 [ 41 ].', 'these 13 benchmarks contain different numbers of pairs of english words that have been assigned similarity ratings by humans, and each of them evaluates the word embeddings in terms of different aspects.', 'for example, rg - 65 and mc - 30 focus on nouns, yc - 130 and simverb - 3500 focus on verbs, and rare - word focuses on rare - words.', 'the similarity between a given pair of words was calculated by computing the cosine similarity between their corresponding word embeddings.', '']",5
"['aligns with the empirical fact that skipgrams word2vec is likely to work better than cbow word2vec with small training corpus size  #TAUTHOR_TAG.', 'training size']","['aligns with the empirical fact that skipgrams word2vec is likely to work better than cbow word2vec with small training corpus size  #TAUTHOR_TAG.', 'training size']","['cbow speech2vec on all benchmarks for all embedding sizes.', 'this result aligns with the empirical fact that skipgrams word2vec is likely to work better than cbow word2vec with small training corpus size  #TAUTHOR_TAG.', '']","['', 'for cbow speech2vec, skipgrams speech2vec, and cbow word2vec, word embeddings of 50 - dimensions are able to capture enough semantic information of the words, as the best performance ( highest ρ ) of each benchmark is mostly achieved by them.', 'for skipgrams word2vec, although the best performance of 7 out of 13 benchmarks is achieved by word embeddings of 200 - dims, there are 6 benchmarks whose best performance is achieved by word embeddings of other sizes.', 'comparing speech2vec to word2vec.', 'from table 1 we see that skipgrams speech2vec achieves the highest ρ in 8 out of 13 benchmarks, outperforming cbow and skipgrams word2vec in combination.', ""we believe a possible reason for such results is due to skipgrams speech2vec's ability to capture semantic information present in speech such as prosody that is not in text."", 'comparing skipgrams to cbow speech2vec.', 'from table 1 we observe that skipgrams speech2vec consistently outperforms cbow speech2vec on all benchmarks for all embedding sizes.', 'this result aligns with the empirical fact that skipgrams word2vec is likely to work better than cbow word2vec with small training corpus size  #TAUTHOR_TAG.', '']",3
['on machine translation ( mt ) evaluation metrics :  #TAUTHOR_TAG conduct a study on the usefulness of automated mt evaluation metrics ('],"['on machine translation ( mt ) evaluation metrics :  #TAUTHOR_TAG conduct a study on the usefulness of automated mt evaluation metrics ( e. g., bleu, nist and meteor )']",['on machine translation ( mt ) evaluation metrics :  #TAUTHOR_TAG conduct a study on the usefulness of automated mt evaluation metrics ('],"['feature approaches :  #AUTHOR_TAG edit distance, which is based on characterlevel removal, insertion, and replacement operations, can be considered as one of the earliest works to measure text similarity.', 'buchler et al. ( 2012 ) use overlapping bi - grams to maximize recall in a reuse detection task of homeric quotations, showing a good precision of more than 70 % at the same time.', 'those techniques rely on surface features ( token and character - level ) only.', 'thus, our proposed method differs by also incorporating semantic information ( lexico - semantic relationships between aligned word pairs ).', 'semantic approaches : computing the semantic similarity between two sentences is a popular task in nlp  #AUTHOR_TAG.', ' #AUTHOR_TAG present a plagiarism detection technique based on semantic role labeling.', 'they analyze text by identifying the semantic space of each term in a sentence and find semantic arguments for each sentence.', 'they also assign weights to the arguments and find that not all of them affect plagiarism detection.', 'techniques from the field of paraphrase detection can be used for e. g., sentence similarity, entailment, and sentiment classification.', ' #AUTHOR_TAG use embedding models to identify paraphrastic sentences in such a mixed nlp task employing a large corpus of short phrases associated with paraphrastic relatives.', 'their simplest model represents a sentence embedding by the averaged vectors of its tokens, the most complex model is a long short - term memory ( lstm ) recurrent neural network.', 'they find that the word averaging model performs best on sentence similarity and entailment, and the lstm performs best on sentiment classification.', 'although these methods generally show good results, they typically allow no manual inspection of why a specific judgment is made and are thus ill - suited for applications in the humanities.', 'approaches based on machine translation ( mt ) evaluation metrics :  #TAUTHOR_TAG conduct a study on the usefulness of automated mt evaluation metrics ( e. g., bleu, nist and meteor ) for the task of paraphrase identification.', 'they train an ensemble of different classifiers using scores of mt metrics as features.', 'they evaluate their model on two corpora for paraphrase and plagiarism detection, respectively, finding that it performs very satisfyingly.', 'this approach to paraphrase and plagiarism detection based on mt metrics combines surface and semantic features since meteor incorporates synonymy information ( see below ).', 'yet, the number of semantic features used is limited and so is also the interpretability of this approach']",0
,,,,0
,,,,5
,,,,3
['recently by  #TAUTHOR_TAG'],['recently by  #TAUTHOR_TAG'],[' #AUTHOR_TAG berg -  #AUTHOR_TAG and recently by  #TAUTHOR_TAG'],"['', 'first explored for weakly supervised learning  #AUTHOR_TAG berg -  #AUTHOR_TAG and recently by  #TAUTHOR_TAG for multisource cross - lingual transfer. in particular,  #TAUTHOR_TAG showed that', 'by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of', 'parameters for all languages. however, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature - rich discriminative parsers', '( mc  #AUTHOR_TAG. in this paper, we improve upon the state of the art in cross - lingual transfer of dependency parsers from multiple source languages by adapting feature - rich discriminative', '##ly trained parsers to a specific target language. first, in § 4 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph - based parsing', '']",0
['recently by  #TAUTHOR_TAG'],['recently by  #TAUTHOR_TAG'],[' #AUTHOR_TAG berg -  #AUTHOR_TAG and recently by  #TAUTHOR_TAG'],"['', 'first explored for weakly supervised learning  #AUTHOR_TAG berg -  #AUTHOR_TAG and recently by  #TAUTHOR_TAG for multisource cross - lingual transfer. in particular,  #TAUTHOR_TAG showed that', 'by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of', 'parameters for all languages. however, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature - rich discriminative parsers', '( mc  #AUTHOR_TAG. in this paper, we improve upon the state of the art in cross - lingual transfer of dependency parsers from multiple source languages by adapting feature - rich discriminative', '##ly trained parsers to a specific target language. first, in § 4 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph - based parsing', '']",0
"[',  #TAUTHOR_TAG recently introduced a novel generative model of dependency parsing, in which the generative process is']","[',  #TAUTHOR_TAG recently introduced a novel generative model of dependency parsing, in which the generative process is']","[',  #TAUTHOR_TAG recently introduced a novel generative model of dependency parsing, in which the generative process is factored', 'into separate steps for the selection of dependents and their ordering.', 'the parameters used in the selection']","['dictionaries  #AUTHOR_TAG. 1 this parser is then directly used to parse the target language. for languages with similar typology, this method can be quite', 'accurate, especially when compared to purely unsupervised methods. for instance, a parser trained on english with only part -', 'of - speech features can correctly parse the greek sentence in figure 1, even without knowledge of the lexical items since the sequence of part - of - speech tags determines the syntactic structure almost unambiguously. learning with multiple languages has been shown to benefit unsupervised learning ( cohen and smith, 1 note that and  #AUTHOR_TAG', 'do require bitext or a bilingual dictionary. the same holds for', 'most cross - lingual representations, e. g.,  #AUTHOR_TAG. [UNK] τζον εδω', '##σε στην μαρια το βιβλιο. ( the ) ( john ) ( gave ) ( to - the ) ( maria ) ( the ) ( book ). 2009 ;  #AUTHOR_TAG berg -  #AUTHOR_TAG', '. annotations in multiple languages can be combined in delexicalized transfer as well, as long as the parser features are available across the involved languages.', 'this idea was explored by mc  #AUTHOR_TAG, who showed that target language accuracy can be improved by', 'simply concatenating delexicalized treebanks in multiple languages. in similar work,  #AUTHOR_TAG proposed a mixture model in which the parameters of', 'a generative target language parser is expressed as a linear interpolation of source language parameters, whereas søgaard ( 2011 ) showed that target side language models can be', 'used to selectively subsample training sentences to improve accuracy. recently, inspired by the phylogenetic prior of berg -  #AUTHOR_TAG, søgaard and  #AUTHOR_TAG proposed', '- among other ideas - a typologically informed weighting heuristic for linearly interpolating source language parameters. however, this weighting did not provide significant improvements over uniform weighting.', 'the aforementioned approaches work well for transfer between similar languages. however, their assumptions cease to hold for typologically divergent languages ; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological', 'traits ; this critical insight is discussed further in § 4. to account for this issue,  #TAUTHOR_TAG recently introduced a novel generative model of dependency parsing, in which the generative process is factored', 'into separate steps for the selection of dependents and their ordering.', 'the parameters used in the selection step are all language independent', ', capturing only head - dependent attachment preferences. in the ordering step, however, parameters are selectively shared between subsets of  #TAUTHOR_TAG restricts its potential performance']",0
['treebanks and experimental setup as  #TAUTHOR_TAG'],['treebanks and experimental setup as  #TAUTHOR_TAG'],"['facilitate comparison with the state of the art, we use the same treebanks and experimental setup as  #TAUTHOR_TAG.', 'notably, we use the mapping proposed by  #AUTHOR_TAG to map from fine -']","['facilitate comparison with the state of the art, we use the same treebanks and experimental setup as  #TAUTHOR_TAG.', 'notably, we use the mapping proposed by  #AUTHOR_TAG to map from fine - grained treebank specific part - of - speech tags to coarse - grained "" universal "" tags, rather than the more recent mapping proposed by  #AUTHOR_TAG.', 'for', 'figure 2 : arc - factored feature templates for graph - based parsing.', 'direction : d ∈ { left, right } ; dependency length : l ∈ { 1, 2, 3, 4, 5 + } ; part of speech of head / dependent / words between head and dependent : h. p / m. p / between. p ∈ { noun, verb, adj, adv, pron, det, adp, num, conj, prt, punc, x } ; token to the left / right of z : z −1 / z + 1 ; wals features : w.', '']",0
"['', ' #AUTHOR_TAG and  #TAUTHOR_TAG have shown that using expectation - maximization ( em ) to this end can in']","['some higher - level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed.', ' #AUTHOR_TAG and  #TAUTHOR_TAG have shown that using expectation - maximization ( em ) to this end can in']","['', ' #AUTHOR_TAG and  #TAUTHOR_TAG have shown that using expectation - maximization ( em ) to this end can in some cases bring substantial accuracy gains.', 'for discriminative models, self - training has been shown to be quite effective']","['some higher - level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed.', ' #AUTHOR_TAG and  #TAUTHOR_TAG have shown that using expectation - maximization ( em ) to this end can in some cases bring substantial accuracy gains.', 'for discriminative models, self - training has been shown to be quite effective for adapting monolingual parsers to new domains ( mc  #AUTHOR_TAG, as well as for relexicalizing delexicalized parsers using unlabeled target language data  #AUTHOR_TAG.', 'similarly tackstrom ( 2012 ) used self - training to adapt a multi - source direct transfer named - entity recognizer to different target languages, "" relexicalizing "" the model with word cluster features.', 'however, as discussed in § 5. 2, standard self - training is not optimal for target language adaptation']",0
"['##erbi arc set from the  #TAUTHOR_TAG in table 2.', '4 thus, the latter']","['second, the viterbi arc set from the  #TAUTHOR_TAG in table 2.', '4 thus, the latter']","['second, the viterbi arc set from the  #TAUTHOR_TAG in table 2.', '4 thus, the latter']","['', 'we propose an ambiguity - aware ensemble - training ( aaet ) method that treats the union of the ensemble predictions for a sentence x as an ambiguous labeling y ( x ).', 'an additional advantage of this approach is that the ensemble is compiled into a single model and therefore does not require multiple models to be stored and used at runtime.', 'it is straightforward to constructy ( x ) from multiple parsers.', 'let a k ( x, m ) be the set of arcs for the mth token in x according to the kth parser in the ensemble.', 'when arc - marginals are used to construct the ambiguity set, | a k ( x, m ) | ≥ 1, but when the viterbiparse is used, a k ( x, m ) is a singleton.', 'we next form, m ) as the ensemble arc ambiguity set from whichy ( x ) is assembled.', 'in this study, we combine the arc sets of two base parsers : first, the arc - marginal ambiguity set of the base parser ( § 5. 2 ) ; and second, the viterbi arc set from the  #TAUTHOR_TAG in table 2.', '4 thus, the latter']",0
"[' #TAUTHOR_TAG baseline, we observe an']","[' #TAUTHOR_TAG baseline, we observe an']","['comparing this model to the  #TAUTHOR_TAG baseline, we observe an']",[' #TAUTHOR_TAG'],0
"[',  #TAUTHOR_TAG recently introduced a novel generative model of dependency parsing, in which the generative process is']","[',  #TAUTHOR_TAG recently introduced a novel generative model of dependency parsing, in which the generative process is']","[',  #TAUTHOR_TAG recently introduced a novel generative model of dependency parsing, in which the generative process is factored', 'into separate steps for the selection of dependents and their ordering.', 'the parameters used in the selection']","['dictionaries  #AUTHOR_TAG. 1 this parser is then directly used to parse the target language. for languages with similar typology, this method can be quite', 'accurate, especially when compared to purely unsupervised methods. for instance, a parser trained on english with only part -', 'of - speech features can correctly parse the greek sentence in figure 1, even without knowledge of the lexical items since the sequence of part - of - speech tags determines the syntactic structure almost unambiguously. learning with multiple languages has been shown to benefit unsupervised learning ( cohen and smith, 1 note that and  #AUTHOR_TAG', 'do require bitext or a bilingual dictionary. the same holds for', 'most cross - lingual representations, e. g.,  #AUTHOR_TAG. [UNK] τζον εδω', '##σε στην μαρια το βιβλιο. ( the ) ( john ) ( gave ) ( to - the ) ( maria ) ( the ) ( book ). 2009 ;  #AUTHOR_TAG berg -  #AUTHOR_TAG', '. annotations in multiple languages can be combined in delexicalized transfer as well, as long as the parser features are available across the involved languages.', 'this idea was explored by mc  #AUTHOR_TAG, who showed that target language accuracy can be improved by', 'simply concatenating delexicalized treebanks in multiple languages. in similar work,  #AUTHOR_TAG proposed a mixture model in which the parameters of', 'a generative target language parser is expressed as a linear interpolation of source language parameters, whereas søgaard ( 2011 ) showed that target side language models can be', 'used to selectively subsample training sentences to improve accuracy. recently, inspired by the phylogenetic prior of berg -  #AUTHOR_TAG, søgaard and  #AUTHOR_TAG proposed', '- among other ideas - a typologically informed weighting heuristic for linearly interpolating source language parameters. however, this weighting did not provide significant improvements over uniform weighting.', 'the aforementioned approaches work well for transfer between similar languages. however, their assumptions cease to hold for typologically divergent languages ; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological', 'traits ; this critical insight is discussed further in § 4. to account for this issue,  #TAUTHOR_TAG recently introduced a novel generative model of dependency parsing, in which the generative process is factored', 'into separate steps for the selection of dependents and their ordering.', 'the parameters used in the selection step are all language independent', ', capturing only head - dependent attachment preferences. in the ordering step, however, parameters are selectively shared between subsets of  #TAUTHOR_TAG restricts its potential performance']",1
"['', ' #AUTHOR_TAG and  #TAUTHOR_TAG have shown that using expectation - maximization ( em ) to this end can in']","['some higher - level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed.', ' #AUTHOR_TAG and  #TAUTHOR_TAG have shown that using expectation - maximization ( em ) to this end can in']","['', ' #AUTHOR_TAG and  #TAUTHOR_TAG have shown that using expectation - maximization ( em ) to this end can in some cases bring substantial accuracy gains.', 'for discriminative models, self - training has been shown to be quite effective']","['some higher - level linguistic properties of the target language have been incorporated through selective sharing, so far no features specific to the target language have been employed.', ' #AUTHOR_TAG and  #TAUTHOR_TAG have shown that using expectation - maximization ( em ) to this end can in some cases bring substantial accuracy gains.', 'for discriminative models, self - training has been shown to be quite effective for adapting monolingual parsers to new domains ( mc  #AUTHOR_TAG, as well as for relexicalizing delexicalized parsers using unlabeled target language data  #AUTHOR_TAG.', 'similarly tackstrom ( 2012 ) used self - training to adapt a multi - source direct transfer named - entity recognizer to different target languages, "" relexicalizing "" the model with word cluster features.', 'however, as discussed in § 5. 2, standard self - training is not optimal for target language adaptation']",1
"['discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of  #TAUTHOR_TAG on selective parameter sharing']","['discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of  #TAUTHOR_TAG on selective parameter sharing']","['discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of  #TAUTHOR_TAG on selective parameter sharing can be incorporated into such models in the transfer scenario.', 'we first']","['by the superiority of discriminative graphbased parsing in the supervised scenario, we investigate whether the insights of  #TAUTHOR_TAG on selective parameter sharing can be incorporated into such models in the transfer scenario.', 'we first review the basic graph - based parser framework and the experimental setup that we will use throughout.', 'we then delve into details on how to incorporate selective sharing in this model in § 4.', 'in § 5, we show how learning with ambiguous labelings in this parser can be used for further target language adaptation, both through self - training and through ensemble - training']",5
['treebanks and experimental setup as  #TAUTHOR_TAG'],['treebanks and experimental setup as  #TAUTHOR_TAG'],"['facilitate comparison with the state of the art, we use the same treebanks and experimental setup as  #TAUTHOR_TAG.', 'notably, we use the mapping proposed by  #AUTHOR_TAG to map from fine -']","['facilitate comparison with the state of the art, we use the same treebanks and experimental setup as  #TAUTHOR_TAG.', 'notably, we use the mapping proposed by  #AUTHOR_TAG to map from fine - grained treebank specific part - of - speech tags to coarse - grained "" universal "" tags, rather than the more recent mapping proposed by  #AUTHOR_TAG.', 'for', 'figure 2 : arc - factored feature templates for graph - based parsing.', 'direction : d ∈ { left, right } ; dependency length : l ∈ { 1, 2, 3, 4, 5 + } ; part of speech of head / dependent / words between head and dependent : h. p / m. p / between. p ∈ { noun, verb, adj, adv, pron, det, adp, num, conj, prt, punc, x } ; token to the left / right of z : z −1 / z + 1 ; wals features : w.', '']",5
['treebanks and experimental setup as  #TAUTHOR_TAG'],['treebanks and experimental setup as  #TAUTHOR_TAG'],"['facilitate comparison with the state of the art, we use the same treebanks and experimental setup as  #TAUTHOR_TAG.', 'notably, we use the mapping proposed by  #AUTHOR_TAG to map from fine -']","['facilitate comparison with the state of the art, we use the same treebanks and experimental setup as  #TAUTHOR_TAG.', 'notably, we use the mapping proposed by  #AUTHOR_TAG to map from fine - grained treebank specific part - of - speech tags to coarse - grained "" universal "" tags, rather than the more recent mapping proposed by  #AUTHOR_TAG.', 'for', 'figure 2 : arc - factored feature templates for graph - based parsing.', 'direction : d ∈ { left, right } ; dependency length : l ∈ { 1, 2, 3, 4, 5 + } ; part of speech of head / dependent / words between head and dependent : h. p / m. p / between. p ∈ { noun, verb, adj, adv, pron, det, adp, num, conj, prt, punc, x } ; token to the left / right of z : z −1 / z + 1 ; wals features : w.', '']",5
"['first baseline,  #TAUTHOR_TAG.', '3  #TAUTHOR_TAG is']","['first baseline,  #TAUTHOR_TAG.', '3  #TAUTHOR_TAG is']","['two multi - source baseline models.', 'the first baseline,  #TAUTHOR_TAG.', '3  #TAUTHOR_TAG is trained without target language data, but we investigate the use of such data in § 5']","['compare our models to two multi - source baseline models.', 'the first baseline,  #TAUTHOR_TAG.', '3  #TAUTHOR_TAG is trained without target language data, but we investigate the use of such data in § 5. 4.', 'the second baseline, delex, is a delexicalized projective version of the well - known graph - based mstparser ( mc  #AUTHOR_TAG.', 'the feature templates used by this model are shown to the left in figure 2.', 'note that there is no selective sharing in this model.', 'the second and third columns of table 2 show the unlabeled attachment scores of the baseline models for each target language.', 'we see that delex performs well on target languages that are related to the majority of the source languages.', 'however, for languages 3 model "" d -, to "" in table 2 from  #TAUTHOR_TAG.', 'that diverge from the indo - european majority family, the selective sharing model,  #TAUTHOR_TAG, achieves substantially higher accuracies']",5
"['##troduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['removing all directional features, we now carefully reintroduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['##troduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['removing all directional features, we now carefully reintroduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features from wals  #AUTHOR_TAG, listed in table 1, to selectively share directional parameters between languages.', 'as a natural first attempt at sharing parameters, one might consider forming the crossproduct of all features of delex with all wals properties, similarly to a common domain adaptation technique ( daume iii, 2007 ;  #AUTHOR_TAG.', 'however, this approach has two issues.', 'first, it results in a huge number of features, making the model prone to overfitting.', 'second, and more critically, it ties together languages via features for which they are not typologically similar.', 'consider english and french, which are both prepositional and thus have the same value for wals property 85a.', 'these languages will end up sharing a parameter for the feature', '85a ; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives.', 'this problem applies to any method for parameter mixing that treats all the parameters as equal.', 'like  #TAUTHOR_TAG, we instead share parameters more selectively.', 'our strategy is to use the relevant part - of - speech tags of the head and dependent to select which parameters to share, based on very basic linguistic knowledge.', 'the resulting features are shown to the right in figure 2.', 'for example, there is a shared directional feature that models the order of subject, object and verb by conjoining wals feature 81a with the arc direction and an indicator feature that fires only if the head is a verb and the dependent is a noun.', 'these features would not be very useful by themselves, so we combine them with the bare features.', 'the accuracy of the resulting share model is shown in column five of table 2.', 'although this model still performs worse than  #TAUTHOR_TAG, it is an improvement over the delex baseline and actually outperforms the former on 5 out of the 16 languages']",5
"['##troduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['removing all directional features, we now carefully reintroduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['##troduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['removing all directional features, we now carefully reintroduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features from wals  #AUTHOR_TAG, listed in table 1, to selectively share directional parameters between languages.', 'as a natural first attempt at sharing parameters, one might consider forming the crossproduct of all features of delex with all wals properties, similarly to a common domain adaptation technique ( daume iii, 2007 ;  #AUTHOR_TAG.', 'however, this approach has two issues.', 'first, it results in a huge number of features, making the model prone to overfitting.', 'second, and more critically, it ties together languages via features for which they are not typologically similar.', 'consider english and french, which are both prepositional and thus have the same value for wals property 85a.', 'these languages will end up sharing a parameter for the feature', '85a ; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives.', 'this problem applies to any method for parameter mixing that treats all the parameters as equal.', 'like  #TAUTHOR_TAG, we instead share parameters more selectively.', 'our strategy is to use the relevant part - of - speech tags of the head and dependent to select which parameters to share, based on very basic linguistic knowledge.', 'the resulting features are shown to the right in figure 2.', 'for example, there is a shared directional feature that models the order of subject, object and verb by conjoining wals feature 81a with the arc direction and an indicator feature that fires only if the head is a verb and the dependent is a noun.', 'these features would not be very useful by themselves, so we combine them with the bare features.', 'the accuracy of the resulting share model is shown in column five of table 2.', 'although this model still performs worse than  #TAUTHOR_TAG, it is an improvement over the delex baseline and actually outperforms the former on 5 out of the 16 languages']",5
"['##erbi arc set from the  #TAUTHOR_TAG in table 2.', '4 thus, the latter']","['second, the viterbi arc set from the  #TAUTHOR_TAG in table 2.', '4 thus, the latter']","['second, the viterbi arc set from the  #TAUTHOR_TAG in table 2.', '4 thus, the latter']","['', 'we propose an ambiguity - aware ensemble - training ( aaet ) method that treats the union of the ensemble predictions for a sentence x as an ambiguous labeling y ( x ).', 'an additional advantage of this approach is that the ensemble is compiled into a single model and therefore does not require multiple models to be stored and used at runtime.', 'it is straightforward to constructy ( x ) from multiple parsers.', 'let a k ( x, m ) be the set of arcs for the mth token in x according to the kth parser in the ensemble.', 'when arc - marginals are used to construct the ambiguity set, | a k ( x, m ) | ≥ 1, but when the viterbiparse is used, a k ( x, m ) is a singleton.', 'we next form, m ) as the ensemble arc ambiguity set from whichy ( x ) is assembled.', 'in this study, we combine the arc sets of two base parsers : first, the arc - marginal ambiguity set of the base parser ( § 5. 2 ) ; and second, the viterbi arc set from the  #TAUTHOR_TAG in table 2.', '4 thus, the latter']",5
"[' #TAUTHOR_TAG baseline, we observe an']","[' #TAUTHOR_TAG baseline, we observe an']","['comparing this model to the  #TAUTHOR_TAG baseline, we observe an']",[' #TAUTHOR_TAG'],5
"[' #TAUTHOR_TAG baseline, we observe an']","[' #TAUTHOR_TAG baseline, we observe an']","['comparing this model to the  #TAUTHOR_TAG baseline, we observe an']",[' #TAUTHOR_TAG'],5
['treebanks and experimental setup as  #TAUTHOR_TAG'],['treebanks and experimental setup as  #TAUTHOR_TAG'],"['facilitate comparison with the state of the art, we use the same treebanks and experimental setup as  #TAUTHOR_TAG.', 'notably, we use the mapping proposed by  #AUTHOR_TAG to map from fine -']","['facilitate comparison with the state of the art, we use the same treebanks and experimental setup as  #TAUTHOR_TAG.', 'notably, we use the mapping proposed by  #AUTHOR_TAG to map from fine - grained treebank specific part - of - speech tags to coarse - grained "" universal "" tags, rather than the more recent mapping proposed by  #AUTHOR_TAG.', 'for', 'figure 2 : arc - factored feature templates for graph - based parsing.', 'direction : d ∈ { left, right } ; dependency length : l ∈ { 1, 2, 3, 4, 5 + } ; part of speech of head / dependent / words between head and dependent : h. p / m. p / between. p ∈ { noun, verb, adj, adv, pron, det, adp, num, conj, prt, punc, x } ; token to the left / right of z : z −1 / z + 1 ; wals features : w.', '']",3
"[' #TAUTHOR_TAG.', 'the']","[' #TAUTHOR_TAG.', 'the']","[' #TAUTHOR_TAG.', 'the performance of bare is shown in the fourth column of table 2.', 'the removal of']","['', 'p, which models direction as well as word order in the local contexts of the head and the dependent.', 'such features do not transfer well across typologically different languages.', 'in order to verify that these issues are the cause of the poor performance of the delex model, we remove all directional features and all features that model local word order from delex.', 'the feature templates of the resulting bare model are shown in the center of figure 2.', 'these features only model selectional preferences and dependency length, analogously to the selection component of  #TAUTHOR_TAG.', 'the performance of bare is shown in the fourth column of table 2.', 'the removal of most of the features results in a performance drop on average.', 'however, for languages outside of the indo - european family, bare is often more accurate, especially for basque, hungarian and japanese, which supports our hypothesis']",3
"['##troduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['removing all directional features, we now carefully reintroduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['##troduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features']","['removing all directional features, we now carefully reintroduce them.', 'inspired by  #TAUTHOR_TAG.', '( 2012 ), we make use of the typological features from wals  #AUTHOR_TAG, listed in table 1, to selectively share directional parameters between languages.', 'as a natural first attempt at sharing parameters, one might consider forming the crossproduct of all features of delex with all wals properties, similarly to a common domain adaptation technique ( daume iii, 2007 ;  #AUTHOR_TAG.', 'however, this approach has two issues.', 'first, it results in a huge number of features, making the model prone to overfitting.', 'second, and more critically, it ties together languages via features for which they are not typologically similar.', 'consider english and french, which are both prepositional and thus have the same value for wals property 85a.', 'these languages will end up sharing a parameter for the feature', '85a ; yet they have the exact opposite direction of attachment preference when it comes to nouns and adjectives.', 'this problem applies to any method for parameter mixing that treats all the parameters as equal.', 'like  #TAUTHOR_TAG, we instead share parameters more selectively.', 'our strategy is to use the relevant part - of - speech tags of the head and dependent to select which parameters to share, based on very basic linguistic knowledge.', 'the resulting features are shown to the right in figure 2.', 'for example, there is a shared directional feature that models the order of subject, object and verb by conjoining wals feature 81a with the arc direction and an indicator feature that fires only if the head is a verb and the dependent is a noun.', 'these features would not be very useful by themselves, so we combine them with the bare features.', 'the accuracy of the resulting share model is shown in column five of table 2.', 'although this model still performs worse than  #TAUTHOR_TAG, it is an improvement over the delex baseline and actually outperforms the former on 5 out of the 16 languages']",3
"[' #TAUTHOR_TAG baseline, we observe an']","[' #TAUTHOR_TAG baseline, we observe an']","['comparing this model to the  #TAUTHOR_TAG baseline, we observe an']",[' #TAUTHOR_TAG'],3
"['applications  #TAUTHOR_TAG.', 'an']","['applications  #TAUTHOR_TAG.', 'an']","['- oriented applications  #TAUTHOR_TAG.', '']","['', 'conversation modelling is one such domain where end - to - end trained systems have matched or surpassed traditional dialog systems in both open - ended  #AUTHOR_TAG and goal - oriented applications  #TAUTHOR_TAG.', '']",0
"['dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles']","['dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are']","['- oriented dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are trained specifically for the task']","['widely known for learning unsupervised embeddings from raw text like in word2vec  #AUTHOR_TAG, embeddings can also be learned in a supervised manner specifically for a given task.', 'supervised word embedding models which score ( conversation history, response ) pairs have been shown to be a strong baseline for both open - ended and goal - oriented dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are trained specifically for the task of predicting the next response given the previous conversation : a candidate response y is scored against the input  #TAUTHOR_TAG.', 'for dialogs, the entire conversation history is stored in the memory component of the model.', 'it can be iteratively read from to perform reasoning and select the best possible responses based on the context.', 'implementing the modifications to the memory network architecture described by  #TAUTHOR_TAG, we use the model as an end - to - end baseline and analyze its performance.', 'the user profile information is stored in the memory of the model as if it were the first turn of the conversation history spoken by the user, i. e. the model builds an embedding of the profile by combining the values of the embeddings of each attribute in the profile.', 'unlike  #TAUTHOR_TAG, we do not make use of any match type features.', 'our goal is to analyse the capabilities of the existing memory network model to leverage profile information when conducting dialog.', 'appendix b shows illustrative examples of memory network predictions based on the experiments described in the following section']",0
"['applications  #TAUTHOR_TAG.', 'an']","['applications  #TAUTHOR_TAG.', 'an']","['- oriented applications  #TAUTHOR_TAG.', '']","['', 'conversation modelling is one such domain where end - to - end trained systems have matched or surpassed traditional dialog systems in both open - ended  #AUTHOR_TAG and goal - oriented applications  #TAUTHOR_TAG.', '']",5
"[' #TAUTHOR_TAG, we provide baselines on the modified dataset by evaluating several learning methods : rule']","[' #TAUTHOR_TAG, we provide baselines on the modified dataset by evaluating several learning methods :']","[' #TAUTHOR_TAG, we provide baselines on the modified dataset by evaluating several learning methods : rule - based systems, supervised embeddings, and end - to - end memory networks']","[' #TAUTHOR_TAG, we provide baselines on the modified dataset by evaluating several learning methods : rule - based systems, supervised embeddings, and end - to - end memory networks']",5
"['dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles']","['dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are']","['- oriented dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are trained specifically for the task']","['widely known for learning unsupervised embeddings from raw text like in word2vec  #AUTHOR_TAG, embeddings can also be learned in a supervised manner specifically for a given task.', 'supervised word embedding models which score ( conversation history, response ) pairs have been shown to be a strong baseline for both open - ended and goal - oriented dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are trained specifically for the task of predicting the next response given the previous conversation : a candidate response y is scored against the input  #TAUTHOR_TAG.', 'for dialogs, the entire conversation history is stored in the memory component of the model.', 'it can be iteratively read from to perform reasoning and select the best possible responses based on the context.', 'implementing the modifications to the memory network architecture described by  #TAUTHOR_TAG, we use the model as an end - to - end baseline and analyze its performance.', 'the user profile information is stored in the memory of the model as if it were the first turn of the conversation history spoken by the user, i. e. the model builds an embedding of the profile by combining the values of the embeddings of each attribute in the profile.', 'unlike  #TAUTHOR_TAG, we do not make use of any match type features.', 'our goal is to analyse the capabilities of the existing memory network model to leverage profile information when conducting dialog.', 'appendix b shows illustrative examples of memory network predictions based on the experiments described in the following section']",5
"['dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles']","['dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are']","['- oriented dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are trained specifically for the task']","['widely known for learning unsupervised embeddings from raw text like in word2vec  #AUTHOR_TAG, embeddings can also be learned in a supervised manner specifically for a given task.', 'supervised word embedding models which score ( conversation history, response ) pairs have been shown to be a strong baseline for both open - ended and goal - oriented dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are trained specifically for the task of predicting the next response given the previous conversation : a candidate response y is scored against the input  #TAUTHOR_TAG.', 'for dialogs, the entire conversation history is stored in the memory component of the model.', 'it can be iteratively read from to perform reasoning and select the best possible responses based on the context.', 'implementing the modifications to the memory network architecture described by  #TAUTHOR_TAG, we use the model as an end - to - end baseline and analyze its performance.', 'the user profile information is stored in the memory of the model as if it were the first turn of the conversation history spoken by the user, i. e. the model builds an embedding of the profile by combining the values of the embeddings of each attribute in the profile.', 'unlike  #TAUTHOR_TAG, we do not make use of any match type features.', 'our goal is to analyse the capabilities of the existing memory network model to leverage profile information when conducting dialog.', 'appendix b shows illustrative examples of memory network predictions based on the experiments described in the following section']",5
"['applications  #TAUTHOR_TAG.', 'an']","['applications  #TAUTHOR_TAG.', 'an']","['- oriented applications  #TAUTHOR_TAG.', '']","['', 'conversation modelling is one such domain where end - to - end trained systems have matched or surpassed traditional dialog systems in both open - ended  #AUTHOR_TAG and goal - oriented applications  #TAUTHOR_TAG.', '']",6
"['babi dialog dataset described in  #TAUTHOR_TAG,']","['babi dialog dataset described in  #TAUTHOR_TAG,']","['work builds upon the babi dialog dataset described in  #TAUTHOR_TAG,']",[' #TAUTHOR_TAG'],6
"[' #TAUTHOR_TAG, crucial aspects of goal -']","[' #TAUTHOR_TAG, crucial aspects of goal - oriented conversation have been split into various synthetically generated tasks to']","[' #TAUTHOR_TAG, crucial aspects of goal - oriented conversation have been split into various synthetically generated tasks to']","['paper aims to bridge a gap in research on neural conversational agents by introducing a new open dataset of goal - oriented dialogs with user profiles associated with each dialog.', ""the dataset acts as a testbed for the training and analysis of end - to - end goal - oriented conversational agents which must personalize their conversation with the user based on attributes in the user's profile."", 'as this work builds on top of the babi dialog dataset proposed by  #TAUTHOR_TAG, crucial aspects of goal - oriented conversation have been split into various synthetically generated tasks to evaluate the strengths and weaknesses of models in a systematic way before applying them on real data.', 'we demonstrated how to use our tasks to break down one such system, end - to - end memory networks.', 'the model was unable to sufficiently perform reasoning or personalization to solve the tasks, indicating that further work needs to be done on learning methods for these aspects.', 'despite the scenarios and language of the tasks being artificial, we believe that building mechanisms that can solve them is a reasonable starting point towards the development of sophisticated dialog systems in domains such as restaurant reservation, customer care or personal assistants.', 'we hope that future research in this field will focus on developing better models to solve our tasks and on releasing datasets with real human - bot dialogs influenced by speaker profiles.', ""at any turn of the dialog, the memory network stores the conversation history in its memory and, based on the user's input for that turn, pays attention to specific utterances from the memory."", ""it can iteratively reason over the memory and uses a weighted combination of these utterances to predict the bot's response to the user."", 'in our visualization, we take the model state at a specific turn in the conversation and highlight the values of the attention weights over the memory for each iteration ( called a hop )']",6
"['dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles']","['dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are']","['- oriented dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are trained specifically for the task']","['widely known for learning unsupervised embeddings from raw text like in word2vec  #AUTHOR_TAG, embeddings can also be learned in a supervised manner specifically for a given task.', 'supervised word embedding models which score ( conversation history, response ) pairs have been shown to be a strong baseline for both open - ended and goal - oriented dialog  #TAUTHOR_TAG.', 'we do not handcraft any special embeddings for the user profiles.', 'the embedding vectors are trained specifically for the task of predicting the next response given the previous conversation : a candidate response y is scored against the input  #TAUTHOR_TAG.', 'for dialogs, the entire conversation history is stored in the memory component of the model.', 'it can be iteratively read from to perform reasoning and select the best possible responses based on the context.', 'implementing the modifications to the memory network architecture described by  #TAUTHOR_TAG, we use the model as an end - to - end baseline and analyze its performance.', 'the user profile information is stored in the memory of the model as if it were the first turn of the conversation history spoken by the user, i. e. the model builds an embedding of the profile by combining the values of the embeddings of each attribute in the profile.', 'unlike  #TAUTHOR_TAG, we do not make use of any match type features.', 'our goal is to analyse the capabilities of the existing memory network model to leverage profile information when conducting dialog.', 'appendix b shows illustrative examples of memory network predictions based on the experiments described in the following section']",4
"['babi dialog tasks in  #TAUTHOR_TAG, supervised embeddings']","['babi dialog tasks in  #TAUTHOR_TAG, supervised embeddings']","['on the babi dialog tasks in  #TAUTHOR_TAG, supervised embeddings']",[' #TAUTHOR_TAG'],4
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['art deep learning methods reach better performance  #TAUTHOR_TAG'],"['essay scoring ( aes ) is the task of assigning grades to essays written in an educational setting, using a computer - based system with natural language processing capabilities.', 'the aim of designing such systems is to reduce the involvement of human graders as far as possible.', 'aes is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse  #AUTHOR_TAG.', 'although traditional aes methods typically rely on handcrafted features  #AUTHOR_TAG, recent results indicate that state - of - the - art deep learning methods reach better performance  #TAUTHOR_TAG are able to capture subtle and complex information that is relevant to the task  #AUTHOR_TAG.', 'in this paper, we propose to combine string kernels ( low - level character n - gram features ) and word embeddings ( high - level semantic features ) to obtain state - of - the - art aes results.', 'since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification  #AUTHOR_TAG and sentiment analysis ( gimenez - perez et al., 2017 ; to native language identification  #AUTHOR_TAG dialect identification, we believe that string kernels can reach equally good results in aes.', 'to the best of our knowledge, string kernels have never been used for this task.', 'as string kernels are a simple approach that relies solely on character n - grams as features, it is fairly obvious that such an approach will not to cover several aspects ( e. g. : semantics, discourse ) required for the aes task.', 'to solve this problem, we propose to combine string kernels with a recent approach based on word embeddings, namely the bag - of - super - wordembeddings ( boswe ).', 'to our knowledge, this is the first successful attempt to combine string kernels and word embeddings.', 'we evaluate our approach on the automated student assessment prize data set, in both in - domain and cross - domain settings.', '']",0
[' #TAUTHOR_TAG'],[' #TAUTHOR_TAG'],['art deep learning methods reach better performance  #TAUTHOR_TAG'],"['essay scoring ( aes ) is the task of assigning grades to essays written in an educational setting, using a computer - based system with natural language processing capabilities.', 'the aim of designing such systems is to reduce the involvement of human graders as far as possible.', 'aes is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse  #AUTHOR_TAG.', 'although traditional aes methods typically rely on handcrafted features  #AUTHOR_TAG, recent results indicate that state - of - the - art deep learning methods reach better performance  #TAUTHOR_TAG are able to capture subtle and complex information that is relevant to the task  #AUTHOR_TAG.', 'in this paper, we propose to combine string kernels ( low - level character n - gram features ) and word embeddings ( high - level semantic features ) to obtain state - of - the - art aes results.', 'since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification  #AUTHOR_TAG and sentiment analysis ( gimenez - perez et al., 2017 ; to native language identification  #AUTHOR_TAG dialect identification, we believe that string kernels can reach equally good results in aes.', 'to the best of our knowledge, string kernels have never been used for this task.', 'as string kernels are a simple approach that relies solely on character n - grams as features, it is fairly obvious that such an approach will not to cover several aspects ( e. g. : semantics, discourse ) required for the aes task.', 'to solve this problem, we propose to combine string kernels with a recent approach based on word embeddings, namely the bag - of - super - wordembeddings ( boswe ).', 'to our knowledge, this is the first successful attempt to combine string kernels and word embeddings.', 'we evaluate our approach on the automated student assessment prize data set, in both in - domain and cross - domain settings.', '']",4
,,,,4
,,,,4
"['approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the']","['state - of - the - art approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels,']","[' #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels,']","['this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring.', 'we compared our approach on the automated student assessment prize data set, in both in - domain and crossdomain settings, with several state - of - the - art approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.', 'using a shallow approach, we report better results compared to recent deep learning approaches  #TAUTHOR_TAG']",4
,,,,3
,,,,3
,,,,5
,,,,5
"['approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the']","['state - of - the - art approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels,']","[' #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels,']","['this paper, we described an approach based on combining string kernels and word embeddings for automatic essay scoring.', 'we compared our approach on the automated student assessment prize data set, in both in - domain and crossdomain settings, with several state - of - the - art approaches  #TAUTHOR_TAG.', 'overall, the in - domain and the cross - domain comparative studies indicate that string kernels, both alone and in combination with word embeddings, attain the best performance on the automatic essay scoring task.', 'using a shallow approach, we report better results compared to recent deep learning approaches  #TAUTHOR_TAG']",5
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', '']","['recent years, sentiment analysis has received considerable attentions in natural language processing ( nlp ) community  #AUTHOR_TAG.', 'polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', 'sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic.', 'labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success  #AUTHOR_TAG and is helpful in business intelligence applications, recommender systems, and message filtering  #AUTHOR_TAG.', 'while topics are often identifiable by keywords alone, sentiment classification appears to be a more challenge task  #AUTHOR_TAG.', 'first, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain - specific contextual cues  #AUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', '']","['recent years, sentiment analysis has received considerable attentions in natural language processing ( nlp ) community  #AUTHOR_TAG.', 'polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', 'sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic.', 'labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success  #AUTHOR_TAG and is helpful in business intelligence applications, recommender systems, and message filtering  #AUTHOR_TAG.', 'while topics are often identifiable by keywords alone, sentiment classification appears to be a more challenge task  #AUTHOR_TAG.', 'first, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain - specific contextual cues  #AUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', '']","['recent years, sentiment analysis has received considerable attentions in natural language processing ( nlp ) community  #AUTHOR_TAG.', 'polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', 'sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic.', 'labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success  #AUTHOR_TAG and is helpful in business intelligence applications, recommender systems, and message filtering  #AUTHOR_TAG.', 'while topics are often identifiable by keywords alone, sentiment classification appears to be a more challenge task  #AUTHOR_TAG.', 'first, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain - specific contextual cues  #AUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', '']","['recent years, sentiment analysis has received considerable attentions in natural language processing ( nlp ) community  #AUTHOR_TAG.', 'polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', 'sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic.', 'labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success  #AUTHOR_TAG and is helpful in business intelligence applications, recommender systems, and message filtering  #AUTHOR_TAG.', 'while topics are often identifiable by keywords alone, sentiment classification appears to be a more challenge task  #AUTHOR_TAG.', 'first, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain - specific contextual cues  #AUTHOR_TAG.', '']",0
"['domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck']","['domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first']","['.', 'supervised sentiment classification systems are domain - specific and annotating a large scale corpus for each domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first type of']","['classification can be performed on words, sentences or documents, and is generally categorized into lexicon - based and corpus - based classification method  #AUTHOR_TAG.', 'the detailed survey about techniques and approaches of sentiment classification can be seen in the book  #AUTHOR_TAG.', 'in this paper we focus on corpus - based classification method.', 'corpus - based methods use a labeled corpus to train a sentiment classifier  #AUTHOR_TAG.', ' #AUTHOR_TAG apply machine learning approach to corpus - based sentiment classification firstly.', 'they found that standard machine learning techniques outperform human - produced baselines.', ' #AUTHOR_TAG apply text - categorization techniques to the subjective portions of the sentiment document.', 'these portions are extracted by efficient techniques for finding minimum cuts in graphs.', ' #AUTHOR_TAG demonstrate that using large feature vectors in combination with feature reduction, high accuracy can be achieved in the very noisy domain of customer feedback data.', ' #AUTHOR_TAG propose the sentiment vector space model to represent song lyric document, assign the sentiment labels such as light - hearted and heavy - hearted.', 'supervised sentiment classification systems are domain - specific and annotating a large scale corpus for each domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first type of solution is using old domain labeled examples to new domain sentiment clas - sification.', '']",0
"['domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck']","['domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first']","['.', 'supervised sentiment classification systems are domain - specific and annotating a large scale corpus for each domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first type of']","['classification can be performed on words, sentences or documents, and is generally categorized into lexicon - based and corpus - based classification method  #AUTHOR_TAG.', 'the detailed survey about techniques and approaches of sentiment classification can be seen in the book  #AUTHOR_TAG.', 'in this paper we focus on corpus - based classification method.', 'corpus - based methods use a labeled corpus to train a sentiment classifier  #AUTHOR_TAG.', ' #AUTHOR_TAG apply machine learning approach to corpus - based sentiment classification firstly.', 'they found that standard machine learning techniques outperform human - produced baselines.', ' #AUTHOR_TAG apply text - categorization techniques to the subjective portions of the sentiment document.', 'these portions are extracted by efficient techniques for finding minimum cuts in graphs.', ' #AUTHOR_TAG demonstrate that using large feature vectors in combination with feature reduction, high accuracy can be achieved in the very noisy domain of customer feedback data.', ' #AUTHOR_TAG propose the sentiment vector space model to represent song lyric document, assign the sentiment labels such as light - hearted and heavy - hearted.', 'supervised sentiment classification systems are domain - specific and annotating a large scale corpus for each domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first type of solution is using old domain labeled examples to new domain sentiment clas - sification.', '']",0
"['svms  #TAUTHOR_TAG,']","['svms  #TAUTHOR_TAG,']","['svms  #TAUTHOR_TAG, we define the uncertainty of']",[' #TAUTHOR_TAG'],0
"['), active learning ( active )  #AUTHOR_TAG, mine the easy classify the hard ( mech )  #TAUTHOR_TAG, and deep belief networks (']","['( tsvm ), active learning ( active )  #AUTHOR_TAG, mine the easy classify the hard ( mech )  #TAUTHOR_TAG, and deep belief networks ( dbn )  #AUTHOR_TAG.', 'spectral learning, tsvm, and active learning method are three baseline methods']","['), active learning ( active )  #AUTHOR_TAG, mine the easy classify the hard ( mech )  #TAUTHOR_TAG, and deep belief networks (']","['', 'the initial momentum is 0. 5 and after 5 epochs, the momentum is set to 0. 9.', 'for supervised learning, we run 10 epochs, three times of linear searches are performed in each epoch.', 'we compare the classification performance of adn with five representative classifiers, i. e., semi - supervised spectral learning ( spectral )  #AUTHOR_TAG, transductive svm ( tsvm ), active learning ( active )  #AUTHOR_TAG, mine the easy classify the hard ( mech )  #TAUTHOR_TAG, and deep belief networks ( dbn )  #AUTHOR_TAG.', 'spectral learning, tsvm, and active learning method are three baseline methods for sentiment classification.', 'mech is a new semi - supervised method for sentiment classification  #TAUTHOR_TAG.', 'dbn  #AUTHOR_TAG is the classical deep learning method proposed recently']",0
"['), active learning ( active )  #AUTHOR_TAG, mine the easy classify the hard ( mech )  #TAUTHOR_TAG, and deep belief networks (']","['( tsvm ), active learning ( active )  #AUTHOR_TAG, mine the easy classify the hard ( mech )  #TAUTHOR_TAG, and deep belief networks ( dbn )  #AUTHOR_TAG.', 'spectral learning, tsvm, and active learning method are three baseline methods']","['), active learning ( active )  #AUTHOR_TAG, mine the easy classify the hard ( mech )  #TAUTHOR_TAG, and deep belief networks (']","['', 'the initial momentum is 0. 5 and after 5 epochs, the momentum is set to 0. 9.', 'for supervised learning, we run 10 epochs, three times of linear searches are performed in each epoch.', 'we compare the classification performance of adn with five representative classifiers, i. e., semi - supervised spectral learning ( spectral )  #AUTHOR_TAG, transductive svm ( tsvm ), active learning ( active )  #AUTHOR_TAG, mine the easy classify the hard ( mech )  #TAUTHOR_TAG, and deep belief networks ( dbn )  #AUTHOR_TAG.', 'spectral learning, tsvm, and active learning method are three baseline methods for sentiment classification.', 'mech is a new semi - supervised method for sentiment classification  #TAUTHOR_TAG.', 'dbn  #AUTHOR_TAG is the classical deep learning method proposed recently']",0
"['methods are reported by  #TAUTHOR_TAG.', 'for']","['methods are reported by  #TAUTHOR_TAG.', 'for']","['are reported by  #TAUTHOR_TAG.', 'for']","['', 'all theses parameters are set based on the dimension of the input data and the scale of the dataset.', 'because that the number of vocabulary in mov dataset is more than other four datasets, so the number of units in previous two hidden layers for mov dataset are more than other four datasets.', 'we perform active learning for 5 iterations.', 'in each iteration, we select and label 20 of the most uncertain points, and then re - train the adn on all of the unlabeled data and labeled data annotated so far.', 'after 5 iterations, 100 labeled data are used for training.', 'the classification accuracies on test data in cross validation for five datasets and six methods are shown in table 1.', 'the results of previous four methods are reported by  #TAUTHOR_TAG.', '']",0
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', '']","['recent years, sentiment analysis has received considerable attentions in natural language processing ( nlp ) community  #AUTHOR_TAG.', 'polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', 'sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic.', 'labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success  #AUTHOR_TAG and is helpful in business intelligence applications, recommender systems, and message filtering  #AUTHOR_TAG.', 'while topics are often identifiable by keywords alone, sentiment classification appears to be a more challenge task  #AUTHOR_TAG.', 'first, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain - specific contextual cues  #AUTHOR_TAG.', '']",1
"[' #TAUTHOR_TAG.', '']","[' #TAUTHOR_TAG.', '']","['of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', '']","['recent years, sentiment analysis has received considerable attentions in natural language processing ( nlp ) community  #AUTHOR_TAG.', 'polarity classification, which determine whether the sentiment expressed in a document is positive or negative, is one of the most popular tasks of sentiment analysis  #TAUTHOR_TAG.', 'sentiment classification is a special type of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the subject or topic.', 'labeling the reviews with their sentiment would provide succinct summaries to readers, which makes it possible to focus the text mining on areas in need of improvement or on areas of success  #AUTHOR_TAG and is helpful in business intelligence applications, recommender systems, and message filtering  #AUTHOR_TAG.', 'while topics are often identifiable by keywords alone, sentiment classification appears to be a more challenge task  #AUTHOR_TAG.', 'first, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain - specific contextual cues  #AUTHOR_TAG.', '']",1
"['domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck']","['domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first']","['.', 'supervised sentiment classification systems are domain - specific and annotating a large scale corpus for each domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first type of']","['classification can be performed on words, sentences or documents, and is generally categorized into lexicon - based and corpus - based classification method  #AUTHOR_TAG.', 'the detailed survey about techniques and approaches of sentiment classification can be seen in the book  #AUTHOR_TAG.', 'in this paper we focus on corpus - based classification method.', 'corpus - based methods use a labeled corpus to train a sentiment classifier  #AUTHOR_TAG.', ' #AUTHOR_TAG apply machine learning approach to corpus - based sentiment classification firstly.', 'they found that standard machine learning techniques outperform human - produced baselines.', ' #AUTHOR_TAG apply text - categorization techniques to the subjective portions of the sentiment document.', 'these portions are extracted by efficient techniques for finding minimum cuts in graphs.', ' #AUTHOR_TAG demonstrate that using large feature vectors in combination with feature reduction, high accuracy can be achieved in the very noisy domain of customer feedback data.', ' #AUTHOR_TAG propose the sentiment vector space model to represent song lyric document, assign the sentiment labels such as light - hearted and heavy - hearted.', 'supervised sentiment classification systems are domain - specific and annotating a large scale corpus for each domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first type of solution is using old domain labeled examples to new domain sentiment clas - sification.', '']",1
"['domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck']","['domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first']","['.', 'supervised sentiment classification systems are domain - specific and annotating a large scale corpus for each domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first type of']","['classification can be performed on words, sentences or documents, and is generally categorized into lexicon - based and corpus - based classification method  #AUTHOR_TAG.', 'the detailed survey about techniques and approaches of sentiment classification can be seen in the book  #AUTHOR_TAG.', 'in this paper we focus on corpus - based classification method.', 'corpus - based methods use a labeled corpus to train a sentiment classifier  #AUTHOR_TAG.', ' #AUTHOR_TAG apply machine learning approach to corpus - based sentiment classification firstly.', 'they found that standard machine learning techniques outperform human - produced baselines.', ' #AUTHOR_TAG apply text - categorization techniques to the subjective portions of the sentiment document.', 'these portions are extracted by efficient techniques for finding minimum cuts in graphs.', ' #AUTHOR_TAG demonstrate that using large feature vectors in combination with feature reduction, high accuracy can be achieved in the very noisy domain of customer feedback data.', ' #AUTHOR_TAG propose the sentiment vector space model to represent song lyric document, assign the sentiment labels such as light - hearted and heavy - hearted.', 'supervised sentiment classification systems are domain - specific and annotating a large scale corpus for each domain is very expensive  #TAUTHOR_TAG.', 'there are several solutions for this corpus annotation bottleneck.', 'the first type of solution is using old domain labeled examples to new domain sentiment clas - sification.', '']",1
"['these reviews to be classified, which is similar with  #TAUTHOR_TAG.', '']","['these reviews to be classified, which is similar with  #TAUTHOR_TAG.', '']","['these reviews to be classified, which is similar with  #TAUTHOR_TAG.', '']","['are many review documents in the dataset.', 'we preprocess these reviews to be classified, which is similar with  #TAUTHOR_TAG.', 'each review is represented as a vector of unigrams, using binary weight equal to 1 for terms present in a vector.', 'moreover, the punctuations, numbers, and words of length one are removed from the vector.', '']",3
"['svms  #TAUTHOR_TAG,']","['svms  #TAUTHOR_TAG,']","['svms  #TAUTHOR_TAG, we define the uncertainty of']",[' #TAUTHOR_TAG'],3
"['svms  #TAUTHOR_TAG,']","['svms  #TAUTHOR_TAG,']","['svms  #TAUTHOR_TAG, we define the uncertainty of']",[' #TAUTHOR_TAG'],6
"['negative reviews.', 'similar with  #TAUTHOR_TAG, we']","['negative reviews.', 'similar with  #TAUTHOR_TAG, we']","['000 negative reviews.', 'similar with  #TAUTHOR_TAG, we divide the 2, 000 reviews into ten equal - sized folds randomly']","['evaluate the performance of the proposed adn method using five sentiment classification datasets.', 'the first dataset is mov  #AUTHOR_TAG, which is a widely - used movie review dataset.', 'the other four dataset contain reviews of four different types of products, including books ( boo ), dvds ( dvd ), electronics ( ele ), and kitchen appliances ( kit )  #AUTHOR_TAG.', 'each dataset includes 1, 000 positive and 1, 000 negative reviews.', 'similar with  #TAUTHOR_TAG, we divide the 2, 000 reviews into ten equal - sized folds randomly and test all the algorithms with crossvalidation.', 'in each folds, 100 reviews are random selected as training data and the remaining 100 data are used for test.', 'only the reviews in the training data set are used for the selection of labeled data by active learning.', 'the adn architecture has different number of hidden units for each hidden layer.', 'for']",6
[' #TAUTHOR_TAG proposed to use noise contrastive estimation ( nc'],[' #TAUTHOR_TAG proposed to use noise contrastive estimation ( nce ) [ 14 ] to speed - up'],['.  #TAUTHOR_TAG proposed to use noise contrastive estimation ( nc'],"['', 'unfortunately, this approach requires a control of the samples variance, which can lead otherwise to unstable learning [ 12 ].', 'in a similar work, mnih et al.  #TAUTHOR_TAG proposed to use noise contrastive estimation ( nce ) [ 14 ] to speed - up the training.', 'nce treats the learning as a binary classification problem between a target word and noise samples, which are drawn from a noise distribution.', 'moreover, nce considers the normalization term as an additional parameter that can be learned during training or fixed beforehand.', 'in this case, the network learns to self - normalize.', 'this property makes nce more attractive compared to other sampling methods, such as importance sampling, which would still require the use of the softmax function during evaluation.', 'in batch mode training, however, the implementation of nce cannot be directly formulated using dense matrix operations, which compromises their speed - up gains.', 'this paper proposes a new solution to train large vocabulary lms using nce in batch mode.', 'the main idea here is to restrict the vocabulary, at each iteration, to the words in the batch and replace the standard softmax function by nce.', 'in particular, the target words ( to predict ) in the batch play the role of targets and']",0
"[' #TAUTHOR_TAG 16 ].', 'furthermore, the network learns to self - normalize']","[' #TAUTHOR_TAG 16 ].', 'furthermore, the network learns to self - normalize']","['requires a fixed small number of noise samples ( e. g., 100 ) to achieve a good performance  #TAUTHOR_TAG 16 ].', 'furthermore, the network learns to self - normalize']","['authors of [ 18 ] have shown that nce and importance sampling ( is ) are closely related, with the main difference is that nce is defined as a binary classifier between samples drawn from data or noise distributions with a logistic loss, whereas is is a multi - class classifier, which uses softmax and a crossentropy loss.', 'hence, the authors concluded that is is theoretically a better choice than nce.', 'the results reported, however, showed a minor difference in performance ( 2. 4 points in perplexity ).', 'moreover, training using is can be very difficult and requires a careful control of the samples variance, which can lead otherwise to unstable learning as was reported in [ 12 ].', 'hence, an adaptive is may use a large number of samples to solve this problem whereas nce is more stable and requires a fixed small number of noise samples ( e. g., 100 ) to achieve a good performance  #TAUTHOR_TAG 16 ].', 'furthermore, the network learns to self - normalize during training using nce.', 'as a results, and on the contrary to is, the softmax is no longer required during evaluation, which makes nce an attractive choice to train large vocabulary nnlm.', 'the next section will show how nce can be efficiently implemented in batch mode training']",0
"['approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice  #TAUTHOR_TAG 16 ]']","['approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice  #TAUTHOR_TAG 16 ]']","['approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice  #TAUTHOR_TAG 16 ].', 'the main idea here is to restrict the vocabulary,']","['nce is a good alternative to train large vocabulary lms, it is not well - suited for batch mode training on gpus.', 'more particularly, each target word in the batch uses a different set of noise samples, which makes it difficult to formulate the learning using dense matrix operations.', 'as a result, the training time significantly increases.', 'to alleviate this problem, noise samples can be shared across the batch [ 16 ].', 'this paper proposes an extension of nce to batch mode ( b - nce ) training.', 'this approach does not require any sampling and can be formulated using dense matrix operations.', 'furthermore, we can show that this solution optimally approximates the sampling from a unigram distribution, which has been shown to be a good noise distribution choice  #TAUTHOR_TAG 16 ].', 'the main idea here is to restrict the vocabulary, at each forward - backward pass, to the target words in the batch ( words to predict ) and then replace the softmax function by nce.', 'in particular, these words play alternatively the role of targets and noise samples.', 'that is, for a target word wi, at batch index i, the rest of the target batch ( the remaining target words at the other batch indices j, j = i ) are considered to be the noise samples.', 'the rest of this section introduces the mathematical formulation of b - nce to efficiently calculate the error with respect to the output layer weights and biases, as well as the error at the previous layer in batch training, using the objective function ( 5 ) and its gradient ( 6 )']",0
"['17,  #TAUTHOR_TAG 16 ] in comparison']","['[ 17,  #TAUTHOR_TAG 16 ] in comparison']",[' #TAUTHOR_TAG 16 ] in comparison'],"['', 'our rnn implementation uses a projection weight matrix to decouple the word embedding and the hidden layer sizes.', 'we also report results after adding a bottleneck fully - connected relu layer right before the output layer in the recurrent models.', 'these models are marked with the prefix relu in the tables below.', 'each of the models is trained using the proposed b - nce approach and the shared noise nce ( s - nce ) [ 16 ].', 'for the ltcb corpus, we also report results of the models trained with the full softmax function.', 'this is the primary motive for using this corpus.', 'we would like also to highlight that the goal of this paper is not about improving lms performance but rather showing how a significant training speed - up can be achieved without compromising the models performance for large vocabulary lms.', 'hence, we solely focus our experiments on nce as a major approach to achieve this goal [ 17,  #TAUTHOR_TAG 16 ] in comparison to the reference full softmax function.', 'comparison to other training approaches such as importance sampling will be conducted in future work']",5
"['in  #TAUTHOR_TAG 16 ], s -']","['in  #TAUTHOR_TAG 16 ],']","['in  #TAUTHOR_TAG 16 ], s -']","['', 'the latter is halved when no improvement on the validation data is observed for an additional 7 epochs.', 'we also use a norm - based gradient clipping with a threshold of 5 but we do not use dropout.', 'moreover, b - nce and s - nce use the unigram as noise distribution pn.', 'following the setup proposed in  #TAUTHOR_TAG 16 ], s - nce uses k = 100 noise samples, whereas b - nce uses only the target words in the batch ( k = 0 ).', '']",5
"['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['relations reveal the structural organization of text, potentially supporting applications such as summarization  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, and coherence evaluation  #AUTHOR_TAG.', 'while some relations are signaled explicitly with connectives such as however  #AUTHOR_TAG, many more are implicit.', 'expert - annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives  #AUTHOR_TAG.', ' #AUTHOR_TAG show that models trained on explicitly marked examples generalize poorly to implicit relation identification.', 'they argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context  #AUTHOR_TAG.', 'similar observations are made by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances to improve supervised classification of implicit discourse relations.', 'in this paper, we approach this problem from the perspective of domain adaptation.', 'specifically, we argue that the reason that automatically - labeled examples generalize poorly is due to domain mismatch from the explicit relations ( source domain ) to the implicit relations ( target domain ).', 'we propose to close the gap by using two simple methods from domain adaptation : ( 1 ) feature representation learning : mapping the source domain and target domain to a shared latent feature space ; ( 2 ) resampling : modifying the relation distribution in the explicit relations to match the distribution over implicit relations.', 'our results on the penn discourse treebank  #AUTHOR_TAG show that these two methods improve the performance on unsupervised discourse relation identification by more than 8. 4 % on average f 1 score across all relation types, an 82 % reduction on the transfer loss incurred by training on explicitly - marked discourse relations.', '2  #AUTHOR_TAG train a classifier for implicit intra - sentence discourse relations from explicitly - marked examples in the rhetorical structure theory ( rst ) treebank, where the relations are automatically labeled by their discourse connectives : for example, labeling the relation as contrast if the connective is but.', ' #AUTHOR_TAG argue that explicitly marked relations are too different from implicit relations to serve as an adequate supervision signal, obtaining negative results in segmented discourse representation theory ( sdrt ) relations.', 'more recent work has focused on the penn discourse treebank ( pdtb ), using explicitly - marked relations to supplement, rather than replace, a labeled corpus of implicit relations.', 'for example, biran and mc  #AUTHOR_TAG collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identification.', ' #AUTHOR_TAG present a multi - task learning framework, using explicit relation identification as auxiliary tasks to help main task on implicit relation identification.', ' #TAUTHOR_TAG explore several selection heuristics for adding automatically - labeled examples from gigaword to']",0
"['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['relations reveal the structural organization of text, potentially supporting applications such as summarization  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, and coherence evaluation  #AUTHOR_TAG.', 'while some relations are signaled explicitly with connectives such as however  #AUTHOR_TAG, many more are implicit.', 'expert - annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives  #AUTHOR_TAG.', ' #AUTHOR_TAG show that models trained on explicitly marked examples generalize poorly to implicit relation identification.', 'they argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context  #AUTHOR_TAG.', 'similar observations are made by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances to improve supervised classification of implicit discourse relations.', 'in this paper, we approach this problem from the perspective of domain adaptation.', 'specifically, we argue that the reason that automatically - labeled examples generalize poorly is due to domain mismatch from the explicit relations ( source domain ) to the implicit relations ( target domain ).', 'we propose to close the gap by using two simple methods from domain adaptation : ( 1 ) feature representation learning : mapping the source domain and target domain to a shared latent feature space ; ( 2 ) resampling : modifying the relation distribution in the explicit relations to match the distribution over implicit relations.', 'our results on the penn discourse treebank  #AUTHOR_TAG show that these two methods improve the performance on unsupervised discourse relation identification by more than 8. 4 % on average f 1 score across all relation types, an 82 % reduction on the transfer loss incurred by training on explicitly - marked discourse relations.', '2  #AUTHOR_TAG train a classifier for implicit intra - sentence discourse relations from explicitly - marked examples in the rhetorical structure theory ( rst ) treebank, where the relations are automatically labeled by their discourse connectives : for example, labeling the relation as contrast if the connective is but.', ' #AUTHOR_TAG argue that explicitly marked relations are too different from implicit relations to serve as an adequate supervision signal, obtaining negative results in segmented discourse representation theory ( sdrt ) relations.', 'more recent work has focused on the penn discourse treebank ( pdtb ), using explicitly - marked relations to supplement, rather than replace, a labeled corpus of implicit relations.', 'for example, biran and mc  #AUTHOR_TAG collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identification.', ' #AUTHOR_TAG present a multi - task learning framework, using explicit relation identification as auxiliary tasks to help main task on implicit relation identification.', ' #TAUTHOR_TAG explore several selection heuristics for adding automatically - labeled examples from gigaword to']",0
['instance  #TAUTHOR_TAG'],['instance  #TAUTHOR_TAG'],['least one target domain instance  #TAUTHOR_TAG'],"['', 'given the label distribution from the target domain, we resample training examples from the source domain with replacement, in order to match the label distribution in the target domain.', 'as this requires the label distribution from the target domain, it is no longer purely unsupervised domain adaptation ; instead, we call it resampling with minimal supervision.', 'it may also be desirable to ensure that the source and target training instances are similar in terms of their observed features ; this is the idea behind the instance weighting approach to domain adaptation  #AUTHOR_TAG.', 'motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least τ with at least one target domain instance  #TAUTHOR_TAG']",0
"['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['relations reveal the structural organization of text, potentially supporting applications such as summarization  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, and coherence evaluation  #AUTHOR_TAG.', 'while some relations are signaled explicitly with connectives such as however  #AUTHOR_TAG, many more are implicit.', 'expert - annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives  #AUTHOR_TAG.', ' #AUTHOR_TAG show that models trained on explicitly marked examples generalize poorly to implicit relation identification.', 'they argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context  #AUTHOR_TAG.', 'similar observations are made by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances to improve supervised classification of implicit discourse relations.', 'in this paper, we approach this problem from the perspective of domain adaptation.', 'specifically, we argue that the reason that automatically - labeled examples generalize poorly is due to domain mismatch from the explicit relations ( source domain ) to the implicit relations ( target domain ).', 'we propose to close the gap by using two simple methods from domain adaptation : ( 1 ) feature representation learning : mapping the source domain and target domain to a shared latent feature space ; ( 2 ) resampling : modifying the relation distribution in the explicit relations to match the distribution over implicit relations.', 'our results on the penn discourse treebank  #AUTHOR_TAG show that these two methods improve the performance on unsupervised discourse relation identification by more than 8. 4 % on average f 1 score across all relation types, an 82 % reduction on the transfer loss incurred by training on explicitly - marked discourse relations.', '2  #AUTHOR_TAG train a classifier for implicit intra - sentence discourse relations from explicitly - marked examples in the rhetorical structure theory ( rst ) treebank, where the relations are automatically labeled by their discourse connectives : for example, labeling the relation as contrast if the connective is but.', ' #AUTHOR_TAG argue that explicitly marked relations are too different from implicit relations to serve as an adequate supervision signal, obtaining negative results in segmented discourse representation theory ( sdrt ) relations.', 'more recent work has focused on the penn discourse treebank ( pdtb ), using explicitly - marked relations to supplement, rather than replace, a labeled corpus of implicit relations.', 'for example, biran and mc  #AUTHOR_TAG collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identification.', ' #AUTHOR_TAG present a multi - task learning framework, using explicit relation identification as auxiliary tasks to help main task on implicit relation identification.', ' #TAUTHOR_TAG explore several selection heuristics for adding automatically - labeled examples from gigaword to']",1
"['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances']","['relations reveal the structural organization of text, potentially supporting applications such as summarization  #AUTHOR_TAG, sentiment analysis  #AUTHOR_TAG, and coherence evaluation  #AUTHOR_TAG.', 'while some relations are signaled explicitly with connectives such as however  #AUTHOR_TAG, many more are implicit.', 'expert - annotated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives  #AUTHOR_TAG.', ' #AUTHOR_TAG show that models trained on explicitly marked examples generalize poorly to implicit relation identification.', 'they argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context  #AUTHOR_TAG.', 'similar observations are made by  #TAUTHOR_TAG, who attempt to add automatically - labeled instances to improve supervised classification of implicit discourse relations.', 'in this paper, we approach this problem from the perspective of domain adaptation.', 'specifically, we argue that the reason that automatically - labeled examples generalize poorly is due to domain mismatch from the explicit relations ( source domain ) to the implicit relations ( target domain ).', 'we propose to close the gap by using two simple methods from domain adaptation : ( 1 ) feature representation learning : mapping the source domain and target domain to a shared latent feature space ; ( 2 ) resampling : modifying the relation distribution in the explicit relations to match the distribution over implicit relations.', 'our results on the penn discourse treebank  #AUTHOR_TAG show that these two methods improve the performance on unsupervised discourse relation identification by more than 8. 4 % on average f 1 score across all relation types, an 82 % reduction on the transfer loss incurred by training on explicitly - marked discourse relations.', '2  #AUTHOR_TAG train a classifier for implicit intra - sentence discourse relations from explicitly - marked examples in the rhetorical structure theory ( rst ) treebank, where the relations are automatically labeled by their discourse connectives : for example, labeling the relation as contrast if the connective is but.', ' #AUTHOR_TAG argue that explicitly marked relations are too different from implicit relations to serve as an adequate supervision signal, obtaining negative results in segmented discourse representation theory ( sdrt ) relations.', 'more recent work has focused on the penn discourse treebank ( pdtb ), using explicitly - marked relations to supplement, rather than replace, a labeled corpus of implicit relations.', 'for example, biran and mc  #AUTHOR_TAG collect word pairs from arguments of explicit examples to help the supervised learning on implicit relation identification.', ' #AUTHOR_TAG present a multi - task learning framework, using explicit relation identification as auxiliary tasks to help main task on implicit relation identification.', ' #TAUTHOR_TAG explore several selection heuristics for adding automatically - labeled examples from gigaword to']",4
['instance  #TAUTHOR_TAG'],['instance  #TAUTHOR_TAG'],['least one target domain instance  #TAUTHOR_TAG'],"['', 'given the label distribution from the target domain, we resample training examples from the source domain with replacement, in order to match the label distribution in the target domain.', 'as this requires the label distribution from the target domain, it is no longer purely unsupervised domain adaptation ; instead, we call it resampling with minimal supervision.', 'it may also be desirable to ensure that the source and target training instances are similar in terms of their observed features ; this is the idea behind the instance weighting approach to domain adaptation  #AUTHOR_TAG.', 'motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least τ with at least one target domain instance  #TAUTHOR_TAG']",3
"['recent results of  #TAUTHOR_TAG.', 'model selection we use a linear support vector machine  #AUTHOR_TAG as the classification model.', 'our model includes five tunable parameters :']","['recent results of  #TAUTHOR_TAG.', 'model selection we use a linear support vector machine  #AUTHOR_TAG as the classification model.', 'our model includes five tunable parameters :']","['and mc  #AUTHOR_TAG.', 'in a pilot study we found that larger amounts of additional training data yielded no further improvements, which is consistent with the recent results of  #TAUTHOR_TAG.', 'model selection we use a linear support vector machine  #AUTHOR_TAG as the classification model.', 'our model includes five tunable parameters : the number of pivot features κ, the size of the feature subset k, the noise level for the denoising autoencoder q, the cosine similarity threshold for resampling τ, and the penalty parameter c for the svm classifier.', 'we consider κ ∈ { 1000, 2000, 3000 } for pivot features and c ∈']","['', 'to test this idea, we collected 1, 000 news articles from cnn. com as extra training data.', 'explicitly - marked discourse relations from this data are automatically extracted by matching the pdtb discourse connectives  #AUTHOR_TAG.', 'for this data, we also need to extract the arguments of the identified connectives : for every identified connective, the sentence following this connective is labeled as arg2 and the preceding sentence is labeled as arg1, as suggested by biran and mc  #AUTHOR_TAG.', 'in a pilot study we found that larger amounts of additional training data yielded no further improvements, which is consistent with the recent results of  #TAUTHOR_TAG.', 'model selection we use a linear support vector machine  #AUTHOR_TAG as the classification model.', 'our model includes five tunable parameters : the number of pivot features κ, the size of the feature subset k, the noise level for the denoising autoencoder q, the cosine similarity threshold for resampling τ, and the penalty parameter c for the svm classifier.', 'we consider κ ∈ { 1000, 2000, 3000 } for pivot features and c ∈ { 0. 001, 0. 01, 0. 1, 1. 0, 10. 0 } for penalty']",3
"['with more sophisticated techniques for instance selection  #TAUTHOR_TAG and feature selection  #AUTHOR_TAG biran and mc  #AUTHOR_TAG, while also tackling the more difficult problems of multi - class relation']","['with more sophisticated techniques for instance selection  #TAUTHOR_TAG and feature selection  #AUTHOR_TAG biran and mc  #AUTHOR_TAG, while also tackling the more difficult problems of multi - class relation']","['.', 'future work will explore the combination of this approach with more sophisticated techniques for instance selection  #TAUTHOR_TAG and feature selection  #AUTHOR_TAG biran and mc  #AUTHOR_TAG, while also tackling the more difficult problems of multi - class relation classification and fine -']","['have presented two methods - feature representation learning and resampling - from domain adaptation to close the gap of using explicit examples for unsupervised implicit discourse relation identification.', 'experiments on the pdtb show the combination of these two methods eliminates more than 80 % of the transfer loss caused by training on explicit examples, increasing average f 1 from 31 % to 39. 5 %, against a supervised upper bound of 41. 3 %.', 'future work will explore the combination of this approach with more sophisticated techniques for instance selection  #TAUTHOR_TAG and feature selection  #AUTHOR_TAG biran and mc  #AUTHOR_TAG, while also tackling the more difficult problems of multi - class relation classification and fine - grained level - 2 discourse relations']",2
"['', 'recently, transformer networks have been shown to perform well for neural machine translation  #TAUTHOR_TAG and many other nlp tasks']","['target units.', 'recently, transformer networks have been shown to perform well for neural machine translation  #TAUTHOR_TAG and many other nlp tasks [ 12 ]']","['generate the target units.', 'recently, transformer networks have been shown to perform well for neural machine translation  #TAUTHOR_TAG and many other nlp tasks [ 12 ].', 'a transformer layer distinguishes']","['recognition systems have experienced many advances over the past decade, with neural acoustic and language models leading to impressive new levels of performance across many challenging tasks [ 1, 2 ].', 'advances in alignment - free sequence - level loss functions like ctc and asg [ 3, 4 ] enabled easier training with letters as output units [ 5, 6 ].', 'the success of sequence - to - sequence models in neural machine translation systems [ 7, 8 ] offered further simplification to asr systems by integrating the acoustic and the language models into a single encoder - decoder architecture that is jointly optimized [ 9, 10 ].', 'the encoder focuses on building acoustic representations that the decoder, through different attention mechanisms, can use to generate the target units.', 'recently, transformer networks have been shown to perform well for neural machine translation  #TAUTHOR_TAG and many other nlp tasks [ 12 ].', 'a transformer layer distinguishes itself from a regular recurrent network by entirely relying on a key - value "" self "" - attention mechanism for learning relationships between distant concepts, rather than relying on recurrent connections and memory cells to preserve information, as in lstms, that can fade over time steps.', ""transformer layers can be seen as bagof - concept layers because they don't preserve location information in the weighted sum self - attention operation."", 'to model word order, sinusoidal positional embeddings are used  #TAUTHOR_TAG.', 'there has been recent research interest in using transformer networks for end - to - end asr both with ctc loss [ 13 ] and in an encoder - decoder framework [ 14, 15 ] with modest performance compared to baseline systems.', 'for a standard hybrid asr system, [ 16 ] introduced a time - constrained key - value self - attention layer to be used in tandem with other tdnn and recurrent layers.', ""using time - restricted self - attention context enabled the authors to model input positions as 1 - hot vectors, however, they didn't show a conclusive evidence for the impact of the self - attention context size."", 'one interesting research question in all previous work was : how to best introduce positional information for input speech features.', 'answers range from dropping it altogether, adding it to input features / embedding, and concatenating it with input features leaving it to the neural network to decide how to combine them.', 'in this paper, we take an alternative approach.', 'we propose replacing sinusoidal positional embedding with contextually augmented inputs learned by 2 - d convolutional layers over input speech features in the encoder, and by 1 - d convolutional layers over previously generated outputs in the decoder.', 'lower layers build atomic concepts, both in encoders and decoder']",0
"['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by the inverse square root of the key dimension.', 'this self - attention operation is done h times in parallel, for the case of h attention heads, with different projection matrices from dinput to d k, d k, and dv.', '']",0
"['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by the inverse square root of the key dimension.', 'this self - attention operation is done h times in parallel, for the case of h attention heads, with different projection matrices from dinput to d k, d k, and dv.', '']",0
['baseline machine translation transformers  #TAUTHOR_TAG and adopted in'],"['baseline machine translation transformers  #TAUTHOR_TAG and adopted in [ 13, 15 ], shows inferior wer performance.', ""by combining sinusoidal and convolutional position embedding ( rows 1 + 2 ), we don't observe any gains."", 'this supports our intuition that the relative convolutional positional information provides sufficient signal for']","['proposed transformer encoder - decoder model with convolutional context using the canonical configuration in the first row.', 'replacing the 1 - d convolutional context in the decoder with sinusoidal positional embedding, as proposed in the baseline machine translation transformers  #TAUTHOR_TAG and adopted in']","['first studied performance of our approach to alternative architectures and positional encoding schemes.', 'in table ( 1 ) we show the wer of the proposed transformer encoder - decoder model with convolutional context using the canonical configuration in the first row.', 'replacing the 1 - d convolutional context in the decoder with sinusoidal positional embedding, as proposed in the baseline machine translation transformers  #TAUTHOR_TAG and adopted in [ 13, 15 ], shows inferior wer performance.', ""by combining sinusoidal and convolutional position embedding ( rows 1 + 2 ), we don't observe any gains."", 'this supports our intuition that the relative convolutional positional information provides sufficient signal for the transformer layers to recreate more global word order.', 'we also found that having multiple encoder - side attention layers is critical for achieving the best wer.', 'increasing the intermediate relu layer in each encoder and decoder layer was found to greatly improve the overall wer across different sets, however increasing the number of attention heads, while keeping the attention dimension the same, deteriorates the performance.', 'to better understand these results, we also studied the effects of different hyperparameter settings.', 'table ( 2 ) shows the effect of different decoder convolutional context sizes spread over different depths.', 'all configurations in table ( 2 ) share the same canonical configuration and the number of 1 - d conv.', 'feature maps were chosen to ensure the total number of parameters are fixed between all configurations.', 'the best performance comes from using the same parameter budget over wider context that is built over multiple convolutional layers.', 'however, the decoder is able to get reasonable wer even with a context of just 3 words as input to the transformer layers.', 'using a deep transformer encoder capture long range structure of the data as an acoustic lm built on top of learned concepts from']",0
"['', 'recently, transformer networks have been shown to perform well for neural machine translation  #TAUTHOR_TAG and many other nlp tasks']","['target units.', 'recently, transformer networks have been shown to perform well for neural machine translation  #TAUTHOR_TAG and many other nlp tasks [ 12 ]']","['generate the target units.', 'recently, transformer networks have been shown to perform well for neural machine translation  #TAUTHOR_TAG and many other nlp tasks [ 12 ].', 'a transformer layer distinguishes']","['recognition systems have experienced many advances over the past decade, with neural acoustic and language models leading to impressive new levels of performance across many challenging tasks [ 1, 2 ].', 'advances in alignment - free sequence - level loss functions like ctc and asg [ 3, 4 ] enabled easier training with letters as output units [ 5, 6 ].', 'the success of sequence - to - sequence models in neural machine translation systems [ 7, 8 ] offered further simplification to asr systems by integrating the acoustic and the language models into a single encoder - decoder architecture that is jointly optimized [ 9, 10 ].', 'the encoder focuses on building acoustic representations that the decoder, through different attention mechanisms, can use to generate the target units.', 'recently, transformer networks have been shown to perform well for neural machine translation  #TAUTHOR_TAG and many other nlp tasks [ 12 ].', 'a transformer layer distinguishes itself from a regular recurrent network by entirely relying on a key - value "" self "" - attention mechanism for learning relationships between distant concepts, rather than relying on recurrent connections and memory cells to preserve information, as in lstms, that can fade over time steps.', ""transformer layers can be seen as bagof - concept layers because they don't preserve location information in the weighted sum self - attention operation."", 'to model word order, sinusoidal positional embeddings are used  #TAUTHOR_TAG.', 'there has been recent research interest in using transformer networks for end - to - end asr both with ctc loss [ 13 ] and in an encoder - decoder framework [ 14, 15 ] with modest performance compared to baseline systems.', 'for a standard hybrid asr system, [ 16 ] introduced a time - constrained key - value self - attention layer to be used in tandem with other tdnn and recurrent layers.', ""using time - restricted self - attention context enabled the authors to model input positions as 1 - hot vectors, however, they didn't show a conclusive evidence for the impact of the self - attention context size."", 'one interesting research question in all previous work was : how to best introduce positional information for input speech features.', 'answers range from dropping it altogether, adding it to input features / embedding, and concatenating it with input features leaving it to the neural network to decide how to combine them.', 'in this paper, we take an alternative approach.', 'we propose replacing sinusoidal positional embedding with contextually augmented inputs learned by 2 - d convolutional layers over input speech features in the encoder, and by 1 - d convolutional layers over previously generated outputs in the decoder.', 'lower layers build atomic concepts, both in encoders and decoder']",5
"['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by']","['layers  #TAUTHOR_TAG have the ability to learn long range relationships for many sequential classification tasks [ 12 ].', 'multi -', 'the dot product between keys and queries is scaled by the inverse square root of the key dimension.', 'this self - attention operation is done h times in parallel, for the case of h attention heads, with different projection matrices from dinput to d k, d k, and dv.', '']",5
['baseline machine translation transformers  #TAUTHOR_TAG and adopted in'],"['baseline machine translation transformers  #TAUTHOR_TAG and adopted in [ 13, 15 ], shows inferior wer performance.', ""by combining sinusoidal and convolutional position embedding ( rows 1 + 2 ), we don't observe any gains."", 'this supports our intuition that the relative convolutional positional information provides sufficient signal for']","['proposed transformer encoder - decoder model with convolutional context using the canonical configuration in the first row.', 'replacing the 1 - d convolutional context in the decoder with sinusoidal positional embedding, as proposed in the baseline machine translation transformers  #TAUTHOR_TAG and adopted in']","['first studied performance of our approach to alternative architectures and positional encoding schemes.', 'in table ( 1 ) we show the wer of the proposed transformer encoder - decoder model with convolutional context using the canonical configuration in the first row.', 'replacing the 1 - d convolutional context in the decoder with sinusoidal positional embedding, as proposed in the baseline machine translation transformers  #TAUTHOR_TAG and adopted in [ 13, 15 ], shows inferior wer performance.', ""by combining sinusoidal and convolutional position embedding ( rows 1 + 2 ), we don't observe any gains."", 'this supports our intuition that the relative convolutional positional information provides sufficient signal for the transformer layers to recreate more global word order.', 'we also found that having multiple encoder - side attention layers is critical for achieving the best wer.', 'increasing the intermediate relu layer in each encoder and decoder layer was found to greatly improve the overall wer across different sets, however increasing the number of attention heads, while keeping the attention dimension the same, deteriorates the performance.', 'to better understand these results, we also studied the effects of different hyperparameter settings.', 'table ( 2 ) shows the effect of different decoder convolutional context sizes spread over different depths.', 'all configurations in table ( 2 ) share the same canonical configuration and the number of 1 - d conv.', 'feature maps were chosen to ensure the total number of parameters are fixed between all configurations.', 'the best performance comes from using the same parameter budget over wider context that is built over multiple convolutional layers.', 'however, the decoder is able to get reasonable wer even with a context of just 3 words as input to the transformer layers.', 'using a deep transformer encoder capture long range structure of the data as an acoustic lm built on top of learned concepts from']",5
[') by  #TAUTHOR_TAG and achieved'],['( smt ) by  #TAUTHOR_TAG and achieved state - of - the - art results as reported in the'],[') by  #TAUTHOR_TAG and achieved'],"['language side dependency structures have been successfully used in statistical machine translation ( smt ) by  #TAUTHOR_TAG and achieved state - of - the - art results as reported in the nist 2008 open mt evaluation workshop and the ntcir - 9 chinese - to - english patent translation task  #AUTHOR_TAG.', 'a primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to longdistance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order ( mc  #AUTHOR_TAG.', 'it is known that dependency - style structures can be transformed from a number of linguistic struc - * now at baidu inc.', '† now at nara institute of science & technology ( naist ) tures.', 'for example, using the constituent - todependency conversion approach proposed by  #AUTHOR_TAG, we can easily yield dependency trees from pcfg style trees.', 'a semantic dependency representation of a whole sentence, predicate - argument structures ( pass ), are also included in the output trees of ( 1 ) a state - of - the - art head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG parser, enju 1  #AUTHOR_TAG and ( 2 ) a state - of - the - art ccg parser 2  #AUTHOR_TAG.', 'the motivation of this paper is to investigate the impact of these non - isomorphic dependency structures to be used for smt.', 'that is, we would like to provide a comparative evaluation of these dependencies in a string - to - dependency decoder  #TAUTHOR_TAG']",0
[') by  #TAUTHOR_TAG and achieved'],['( smt ) by  #TAUTHOR_TAG and achieved state - of - the - art results as reported in the'],[') by  #TAUTHOR_TAG and achieved'],"['language side dependency structures have been successfully used in statistical machine translation ( smt ) by  #TAUTHOR_TAG and achieved state - of - the - art results as reported in the nist 2008 open mt evaluation workshop and the ntcir - 9 chinese - to - english patent translation task  #AUTHOR_TAG.', 'a primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to longdistance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order ( mc  #AUTHOR_TAG.', 'it is known that dependency - style structures can be transformed from a number of linguistic struc - * now at baidu inc.', '† now at nara institute of science & technology ( naist ) tures.', 'for example, using the constituent - todependency conversion approach proposed by  #AUTHOR_TAG, we can easily yield dependency trees from pcfg style trees.', 'a semantic dependency representation of a whole sentence, predicate - argument structures ( pass ), are also included in the output trees of ( 1 ) a state - of - the - art head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG parser, enju 1  #AUTHOR_TAG and ( 2 ) a state - of - the - art ccg parser 2  #AUTHOR_TAG.', 'the motivation of this paper is to investigate the impact of these non - isomorphic dependency structures to be used for smt.', 'that is, we would like to provide a comparative evaluation of these dependencies in a string - to - dependency decoder  #TAUTHOR_TAG']",0
['has been originally used by  #TAUTHOR_TAG'],['has been originally used by  #TAUTHOR_TAG'],"['from the pcfg tree, we use the lth constituent - to - dependency conversion tool 3 written by  #AUTHOR_TAG.', 'the head finding rules 4 are according to  #AUTHOR_TAG and  #AUTHOR_TAG.', 'similar approach has been originally used by  #TAUTHOR_TAG']","['pcfg parsing, we select the berkeley parser  #AUTHOR_TAG.', 'in order to generate wordlevel dependency trees from the pcfg tree, we use the lth constituent - to - dependency conversion tool 3 written by  #AUTHOR_TAG.', 'the head finding rules 4 are according to  #AUTHOR_TAG and  #AUTHOR_TAG.', 'similar approach has been originally used by  #TAUTHOR_TAG']",0
[') by  #TAUTHOR_TAG and achieved'],['( smt ) by  #TAUTHOR_TAG and achieved state - of - the - art results as reported in the'],[') by  #TAUTHOR_TAG and achieved'],"['language side dependency structures have been successfully used in statistical machine translation ( smt ) by  #TAUTHOR_TAG and achieved state - of - the - art results as reported in the nist 2008 open mt evaluation workshop and the ntcir - 9 chinese - to - english patent translation task  #AUTHOR_TAG.', 'a primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to longdistance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order ( mc  #AUTHOR_TAG.', 'it is known that dependency - style structures can be transformed from a number of linguistic struc - * now at baidu inc.', '† now at nara institute of science & technology ( naist ) tures.', 'for example, using the constituent - todependency conversion approach proposed by  #AUTHOR_TAG, we can easily yield dependency trees from pcfg style trees.', 'a semantic dependency representation of a whole sentence, predicate - argument structures ( pass ), are also included in the output trees of ( 1 ) a state - of - the - art head - driven phrase structure grammar ( hpsg )  #AUTHOR_TAG parser, enju 1  #AUTHOR_TAG and ( 2 ) a state - of - the - art ccg parser 2  #AUTHOR_TAG.', 'the motivation of this paper is to investigate the impact of these non - isomorphic dependency structures to be used for smt.', 'that is, we would like to provide a comparative evaluation of these dependencies in a string - to - dependency decoder  #TAUTHOR_TAG']",1
"['extracting string - to - dependency transfer rules, we use well - formed dependency structures, either fixed or floating, as defined in  #TAUTHOR_TAG']","['extracting string - to - dependency transfer rules, we use well - formed dependency structures, either fixed or floating, as defined in  #TAUTHOR_TAG']","['extracting string - to - dependency transfer rules, we use well - formed dependency structures, either fixed or floating, as defined in  #TAUTHOR_TAG.', 'similarly, we ignore the syntactic roles both during rule extracting and target dependency language model ( lm ) training']","['', 'for extracting string - to - dependency transfer rules, we use well - formed dependency structures, either fixed or floating, as defined in  #TAUTHOR_TAG.', 'similarly, we ignore the syntactic roles both during rule extracting and target dependency language model ( lm ) training']",5
"['decoder described in  #TAUTHOR_TAG  #AUTHOR_TAG, was employed']","['re - implemented the string - to - dependency decoder described in  #TAUTHOR_TAG  #AUTHOR_TAG, was employed']","['re - implemented the string - to - dependency decoder described in  #TAUTHOR_TAG  #AUTHOR_TAG, was employed']","['re - implemented the string - to - dependency decoder described in  #TAUTHOR_TAG  #AUTHOR_TAG, was employed to train ( 1 ) a five - gram lm on the xinhua portion of ldc english gigaword corpus v3 ( ldc2007t07 ) and ( 2 ) a tri - gram dependency lm on the english dependency structures of the training data.', 'we report the translation quality using the case - insensitive bleu - 4 metric  #AUTHOR_TAG']",5
['has been originally used by  #TAUTHOR_TAG'],['has been originally used by  #TAUTHOR_TAG'],"['from the pcfg tree, we use the lth constituent - to - dependency conversion tool 3 written by  #AUTHOR_TAG.', 'the head finding rules 4 are according to  #AUTHOR_TAG and  #AUTHOR_TAG.', 'similar approach has been originally used by  #TAUTHOR_TAG']","['pcfg parsing, we select the berkeley parser  #AUTHOR_TAG.', 'in order to generate wordlevel dependency trees from the pcfg tree, we use the lth constituent - to - dependency conversion tool 3 written by  #AUTHOR_TAG.', 'the head finding rules 4 are according to  #AUTHOR_TAG and  #AUTHOR_TAG.', 'similar approach has been originally used by  #TAUTHOR_TAG']",3
['adversarial training or selflearning  #TAUTHOR_TAG'],['adversarial training or selflearning  #TAUTHOR_TAG'],"['the initial alignment  #AUTHOR_TAG, fully unsupervised methods have managed to obtain comparable results based on either adversarial training or selflearning  #TAUTHOR_TAG.', 'a prominent application of these methods is']","['- lingual word embedding mappings have attracted a lot of attention in recent times.', 'these methods work by independently training word embeddings in different languages, and mapping them to a shared space through linear transformations.', 'while early methods required a training dictionary to find the initial alignment  #AUTHOR_TAG, fully unsupervised methods have managed to obtain comparable results based on either adversarial training or selflearning  #TAUTHOR_TAG.', 'a prominent application of these methods is bilingual lexicon induction ( bli ), that is, using the resulting cross - lingual embeddings to build a bilingual dictionary.', '']",0
['adversarial training or selflearning  #TAUTHOR_TAG'],['adversarial training or selflearning  #TAUTHOR_TAG'],"['the initial alignment  #AUTHOR_TAG, fully unsupervised methods have managed to obtain comparable results based on either adversarial training or selflearning  #TAUTHOR_TAG.', 'a prominent application of these methods is']","['- lingual word embedding mappings have attracted a lot of attention in recent times.', 'these methods work by independently training word embeddings in different languages, and mapping them to a shared space through linear transformations.', 'while early methods required a training dictionary to find the initial alignment  #AUTHOR_TAG, fully unsupervised methods have managed to obtain comparable results based on either adversarial training or selflearning  #TAUTHOR_TAG.', 'a prominent application of these methods is bilingual lexicon induction ( bli ), that is, using the resulting cross - lingual embeddings to build a bilingual dictionary.', '']",6
"['.', 'in our experiments, we use fasttext embeddings  #AUTHOR_TAG mapped through vecmap  #TAUTHOR_TAG, but the algorithm described next can also']","['train them.', 'in our experiments, we use fasttext embeddings  #AUTHOR_TAG mapped through vecmap  #TAUTHOR_TAG, but the algorithm described next can also']","['.', 'in our experiments, we use fasttext embeddings  #AUTHOR_TAG mapped through vecmap  #TAUTHOR_TAG, but the algorithm described next can also']","['input of our method is a set of cross - lingual word embeddings and the monolingual corpora used to train them.', 'in our experiments, we use fasttext embeddings  #AUTHOR_TAG mapped through vecmap  #TAUTHOR_TAG, but the algorithm described next can also work with any other word embedding and cross - lingual mapping method.', 'the general idea of our method is to to build an unsupervised phrase - based statistical machine translation system  #AUTHOR_TAG c artetxe et al.,, 2019, and use it to generate a synthetic parallel corpus from which to extract a bilingual dictionary.', 'for that purpose, we first derive phrase embeddings from the input word embeddings by taking the 400, 000 most frequent bigrams and and the 400, 000 most frequent trigrams in each language, and assigning them the centroid of the words they contain.', 'having done that, we use the resulting cross - lingual phrase embeddings to build a phrase - table as described in  #AUTHOR_TAG c ).', '']",5
"['.', 'having done that, we map these word embeddings to a cross - lingual space using the unsupervised mode in vecmap  #TAUTHOR_TAG, which builds an initial solution based on the intralingual similarity distribution of the embeddings']","['original dataset.', 'having done that, we map these word embeddings to a cross - lingual space using the unsupervised mode in vecmap  #TAUTHOR_TAG, which builds an initial solution based on the intralingual similarity distribution of the embeddings']","[', which is not documented in the original dataset.', 'having done that, we map these word embeddings to a cross - lingual space using the unsupervised mode in vecmap  #TAUTHOR_TAG, which builds an initial solution based on the intralingual similarity distribution of the embeddings']","['order to compare our proposed method headto - head with other bli methods, the experimental setting needs to fix the monolingual embedding training method, as well as the cross - lingual mapping algorithm and the evaluation dictionaries.', 'in addition, in order to avoid any advantage, our method should not see any further monolingual corpora than those used to train the monolingual embeddings.', 'unfortunately, existing bli datasets distribute pre - trained word embeddings alone, but not the monolingual corpora used to train them.', 'for that reason, we decide to use the evaluation dictionaries from the standard muse dataset but, instead of using the pre - trained wikipedia embeddings distributed with it, we extract monolingual corpora from wikipedia ourselves and train our own embeddings trying to be as faithful as possible to the original settings.', 'this allows us to compare our proposed method to previous retrieval techniques in the exact same conditions, while keeping our results as comparable as possible to previous work reporting results for the muse dataset.', 'more concretely, we use wikiextractor 4 to extract plain text from wikipedia dumps, and preprocess the resulting corpus using standard moses tools  #AUTHOR_TAG by applying sentence splitting, punctuation normalization, tokenization with aggressive hyphen splitting, and lowercasing.', 'we then train word embeddings for each language using the skip - gram implementation of fasttext  #AUTHOR_TAG table 1 : p @ 1 of proposed system and previous retrieval methods, using the same cross - lingual embeddings.', 'the muse dataset were trained using these exact same settings, so our embeddings only differ in the wikipedia dump used to extract the training corpus and the pre - processing applied to it, which is not documented in the original dataset.', 'having done that, we map these word embeddings to a cross - lingual space using the unsupervised mode in vecmap  #TAUTHOR_TAG, which builds an initial solution based on the intralingual similarity distribution of the embeddings and iteratively improves it through self - learning.', 'finally, we induce a bilingual dictionary using our proposed method and evaluate it in comparison to previous retrieval methods ( standard nearest neighbor, inverted nearest neighbor, inverted softmax 5 and csls ).', 'following common practice, we use precision at 1 as our evaluation measure.', '6 4 results and discussion table 1 reports the results of our proposed system in comparison to previous retrieval methods.', 'as it can be seen, our method obtains the best results in all language pairs and directions, with an average improvement of 6 points over nearest neighbor and 4 points over csls, which is the best performing previous method.', 'these results are very consistent']",5
['combined with initialization heuristics  #TAUTHOR_TAG'],['combined with initialization heuristics  #TAUTHOR_TAG'],"['adversarial training  #AUTHOR_TAG a ; or more robust iterative approaches combined with initialization heuristics  #TAUTHOR_TAG.', 'at the same time, several recent methods have formulated embedding mappings as an optimal transport problem  #AUTHOR_TAG b ;.', 'in addition to that, a large body']","['', 'the amount of required supervision was later reduced through self - learning methods  #AUTHOR_TAG, and then completely eliminated through adversarial training  #AUTHOR_TAG a ; or more robust iterative approaches combined with initialization heuristics  #TAUTHOR_TAG.', 'at the same time, several recent methods have formulated embedding mappings as an optimal transport problem  #AUTHOR_TAG b ;.', 'in addition to that, a large body of work has focused on addressing the hubness problem that arises when directly inducing bilingual dictionaries from cross - lingual embeddings, either through the retrieval method  #AUTHOR_TAG or the mapping itself  #AUTHOR_TAG.', 'while all these previous methods directly induce bilingual dictionaries from cross - lingually mapped embeddings, our proposed method combines them with unsupervised machine translation techniques, outperforming them all by a substantial margin']",5
['recently proposed dependency parser  #TAUTHOR_TAG 1 which has demonstrated state -'],['recently proposed dependency parser  #TAUTHOR_TAG 1 which has demonstrated state - of - theart'],['recently proposed dependency parser  #TAUTHOR_TAG 1 which has demonstrated state -'],"['multilingual track of the conll - 2007 shared task  #AUTHOR_TAG considers dependency parsing of texts written in different languages.', 'it requires use of a single dependency parsing model for the entire set of languages ; model parameters are estimated individually for each language on the basis of provided training sets.', 'we use a recently proposed dependency parser  #TAUTHOR_TAG 1 which has demonstrated state - of - theart performance on a selection of languages from the conll - x shared task  #AUTHOR_TAG.', 'this parser employs a latent variable model, incremental sigmoid belief networks ( isbns ), to define a generative history - based model of projective parsing.', 'we used the pseudo - projective transformation introduced in  #AUTHOR_TAG to cast non - projective parsing tasks as projective.', 'following  #AUTHOR_TAG, the encoding scheme called head in  #AUTHOR_TAG was used to encode the original non - projective dependencies in the labels of the projectivized dependency tree.', 'in the following sections we will briefly discuss our modifications to the isbn parser, experimental setup, and achieved results']",5
['isbn dependency parser in  #TAUTHOR_TAG. the current state is'],['isbn dependency parser in  #TAUTHOR_TAG. the current state is'],"['was not foreseen by the designer. this provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality appropriate for the nature of the problem. in our experiments we use the same definition', 'of structural locality as was proposed for the isbn dependency parser in  #TAUTHOR_TAG. the current state is connected']","['', 'related states are then connected with conditional dependency edges in the isbn graphical model. longer conditional dependencies are then possible through chains of these immediate conditional dependencies, but there is an inductive bias toward shorter chains. this', 'bias makes it important that the set of chosen relationships defines an appropriate notion of locality. however, as long as', 'there exists some chain of relationships between any two states, then any statistical dependency which is clearly manifested in the data can be learned, even if', 'it was not foreseen by the designer. this provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality appropriate for the nature of the problem. in our experiments we use the same definition', 'of structural locality as was proposed for the isbn dependency parser in  #TAUTHOR_TAG. the current state is connected to previous states using a set of 7 distinct relationships defined in terms', ""of each state's parser configuration, which includes of a stack and a queue. specifically, the current"", '']",5
"['in isbn models is not tractable, but effective approximations were proposed in  #AUTHOR_TAG a ).', 'unlike  #TAUTHOR_TAG, in the shared']","['in isbn models is not tractable, but effective approximations were proposed in  #AUTHOR_TAG a ).', 'unlike  #TAUTHOR_TAG, in the shared']","['inference in isbn models is not tractable, but effective approximations were proposed in  #AUTHOR_TAG a ).', 'unlike  #TAUTHOR_TAG, in the shared task we used only the simplest feed - forward approximation, which replicates the computation of a neural network of the type proposed in  #AUTHOR_TAG.', 'we would expect better performance with the more accurate approximation based on variational inference proposed and']","['inference in isbn models is not tractable, but effective approximations were proposed in  #AUTHOR_TAG a ).', 'unlike  #TAUTHOR_TAG, in the shared task we used only the simplest feed - forward approximation, which replicates the computation of a neural network of the type proposed in  #AUTHOR_TAG.', 'we would expect better performance with the more accurate approximation based on variational inference proposed and evaluated in  #AUTHOR_TAG a ).', 'we did not try this because, on larger treebanks it would have taken too long to tune the model with this better approximation, and using different approximation methods for different languages would not be compatible with the shared task rules.', 'to search for the most probable parse, we use the heuristic search algorithm described in  #TAUTHOR_TAG, which is a form of beam search.', 'in section 4 we show that this search leads to quite efficient parsing.', 'to overcome a minor shortcoming of the parsing algorithm of  #AUTHOR_TAG we introduce a simple language independent post - processing step.', ""nivre's parsing algorithm allows unattached nodes to stay on the stack at the end of parsing, which is reasonable for treebanks with unlabeled attachment to root."", 'however, this sometimes happens with languages where only labeled attachment to root is allowed.', 'in these cases ( only 35 tokens in greek, 17 in czech, 1 in arabic, on the final testing set ) we attached them using a simple rule : if there are no tokens in the sentence attached to root, then the considered token is attached to root with the most frequent root - attachment relation used for its part - ofspeech tag.', 'if there are other root - attached tokens in the sentence, it is attached to the next root - attached token with the most frequent relation.', 'preference is given to the most frequent attachment direction for its part - of - speech tag.', 'this rule guarantees that no loops are introduced by the post - processing']",5
['isbn dependency parser in  #TAUTHOR_TAG. the current state is'],['isbn dependency parser in  #TAUTHOR_TAG. the current state is'],"['was not foreseen by the designer. this provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality appropriate for the nature of the problem. in our experiments we use the same definition', 'of structural locality as was proposed for the isbn dependency parser in  #TAUTHOR_TAG. the current state is connected']","['', 'related states are then connected with conditional dependency edges in the isbn graphical model. longer conditional dependencies are then possible through chains of these immediate conditional dependencies, but there is an inductive bias toward shorter chains. this', 'bias makes it important that the set of chosen relationships defines an appropriate notion of locality. however, as long as', 'there exists some chain of relationships between any two states, then any statistical dependency which is clearly manifested in the data can be learned, even if', 'it was not foreseen by the designer. this provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality appropriate for the nature of the problem. in our experiments we use the same definition', 'of structural locality as was proposed for the isbn dependency parser in  #TAUTHOR_TAG. the current state is connected to previous states using a set of 7 distinct relationships defined in terms', ""of each state's parser configuration, which includes of a stack and a queue. specifically, the current"", '']",1
['isbn dependency parser in  #TAUTHOR_TAG. the current state is'],['isbn dependency parser in  #TAUTHOR_TAG. the current state is'],"['was not foreseen by the designer. this provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality appropriate for the nature of the problem. in our experiments we use the same definition', 'of structural locality as was proposed for the isbn dependency parser in  #TAUTHOR_TAG. the current state is connected']","['', 'related states are then connected with conditional dependency edges in the isbn graphical model. longer conditional dependencies are then possible through chains of these immediate conditional dependencies, but there is an inductive bias toward shorter chains. this', 'bias makes it important that the set of chosen relationships defines an appropriate notion of locality. however, as long as', 'there exists some chain of relationships between any two states, then any statistical dependency which is clearly manifested in the data can be learned, even if', 'it was not foreseen by the designer. this provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality appropriate for the nature of the problem. in our experiments we use the same definition', 'of structural locality as was proposed for the isbn dependency parser in  #TAUTHOR_TAG. the current state is connected to previous states using a set of 7 distinct relationships defined in terms', ""of each state's parser configuration, which includes of a stack and a queue. specifically, the current"", '']",3
"['not require much effort in feature engineering.', 'as was demonstrated in  #TAUTHOR_TAG, even a minimal set of local explicit features']","['not require much effort in feature engineering.', 'as was demonstrated in  #TAUTHOR_TAG, even a minimal set of local explicit features achieves results']","['require much effort in feature engineering.', 'as was demonstrated in  #TAUTHOR_TAG, even a minimal set of local explicit features']","['', 'important highly predictive features. this makes an isbn parser a particularly good baseline when considering a new treebank or language, be - ara bas', 'cat chi cze eng gre hun ita cause it does not require much effort in feature engineering.', 'as was demonstrated in  #TAUTHOR_TAG, even a minimal set of local explicit features achieves results which are non - significantly different from a carefully chosen set of explicit features', ', given the language independent definition of locality described in section 2. it is also important to note that the model is quite efficient. figure 1 shows the tradeoff between accuracy and parsing time as the width of the', 'search beam is varied, on the development set. this curve plots the average labeled attachment score over basque, chinese, english, and turkish as a function of parsing time per token. 4 accuracy of only 1 %', 'below the maximum can be achieved with average processing time of 17 ms', 'per token, or 60 tokens per second. 5 we also refer the reader to  #TAUTHOR_TAG for more detailed analysis of the isbn dependency', 'parser results, where, among other things, it was shown that the isbn model is especially accurate at modeling long dependencies']",3
"['in isbn models is not tractable, but effective approximations were proposed in  #AUTHOR_TAG a ).', 'unlike  #TAUTHOR_TAG, in the shared']","['in isbn models is not tractable, but effective approximations were proposed in  #AUTHOR_TAG a ).', 'unlike  #TAUTHOR_TAG, in the shared']","['inference in isbn models is not tractable, but effective approximations were proposed in  #AUTHOR_TAG a ).', 'unlike  #TAUTHOR_TAG, in the shared task we used only the simplest feed - forward approximation, which replicates the computation of a neural network of the type proposed in  #AUTHOR_TAG.', 'we would expect better performance with the more accurate approximation based on variational inference proposed and']","['inference in isbn models is not tractable, but effective approximations were proposed in  #AUTHOR_TAG a ).', 'unlike  #TAUTHOR_TAG, in the shared task we used only the simplest feed - forward approximation, which replicates the computation of a neural network of the type proposed in  #AUTHOR_TAG.', 'we would expect better performance with the more accurate approximation based on variational inference proposed and evaluated in  #AUTHOR_TAG a ).', 'we did not try this because, on larger treebanks it would have taken too long to tune the model with this better approximation, and using different approximation methods for different languages would not be compatible with the shared task rules.', 'to search for the most probable parse, we use the heuristic search algorithm described in  #TAUTHOR_TAG, which is a form of beam search.', 'in section 4 we show that this search leads to quite efficient parsing.', 'to overcome a minor shortcoming of the parsing algorithm of  #AUTHOR_TAG we introduce a simple language independent post - processing step.', ""nivre's parsing algorithm allows unattached nodes to stay on the stack at the end of parsing, which is reasonable for treebanks with unlabeled attachment to root."", 'however, this sometimes happens with languages where only labeled attachment to root is allowed.', 'in these cases ( only 35 tokens in greek, 17 in czech, 1 in arabic, on the final testing set ) we attached them using a simple rule : if there are no tokens in the sentence attached to root, then the considered token is attached to root with the most frequent root - attachment relation used for its part - ofspeech tag.', 'if there are other root - attached tokens in the sentence, it is attached to the next root - attached token with the most frequent relation.', 'preference is given to the most frequent attachment direction for its part - of - speech tag.', 'this rule guarantees that no loops are introduced by the post - processing']",4
"['not require much effort in feature engineering.', 'as was demonstrated in  #TAUTHOR_TAG, even a minimal set of local explicit features']","['not require much effort in feature engineering.', 'as was demonstrated in  #TAUTHOR_TAG, even a minimal set of local explicit features achieves results']","['require much effort in feature engineering.', 'as was demonstrated in  #TAUTHOR_TAG, even a minimal set of local explicit features']","['', 'important highly predictive features. this makes an isbn parser a particularly good baseline when considering a new treebank or language, be - ara bas', 'cat chi cze eng gre hun ita cause it does not require much effort in feature engineering.', 'as was demonstrated in  #TAUTHOR_TAG, even a minimal set of local explicit features achieves results which are non - significantly different from a carefully chosen set of explicit features', ', given the language independent definition of locality described in section 2. it is also important to note that the model is quite efficient. figure 1 shows the tradeoff between accuracy and parsing time as the width of the', 'search beam is varied, on the development set. this curve plots the average labeled attachment score over basque, chinese, english, and turkish as a function of parsing time per token. 4 accuracy of only 1 %', 'below the maximum can be achieved with average processing time of 17 ms', 'per token, or 60 tokens per second. 5 we also refer the reader to  #TAUTHOR_TAG for more detailed analysis of the isbn dependency', 'parser results, where, among other things, it was shown that the isbn model is especially accurate at modeling long dependencies']",7
"['in an unsupervised way  #TAUTHOR_TAG 8 ], which achieves very promising']","['in an unsupervised way  #TAUTHOR_TAG 8 ], which achieves very promising performance.', 'the first']","['the inherent word - level knowledge in an unsupervised way  #TAUTHOR_TAG 8 ], which achieves very promising performance.', 'the first line of']","['', 'recently, the other line has studied to pre - train a language model over large corpus to learn the inherent word - level knowledge in an unsupervised way  #TAUTHOR_TAG 8 ], which achieves very promising performance.', 'the first line of work is usually carefully designed for the target task, which is not widely applicable.', 'the second line can only learn the co - occurrence of words or entities in the context, while it may not be that robust for some complex scenarios such as reasoning task.', 'for example, to answer the question "" was the light bulb still hot? "" when the document is given as "" i went into my bedroom and flipped the light switch.', 'oh, i see that the ceiling lamp is']",0
['as bert and gpt  #TAUTHOR_TAG 8 ]'],['as bert and gpt  #TAUTHOR_TAG 8 ]'],['as bert and gpt  #TAUTHOR_TAG 8 ] is also used as'],"['', 'these methods relied on task - specific model structures which are difficult to adapt to other tasks.', 'pre - trained language model such as bert and gpt  #TAUTHOR_TAG 8 ] is also used as a kind of commonsense knowledge source.', 'however, the lm method mainly captures the co - occurrence of words and phrases and cannot address some more complex problems which may require the reasoning ability.', 'unlike previous work, we incorporate external knowledge by jointly training mrc model with two auxiliary tasks which are relevant to commonsense knowledge.', 'the model can learn to fill in the knowledge gap without changing the original model structure']",0
['bert  #TAUTHOR_TAG as the pretrained encoder'],['bert  #TAUTHOR_TAG as the pretrained encoder'],"['', 'here we utilize bert  #TAUTHOR_TAG as the pretrained encoder']","['', 'the overall framework is shown in figure 1.', 'the pre - trained lm encoder acts as the foundation of the model, which is used to capture the relationship between question, document and answer options.', 'here we utilize bert  #TAUTHOR_TAG as the pretrained encoder for its superior performance in a range of natural language understanding tasks.', '']",5
"['1 is the relation - existence task.', 'following  #TAUTHOR_TAG, we first convert the concept to a set of bpe tokens tokens a and tokens b, with beginning index i and j in the input sequence respectively.', 'the probability of whether there is a']","['1 is the relation - existence task.', 'following  #TAUTHOR_TAG, we first convert the concept to a set of bpe tokens tokens a and tokens b, with beginning index i and j in the input sequence respectively.', 'the probability of whether there is a']","['1 is the relation - existence task.', 'following  #TAUTHOR_TAG, we first convert the concept to a set of bpe tokens tokens a and tokens b, with beginning index i and j in the input sequence respectively.', 'the probability of whether there is a relation in each pair ( tokens a,', 'where w 1 ∈ r h ×h is a trainable matrix.', 'we define the pair ( tokens a, tokens b ) that has relation in conceptnet as a positive example and others as negative examples.', 'we down - sample the negative examples and keep ratio']","['1 is the relation - existence task.', 'following  #TAUTHOR_TAG, we first convert the concept to a set of bpe tokens tokens a and tokens b, with beginning index i and j in the input sequence respectively.', 'the probability of whether there is a relation in each pair ( tokens a,', 'where w 1 ∈ r h ×h is a trainable matrix.', 'we define the pair ( tokens a, tokens b ) that has relation in conceptnet as a positive example and others as negative examples.', 'we down - sample the negative examples and keep ratio of positive vs negative is 1 : γ.', 'we define the relation - existence loss as the average binary cross - entropy ( bce ) loss :', 'where | a |, | b | are the number of sampled concepts in sentence a and sentence b respectively, y re is the label of whether there is a relation between concepts.', 'task 2 is the relation - type task.', 'we predict the relation type between tokens a and tokens b. the relation - type probabilities are computed as :', 'where w 2 ∈ r h ×2h and w 3 ∈ r r×h are new trainable matrices, r is the number of selected relation types 2.', 'the relation - type loss is computed as :', 'we define s i j as the label whether there is a relation from sentence a to sentence b. k is the index of ground - truth relation in conceptnet, | s | is the number of relations among the tokens in two sentences.', 'as the three tasks share the same bert architecture with only different linear head layers, we propose to train them together.', 'the joint objective function is formulated as follows :', 'where λ 1 and λ 2 are two hyper - parameters that control the weight of the tasks, n is number of options']",5
"['bert ( base )  #TAUTHOR_TAG as pre - trained language model.', 'we']","['use the uncased bert ( base )  #TAUTHOR_TAG as pre - trained language model.', 'we']","['use the uncased bert ( base )  #TAUTHOR_TAG as pre - trained language model.', 'we set the batch size']","['use the uncased bert ( base )  #TAUTHOR_TAG as pre - trained language model.', 'we set the batch size to 24, learning rate to 2e - 5.', 'the maximum sequence length is 384 for semeval - 2018 task 11 and 512 for story cloze test.', 'we fine - tune for 3 epochs on each dataset.', 'the taskspecific hyper - parameters λ 1 and λ 2 are set to 0. 5 and the ratio γ is set to 4. 0']",5
['models  #TAUTHOR_TAG 8 ]'],['models  #TAUTHOR_TAG 8 ]'],"['decoder models  #TAUTHOR_TAG 8 ].', 'several methods have been proposed to']",[' #TAUTHOR_TAG'],0
"['28 ]. specaugment masks', 'blocks of frequency channels and blocks of time steps  #TAUTHOR_TAG and also warps the spectrogram along the time axis to perform data augmentation.', 'it is closely related to']","['28 ]. specaugment masks', 'blocks of frequency channels and blocks of time steps  #TAUTHOR_TAG and also warps the spectrogram along the time axis to perform data augmentation.', 'it is closely related to [ 29 ]']","['parameters to improve generalization [ 27 ]. this form of noise can be interpreted as a simplified form of bayesian inference', 'that optimizes a minimum description length loss [ 28 ]. specaugment masks', 'blocks of frequency channels and blocks of time steps  #TAUTHOR_TAG and also warps the spectrogram along the time axis to perform data augmentation.', 'it is closely related to [ 29 ]']","['dropout, randomly deactivates connections between neurons by temporarily zeroing out weights [ 15 ]. zoneout, which is also inspired by dropout and was especially developed for recurrent models [ 16 ], stochastic', '##ally forces some hidden units to maintain their previous values. in lstms, the method is applied on the cell state or on the recurrent feedback of the output. label smoothing interpolates the', 'hard label targets with a uniform distribution over targets, and improves generalization in many classification tasks [ 17 ]. batch normalization ( bn ) accelerate', ""##s training by standardizing the distribution of each layer's input [ 18 ]. in order to reduce the normalization mismatch between training and testing, we modify the original approach by freezing the batch normalization layers"", 'in the middle of the training when the magnitude of parameter updates is small. after freezing, the running statistics are not updated,', 'batch statistics are ignored, and bn layers approximately operate as global normalization. scheduled sampling stochastically uses the token produced by a sequence', 'model instead of the true previous token during training to mitigate the effects of exposure bias [ 19 ]. residual networks address the problem of vanishing and explo', '##ding gradients by including skip connections [ 20 ] in the model that force the neural network to learn a residual mapping function using a stack of layers. optimization of this', 'residual mapping is easier, allowing the use of much deeper structures. curriculum learning simplifies deep neural network training by presenting', 'training examples in a meaningful order, usually by increasing order of difficulty [ 21 ]. in seq2seq models, the input acoustic', 'sequences are frequently sorted in order of in - creasing length [ 22 ]. speed and tempo perturbation changes the rate', 'of speech, typically by ±10 %, with or without altering the pitch and timbre of the speech signal [ 23, 24 ]. the goal of these methods is to increase the amount', 'of training data for the model. sequence noise injection adds structured sequence level noise generated from speech utterances to training examples to improve', 'the generalization of seq2seq models [ 25 ]. as previously shown, input noise during neural network training encourages convergence to a local optimum with lower curvature,', 'which indicates better generalization [ 26 ]. weight noise adds noise directly to the network parameters to improve generalization [ 27 ]. this form of noise can be interpreted as a simplified form of bayesian inference', 'that optimizes a minimum description length loss [ 28 ]. specaugment masks', 'blocks of frequency channels and blocks of time steps  #TAUTHOR_TAG and also warps the spectrogram along the time axis to perform data augmentation.', 'it is closely related to [ 29 ]']",0
['uses the sm policy  #TAUTHOR_TAG with p = 0'],"['uses the sm policy  #TAUTHOR_TAG with p = 0. 3 and no time warping.', 'the encoder network comprises']",['uses the sm policy  #TAUTHOR_TAG with p = 0'],"['study focuses on switchboard - 300, a standard 300 - hour english conversational speech recognition task.', 'our acoustic and text data preparation follows the kaldi [ 30 ] s5c recipe.', 'our attention based seq2seq model is similar to [ 31, 32 ] and follows the structure of [ 33 ].', 'we extract 80 - dimensional log - mel filterbank features over 25ms frames every 10ms from the input speech signal.', 'the input audio is speed and / or tempo perturbed with 5 / 6 probability.', 'following [ 25 ], sequence noise mixed from up to 4 utterances is injected with 40 % probability and 0. 3 weight.', 'the filterbank output is mean - and - variance normalized at the speaker level, and first ( ∆ ) and second ( ∆∆ ) derivatives are also calculated.', 'the final features presented to the network are also processed through a specaugment block that uses the sm policy  #TAUTHOR_TAG with p = 0. 3 and no time warping.', 'the encoder network comprises 8 bidirectional lstm layers with 1536 nodes per direction per layer [ 34, 35 ].', '']",5
"['300 results in  #TAUTHOR_TAG 8, 52, 53 ] and the switch']","['results in  #TAUTHOR_TAG 8, 52, 53 ] and the switchboard - 2000 results in [ 51, 52, 54, 55, 56, 57 ].', 'our 300hour model not only']","['in  #TAUTHOR_TAG 8, 52, 53 ] and the switchboard - 2000 results in [ 51, 52, 54, 55, 56, 57 ].', 'our 300hour model not only']","['comparison with results in the literature we refer to the switchboard - 300 results in  #TAUTHOR_TAG 8, 52, 53 ] and the switchboard - 2000 results in [ 51, 52, 54, 55, 56, 57 ].', 'our 300hour model not only outperforms the previous best attention based encoder - decoder model  #TAUTHOR_TAG by a large margin, it also surpasses the best hybrid systems with multiple lms [ 8 ].', 'our result on switchboard - 2000 is also better than any single system results reported to date, and reaches the performance of the best system combinations']",5
"['the best published attention based seq2seq model  #TAUTHOR_TAG, with']","['the best published attention based seq2seq model  #TAUTHOR_TAG, with']","['the best published attention based seq2seq model  #TAUTHOR_TAG, with']","['', 'as table 4 shows, although our smallest attention based model achieves reasonable results on this task, a significant loss is indeed observed with decreasing model size, especially on call - home.', 'nevertheless, an external language model reduces the performance gap.', 'a small, 57m - parameter model together with a similar size language model is only 5 % relative worse than our largest model.', 'we note that this model already outperforms the best published attention based seq2seq model  #TAUTHOR_TAG, with']",4
"['300 results in  #TAUTHOR_TAG 8, 52, 53 ] and the switch']","['results in  #TAUTHOR_TAG 8, 52, 53 ] and the switchboard - 2000 results in [ 51, 52, 54, 55, 56, 57 ].', 'our 300hour model not only']","['in  #TAUTHOR_TAG 8, 52, 53 ] and the switchboard - 2000 results in [ 51, 52, 54, 55, 56, 57 ].', 'our 300hour model not only']","['comparison with results in the literature we refer to the switchboard - 300 results in  #TAUTHOR_TAG 8, 52, 53 ] and the switchboard - 2000 results in [ 51, 52, 54, 55, 56, 57 ].', 'our 300hour model not only outperforms the previous best attention based encoder - decoder model  #TAUTHOR_TAG by a large margin, it also surpasses the best hybrid systems with multiple lms [ 8 ].', 'our result on switchboard - 2000 is also better than any single system results reported to date, and reaches the performance of the best system combinations']",4
"['errors.', 'following  #TAUTHOR_TAG, we have assigned 9 and 999 ( 9 and 999 times more important ) penalties to']","['errors.', 'following  #TAUTHOR_TAG, we have assigned 9 and 999 ( 9 and 999 times more important ) penalties to']","['errors.', 'following  #TAUTHOR_TAG, we have assigned 9 and 999 ( 9 and 999 times more important ) penalties to the missclassification of legitimate', 'messages as uce. this means that every instance of a legitimate message has been replaced by', '9 and 999 instances of the same message respectively for nb, c4. 5', 'and part. however, for knn the data have been downsampled']","['', 'a popular technique for doing this is resampling the training collection by multiplying the number of instances of the preferred class by the cost ratio. also, the unpreferred class can be downsampled by eliminating some instances. the software package', 'we use for our experiments applies both methods depending', 'on the algorithm tested. we have tested four learning algorithms : naive bayes ( nb ), c4. 5, part and k - nearest neighbor ( knn ), all', 'implemented in the weka package  #AUTHOR_TAG. the version of weka used in this work is we', '##ka 3. 0. 1. the algorithms used can be biased to prefer the mistake of classify a uce message', 'as not uce to the opposite, assigning a penalty to the second kind of errors.', 'following  #TAUTHOR_TAG, we have assigned 9 and 999 ( 9 and 999 times more important ) penalties to the missclassification of legitimate', 'messages as uce. this means that every instance of a legitimate message has been replaced by', '9 and 999 instances of the same message respectively for nb, c4. 5', 'and part. however, for knn the data have been downsampled']",5
"[', respectively  #TAUTHOR_TAG.', 'in table 1, no costs were used.', 'tables 2 and 3 show the results of our experiments for cost ratios']","['effectiveness letting legitimate messages pass the filter, respectively  #TAUTHOR_TAG.', 'in table 1, no costs were used.', 'tables 2 and 3 show the results of our experiments for cost ratios']","['effectiveness letting legitimate messages pass the filter, respectively  #TAUTHOR_TAG.', 'in table 1, no costs were used.', 'tables 2 and 3 show the results of our experiments for cost ratios']","['experiments results are summarized in the table 1, 2 and 3.', 'the learning algorithms naive bayes ( nb ), 5 - nearest neighbor ( 5nn ), c4. 5 and part were tested on words ( - w ), heuristic features ( - h ), and both ( - wh ).', 'the knn algorithm was tested with values of k equal to 1, 2, 5 and 8, being 5 the optimal number of neighbors.', 'we present the weighted accuracy ( wacc ), and also the recall ( rec ) and precision ( pre ) for the class uce.', 'weighted accuracy is a measure that weights higher the hits and misses for the preferred class.', 'recall and precision for the uce class show how effective the filter is blocking uce, and what is its effectiveness letting legitimate messages pass the filter, respectively  #TAUTHOR_TAG.', 'in table 1, no costs were used.', 'tables 2 and 3 show the results of our experiments for cost ratios of 9 and 999.', 'for these last cases, there were not enough training instances for the knn algorithm to perform classification, due to the downsampling method applied by weka']",0
"[').', 'in  #TAUTHOR_TAG, the method followed is the variation of the probability threshold, which leads to a high']","['to the sampling method, which only slightly affects to the estimation of probabilities ( done by approximation to a normal distribution ).', 'in  #TAUTHOR_TAG, the method followed is the variation of the probability threshold, which leads to a high']","['to the sampling method, which only slightly affects to the estimation of probabilities ( done by approximation to a normal distribution ).', 'in  #TAUTHOR_TAG, the method followed is the variation of the probability threshold, which leads to a high variation of results.', 'in future experiments, we plan to apply the uniform method metacost  #AUTHOR_TAG to the algorithms tested in this work,']","['', 'naive bayes has not shown high variability with respect to costs.', 'this is probably due to the sampling method, which only slightly affects to the estimation of probabilities ( done by approximation to a normal distribution ).', 'in  #TAUTHOR_TAG, the method followed is the variation of the probability threshold, which leads to a high variation of results.', 'in future experiments, we plan to apply the uniform method metacost  #AUTHOR_TAG to the algorithms tested in this work, for getting more comparable results.', 'with respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words.', 'the improvement shown in our experiments is modest, due to the heuristics used.', 'we are not able to add other heuristics in this case because the spambase collection comes in a preprocessed fashion.', 'for future experiments, we will use the collection from  #TAUTHOR_TAG, which is in raw form.', 'this fact will enable us to search for more powerful heuristics.', '']",0
"[').', 'in  #TAUTHOR_TAG, the method followed is the variation of the probability threshold, which leads to a high']","['to the sampling method, which only slightly affects to the estimation of probabilities ( done by approximation to a normal distribution ).', 'in  #TAUTHOR_TAG, the method followed is the variation of the probability threshold, which leads to a high']","['to the sampling method, which only slightly affects to the estimation of probabilities ( done by approximation to a normal distribution ).', 'in  #TAUTHOR_TAG, the method followed is the variation of the probability threshold, which leads to a high variation of results.', 'in future experiments, we plan to apply the uniform method metacost  #AUTHOR_TAG to the algorithms tested in this work,']","['', 'naive bayes has not shown high variability with respect to costs.', 'this is probably due to the sampling method, which only slightly affects to the estimation of probabilities ( done by approximation to a normal distribution ).', 'in  #TAUTHOR_TAG, the method followed is the variation of the probability threshold, which leads to a high variation of results.', 'in future experiments, we plan to apply the uniform method metacost  #AUTHOR_TAG to the algorithms tested in this work, for getting more comparable results.', 'with respect to the use of heuristics, we can see that this information alone is not competitive, but it can improve classification based on words.', 'the improvement shown in our experiments is modest, due to the heuristics used.', 'we are not able to add other heuristics in this case because the spambase collection comes in a preprocessed fashion.', 'for future experiments, we will use the collection from  #TAUTHOR_TAG, which is in raw form.', 'this fact will enable us to search for more powerful heuristics.', '']",2
['keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],"['', 'leverages word embeddings pre - trained by word2vec ( mikolov et al. 2013 ) ; the multi - filter convolutional', 'layer consists of multiple convolutional filters ( kim 2014 ) ; the residual convolutional layer contains multiple residual blocks ( he et al. 2016 ) ; the attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ; the output layer utilizes the sigmoid function to predict the probability of each icd code. our main contribution is that we proposed a novel cnn architecture that combines the', 'multi - filter cnn', '( kim 2014 ) and residual cnn', '']",0
['keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],"['', 'leverages word embeddings pre - trained by word2vec ( mikolov et al. 2013 ) ; the multi - filter convolutional', 'layer consists of multiple convolutional filters ( kim 2014 ) ; the residual convolutional layer contains multiple residual blocks ( he et al. 2016 ) ; the attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ; the output layer utilizes the sigmoid function to predict the probability of each icd code. our main contribution is that we proposed a novel cnn architecture that combines the', 'multi - filter cnn', '( kim 2014 ) and residual cnn', '']",0
['keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],"['', 'leverages word embeddings pre - trained by word2vec ( mikolov et al. 2013 ) ; the multi - filter convolutional', 'layer consists of multiple convolutional filters ( kim 2014 ) ; the residual convolutional layer contains multiple residual blocks ( he et al. 2016 ) ; the attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ; the output layer utilizes the sigmoid function to predict the probability of each icd code. our main contribution is that we proposed a novel cnn architecture that combines the', 'multi - filter cnn', '( kim 2014 ) and residual cnn', '']",0
"['iii datasets.', ' #TAUTHOR_TAG incorporated the convolutional neural']","['iii datasets.', ' #TAUTHOR_TAG incorporated the convolutional neural']","['iii datasets.', ' #TAUTHOR_TAG incorporated the convolutional neural network ( cnn ) with per - label attention mechanism.', 'their model achieved']","['', ' #AUTHOR_TAG evaluated coverage - based feature selection methods and random forests on seven medical specialties for icd9 code prediction and two for icd10, incorporating structured and unstructured text.', 'with the development of deep learning, researchers also explored neural networks for this task.', ' #AUTHOR_TAG utilized the long short - term memory ( lstm ) and attention mechanism for automated icd coding from diagnosis descriptions.', ' #AUTHOR_TAG also adopted the lstm but they introduced the tree structure and adversarial learning to utilize code descriptions.', ' #AUTHOR_TAG exploited condensed memory neural networks and evaluated it on the free - text medical notes of the mimic - iii dataset.', ' #AUTHOR_TAG proposed a hierarchical gated recurrent unit ( gru ) network, which encodes sentences and documents with two stacked layers, to assign multiple icd codes to discharge summaries of the mimic ii and iii datasets.', ' #TAUTHOR_TAG incorporated the convolutional neural network ( cnn ) with per - label attention mechanism.', 'their model achieved the state - of - the - art performance among the work using only unstructured text of the mimic dataset.', ' #AUTHOR_TAG built a hybrid system that includes the cnn, lstm and decision tree to predict icd codes from unstructured, semi - structured and structured tabular data.', 'in addition,  #AUTHOR_TAG utilized lstms to predict diagnostic codes from time series of clinical measurements, while our work focuses on text']",0
"['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size d e is 100, the out - channel size d f of a filter in']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size d e is 100, the out - channel size d f of a filter in the multi - filter convolutional layer is 100, the learning rate is 0. 0001, the batch size is 16 and the dropout rate is 0. 2.', '']",0
"['( caml ) was proposed by  #TAUTHOR_TAG.', 'it has achieved the']","['( caml ) was proposed by  #TAUTHOR_TAG.', 'it has achieved the state - of - theart results on the mimic - iii and mimic - ii datasets among the models using unstructured text.', 'it consists of one convolutional']","['multi - label classification ( caml ) was proposed by  #TAUTHOR_TAG.', 'it has achieved the state - of - theart results on the mimic - iii and mimic - ii datasets among the models using unstructured text.', 'it consists of one convolutional layer and one attention layer']","['##l & dr - caml the convolutional attention network for multi - label classification ( caml ) was proposed by  #TAUTHOR_TAG.', 'it has achieved the state - of - theart results on the mimic - iii and mimic - ii datasets among the models using unstructured text.', 'it consists of one convolutional layer and one attention layer to generate label - aware features for multi - label classification ( mccallum 1999 ).', 'the description regularized caml ( dr - caml ) is an extension of caml and incorporates the text description of each code to regularize the model']",0
['keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],"['', 'leverages word embeddings pre - trained by word2vec ( mikolov et al. 2013 ) ; the multi - filter convolutional', 'layer consists of multiple convolutional filters ( kim 2014 ) ; the residual convolutional layer contains multiple residual blocks ( he et al. 2016 ) ; the attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ; the output layer utilizes the sigmoid function to predict the probability of each icd code. our main contribution is that we proposed a novel cnn architecture that combines the', 'multi - filter cnn', '( kim 2014 ) and residual cnn', '']",1
['keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],['attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ;'],"['', 'leverages word embeddings pre - trained by word2vec ( mikolov et al. 2013 ) ; the multi - filter convolutional', 'layer consists of multiple convolutional filters ( kim 2014 ) ; the residual convolutional layer contains multiple residual blocks ( he et al. 2016 ) ; the attention layer keeps the interpretability for the model following  #TAUTHOR_TAG ; the output layer utilizes the sigmoid function to predict the probability of each icd code. our main contribution is that we proposed a novel cnn architecture that combines the', 'multi - filter cnn', '( kim 2014 ) and residual cnn', '']",3
"[' #TAUTHOR_TAG, we employed the perlabel attention mechanism']","[' #TAUTHOR_TAG, we employed the perlabel attention mechanism']","[' #TAUTHOR_TAG, we employed the perlabel attention mechanism']","[' #TAUTHOR_TAG, we employed the perlabel attention mechanism to make each icd code attend to different parts of the document representation h. the attention layer is formalized as :', 'where u ∈ r ( m×d p ) ×l represents the parameter matrix of the attention layer, a ∈ r n×l represents the attention weights for each pair of an icd code and a word, v ∈ r l× ( m×d p ) represents the output of the attention layer.', 'here l denotes the number of icd codes']",5
"['##um 1999 ;  #TAUTHOR_TAG.', 'the training objective is to minimize the binary cross entropy loss between the prediction']","['( mccallum 1999 ;  #TAUTHOR_TAG.', 'the training objective is to minimize the binary cross entropy loss between the predictiony and']","['##um 1999 ;  #TAUTHOR_TAG.', 'the training objective is to minimize the binary cross entropy loss between the predictiony and the target y :', 'where w denotes the input word sequence and θ denotes all the parameters.', 'we utilized the back - propagation algorithm and adam optimize']","['', 'for training, we treated the icd coding task as a multi - label classification problem following previous work ( mccallum 1999 ;  #TAUTHOR_TAG.', 'the training objective is to minimize the binary cross entropy loss between the predictiony and the target y :', 'where w denotes the input word sequence and θ denotes all the parameters.', 'we utilized the back - propagation algorithm and adam optimizer ( kingma and ba 2014 ) to train our model']",5
"['third version of medical information mart for intensive care ( mimic - iii ) ( johnson et al. 2016 ) as the first dataset to evaluate our models.', 'following  #TAUTHOR_TAG,']","['third version of medical information mart for intensive care ( mimic - iii ) ( johnson et al. 2016 ) as the first dataset to evaluate our models.', 'following  #TAUTHOR_TAG,']","['mimic - iii in this paper, we employed the third version of medical information mart for intensive care ( mimic - iii ) ( johnson et al. 2016 ) as the first dataset to evaluate our models.', 'following  #TAUTHOR_TAG, we used']","['mimic - iii in this paper, we employed the third version of medical information mart for intensive care ( mimic - iii ) ( johnson et al. 2016 ) as the first dataset to evaluate our models.', 'following  #TAUTHOR_TAG, we used discharge summaries, split them by patient ids, and conducted experiments using the full codes as well as the top - 50 most frequent codes.', 'finally, the mimic - iii dataset using 8, 921 icd - 9 codes consists of 47, 719, 1, 631 and 3, 372 discharge summaries for training, development and testing respectively.', '']",5
"['al. 2013 ;  #TAUTHOR_TAG ; baumel et al. 2018 ).', 'follow their experimental setting, there are 20,']","['al. 2013 ;  #TAUTHOR_TAG ; baumel et al. 2018 ).', 'follow their experimental setting, there are']","['##e et al. 2013 ;  #TAUTHOR_TAG ; baumel et al. 2018 ).', 'follow their experimental setting, there are 20,']","['the mimic - iii dataset, we also leveraged the mimic - ii dataset to compare our models with the ones in previous work ( perotte et al. 2013 ;  #TAUTHOR_TAG ; baumel et al. 2018 ).', 'follow their experimental setting, there are 20, 533 and 2, 282 clinical notes for training and testing, and 5, 031 unique icd - 9 codes in the dataset.', 'preprocessing following previous work  #TAUTHOR_TAG, the text was tokenized, and each token were transformed into its lowercase.', 'the tokens that contain no alphabetic characters were removed such as numbers and punctuations.', 'the maximum length of a token sequence is 2, 500 and the one that exceeds this length will be truncated.', '']",5
"['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size d e is 100, the out - channel size d f of a filter in']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size d e is 100, the out - channel size d f of a filter in the multi - filter convolutional layer is 100, the learning rate is 0. 0001, the batch size is 16 and the dropout rate is 0. 2.', '']",5
"['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size d e is 100, the out - channel size d f of a filter in']","['our model has a number of hyper - parameters, it is infeasible to search optimal values for all hyper - parameters.', 'therefore, some hyper - parameter values were chosen empirically or following prior work  #TAUTHOR_TAG.', 'the word embedding size d e is 100, the out - channel size d f of a filter in the multi - filter convolutional layer is 100, the learning rate is 0. 0001, the batch size is 16 and the dropout rate is 0. 2.', '']",5
"['used the optimal hyper - parameter setting reported in their paper  #TAUTHOR_TAG.', '']","['used the optimal hyper - parameter setting reported in their paper  #TAUTHOR_TAG.', '']","[', we used the optimal hyper - parameter setting reported in their paper  #TAUTHOR_TAG.', '']","['', 'our experimental settings are as follows.', 'for caml, we used the optimal hyper - parameter setting reported in their paper  #TAUTHOR_TAG.', 'for mul - tirescnn, we used six filters and 1 residual block, which obtained the best result in our hyper - parameter tuning experiments.', 'the batch size, learning rate and dropout rate are identical in every experiment.', 'we used the training set and development set of mimic - iii ( full codes ) as experimental data.', 'the experiments were conducted on nvidia tesla p40 gpus.', 'training will terminate if the performance on the development set does not increase for 10 times.', 'as shown in table 6, the parameter of multirescnn is approximately 1. 9 times as many as that of caml.', 'the training time of multirescnn is about 2. 3 times more than that of caml.', '']",5
"['of infix removal is sometimes impossible  #TAUTHOR_TAG.', 'informal / irregular forms']","['of infix removal is sometimes impossible  #TAUTHOR_TAG.', 'informal / irregular forms']","['of infix removal is sometimes impossible  #TAUTHOR_TAG.', 'informal / irregular forms']","['', 'conventional rule - based stemmers tailor the linguistic knowledge of experts.', 'on the other hand, statistical stemmers provide languageindependent approaches which generally group related words based on various string - similarity measures.', 'such approaches often involve n - grams ; equivalence classes can be formed from words that share the same properties : word - initial letter n - grams, common n - grams throughout the word, or by refining these classes with clustering techniques.', 'this kind of statistical stemming has been shown to be effective for many languages, including english, turkish, and malay.', 'for example, bhat introduced a method for kannada where the similarity of two words is determined by three distance measures based on prefix and suffix matching and the first mismatch point in the words [ 2 ].', 'defining precise rules for morphologically complex texts, especially for the purpose of infix removal is sometimes impossible  #TAUTHOR_TAG.', 'informal / irregular forms usually do not obey the conventional rules in the languages.', ""for instance,'khunh'( home ) is a frequent form for'khanh'in persian conversations or'goood'and'good'are used interchangeably in english tweets."", 'in this paper, we propose a statistical technique for finding inflectional and derivation formations of words.', 'to this end, we introduce an unsupervised method to cluster all morphological variants of a word.', 'the proposed algorithm learns linguistic patterns to match a word and its morphological variants based on a given large collection of documents, which is readily available on the web.', 'a linguistic pattern captures a transformation rule between a word and its morphological variant.', 'the extracted rules indicate which letters in which positions of a word should be modified.', 'affix characters, positions of the characters, operations on the characters based on the minimum edit distance ( med ) algorithm ( i. e., insertion or deletion ) [ 10 ], and part - of - speech ( pos ) tag of the input word are the attributes of a rule.', 'our algorithm assigns a score to each rule, indicating its confidence.', 'the higher the frequency of a rule in the input collection, the higher the confidence value of that rule.', 'finally, a small subset of the']",1
"['shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', '']","['shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', '']","['to compensate the shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', 'we used the following probabilistic framework']","['', 'we use dictionary - based cross - lingual information retrieval ( clir ) to this end.', 'in highly inflected languages, bilingual dictionaries contain only original forms of the words.', 'therefore, in dictionary - based clir, retrieval systems are obliged either to stem documents and queries, or to leave them intact [ 8, 4, 12 ], or expand the query with inflections.', 'we opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', 'we used the following probabilistic framework to this end  #TAUTHOR_TAG :', 'where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i.', 'c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold.', 'then, we compute the translation probability of c i, j or c i, j for the given q i.', 'to avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary ( c i, j and c i, j ) or a pair of a candidate from the dictionary and an inflection from the collection ( c i, j and c i, j )  #TAUTHOR_TAG.', 'our goal is to findc i using the proposed ss4mct ( i. e. set of top - ranked c i, j according to p t ( c i, j | c i, j ) ) and then evaluate its impact on the performance of the clir task.', 'figure 1 shows the whole process of extracting rules ( off - line part ) and the evaluation framework ( on - line part )']",5
"['shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', '']","['shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', '']","['to compensate the shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', 'we used the following probabilistic framework']","['', 'we use dictionary - based cross - lingual information retrieval ( clir ) to this end.', 'in highly inflected languages, bilingual dictionaries contain only original forms of the words.', 'therefore, in dictionary - based clir, retrieval systems are obliged either to stem documents and queries, or to leave them intact [ 8, 4, 12 ], or expand the query with inflections.', 'we opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', 'we used the following probabilistic framework to this end  #TAUTHOR_TAG :', 'where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i.', 'c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold.', 'then, we compute the translation probability of c i, j or c i, j for the given q i.', 'to avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary ( c i, j and c i, j ) or a pair of a candidate from the dictionary and an inflection from the collection ( c i, j and c i, j )  #TAUTHOR_TAG.', 'our goal is to findc i using the proposed ss4mct ( i. e. set of top - ranked c i, j according to p t ( c i, j | c i, j ) ) and then evaluate its impact on the performance of the clir task.', 'figure 1 shows the whole process of extracting rules ( off - line part ) and the evaluation framework ( on - line part )']",5
"['shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', '']","['shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', '']","['to compensate the shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', 'we used the following probabilistic framework']","['', 'we use dictionary - based cross - lingual information retrieval ( clir ) to this end.', 'in highly inflected languages, bilingual dictionaries contain only original forms of the words.', 'therefore, in dictionary - based clir, retrieval systems are obliged either to stem documents and queries, or to leave them intact [ 8, 4, 12 ], or expand the query with inflections.', 'we opted the query expansion approach which is a widely used approach to compensate the shortage of inflections [ 9, 3,  #TAUTHOR_TAG.', 'we used the following probabilistic framework to this end  #TAUTHOR_TAG :', 'where q i is a query term and c i is the set of translation candidates provided in a bilingual dictionary for q i.', 'c i is the set of the most probable inflections of the words appeared in c i selected by a tuned threshold.', 'then, we compute the translation probability of c i, j or c i, j for the given q i.', 'to avoid adding noisy terms, we only compute the joint probabilities between either a pair of translation candidates from the dictionary ( c i, j and c i, j ) or a pair of a candidate from the dictionary and an inflection from the collection ( c i, j and c i, j )  #TAUTHOR_TAG.', 'our goal is to findc i using the proposed ss4mct ( i. e. set of top - ranked c i, j according to p t ( c i, j | c i, j ) ) and then evaluate its impact on the performance of the clir task.', 'figure 1 shows the whole process of extracting rules ( off - line part ) and the evaluation framework ( on - line part )']",5
"['[ 4,  #TAUTHOR_TAG.', 'we used']","['[ 4,  #TAUTHOR_TAG.', 'we used step1 [ 13 ] in our stemming process in persian.', 'we']","['[ 4,  #TAUTHOR_TAG.', 'we used']","['statistics of the collection used for both rule extraction and evaluation is provided in table 2.', 'we employed the statistical language modeling framework with kullback - leibler similarity measure of lemur toolkit for our retrieval task.', 'dirichlet prior is selected as our document smoothing strategy.', 'top 30 documents are used for the mixture pseudo - relevance feedback algorithm.', 'queries are expanded by the top 50 terms generated by the feedback model [ 14, 6 ].', 'we removed persian stop words from the queries and documents [ 4,  #TAUTHOR_TAG.', 'we used step1 [ 13 ] in our stemming process in persian.', 'we also stem the source english queries in all experiments with the porter stemmer.', 'we use google englishpersian dictionary 1 as the translation resource.', 'dadashkarimi et al., demonstrated that google has better coverage compared to other english - persian dictionaries  #TAUTHOR_TAG.', 'we have exploited 40 persian pos tags in our experiments.', '2 the retrieval results are mainly evaluated by mean average precision ( map ) over top 1000 retrieved documents.', 'significance tests are computed using two - tailed paired t - test with 95 % confidence.', 'precision at top 5 documents ( p @ 5 ) and top 10 documents ( p @ 10 ) are also reported']",5
"['bi - gram coherence translation method ( bictm ), introduced in  #TAUTHOR_TAG ( assume']","['bi - gram coherence translation method ( bictm ), introduced in  #TAUTHOR_TAG ( assume | c i | = 0 ),']","['the bi - gram coherence translation method ( bictm ), introduced in  #TAUTHOR_TAG ( assume | c i | = 0 ),']",[' #TAUTHOR_TAG'],5
"['[ 4,  #TAUTHOR_TAG.', 'we used']","['[ 4,  #TAUTHOR_TAG.', 'we used step1 [ 13 ] in our stemming process in persian.', 'we']","['[ 4,  #TAUTHOR_TAG.', 'we used']","['statistics of the collection used for both rule extraction and evaluation is provided in table 2.', 'we employed the statistical language modeling framework with kullback - leibler similarity measure of lemur toolkit for our retrieval task.', 'dirichlet prior is selected as our document smoothing strategy.', 'top 30 documents are used for the mixture pseudo - relevance feedback algorithm.', 'queries are expanded by the top 50 terms generated by the feedback model [ 14, 6 ].', 'we removed persian stop words from the queries and documents [ 4,  #TAUTHOR_TAG.', 'we used step1 [ 13 ] in our stemming process in persian.', 'we also stem the source english queries in all experiments with the porter stemmer.', 'we use google englishpersian dictionary 1 as the translation resource.', 'dadashkarimi et al., demonstrated that google has better coverage compared to other english - persian dictionaries  #TAUTHOR_TAG.', 'we have exploited 40 persian pos tags in our experiments.', '2 the retrieval results are mainly evaluated by mean average precision ( map ) over top 1000 retrieved documents.', 'significance tests are computed using two - tailed paired t - test with 95 % confidence.', 'precision at top 5 documents ( p @ 5 ) and top 10 documents ( p @ 10 ) are also reported']",0
"['),  #TAUTHOR_TAG have shown that a']","['chinese treebank ( ctb ),  #TAUTHOR_TAG have shown that a']","['),  #TAUTHOR_TAG have shown that']","['', 'for constituent - based parsing using the chinese treebank ( ctb ),  #TAUTHOR_TAG have shown that a shift - reduce parser can give competitive accuracy scores together with high speeds, by using an svm to make a single decision at each point in the parsing process.', ""in this paper we describe a global discriminative model for chinese shift - reduce parsing, and compare it with wang et al.'s approach."", 'we apply the same shift - reduce procedure as  #TAUTHOR_TAG, but instead of using a local classifier for each transition - based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses.', 'we apply beam search to decoding instead of greedy search.', 'the parser still operates in linear time, but the use of beam - search allows the correction of local decision errors by global comparison.', ""using ctb2, our model achieved parseval f - scores comparable to wang et al.'s approach."", 'we also present accuracy scores for the much larger ctb5, using both a constituent - based and dependency - based evaluation.', 'the scores for the dependency - based evaluation were higher than the state - of - the - art dependency parsers for the ctb5 data']",0
"['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['', "" #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. however, there are a small number of sentences ( 14 out of 3475 from the training data ) that have unary - branching roots. for these sentences, wang's"", '']",0
"['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['', "" #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. however, there are a small number of sentences ( 14 out of 3475 from the training data ) that have unary - branching roots. for these sentences, wang's"", '']",0
"['.', 'in the deterministic parser of  #TAUTHOR_TAG, the highest scoring action predicted by the classifier may prevent a valid binary tree from being built.', 'in this case, wang et al. simply']","['valid binarized trees.', 'in the deterministic parser of  #TAUTHOR_TAG, the highest scoring action predicted by the classifier may prevent a valid binary tree from being built.', 'in this case, wang et al. simply']","['.', 'in the deterministic parser of  #TAUTHOR_TAG, the highest scoring action predicted by the classifier may prevent a valid binary tree from being built.', 'in this case, wang et al. simply']","['all sequences of actions produce valid binarized trees.', 'in the deterministic parser of  #TAUTHOR_TAG, the highest scoring action predicted by the classifier may prevent a valid binary tree from being built.', 'in this case, wang et al. simply return a partial parse consisting of all the subtrees on the stack.', 'in our parser a set of restrictions is applied which guarantees a valid parse tree.', 'for example, two simple restrictions are that a shift action can only be applied if the queue of incoming words variables : state item item = ( s, q ), where s is stack and q is incoming queue ; the agenda agenda ; list of state items next ; algorithm :', '']",0
"['0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel function with an svm and did not manually create feature combinations. since we used', 'the linear perceptron algorithm we manually combined unigram features into bigram and trigram features. the "" bracket "" row shows bracket - related features, which were inspired by  #TAUTHOR_TAG. here brackets refer to left brackets including "" [UNK] "", "" "" "" and', '"" [UNK] "" and right brackets including "" [UNK] "", "" "" "" and "" [UNK] "". in the table, b represents the matching status of the last left bracket ( if any ) on the stack. it takes three different values : 1 ( no matching right bracket has', 'been pushed onto stack ), 2 ( a matching right bracket has been pushed onto stack ) and 3 ( a matching right bracket has been pushed onto stack, but then popped off ). the "" separ', '##ator "" row shows features that include one of the separator punctuations ( i. e. "" [UNK] "", "" [UNK] "", "" [UNK] "" and "" [UNK]', '']",0
"['0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel function with an svm and did not manually create feature combinations. since we used', 'the linear perceptron algorithm we manually combined unigram features into bigram and trigram features. the "" bracket "" row shows bracket - related features, which were inspired by  #TAUTHOR_TAG. here brackets refer to left brackets including "" [UNK] "", "" "" "" and', '"" [UNK] "" and right brackets including "" [UNK] "", "" "" "" and "" [UNK] "". in the table, b represents the matching status of the last left bracket ( if any ) on the stack. it takes three different values : 1 ( no matching right bracket has', 'been pushed onto stack ), 2 ( a matching right bracket has been pushed onto stack ) and 3 ( a matching right bracket has been pushed onto stack, but then popped off ). the "" separ', '##ator "" row shows features that include one of the separator punctuations ( i. e. "" [UNK] "", "" [UNK] "", "" [UNK] "" and "" [UNK]', '']",0
"['),  #TAUTHOR_TAG have shown that a']","['chinese treebank ( ctb ),  #TAUTHOR_TAG have shown that a']","['),  #TAUTHOR_TAG have shown that']","['', 'for constituent - based parsing using the chinese treebank ( ctb ),  #TAUTHOR_TAG have shown that a shift - reduce parser can give competitive accuracy scores together with high speeds, by using an svm to make a single decision at each point in the parsing process.', ""in this paper we describe a global discriminative model for chinese shift - reduce parsing, and compare it with wang et al.'s approach."", 'we apply the same shift - reduce procedure as  #TAUTHOR_TAG, but instead of using a local classifier for each transition - based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses.', 'we apply beam search to decoding instead of greedy search.', 'the parser still operates in linear time, but the use of beam - search allows the correction of local decision errors by global comparison.', ""using ctb2, our model achieved parseval f - scores comparable to wang et al.'s approach."", 'we also present accuracy scores for the much larger ctb5, using both a constituent - based and dependency - based evaluation.', 'the scores for the dependency - based evaluation were higher than the state - of - the - art dependency parsers for the ctb5 data']",1
"['),  #TAUTHOR_TAG have shown that a']","['chinese treebank ( ctb ),  #TAUTHOR_TAG have shown that a']","['),  #TAUTHOR_TAG have shown that']","['', 'for constituent - based parsing using the chinese treebank ( ctb ),  #TAUTHOR_TAG have shown that a shift - reduce parser can give competitive accuracy scores together with high speeds, by using an svm to make a single decision at each point in the parsing process.', ""in this paper we describe a global discriminative model for chinese shift - reduce parsing, and compare it with wang et al.'s approach."", 'we apply the same shift - reduce procedure as  #TAUTHOR_TAG, but instead of using a local classifier for each transition - based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses.', 'we apply beam search to decoding instead of greedy search.', 'the parser still operates in linear time, but the use of beam - search allows the correction of local decision errors by global comparison.', ""using ctb2, our model achieved parseval f - scores comparable to wang et al.'s approach."", 'we also present accuracy scores for the much larger ctb5, using both a constituent - based and dependency - based evaluation.', 'the scores for the dependency - based evaluation were higher than the state - of - the - art dependency parsers for the ctb5 data']",5
"['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['', "" #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. however, there are a small number of sentences ( 14 out of 3475 from the training data ) that have unary - branching roots. for these sentences, wang's"", '']",5
"['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the']","['is novel in our parser.', ' #AUTHOR_TAG and  #TAUTHOR_TAG only used']","['', "" #AUTHOR_TAG and  #TAUTHOR_TAG only used the first three transition actions, setting the final state as all incoming words having been processed, and the stack containing only one node. however, there are a small number of sentences ( 14 out of 3475 from the training data ) that have unary - branching roots. for these sentences, wang's"", '']",5
"['0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel']","[', whereas when the corresponding node is non - terminal, c represents its constituent label ; t represents the pos - tag for a word. the context s 0, s 1, s 2,', 's 3 and n 0, n 1, n 2, n 3 for the feature templates is taken from  #TAUTHOR_TAG. however,  #TAUTHOR_TAG used a polynomial kernel function with an svm and did not manually create feature combinations. since we used', 'the linear perceptron algorithm we manually combined unigram features into bigram and trigram features. the "" bracket "" row shows bracket - related features, which were inspired by  #TAUTHOR_TAG. here brackets refer to left brackets including "" [UNK] "", "" "" "" and', '"" [UNK] "" and right brackets including "" [UNK] "", "" "" "" and "" [UNK] "". in the table, b represents the matching status of the last left bracket ( if any ) on the stack. it takes three different values : 1 ( no matching right bracket has', 'been pushed onto stack ), 2 ( a matching right bracket has been pushed onto stack ) and 3 ( a matching right bracket has been pushed onto stack, but then popped off ). the "" separ', '##ator "" row shows features that include one of the separator punctuations ( i. e. "" [UNK] "", "" [UNK] "", "" [UNK] "" and "" [UNK]', '']",5
"[' #TAUTHOR_TAG, and our']","[' #TAUTHOR_TAG, and our parser, respectively.', 'the']","[' #TAUTHOR_TAG, and our']","['experiments in this section were performed using ctb2 to allow comparison with previous work, with the ctb2 data extracted from chinese treebank 5 ( ctb5 table 3 : accuracies on ctb2 with gold - standard pos - tags own implementation of the perceptron - based tagger from  #AUTHOR_TAG.', 'the results of various models measured using sentences with less than 40 words and using goldstandard pos - tags are shown in table 3.', 'the rows represent the model from  #AUTHOR_TAG,  #AUTHOR_TAG, the svm and ensemble models from  #TAUTHOR_TAG, and our parser, respectively.', 'the accuracy of our parser is competitive using this test set.', 'the results of various models using automatically assigned pos - tags are shown in table 4.', 'the rows in the table represent the models from  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG,  #AUTHOR_TAG, the svm model from  #TAUTHOR_TAG, and the parser of this paper, respectively.', 'our parser gave comparable accuracies to the svm and ensemble models from  #TAUTHOR_TAG.', 'however, comparison with table 3 shows that our parser is more sensitive to pos - tagging errors than some of the other models.', '']",5
"['process from  #AUTHOR_TAG and  #TAUTHOR_TAG, and therefore']","['process from  #AUTHOR_TAG and  #TAUTHOR_TAG, and therefore']","['parser is based on the shift - reduce parsing process from  #AUTHOR_TAG and  #TAUTHOR_TAG, and therefore']","['parser is based on the shift - reduce parsing process from  #AUTHOR_TAG and  #TAUTHOR_TAG, and therefore it can be classified as a transition - based parser  #AUTHOR_TAG.', 'an important difference between our parser and the  #TAUTHOR_TAG is based on a local classifier that optimizes each individual choice.', 'instead of greedy local decoding, we used beam search in the decoder.', 'an early work that applies beam search to constituent parsing is  #AUTHOR_TAG.', ""the main difference between our parser and ratnaparkhi's is that we use a global discriminative model, whereas ratnaparkhi's parser has separate probabilities of actions chained together in a conditional model."", 'both our parser and the parser from  #AUTHOR_TAG use a global discriminative model and an incremental parsing process.', '']",5
