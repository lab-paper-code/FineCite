unique_id,citing_id,citing_title,cited_title,cited_authors,section_title,cited_abstract,citation_context,cite_context_paragraph,citation_class_label,dynamic_contexts_combined
CC1,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,a fast and portable realizer for text generation systems,"['B Lavoie', 'Rambow']",,"{PG,1 A mixer block used in rotary drums to improve the efficiency of mixing, drying, cooling, heating, or calcining of solid materials such as gravel, stone, fluxes and the like to produce a more uniform product with minimal production of fines and dust is described. The mixer block is especially useful when used as part of the refractory lining in a rotary kiln to calcine fluxstone such as limestone, dolomite, dolomitic limestone, magnesite and the like.",The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG ) .,"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).', 'The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG ) .']",0,"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).', 'The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG ) .']"
CC2,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,a fast and portable realizer for text generation systems,"['B Lavoie', 'Rambow']",,"{PG,1 A mixer block used in rotary drums to improve the efficiency of mixing, drying, cooling, heating, or calcining of solid materials such as gravel, stone, fluxes and the like to produce a more uniform product with minimal production of fines and dust is described. The mixer block is especially useful when used as part of the refractory lining in a rotary kiln to calcine fluxstone such as limestone, dolomite, dolomitic limestone, magnesite and the like.","Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG ) .","['The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991 In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG ) .']",2,"['The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991 In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG ) .']"
CC3,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,machine translation divergences a formal description and proposed solution,['B J Don'],,"There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system.","More details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) .","['--o II a failli pleuvoir.', 'More details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) .']",0,"['More details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) .']"
CC4,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,applied text generation,['T Korelsky'],introduction,"While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion.","Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .","['In this paper we present a linguistically motivated framework for uniform lexicostructural processing.', 'It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).', 'Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .', 'Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.']",2,"['In this paper we present a linguistically motivated framework for uniform lexicostructural processing.', 'It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).', 'Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .']"
CC5,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,applied text generation,['T Korelsky'],,"While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion.","The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and #AUTHOR_TAG ) .","['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and #AUTHOR_TAG ) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG .']",2,"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and #AUTHOR_TAG ) .']"
CC6,A00-1012,Experiments on sentence boundary detection,a simple rulebased part of speech tagger,['E Brill'],,"Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. 1. I N T R O D U C T I O N There has been a dramat ic increase in the application of probabilistic models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact tha t very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automat ic par t of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12]. Stochastic taggers have *A version of this paper appears in Proceedings of the Third Conference on Applied Computational Linguistics (ACL), Trento, Italy, 1992. Used by permission of the Association for Computational Linguistics; copies of the publication from which this material is derived can can be obtained from Dr. Donald E. Walker (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910, Morristown, NJ 07960-1910, USA. The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. DAAL 03-89-G0031 PRI. 112 obtained a high degree of accuracy without performing any syntactic analysis on the input. These stochastic par t of speech taggers make use of a Markov model which captures lexical and contextual information. The parameters of the model can be est imated from tagged [1, 3, 4, 6, 12] or untagged [2, 9, 11] text. Once the parameters of the model are est imated, a sentence can then be automatical ly tagged by assigning it the tag sequence which is assigned the highest probabil i ty by the model. Performance is often enhanced with the aid of various higher level preand postprocessing procedures or by manually tuning the model. A number of rule-based taggers have been built [10, 7, 8]. [10] and [7] both have error rates substantial ly higher than s tate of the art stochastic taggers. [8] disambiguates words within a deterministic parser. We wanted to determine whether a simple rule-based tagger without any knowledge of syntax can perform as well as a stochastic tagger, or if par t of speech tagging really is a domain to which stochastic techniques are bet ter suited. In this paper we describe a rule-based tagger which performs as well as taggers based upon probabilistic models. The rule-based tagger overcomes the limitations common in rule-based approaches to language processing: it is robust, and the rules are automatical ly acquired. In addition, the tagger has many advantages over stochastic taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules as opposed to the large tables of statistics needed for stochastic taggers, ease of finding and implementing improvements to the tagger, and bet ter portabil i ty from one tag set or corpus genre to another. 2. T H E T A G G E R The tagger works by automatical ly recognizing and remedying its weaknesses, thereby incrementally improving its performance. The tagger initially tags by assigning each word its most likely tag, est imated by examining a large tagged corpus, without regard to context. In both sentences below, run would be tagged as a verb: Report Documentation Page Form Approved OMB No. 0704-0188 Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number.",Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .,"['Figure 1: Example text shown in standard and ASR format ation which is not available in ASR output is sentence boundary information.', 'However, knowledge of sentence boundaries is required by many NLP technologies.', ""Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .""]",0,"[""Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .""]"
CC7,A00-1020,Multilingual coreference resolution,knowledgelean coreference resolution and its relation to textual cohesion and coherence,"['Sanda M Harabagiu', 'Steve J Maiorano']",introduction,"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology.","SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .","['For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver.', 'SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .', 'When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.', 'When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.', 'Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents.']",2,"['SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .']"
CC8,A00-1020,Multilingual coreference resolution,robust pronoun resolution with limited knowledge,['Ruslan Mitkov'],,"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications.","Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) .","['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', 'Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge.', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) .', 'For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference.', 'This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).', 'These constraints are implemented as a set of heuristics ordered by their priority.', 'Moreover, the COCKTAILframework uniformly addresses the prob- lem of interaction between different forms of coref- erence, thus making the extension to multilingual coreference very natural.']",0,"['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', 'Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) .', 'For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.']"
CC9,A00-1020,Multilingual coreference resolution,knowledgelean coreference resolution and its relation to textual cohesion and coherence,"['Sanda M Harabagiu', 'Steve J Maiorano']",,"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology.",Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG ) .,"['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'On other occasions, coercions are obtained as paths of meronyms (e.g. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG ) .']",0,"['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'On other occasions, coercions are obtained as paths of meronyms (e.g. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG ) .']"
CC10,A00-1020,Multilingual coreference resolution,knowledgelean coreference resolution and its relation to textual cohesion and coherence,"['Sanda M Harabagiu', 'Steve J Maiorano']",,"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology.","For this research , we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .","['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', 'Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf.', '(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).', 'For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'For this research , we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', 'This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g.', 'subjects of communication verbs are more likely to refer to the last person mentioned in the text).', 'These constraints are implemented as a set of heuristics ordered by their priority.', 'Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural.']",5,"['For this research , we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', 'This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g.']"
CC11,A00-1024,Categorizing unknown words,language identification with confidence limits,['D Elworthy'],experiments,"A statistical classification algorithm and its application to language identification from noisy input are described. The main innovation is to compute confidence limits on the classification, so that the algorithm terminates when enough evidence to make a clear decision has been made, and so avoiding problems with categories that have similar characteristics. A second application, to genre identification, is briefly ex- amined. The results show that some of the problems of other language identification tech- niques can be avoided, and illustrate a more important point: that a statistical language process can be used to provide feedback about its own success rate","Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG ) .","['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word.']",4,"['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word.']"
CC12,A00-1024,Categorizing unknown words,a stochastic parts program and noun phrase parser for unrestricted text,['K Church'],experiments,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .,"['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns.']",5,"['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .']"
CC13,A00-1024,Categorizing unknown words,detecting and correcting morphosyntactic errors in real texts,['T Vosse'],,,Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG ) .,"['Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', 'Hence, his system would be ineffective on the closed caption domain with which we are working.', '(Granger, 1983) uses expectations generated by scripts to anMyze unknown words.', 'The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages.']",1,"['Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.']"
CC14,A00-1024,Categorizing unknown words,detecting and correcting morphosyntactic errors in real texts,['T Vosse'],experiments,,Corpus frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .,"['Corpus frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', 'Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions.']",5,"['Corpus frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.']"
CC15,A00-2028,An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email,automatic detection of poor speech recognition at the dialogue level,"['D J Litman', 'M A Walker', 'M J Kearns']",conclusion,"The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction. An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand. To do so, a system must be able to identify dialogue properties that suggest adaptation. This paper focuses on identifying situations where the speech recognizer is performing poorly. We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations. Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm.",Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ) .,"['The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ) .', 'However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.', 'In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime.']",2,['Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ) .']
CC16,A00-2028,An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email,automatic detection of poor speech recognition at the dialogue level,"['D J Litman', 'M A Walker', 'M J Kearns']",,"The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction. An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand. To do so, a system must be able to identify dialogue properties that suggest adaptation. This paper focuses on identifying situations where the speech recognizer is performing poorly. We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations. Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm.",The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG ) .,"['The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG ) .']",4,"['The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG ) .']"
CC17,D08-1004,Modeling annotators,thumbs up sentiment classification using machine learning techniques,"['B Pang', 'L Lee', 'S Vaithyanathan']",method,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.","We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .","['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']",5,"['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .']"
CC18,D08-1006,Refining generative language models using discriminative learning,classbased ngram models of natural language,"['F Brown', 'Vincent J Della Pietra', 'Peter V deSouza', 'Jenifer C Lai', 'Robert L Mercer']",conclusion,,"In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #AUTHOR_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .","['The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #AUTHOR_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .""]",3,"[""In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #AUTHOR_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .""]"
CC19,D08-1006,Refining generative language models using discriminative learning,a discriminative language model with pseudonegative samples,"['Daisuke Okanohara', ""Jun'ichi Tsujii""]",,,"Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .","['Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', 'Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data.', 'Đn both cases essentially linear classifiers were used as features.', 'As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples.', 'Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', 'While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.', 'This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.', 'For this reason we use a different sampling scheme which we refer to as rejection sampling.', ""This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline.""]",4,"['Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', 'Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .']"
CC20,D08-1006,Refining generative language models using discriminative learning,a discriminative language model with pseudonegative samples,"['Daisuke Okanohara', ""Jun'ichi Tsujii""]",experiments,,"As shown in ( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .","['For our features we used large-margin classifiers trained using the online algorithm described in (Crammer et al., 2006).', 'The code for the classifier was generously provided by Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in ( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'No special effort was otherwise made in order to optimize the parameters of the classifiers.']",4,"['As shown in ( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .']"
CC21,D08-1006,Refining generative language models using discriminative learning,an empirical study of smoothing techniques for language modeling,['Goodman'],experiments,"We present a tutorial introduction to n-gram models for language modeling and survey the most widely-used smoothing algorithms for such models. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), Bell, Cleary, and Witten (1990), Ney, Essen, and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g., Brown versus Wall Street Journal), count cutoffs, and n-gram order (bigram versus trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. Our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance. We introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser-Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.Engineering and Applied Science","Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .","['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.', 'A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled.', ""The value of the n'th coordinate in the vector representation of Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001."", 'This indicates that satisfying the constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated.']",5,"['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .']"
CC22,D08-1010,Maximum entropy based rule selection model for syntax-based statistical machine translation,treetostring alignment template for statistical machine translation,"['Yang Liu', 'Qun Liu', 'Shouxun Lin']",experiments,"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.",The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .,"['The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .', 'When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.', 'Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately.', 'We set the iteration number to 100 and Gaussian prior to 1.']",2,"['The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .', 'When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.', 'Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately.']"
CC23,D08-1016,Dependency parsing by belief propagation,coarsetofine nbest parsing and maxent discriminative reranking,"['E Charniak', 'M Johnson']",conclusion,,"For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']",3,"['For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .']"
CC24,D08-1016,Dependency parsing by belief propagation,forest reranking discriminative parsing with nonlocal features,['L Huang'],conclusion,"Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.","For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']",3,"['For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .']"
CC25,D08-1016,Dependency parsing by belief propagation,integrating graphbased and transitionbased dependency parsers,"['J Nivre', 'R McDonald']",conclusion,,"For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #AUTHOR_TAG ) .","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #AUTHOR_TAG ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']",3,"['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #AUTHOR_TAG ) .']"
CC26,D08-1016,Dependency parsing by belief propagation,probabilistic cfg with latent annotations,"['T Matsuzaki', 'Y Miyao', 'J Tsujii']",conclusion,"This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences <= 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.","We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005;Huang, 2008) and history-based parsing (Nivre and McDonald, 2008).', 'We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .']",3,"['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .']"
CC27,D08-1022,Forest-based translation rule extraction,forestbased translation,"['Haitao Mi', 'Liang Huang', 'Qun Liu']",introduction,,"Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .","['We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists.', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', 'When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.']",2,"['Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', 'When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.']"
CC28,D08-1022,Forest-based translation rule extraction,forestbased translation,"['Haitao Mi', 'Liang Huang', 'Qun Liu']",related work,,The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .,"['The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).', 'The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', 'This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.']",2,"['The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).', 'The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .']"
CC29,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,using predicateargument structures for information extraction,"['Mihai Surdeanu', 'Sanda Harabagiu', 'John Williams', 'Paul Aarseth']",introduction,"In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.","Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .","['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0,"['Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .']"
CC30,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic semantic role labeling for chinese verbs,"['Nianwen Xue', 'Martha Palmer']",experiments,"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling.","To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , Xue ( 2008 ) .","['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , Xue ( 2008 ) .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.', 'From the table 6, we can find that our system is better than both of the related systems.', 'Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%.']",1,"['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , Xue ( 2008 ) .']"
CC31,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,question answering based on semantic structures,"['Srini Narayanan', 'Sanda Harabagiu']",introduction,The ability to answer complex questions posed in Natural Language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support. In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model. A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM). In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions. The results indicate enhanced accuracy over current state-of-the-art Q/A systems.,"Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .","['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0,"['Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .']"
CC32,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,the specification of the semantic knowledgebase of contemporary chinese,"['Hui Wang', 'Weidong Zhan', 'Shiwen Yu']",,,The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .,"['SemCat (semantic category) of predicate, Sem-Cat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .']",5,['The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .']
CC33,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,calibrating features for semantic role labeling,"['Nianwen Xue', 'Martha Palmer']",introduction,"This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed. We further show that different features are needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis.",#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .,"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,['#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .']
CC34,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,annotating the propositions in the penn chinese treebank,"['Nianwen Xue', 'Martha Palmer']",introduction,"In this paper, we describe an approach to annotate the propositions in the Penn Chinese Treebank. We describe how diathesis alternation patterns can be used to make coarse sense distinctions for Chinese verbs as a necessary step in annotating the predicate-structure of Chinese verbs. We then discuss the representation scheme we use to label the semantic arguments and adjuncts of the predicates. We discuss several complications for this type of annotation and describe our solutions. We then discuss how a lexical database with predicate-argument structure information can be used to ensure consistent annotation. Finally, we discuss possible applications for this resource.","After the PropBank ( #AUTHOR_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank ( #AUTHOR_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['After the PropBank ( #AUTHOR_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .']"
CC35,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic semantic role labeling for chinese verbs,"['Nianwen Xue', 'Martha Palmer']",introduction,"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling.","Experiments on Chinese SRL ( #AUTHOR_TAG , Xue 2008 ) reassured these findings .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( #AUTHOR_TAG , Xue 2008 ) reassured these findings .']",4,"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'Experiments on Chinese SRL ( #AUTHOR_TAG , Xue 2008 ) reassured these findings .']"
CC36,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,semantic argument classification exploiting argument interdependence,"['Zheng Ping Jiang', 'Jia Li', 'Hwee Tou Ng']",,"This paper describes our research on automatic semantic argument classification, using the PropBank data [Kingsbury et al., 2002]. Previous research employed features that were based either on a full parse or shallow parse of a sentence. These features were mostly based on an individual semantic argument and the relation between the predicate and a semantic argument, but they did not capture the interdependence among all arguments of a predicate. In this paper, we propose the use of the neighboring semantic arguments of a predicate as additional features in determining the class of the current semantic argument. Our experimental results show significant improvement in the accuracy of semantic argument classification after exploiting argument interdependence. Argument classification accuracy on the standard Section 23 test set improves to 90.50%, representing a relative error reduction of 18%.",#AUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .,"['#AUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', 'We can use window size to represent the scope of the context.', 'Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu- ments will be utilized for the classification of cur- rent semantic role.', 'There are two kinds of argu- ment sequences in Jiang et al. (2005), and we only test the linear sequence.', 'Take the sentence in fig- ure 1 as an example.', 'The linear sequence of the arguments in this sentence is: ____(until then), ____ (the insurance company), _ (has), _ ____ (for the Sanxia Project), ____ (in- surance services).', 'For the argument _ (has), if the semantic context window size is [-1,2], the seman- tic context features e.g. headword, phrase type and etc. of ____ (the insurance company), __ ___ (for the Sanxia Project) and ____ (insurance services) will be utilized to serve the classification task of _ (has).']",5,['#AUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .']
CC37,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,can semantic roles generalize across genres,"['Szu-ting Yi', 'Edward Loper', 'Martha Palmer']",conclusion,"PropBank has been widely used as training data for Semantic Role Labeling. However, because this training data is taken from the WSJ, the resulting machine learning models tend to overfit on idiosyncrasies of that text's style, and do not port well to other genres. In addition, since PropBank was designed on a verb-by-verb basis, the argument labels Arg2 - Arg5 get used for very diverse argument roles with inconsistent training instances. For example, the verb ""make"" uses Arg2 for the ""Material"" argument; but the verb ""multiply"" uses Arg2 for the ""Extent"" argument. As a result, it can be difficult for automatic classifiers to learn to distinguish arguments Arg2-Arg5. We have created a mapping between PropBank and VerbNet that provides a VerbNet thematic role label for each verb-specific PropBank label. Since VerbNet uses argument labels that are more consistent across verbs, we are able to demonstrate that these new labels are easier to learn.",#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .,"['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', 'However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank.', 'What if we could extend the idea of hierarchical architecture to the single semantic role level?', 'Would that help the improvement of SRC?']",1,"['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .']"
CC38,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,shallow semantic parsing of chinese,"['Honglin Sun', 'Daniel Jurafsky']",introduction,"In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.","Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.']"
CC39,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],experiments,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .","['To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.', 'From the table 6, we can find that our system is better than both of the related systems.', 'Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%.']",1,"['To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .']"
CC40,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,hierarchical semantic role labeling,"['Alessandro Moschitti', 'Ana-Maria Giuglea', 'Bonaventura Coppola', 'Roberto Basili']",,"We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification). To achieve this, we have split the classification step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts). The results show that the non-optimized hierarchical approach is computationally more efficient than the traditional systems and it preserves their accuracy.","be found in figure 2 , which is similar with that in #AUTHOR_TAG .","['Previous semantic role classifiers always did the classification problem in one-step.', 'However, in this paper, we did SRC in two steps.', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in #AUTHOR_TAG .']",1,"['Previous semantic role classifiers always did the classification problem in one-step.', 'be found in figure 2 , which is similar with that in #AUTHOR_TAG .']"
CC41,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG .","['Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG .']",5,"['Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG .']"
CC42,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,bilingual framenet dictionaries for machine translation,['Hans C Boas'],introduction,"This paper describes issues surrounding the planning and design of GermanFrameNet (GFN), a counterpart to the English-based FrameNet project. The goals of GFN are (a) to create lexical entries for German nouns, verbs, and adjectives that correspond to existing FrameNet entries, and (b) to link the parallel lexicon fragments by means of common semantic frames and numerical indexing mechanisms. GFN will take a fine-grained approach towards polysemy that seeks to split word senses based on the semantic frames that underlie their analysis. The parallel lexicon fragments represent an important step towards capturing valuable information about the different syntactic realizations of frame semantic concepts across languages, which is relevant for information retrieval, machine translation, and language generation. 1","Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG ) .","['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0,"['Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG ) .']"
CC43,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],experiments,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","We use the same data setting with #AUTHOR_TAG , however a bit different from Xue and Palmer ( 2005 ) .","['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with #AUTHOR_TAG , however a bit different from Xue and Palmer ( 2005 ) .']",5,"['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'We use the same data setting with #AUTHOR_TAG , however a bit different from Xue and Palmer ( 2005 ) .']"
CC44,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],introduction,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #AUTHOR_TAG .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #AUTHOR_TAG .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #AUTHOR_TAG .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.']"
CC45,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],introduction,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG ) reassured these findings .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG ) reassured these findings .']",0,"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG ) reassured these findings .']"
CC46,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,hierarchical semantic role labeling,"['Alessandro Moschitti', 'Ana-Maria Giuglea', 'Bonaventura Coppola', 'Roberto Basili']",introduction,"We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification). To achieve this, we have split the classification step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts). The results show that the non-optimized hierarchical approach is computationally more efficient than the traditional systems and it preserves their accuracy.",#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.,"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,['#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.']
CC47,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic semantic role labeling for chinese verbs,"['Nianwen Xue', 'Martha Palmer']",introduction,"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling.","Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #AUTHOR_TAG and Xue ( 2008 ) .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #AUTHOR_TAG and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #AUTHOR_TAG and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.']"
CC48,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic semantic role labeling for chinese verbs,"['Nianwen Xue', 'Martha Palmer']",experiments,"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling.","We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .","['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']",1,"['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']"
CC49,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,shallow semantic parsing of chinese,"['Honglin Sun', 'Daniel Jurafsky']",,"In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.",The candidate feature templates include : Voice from #AUTHOR_TAG .,['The candidate feature templates include : Voice from #AUTHOR_TAG .'],5,['The candidate feature templates include : Voice from #AUTHOR_TAG .']
CC50,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,the penn chinese treebank phrase structure annotation of a large corpus,"['Nianwen Xue', 'Fei Xia', 'Fu dong Chiou', 'Martha Palmer']",,"With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.",The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .,"['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1 is an example from the PropBank 1 .', 'We put the word-by-word translation and the translation of the whole sentence below the example.', 'It is quite a complex sentence, as there are many semantic roles in it.', 'In this sentence, all the semantic roles of the verb 提供 (provide) are presented in the syntactic tree.', 'We can separate the semantic roles into two groups.']",5,['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .']
CC51,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic labeling of semantic roles,"['Daniel Gildea', 'Daniel Jurafsky']",introduction,"We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.",Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .,"['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002).', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0,['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .']
CC52,D09-1056,The role of named entities in web people search,automatic entity disambiguation benefits to ner relation extraction link analysis and inference,['Matthias Blume'],related work,"Entity disambiguation resolves the many-to-many correspondence between mentions of entities in text and unique real-world entities. Entity disambiguation can bring to bear global (corpus-level) statistics to improve the performance of named entity recognition systems. More importantly , intelligence analysts are keenly interested in relationships between real-world entities. Entity disambiguation makes possible additional types of relation assertions and affects relation extraction performance assessment. Finally, link analysis and inference inherently operate at the level of entities, not text strings. Thus, entity disambiguation is a prerequisite to carrying out these higher-level operations on information extracted from plain text. This paper describes Fair Isaac's automatic entity disambiguation capability and its performance.","In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .","['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.']"
CC53,D09-1056,The role of named entities in web people search,cucomsem exploring rich features for unsupervised web personal name disambiguation,"['Ying Chen', 'James H Martin']",related work,,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .","['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.']"
CC54,D09-1056,The role of named entities in web people search,disambiguation algorithm for people search on the web in,"['Dmitri V Kalashnikov', 'Stella Chen', 'Rabia Nuray', 'Sharad Mehrotra', 'Naveen Ashish']",related work,,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .","['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.']"
CC55,D09-1056,The role of named entities in web people search,unsupervised name disambiguation via social network similarity,['Bradley Malin'],related work,"Though names reference actual entities it is nontrivial to resolve which entity a particular name observation represents. Even when names are devoid of typographical error, the resolution process is confounded by both ambiguity, where the same name correctly references multiple entities, and by variation, when an entity is correctly referenced by multiple names. Thus, before link analysis for surveillance or intelligence-gathering purposes can proceed, it is necessary to ensure vertices and edges of the network are correct. In this paper, we concentrate on ambiguity and investigate unsupervised methods which simultaneously learn 1) the number of entities represented by a particular name and 2) which observations correspond to the same entity. The disambiguation methods leverage the fact that an entity's name can be listed in multiple sources, each with a number of related entity's names, which permits the construction of name-based relational networks. The methods studied in this paper differ based on the type of network similarity exploited for disambiguation. The first method relies upon exact name similarity and employs hierarchical clustering of sources, where each source is considered a local network. In contrast, the second method employs a less strict similarity requirement by using random walks between ambiguous observations on a global social network constructed from all sources, or a community similarity. While both methods provide better than simple baseline results on a subset of the Internet Movie Database, findings suggest methods which measure similarity based on community, rather than exact, similarity provide more robust disambiguation capability.","Other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']"
CC56,D09-1056,The role of named entities in web people search,named entity disambiguation a hybrid statistical and rulebased incremental approach,"['Hien T Nguyen', 'Tru H Cao']",related work,"The rapidly increasing use of large-scale data on the Web makes named entity disambiguation become one of the main challenges to research in Information Extraction and development of Semantic Web. This paper presents a novel method for detecting proper names in a text and linking them to the right entities in Wikipedia. The method is hybrid, containing two phases of which the first one utilizes some heuristics and patterns to narrow down the candidates, and the second one employs the vector space model to rank the ambiguous cases to choose the right candidate. The novelty is that the disambiguation process is incremental and includes several rounds that filter the candidates, by exploiting previously identified entities and extending the text by those entity attributes every time they are successfully resolved in a round. We test the performance of the proposed method in disambiguation of names of people, locations and organizations in texts of the news domain. The experiment results show that our approach achieves high accuracy and can be used to construct a robust named entity disambiguation system.","Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']"
CC57,D09-1056,The role of named entities in web people search,the semeval2007 weps evaluation establishing a benchmark for the web people search task,"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']",related work,"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.","It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG ) .","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998).', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG ) .']",0,"['It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG ) .']"
CC58,D09-1056,The role of named entities in web people search,multidocument statistical fact extraction and fusion,['Gideon S Mann'],experiments,"This dissertation presents original techniques for statistical fact extraction and fusion from multiple documents. Fact extraction, or relationship extraction, is a process where natural language text is scanned to find instances of a predetermined class of facts (e.g. birthday(x,y)). A framework for training statistical fact extractors from example is used wherein a set of examples and a target model are used to annotate an automatically collected corpus. This annotation is then used to provide training data for classifiers (Phrase Conditional Likelihood and Native Bayes) or sequence models (Conditional Random Fields).  Fact extractors are used in two information retrieval tasks. In question answering the set of candidate answers is narrowed using fine-grained proper noun ontological facts (is-a(X, Y)) extracted from a corpus by rote classifiers leading to higher performance. Extracted facts are also used for name-referent disambiguation, or cross-document coreference, where one personal name may refer to multiple potential people in the world. The distinguishing biographic facts for each person, such as birthday(x,y) and occupation (x,y), are automatically extracted from plain text and these biographic facts are used along with other statistical methods to distinguish between mentions of each of the referents.  This dissertation presents novel techniques for fusion which integrate facts extracted from multiple sources. For the task of biographic fact extraction, fusion of factual information extracted from multiple documents improves the precision of the resulting information. Further improvements result from cascaded fact extraction, where certain facts are extracted and fused and then these facts are used to extract additional information. The technique of cascaded fact extraction and fusion is also applied to time-bounded facts, where a cascade of fact extractors produce a timeline of corporate management succession.  Collectively, this research demonstrates the utility of multi-document fact extraction and fusion. It shows that facts can serve as a building-block for deeper text processing such as finding coreferent names in a series of documents, finding the answers to questions, and constructing a timeline for time-variable facts. The key aspects to the process are training with minimal supervision, high-performance statistical fact extraction, fusion across multiple sources of information, and cascaded extraction.","2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .","['2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']",5,"['2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']"
CC59,D09-1056,The role of named entities in web people search,crossdocument coreference on a large scale corpus,"['Chung Heong Gooi', 'James Allan']",related work,,"Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .']"
CC60,D09-1056,The role of named entities in web people search,a testbed for people searching strategies in the www,"['Javier Artiles', 'Julio Gonzalo', 'Felisa Verdejo']",related work,This paper describes the creation of a testbed to evaluate people searching strategies on the World-Wide-Web. This task involves resolving person names' ambiguity and locating relevant information characterising every individual under the same name.,"It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 ) .","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998).', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 ) .']",0,"['It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 ) .']"
CC61,D09-1056,The role of named entities in web people search,cucomsem exploring rich features for unsupervised web personal name disambiguation,"['Ying Chen', 'James H Martin']",related work,,"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .']"
CC62,D09-1056,The role of named entities in web people search,disambiguation algorithm for people search on the web in,"['Dmitri V Kalashnikov', 'Stella Chen', 'Rabia Nuray', 'Sharad Mehrotra', 'Naveen Ashish']",related work,,"Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ) .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']"
CC63,D09-1056,The role of named entities in web people search,searching for people on web search engines,"['Amanda Spink', 'Bernard Jansen', 'Jan Pedersen']",introduction,"The Web is a communication and information technology that is often used for the distribution and retrieval of personal information. Many people and organizations mount Web sites containing large amounts of information on individuals, particularly about celebrities. However, limited studies have examined how people search for information on other people, using personal names, via Web search engines. Explores the nature of personal name searching on Web search engines. The specific research questions addressed in the study are: ""Do personal names form a major part of queries to Web search engines?""; ""What are the characteristics of personal name Web searching?""; and ""How effective is personal name Web searching?"". Random samples of queries from two Web search engines were analyzed. The findings show that: personal name searching is a common but not a major part of Web searching with few people seeking information on celebrities via Web search engines; few personal name queries include double quotations or additional identifying terms; and name searches on Alta Vista included more advanced search features relative to those on AlltheWeb.com. Discusses the implications of the findings for Web searching and search engines, and further research.",A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ) .,"['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ) .', 'According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (Artiles et al., 2005).', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']",0,['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ) .']
CC64,D09-1056,The role of named entities in web people search,experiments on semanticbased clustering for crossdocument coreference,['Horacio Saggion'],related work,,#AUTHOR_TAG compared the performace of NEs versus BoW features .,"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007)- .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of NEs versus BoW features .', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['#AUTHOR_TAG compared the performace of NEs versus BoW features .', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']"
CC65,D09-1056,The role of named entities in web people search,entitybased crossdocument coreferencing using the vector space model,"['Amit Bagga', 'Breck Baldwin']",related work,,"Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .']"
CC66,D09-1056,The role of named entities in web people search,extended named entity ontology with attribute information,['Satoshi Sekine'],experiments,"Named Entities (NE) are regarded as an important type of semantic knowledge in many natural language processing (NLP) applications. Originally, a limited number of NE categories were proposed. In MUC, it was 7 categories - people, organization, location, time, date, money and percentage expressions. However, it was noticed that such a limited number of NE categories is too small for many applications. The author has proposed Extended Named Entity (ENE), which has about 200 categories (Sekine and Nobata 04). During the development of ENE, we noticed that many ENE categories have specific attributes, and those provide very important information for the entities. For example, rivers have attributes like source location, outflow, and length. Some such information is essential to knowing about the river, while the name is only a label which can be used to refer to the river. Also, such attributes are important information for many NLP applications. In this paper, we report on the design of a set of attributes for ENE categories. We used a bottom up approach to creating the knowledge using a Japanese encyclopedia, which contains abundant descriptions of ENE instances.",It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG ) .,"['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG ) .', 'Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.']",5,"['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG ) .', 'Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.']"
CC67,D09-1056,The role of named entities in web people search,a testbed for people searching strategies in the www,"['Javier Artiles', 'Julio Gonzalo', 'Felisa Verdejo']",introduction,This paper describes the creation of a testbed to evaluate people searching strategies on the World-Wide-Web. This task involves resolving person names' ambiguity and locating relevant information characterising every individual under the same name.,"According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG ) .","['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al., 2004).', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']",0,"['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al., 2004).', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']"
CC68,D09-1056,The role of named entities in web people search,large scale named entity disambiguation based on wikipedia data,['Silviu Cucerzan'],related work,"This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.","Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']"
CC69,D09-1056,The role of named entities in web people search,the semeval2007 weps evaluation establishing a benchmark for the web people search task,"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']",introduction,"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.","The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it .","['This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it .']",0,"['The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it .']"
CC70,D09-1056,The role of named entities in web people search,the semeval2007 weps evaluation establishing a benchmark for the web people search task,"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']",related work,"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.","We have used the testbeds from WePS-1 ( #AUTHOR_TAG , 2007)2 and WePS-2 (Artiles et al., 2009) evaluation campaigns 3.","['We have used the testbeds from WePS-1 ( #AUTHOR_TAG , 2007)2 and WePS-2 (Artiles et al., 2009) evaluation campaigns 3.']",5,"['We have used the testbeds from WePS-1 ( #AUTHOR_TAG , 2007)2 and WePS-2 (Artiles et al., 2009) evaluation campaigns 3.']"
CC71,D09-1056,The role of named entities in web people search,irstbp web people search using name entities,"['Octavian Popescu', 'Bernardo Magnini']",related work,,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .","['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['The most used feature for the Web People Search task, however, are NEs.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .']"
CC72,D09-1056,The role of named entities in web people search,entitybased crossdocument coreferencing using the vector space model,"['Amit Bagga', 'Breck Baldwin']",related work,,"The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own (Artiles et al., 2005;Artiles et al., 2007).']",0,"['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .']"
CC73,D09-1056,The role of named entities in web people search,weps 2 evaluation campaign overview of the web people search clustering task,"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']",related work,"The second WePS (Web People Search) Evaluation cam-paign took place in 2008-2009 with the participation of 19 re-search groups from Europe, Asia and North America. Given the output of a Web Search Engine for a (usually ambiguous) person name as query, two tasks were addressed: a clustering task, which consists of grouping together web pages referring to the same person, and an extraction task, which consists of extracting salient attributes for each of the persons shar-ing the same name. This paper presents the definition, re-sources, methodology and evaluation metrics, participation and comparative results for the clustering task","In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .","['Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation.', 'This makes NEs the second most common type of feature; only the BoW feature was more popular.', 'Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc.', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .', 'Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.', 'But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'In the next Section we describe this dataset and how it has been adapted for our purposes.']",0,"['Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation.', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .', 'Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.']"
CC74,D09-1056,The role of named entities in web people search,titpi web people search task using semisupervised clustering approach,"['Kazunari Sugiyama', 'Manabu Okumura']",related work,,"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).']"
CC75,D09-1056,The role of named entities in web people search,irstbp web people search using name entities,"['Octavian Popescu', 'Bernardo Magnini']",related work,,"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .']"
CC76,D09-1067,Improving verb clustering with automatically acquired selectional preferences,discriminative learning of selectional preference from unlabeled text,"['Shane Bergsma', 'Dekang Lin', 'Randy Goebel']",conclusion,"We present a discriminative method for learning selectional preferences from unlabeled text. Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations. We train a Support Vector Machine classifier to distinguish the positive from the negative instances. We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances. The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments. Compared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and resolves 37 % more pronouns correctly in a pronoun resolution experiment.","Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ) .","['In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ) .', 'The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.']",3,"['Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ) .']"
CC77,D09-1067,Improving verb clustering with automatically acquired selectional preferences,a simple similaritybased model for selectional preferences,['Katrin Erk'],conclusion,,"Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .","['In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .', 'The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.']",3,"['Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .']"
CC78,D09-1087,Self-training PCFG grammars with latent annotations across languages,forest reranking discriminative parsing with nonlocal features,['Liang Huang'],conclusion,"Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.","Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .', 'Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case.']",3,"['Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .']"
CC79,D09-1087,Self-training PCFG grammars with latent annotations across languages,coarsetofine nbest parsing and maxent discriminative reranking,"['Eugene Charniak', 'Mark Johnson']",conclusion,,"Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training .","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training .', 'Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case.']",3,"['Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training .']"
CC80,D09-1087,Self-training PCFG grammars with latent annotations across languages,sparse multiscale grammars for discriminative latent variable parsing,"['Slav Petrov', 'Dan Klein']",conclusion,"We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.","Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case .","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case .']",3,"['Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case .']"
CC81,D09-1143,"Convolution kernels on constituent, dependency and sequential structures for relation extraction",a semantic kernel for predicate argument classification,"['Alessandro Moschitti', 'Cosmin Bejan']",conclusion,"Automatically deriving semantic structures from text is a challenging task for machine learning. The flat feature representations, usually used in learning models, can only partially describe structured data. This makes difficult the processing of the semantic information that is embedded into parse-trees. In this paper a new kernel for automatic classification of predicate arguments has been designed and experimented. It is based on subparse-trees annotated with predicate argument information from PropBank corpus. This kernel, exploiting the convolution properties of the parse-tree kernel, enables us to learn which syntactic structures can be associated with the arguments defined in PropBank. Support Vector Machines (SVMs) using such a kernel classify arguments with a better accuracy than SVMs based on linear kernel.","Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']",3,"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"
CC82,D09-1143,"Convolution kernels on constituent, dependency and sequential structures for relation extraction",semantic role labeling via framenet verbnet and propbank,"['Ana-Maria Giuglea', 'Alessandro Moschitti']",conclusion,"This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources: FrameNet, VerbNet and PropBank. The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs. We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes. The PropBank corpus, which is tightly connected to the VerbNet lexicon, is used to increase the verb coverage and also to test the effectiveness of our approach. The results indicate that our model is an interesting step towards the design of more robust semantic parsers.","Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']",3,"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"
CC83,D09-1143,"Convolution kernels on constituent, dependency and sequential structures for relation extraction",tree kernels for semantic role labeling,"['Alessandro Moschitti', 'Daniele Pighin', 'Roberto Basili']",conclusion,"The availability of large scale data sets of manually annotated predicate-argument structures has recently favored the use of machine learning approaches to the design of automated semantic role labeling (SRL) systems. The main research in this area relates to the design choices for feature representation and for effective decompositions of the task in different learning models. Regarding the former choice, structural properties of full syntactic parses are largely employed as they represent ways to encode different principles suggested by the linking theory between syntax and semantics. The latter choice relates to several learning schemes over global views of the parses. For example, re-ranking stages operating over alternative predicate-argument sequences of the same sentence have shown to be very effective. In this article, we propose several kernel functions to model parse tree properties in kernel-based machines, for example, perceptrons or support vector machines. In particular, we define different kinds of tree kernels as general approaches to feature engineering in SRL. Moreover, we extensively experiment with such kernels to investigate their contribution to individual stages of an SRL architecture both in isolation and in combination with other traditional manually coded features. The results for boundary recognition, classification, and re-ranking stages provide systematic evidence about the significant impact of tree kernels on the overall accuracy, especially when the amount of training data is small. As a conclusive result, tree kernels allow for a general and easily portable feature engineering method which is applicable to a large family of natural language processing tasks.","Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG ) .","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG ) .']",3,"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG ) .']"
CC84,D09-1143,"Convolution kernels on constituent, dependency and sequential structures for relation extraction",effective use of wordnet semantics via kernelbased learning,"['Roberto Basili', 'Marco Cammisa', 'Alessandro Moschitti']",conclusion,"Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.    In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available.","Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']",3,"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"
CC85,D09-1160,Polynomial to linear,small statistical models by random feature mixing,"['Kuzman Ganchev', 'Mark Dredze']",conclusion,The application of statistical NLP systems to resource constrained devices is limited by the need to maintain parameters for a large number of features and an alphabet mapping features to parameters. We introduce random feature mixing to eliminate alphabet storage and reduce the number of parameters without severely impacting model performance.,"When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .","['We plan to apply our method to wider range of classifiers used in various NLP tasks.', 'To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .']",3,"['When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .']"
CC86,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,ordering phrases with function words,"['Hendra Setiawan', 'Min-Yen Kan', 'Haizhou Li']",,"This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios.","The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .","['The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']",2,"['The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.']"
CC87,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,improved word alignment with statistics and linguistic heuristics,['Ulf Hermjakob'],related work,"We present a method to align words in a bitext that combines elements of a tra-ditional statistical approach with linguis-tic knowledge. We demonstrate this ap-proach for Arabic-English, using an align-ment lexicon produced by a statistical word aligner, as well as linguistic re-sources ranging from an English parser to heuristic alignment rules for function words. These linguistic heuristics have been generalized from a development cor-pus of 100 parallel sentences. Our aligner, UALIGN, outperforms both the commonly used GIZA++ aligner and the state-of-the-art LEAF aligner on F-measure and pro-duces superior scores in end-to-end sta-tistical machine translation, +1.3 BLEU points over GIZA++, and +0.7 over LEAF.","With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG ) .","['Our reordering model is closely related to the model proposed by Zhang and Gildea (2005;2007a), with respect to conditioning the reordering predictions on lexical items.', 'These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG ) .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.']",1,"['With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG ) .']"
CC88,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,ordering phrases with function words,"['Hendra Setiawan', 'Min-Yen Kan', 'Haizhou Li']",,"This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios.","To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG .","['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG .', 'Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).', 'In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG).', 'The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro-This heuristic is commonly used in learning phrase pairs from parallel text.', 'The maximality ensures the uniqueness of L and R.']",5,"['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG .']"
CC89,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,topological ordering of function words in hierarchical phrasebased translation,"['Hendra Setiawan', 'Min Yen Kan', 'Haizhou Li', 'Philip Resnik']",,"Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance.","The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .","['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']",2,"['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .']"
CC90,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,topological ordering of function words in hierarchical phrasebased translation,"['Hendra Setiawan', 'Min Yen Kan', 'Haizhou Li', 'Philip Resnik']",,"Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance.","To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .","['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the form']",5,"['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .']"
CC91,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",related work,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","In our previous work ( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .","['In our previous work ( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that is motivated by previous work on textual entailment.', 'We use clauses in the logic-based approaches as the underlying representation of our system.', 'Based on this representation, we apply a two stage entailment process similar to MacCartney et al. (2006) developed for textual entailment: an alignment stage followed by an entailment stage.']",2,"['In our previous work ( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .']"
CC92,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",method,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .","['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']",2,"['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']"
CC93,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",experiments,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","Note that in our original work ( #AUTHOR_TAG ) , only development data were used to show some initial observations.","['Note that in our original work ( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']",1,"['Note that in our original work ( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']"
CC94,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",method,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.",This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG ) .,"['This string representation of paths is used to capture both the subject consistency and the object consistency.', 'Since they are non-numerical features, and the variability of their values can be extremely large, so we applied an instance-based classification model (e.g., k-nearest neighbor) to determine alignments between verb terms.', 'We measure the distance between two path features by their minimal string edit distance, and then simply use the Euclidean distance to measure the closeness between any two verbs.', 'Again this model is trained from our development data described in Zhang and Chai (2009).', 'Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG ) .']",5,['This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG ) .']
CC95,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",introduction,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .","['To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', 'For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', 'While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.', 'It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome.']",2,"['To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .']"
CC96,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","In our previous work ( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .","['In our previous work ( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses.']",2,"['In our previous work ( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .']"
CC97,D10-1100,Automatic Detection of Micro-Arousals,convolution kernels on constituent dependency and sequential structures for relation extraction,"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']",,"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art.","We use the structures previously used by #AUTHOR_TAG , and propose one new structure .","['Linear learning machines are one of the most popular machines used for classification problems.', 'The objective of a typical classification problem is to learn a function that separates the data into different classes.', 'The data is usually in the form of features extracted from abstract objects like strings, trees, etc.', 'A drawback of learning by using complex functions is that complex functions do not generalize well and thus tend to over-fit.', 'The research community therefore prefers linear classifiers over other complex classifiers.', 'But more often than not, the data is not linearly separable.', 'It can be made linearly separable by increasing the dimensionality of data but then learning suffers from the curse of dimensionality and classification becomes computationally intractable.', 'This is where kernels come to the rescue.', 'The well-known kernel trick aids us in finding similarity between feature vectors in a high dimensional space without having to write down the expanded feature space.', 'The essence of kernel methods is that they compare two feature vectors in high dimensional space by using a dot product that is a function of the dot product of feature vectors in the lower dimensional space.', 'Moreover, Convolution Kernels (first introduced by Haussler (1999)) can be used to compare abstract objects instead of feature vectors.', 'This is because these kernels involve a recursive calculation over the ""parts"" of a discrete structure.', 'This calculation is usually made computationally efficient using Dynamic Programming techniques.', 'Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data).', 'Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task.', 'Now we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by #AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).', 'For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).', 'We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs.', 'Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities.', 'Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities.', 'In Figure 1, since the target entities are at the leftmost and rightmost branch of the depen- 3 We omitted SK6, which is the worst performing sequence kernel in (Nguyen et al., 2009).', 'Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corresponding parent in DW, we get a GR tree.', 'For example, in Figure 1, replacing Toujan Faisal by nsubj, 54 by appos, she by nsubjpass and so on.', 'Grammatical Relation Word (GRW) tree: We get this tree by adding the grammatical relations as separate nodes between a node and its parent.', 'For example, in Figure 1, adding nsubj as a node between T1-Individual and Toujan Faisal, appos as a node between 54 and Toujan Faisal, and so on.', 'Sequence Kernel of words (SK1): This is the sequence of words between the two entities, including their tags.', 'For our example in Figure 1, it would be T1-Individual Toujan Faisal 54 said she was informed of the refusal by an T2-Group Interior Ministry committee.', 'Sequence in GRW tree (SqGRW): This is the new structure that we introduce which, to the best of our knowledge, has not been used before for similar tasks.', 'It is the sequence of nodes from one target to the other in the GRW tree.', 'For example, in Figure 1, this would be Toujan Faisal nsubj T1-Individual said ccomp informed prep by T2-Group pobj committee.']",5,"['We use the structures previously used by #AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).', 'For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).', 'We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs.', 'Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities.', 'Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities.', 'In Figure 1, since the target entities are at the leftmost and rightmost branch of the depen- 3 We omitted SK6, which is the worst performing sequence kernel in (Nguyen et al., 2009).', 'Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corresponding parent in DW, we get a GR tree.']"
CC98,D10-1100,Automatic Detection of Micro-Arousals,convolution kernels on constituent dependency and sequential structures for relation extraction,"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']",conclusion,"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art.",This revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .,"['In this paper, we have introduced the novel tasks of social event detection and classification.', 'We show that data sampling techniques play a crucial role for the task of relation detection.', 'Through oversampling we achieve an increase in F1-measure of 22.2% absolute over a baseline system.', 'Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information.', 'Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best.', 'This revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', 'We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks.']",1,['This revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .']
CC99,D10-1100,Automatic Detection of Micro-Arousals,convolution kernels on constituent dependency and sequential structures for relation extraction,"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']",experiments,"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art.","Here , the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where GR performed much worse than PET for ACE data .","['Social event detection is the task of detecting if any social event exists between a pair of entities in a sentence.', 'We formulate the problem as a binary classification task by labeling an example that does not have a social event as class -1 and by labeling an example that either has an INR or COG social event as class 1.', 'First we present results for our baseline system.', 'Our baseline system uses various structures and their combinations but without any data balancing.', '1 presents results for our baseline system.', 'Grammatical relation tree structure (GR), a structure derived from dependency tree by replacing the words by their grammatical relations achieves the best precision.', 'This is probably because the clas-sifier learns that if both the arguments of a predicate contain target entities then it is a social event.', 'Among kernels for single structures, the path enclosed tree for PSTs (PET) achieves the best recall.', 'Furthermore, a combination of structures derived from PSTs and DTs performs best.', 'The sequence kernels, perform much worse than SqGRW (F1-measure as low as 0.45).', 'Since it is the same case for all subsequent experiments, we omit them from the discussion.', 'We now turn to experiments involving sampling.', 'Table 2 presents results for under-sampling, i.e. randomly removing examples belonging to the negative class until its size matches the positive class.', 'Table 2 shows a large gain in F1-measure of 9.72% absolute over the baseline system (Table 1).', 'We found that worst performing kernel with under-sampling is SK1 with an F1-measure of 39.2% which is better than the best performance without undersampling.', 'These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples).', 'This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy.', 'absolute.', 'As in the baseline system, a combination of structures performs best.', 'As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall.', 'Here , the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where GR performed much worse than PET for ACE data .', 'This exemplifies the difference in the nature of our event annotations from that of ACE relations.', 'Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.', 'This means that implicit feature space is much sparser and thus not the best representation.', '4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are ""close"" to the original examples.', 'This method achieves a gain 16.78% over the baseline system.', 'We expected this system to perform better than the over-sampled system but it does not.', 'This suggests that our over-sampled system is not over-fitting; a concern with using oversampling techniques.']",1,"['Here , the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where GR performed much worse than PET for ACE data .']"
CC100,D10-1101,Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields,biographies bollywood boomboxes and blenders domain adaptation for sentiment classification,"['John Blitzer', 'Mark Dredze', 'Fernando Pereira']",experiments,"Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.","Our results also confirm the insights gained by #AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .","['Our results also confirm the insights gained by #AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.']",1,"['Our results also confirm the insights gained by #AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .']"
CC101,D10-1101,Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields,instance weighting for domain adaptation in nlp,"['Jing Jiang', 'ChengXiang Zhai']",conclusion,"Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.","For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .","['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']",3,"['For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .']"
CC102,D10-1101,Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields,biographies bollywood boomboxes and blenders domain adaptation for sentiment classification,"['John Blitzer', 'Mark Dredze', 'Fernando Pereira']",conclusion,"Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.","For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .","['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']",3,"['For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']"
CC103,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,algorithms for deterministic incremental dependency parsing,['J Nivre'],experiments,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.",An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.,"['An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word iden- tity of the syntactic head of the top word on the stack (if available); dependency arc label iden- tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunc- tions are included.']",5,['An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.']
CC104,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,structured output learning with indirect supervision,"['M Chang', 'D Goldwasser', 'D Roth', 'V Srikumar']",introduction,"We present a novel approach for structure prediction that addresses the difficulty of obtaining labeled structures for training. We observe that structured output problems often have a companion learning problem of determining whether a given input possesses a good structure. For example, the companion problem for the part-of-speech (POS) tagging task asks whether a given sequence of words has a corresponding sequence of POS tags that is ""legitimate"". While obtaining direct supervision for structures is difficult and expensive, it is often very easy to obtain indirect supervision from the companion binary decision problem.    In this paper, we develop a large margin framework that jointly learns from both direct and indirect forms of supervision. Our experiments exhibit the significant contribution of the easy-to-get indirect binary supervision on three important NLP structure learning problems.","This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ) .","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']",0,"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.']"
CC105,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,kbest spanning tree parsing,['K Hall'],experiments,"This paper introduces a Maximum Entropy dependency parser based on an efficient kbest Maximum Spanning Tree (MST) algorithm. Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy, we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers. This motivates our parsing approach, which is based on reranking the kbest parses generated by an edge-factored model. Oracle parse accuracy results are presented for the edge-factored model and 1-best results for the reranker on eight languages (seven from CoNLL-X and English).","We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .","['• Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (McDonald et al., 2005).', 'We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .', 'The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w ρ(i) where ρ(i) provides the index of the head word; and partof-speech tags of these words t i .', 'We use the following set of features similar to McDonald et al. ( 2005):']",5,"['* Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (McDonald et al., 2005).', 'We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .']"
CC106,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,uptraining for accurate deterministic question parsing,"['S Petrov', 'P C Chang', 'M Ringgaard', 'H Alshawi']",experiments,"It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.",#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .,"['Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'The question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'Root-F1 scores from Table 2 suggest that one simple question is ""what is the main verb of this sentence?""', 'for sentences that are questions.', 'In most cases this task is straight-forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'We feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data.']",1,['#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .']
CC107,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,using a dependency parser to improve smt for subjectobjectverb languages in,"['P Xu', 'J Kang', 'M Ringgaard', 'F Och']",experiments,,1Our rules are similar to those from #AUTHOR_TAG .,['1Our rules are similar to those from #AUTHOR_TAG .'],1,['1Our rules are similar to those from #AUTHOR_TAG .']
CC108,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,dependency treebased sentiment classification using crfs with hidden variables,"['T Nakagawa', 'K Inui', 'S Kurohashi']",introduction,"In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features.","This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( #AUTHOR_TAG ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( #AUTHOR_TAG ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( #AUTHOR_TAG ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .']"
CC109,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,online largemargin training of dependency parsers,"['R McDonald', 'K Crammer', 'F Pereira']",experiments,"We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.",An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG ) .,"['An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG ) .', 'We use the non-projective k-best MST algorithm to generate k-best lists (Hall, 2007), where k = 8 for the experiments in this paper.', 'The graph- based parser features used in the experiments in this paper are defined over a word, wi at po- sition i; the head of this word w_(i) where _(i) provides the index of the head word; and part- of-speech tags of these words ti.', 'We use the following set of features similar to McDonald et al. (2005):']",5,['An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG ) .']
CC110,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,reranking and selftraining for parser adaptation,"['D McClosky', 'E Charniak', 'M Johnson']",related work,,"The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .","['A recent study by  also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system.', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']",1,"['A recent study by  also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.']"
CC111,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,meteor an automatic metric for mt evaluation with improved correlation with human judgments,"['S Banerjee', 'A Lavie']",experiments,"We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.","Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure .","['In our experiments we work with a set of English-Japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'We use a reordering score based on the reordering penalty from the METEOR scoring metric.', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure .', 'Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates.']",4,"['Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure .']"
CC112,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,using a dependency parser to improve smt for subjectobjectverb languages in,"['P Xu', 'J Kang', 'M Ringgaard', 'F Och']",introduction,,"This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( #AUTHOR_TAG ) , and many other tasks .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( #AUTHOR_TAG ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( #AUTHOR_TAG ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.']"
CC113,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,a tale of two parsers investigating and combining graphbased and transitionbased dependency parsing,"['Y Zhang', 'S Clark']",experiments,"Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beam-search. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.","• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.","['• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-ofspeech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunctions are included.']",5,"['• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.']"
CC114,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,domain adaptation with structural correspondence learning,"['J Blitzer', 'R McDonald', 'F Pereira']",introduction,"Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.","In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.']"
CC115,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,building a large annotated corpus of english the penn treebank computational linguistics,"['M Marcus', 'B Santorini', 'M A Marcinkiewicz']",experiments,,"In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG ) .","['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG ) .', 'We also make use of the Brown corpus, and the Question Treebank (QTB) (Judge et al., 2006']",5,"['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG ) .']"
CC116,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,reranking and selftraining for parser adaptation,"['D McClosky', 'E Charniak', 'M Johnson']",introduction,,"In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .']"
CC117,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,guiding semisupervision with constraintdriven learning,"['M W Chang', 'L Ratinov', 'D Roth']",related work,,"The work that is most similar to ours is that of #AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .","['The work that is most similar to ours is that of #AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented-loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented-loss functions directly (rather than adding a set of examples to the training set).', 'Unlike the CODL approach, we do not perform complete optimization on each iteration over the unlabeled dataset; rather, we incorporate the updates in our online learning algorithm.', 'As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include Mann and Mc-Callum (2010) and Ganchev et al. (2010).', 'Again, these works are typically interested in using the extrinsic metric -or, in general, extrinsic information -to optimize the intrinsic metric in the absence of any labeled intrinsic data.', 'Our goal is to optimize both simultaneously.']",1,"['The work that is most similar to ours is that of #AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .']"
CC118,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,an endtoend discriminative approach to machine translation,"['P Liang', 'A Bouchard-Ct', 'D Klein', 'B Taskar']",related work,,#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .,"['A recent study by  also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']",1,['#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .']
CC119,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,training a parser for machine translation reordering,"['J Katz-Brown', 'S Petrov', 'R McDonald', 'D Talbot', 'F Och', 'H Ichikawa', 'M Seno', 'H Kazawa']",related work,"We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.",A recent study by #AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .,"['A recent study by #AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system.', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']",1,['A recent study by #AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .']
CC120,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,unsupervised methods for determining object and relation synonyms on the web,"['A Yates', 'O Etzioni']",experiments,"The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of RESOLVER's probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic RESOLVER system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus.","Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .","['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .']",0,"['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .']"
CC121,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,discriminative training methods for hidden markov models theory and experiments with perceptron algorithms,['M Collins'],introduction,"We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random elds (CRFs). The al-gorithms rely on Viterbi decoding of training examples, combined with sim-ple additive updates. We describe the-ory justifying the algorithms through a modication of the proof of conver-gence of the perceptron algorithm for classi cation problems. We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.","Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG , by inserting in loss-separability for normal separability .","['A training set D is loss-separable with margin γ > 0 if there exists a vector u with u = 1 such that for all y , y ∈ Y x and (x, y) ∈ D, if L(y , y) < L(y , y), then u•Φ(y )−u•Φ(y ) ≥ γ.', 'Furthermore, let R ≥ ||Φ(y) − Φ(y )||, for all y, y .', 'Assumption 1. Assume training set D is lossseparable with margin γ.', 'Theorem 1.', 'Given Assumption 1.', 'Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y) ∈ D with parameter vector θ when ∃ŷ j ∈ F k-best θ (x) wherê y j =ŷ 1 and L(ŷ j , y) < L(ŷ 1 , y).', 'If training is run indefinitely, then m ≤ R 2 γ 2 .', 'Proof.', 'Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG , by inserting in loss-separability for normal separability .']",0,"['A training set D is loss-separable with margin g > 0 if there exists a vector u with u = 1 such that for all y , y  Y x and (x, y)  D, if L(y , y) < L(y , y), then u*Ph(y )-u*Ph(y ) >= g.', 'Furthermore, let R >= ||Ph(y) - Ph(y )||, for all y, y .', 'Assumption 1. Assume training set D is lossseparable with margin g.', 'Theorem 1.', 'Given Assumption 1.', 'Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y)  D with parameter vector th when y j  F k-best th (x) where y j =y 1 and L(y j , y) < L(y 1 , y).', 'If training is run indefinitely, then m <= R 2 g 2 .', 'Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG , by inserting in loss-separability for normal separability .']"
CC122,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,characterizing the errors of datadriven dependency parsing models,"['R McDonald', 'J Nivre']",experiments,"We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development.","Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).","['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).']",4,"['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).']"
CC123,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,corpus variation and parser performance,['D Gildea'],introduction,"Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model.","In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .']"
CC124,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,discriminative reranking for natural language parsing,['Michael Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation",One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ) .,"['One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extrinsic objective.', 'While this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser.']",0,"['One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.']"
CC125,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,global learning of focused entailment graphs,"['J Berant', 'I Dagan', 'J Goldberger']",experiments,"We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms.","Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG ) .","['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG ) .']",0,"['Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG ) .']"
CC126,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,guiding semisupervision with constraintdriven learning,"['M W Chang', 'L Ratinov', 'D Roth']",introduction,,"This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']",1,"['This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.']"
CC127,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,algorithms for deterministic incremental dependency parsing,['J Nivre'],experiments,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.",An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .,"['An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunctions are included.']",5,['An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .']
CC128,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,generalized expectation criteria for semisupervised learning with weakly labeled data,"['G S Mann', 'A McCallum']",introduction,"In this paper, we present an overview of generalized expectation criteria (GE), a simple, robust, scalable method for semi-supervised training using weakly-labeled data. GE fits model parameters by favoring models that match certain expectation constraints, such as marginal label distributions, on the unlabeled data. This paper shows how to apply generalized expectation criteria to two classes of parametric models: maximum entropy models and conditional random fields. Experimental results demonstrate accuracy improvements over supervised training and a number of other state-of-the-art semi-supervised learning methods for these models.","This includes work on generalized expectation ( #AUTHOR_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( #AUTHOR_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']",0,"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( #AUTHOR_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.']"
CC129,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,conllx shared task on multilingual dependency parsing,"['S Buchholz', 'E Marsi']",experiments,"Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?",For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .,"['In the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one.', 'Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score.', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .']",5,['For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .']
CC130,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,uptraining for accurate deterministic question parsing,"['S Petrov', 'P C Chang', 'M Ringgaard', 'H Alshawi']",introduction,"It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.","In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.']"
CC131,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,what is the jeopardy model a quasisynchronous grammar for qa,"['M Wang', 'N A Smith', 'T Mitamura']",introduction,"This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimentalresultsusing theTRECdataset are shown to significantly outperform strong state-of-the-art baselines.","This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"
CC132,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,a lightweight evaluation framework for machine translation reordering,"['D Talbot', 'H Kazawa', 'H Ichikawa', 'J Katz-Brown', 'M Seno', 'F Och']",experiments,Reordering is a major challenge for machine translation between distant languages. Recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality. Here we present a simple framework for evaluating word order independently of lexical choice by comparing the system's reordering of a source sentence to reference reordering data generated from manually word-aligned translations. When used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming end-to-end machine translation experiments. A novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion. We show that how the alignments are generated can significantly effect the robustness of the evaluation. We also outline some ways in which this framework has allowed our group to analyze reordering errors for English to Japanese machine translation.,criteria and data used in our experiments are based on the work of #AUTHOR_TAG .,['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .'],5,['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']
CC133,D12-1037,Discriminative Training for Log-Linear Based SMT,svmknn discriminative nearest neighbor classification for visual category recognition,"['Hao Zhang', 'Alexander C Berg', 'Michael Maire', 'Jitendra Malik']",introduction,"We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves the distance function on the collection of neighbors. Our method can be applied to large, multiclass data sets for which it outperforms nearest neighbor and support vector machines, and remains efficient when the problem becomes intractable for support vector machines. A wide variety of distance functions can be used and our experiments show state-of-the-art performance on a number of benchmark data sets for shape and texture classification (MNIST, USPS, CUReT) and object recognition (Caltech- 101). On Caltech-101 we achieved a correct classification rate of 59.05%(+-0.56%) at 15 training images per class, and 66.23%(+-0.48%) at 30 training images.","In this paper , inspired by KNN-SVM ( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .","['In this paper , inspired by KNN-SVM ( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', 'Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set.']",4,"['In this paper , inspired by KNN-SVM ( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', 'Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.']"
CC134,D12-1037,Discriminative Training for Log-Linear Based SMT,minimum error rate training in statistical machine translation,['Franz Josef Och'],introduction,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.","Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\u03b1W \u00b7 h(fj, e)] P\u03b1(e|fj; W) = (7) Ee'Ec; exp[\u03b1W \u00b7 h(fj, e')], where \u03b1 > 0 is a real number valued smoother.","['Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', ""Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother."", 'One can see that, in the extreme case, for α —* oc, (6) converges to (5).']",4,"[""Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\\u2212WbII2+ A \\ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\\u03b1W \\u00b7 h(fj, e)] P\\u03b1(e|fj; W) = (7) Ee'Ec; exp[\\u03b1W \\u00b7 h(fj, e')], where \\u03b1 > 0 is a real number valued smoother.""]"
CC135,D12-1037,Discriminative Training for Log-Linear Based SMT,discriminative training and maximum entropy models for statistical machine translation,"['Franz Josef Och', 'Hermann Ney']",introduction,"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.","#AUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :","['#AUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']",0,"['#AUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']"
CC136,D12-1037,Discriminative Training for Log-Linear Based SMT,examplebased decoding for statistical machine translation,"['Taro Watanabe', 'Eiichiro Sumita']",related work,,"Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .","['Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', 'Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.']",1,"['Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', 'Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.']"
CC137,D12-1037,Discriminative Training for Log-Linear Based SMT,nearoptimal hashing algorithms for approximate nearest neighbor in high dimensions,"['Alexandr Andoni', 'Piotr Indyk']",experiments,"We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice","Actually , if we use LSH technique ( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .","['Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 .', 'This shows that the local method is efficient.', 'Further, compared to the retrieval, the local training is not the bottleneck.', 'Actually , if we use LSH technique ( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .']",3,"['Actually , if we use LSH technique ( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .']"
CC138,D12-1037,Discriminative Training for Log-Linear Based SMT,optimal search for minimum error rate training,"['Michel Galley', 'Chris Quirk']",related work,"Abstract Minimum error rate training is a crucial component to many state-of-the-art NLP applications, such as machine translation and speech recognition. However, common evaluation functions such as BLEU or word error rate are generally highly non-convex and thus prone to search errors. In this paper, we present LP-MERT, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming. Given a set of N -best lists produced from S input sentences, this algorithm finds a linear model that is globally optimal with respect to this set. We find that this algorithm is polynomial in N and in the size of the model, but exponential in S. We present extensions of this work that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm","(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza- tion objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.']"
CC139,D12-1037,Discriminative Training for Log-Linear Based SMT,local learning algorithms,"['L´eon Bottou', 'Vladimir Vapnik']",introduction,"The goal of digital image processing is to capture, transmit, and display images as efficiently as possible. Such tasks are computationally intensive because an image is digitally represented by large amounts of data. It is possible to render an image by reconstructing it with a subset of the most relevant data. One such procedure used to accomplish this task is commonly referred to as sparse coding. For our purpose, we use images of handwritten digits that are presented to an artificial neural network. The network implements Rozell u27s locally competitive algorithm (LCA) to generate a sparse code. This sparse code is then presented to another neural network, a classifier that attempts to place the image in one of ten categories, each representing one of the digits zero through nine. Furthermore, the LCA approach is unique in that it produces quality sparse codes by utilizing highly parallel architectures. Pattern recognition problems have been of interest by industries that rely heavily on data as a core part of their business. Social networking companies use it to analyze, predict, and even influence user behavior. However, as data becomes more cost-effective to collect, it will be important for companies in other industries to extract useful information from said data. Manufacturing companies use it to analyze the performance of their products and financial service companies use it to flag customers likely to default on their loans. Interestingly, the image processing techniques described above can be generalized for use on data other than image data","The local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .","['The local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', 'Compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'It is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006).']",0,"['The local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', 'Compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.']"
CC140,D12-1037,Discriminative Training for Log-Linear Based SMT,a simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters,"['Bing Zhao', 'Shengyuan Chen']",introduction,"We propose a variation of simplex-downhill algo-rithm specifically customized for optimizing param-eters in statistical machine translation (SMT) de-coder for better end-user automatic evaluation met-ric scores for translations, such as versions of BLEU, TER and mixtures of them. Traditional simplex-downhill has the advantage of derivative-free com-putations of objective functions, yet still gives satis-factory searching directions in most scenarios. This is suitable for optimizing translation metrics as they are not differentiable in nature. On the other hand, Armijo algorithm usually performs line search ef-ficiently given a searching direction. It is a deep hidden fact that an efficient line search method will change the iterations of simplex, and hence the searching trajectories. We propose to embed the Armijo inexact line search within the simplex-downhill algorithm. We show, in our experiments, the proposed algorithm improves over the widely-applied Minimum Error Rate training algorithm for optimizing machine translation parameters.","Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .","['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']",0,"['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.']"
CC141,D12-1037,Discriminative Training for Log-Linear Based SMT,an empirical study of smoothing techniques for language modeling in,"['Stanley F Chen', 'Joshua Goodman']",experiments,"We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t...","We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( #AUTHOR_TAG ) .","['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( #AUTHOR_TAG ) .', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']",5,"['We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( #AUTHOR_TAG ) .']"
CC142,D12-1037,Discriminative Training for Log-Linear Based SMT,srilm  an extensible language modeling toolkit,['Andreas Stolcke'],experiments,"SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools. 1","We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .","['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']",5,"['We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .']"
CC143,D12-1037,Discriminative Training for Log-Linear Based SMT,optimal search for minimum error rate training,"['Michel Galley', 'Chris Quirk']",introduction,"Abstract Minimum error rate training is a crucial component to many state-of-the-art NLP applications, such as machine translation and speech recognition. However, common evaluation functions such as BLEU or word error rate are generally highly non-convex and thus prone to search errors. In this paper, we present LP-MERT, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming. Given a set of N -best lists produced from S input sentences, this algorithm finds a linear model that is globally optimal with respect to this set. We find that this algorithm is polynomial in N and in the size of the model, but exponential in S. We present extensions of this work that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm","Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .","['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']",0,"['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.']"
CC144,D12-1037,Discriminative Training for Log-Linear Based SMT,incremental and decremental support vector machine learning,"['G Cauwenberghs', 'T Poggio']",,"An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental ""unlearning"" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.","In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .","['Compared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'In this section, we will investigate the incremental trainingmethodsinSMTscenario.']",0,"['In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .']"
CC145,D12-1037,Discriminative Training for Log-Linear Based SMT,a simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters,"['Bing Zhao', 'Shengyuan Chen']",related work,"We propose a variation of simplex-downhill algo-rithm specifically customized for optimizing param-eters in statistical machine translation (SMT) de-coder for better end-user automatic evaluation met-ric scores for translations, such as versions of BLEU, TER and mixtures of them. Traditional simplex-downhill has the advantage of derivative-free com-putations of objective functions, yet still gives satis-factory searching directions in most scenarios. This is suitable for optimizing translation metrics as they are not differentiable in nature. On the other hand, Armijo algorithm usually performs line search ef-ficiently given a searching direction. It is a deep hidden fact that an efficient line search method will change the iterations of simplex, and hence the searching trajectories. We propose to embed the Armijo inexact line search within the simplex-downhill algorithm. We show, in our experiments, the proposed algorithm improves over the widely-applied Minimum Error Rate training algorithm for optimizing machine translation parameters.","( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .']"
CC146,D12-1037,Discriminative Training for Log-Linear Based SMT,random restarts in minimum error rate training for statistical machine translation,"['Robert C Moore', 'Chris Quirk']",related work,"Och's (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its benefits. We compare several ways of performing random restarts with MERT. We find that all of our random restart methods outperform MERT without random restarts, and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time.","( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .']"
CC147,D12-1037,Discriminative Training for Log-Linear Based SMT,statistical significance tests for machine translation evaluation,['Philipp Koehn'],experiments,"If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.",The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .,"['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .']",5,"['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .']"
CC148,D12-1037,Discriminative Training for Log-Linear Based SMT,tuning as ranking,"['Mark Hopkins', 'Jonathan May']",,"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.","(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG, 2011).","['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG, 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'Since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'However, one can obtain e11 and e21 with weights: (1, 1) and (−1, 1), respectively.']",0,"['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG, 2011).']"
CC149,D12-1037,Discriminative Training for Log-Linear Based SMT,discriminative training and maximum entropy models for statistical machine translation,"['Franz Josef Och', 'Hermann Ney']",related work,"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.","( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.","['Several works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['Several works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.']"
CC150,D12-1037,Discriminative Training for Log-Linear Based SMT,examplebased decoding for statistical machine translation,"['Taro Watanabe', 'Eiichiro Sumita']",,,"To retrieve translation examples for a test sentence , ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :","['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']",5,"['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']"
CC151,D12-1037,Discriminative Training for Log-Linear Based SMT,minimum error rate training in statistical machine translation,['Franz Josef Och'],introduction,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.","Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .","['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']",0,"['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.']"
CC152,D12-1037,Discriminative Training for Log-Linear Based SMT,ultraconservative online algorithms for multiclass problems,"['Koby Crammer', 'Yoram Singer']",,"In this paper we study a paradigm to generalize online classification algorithms for binary classification problems to multiclass problems. The particular hypotheses we investigate maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultraconservativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems. We discuss the form the algorithms take in the binary case and show that all the algorithms from the first family reduce to the Perceptron algorithm while MIRA provides a new Perceptron-like algorithm with a margin-dependent learning rate. We then return to multiclass problems and describe an analogous multiplicative family of algorithms with corresponding mistake bounds. We end the formal part by deriving and analyzing a multiclass version of Li and Long's ROMMA algorithm. We conclude with a discussion of experimental results that demonstrate the merits of our algorithms.","We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .","['Following the notations in Algorithm 2, W b is the baseline weight, D i = { f i j , c i j , r i j } K j=1 denotes training examples for t i .', 'For the sake of brevity, we will drop the index i, D i = { f j , c j , r j } K j=1 , in the rest of this paper.', 'Our goal is to find an optimal weight, denoted by W i , which is a local weight and used for decoding the sentence t i .', 'Unlike the global method which performs tuning on the whole development set Dev + D i as in Algorithm 1, W i can be incrementally learned by optimizing on D i based on W b .', 'We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .']",5,"['We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .']"
CC153,D12-1037,Discriminative Training for Log-Linear Based SMT,minimum error rate training in statistical machine translation,['Franz Josef Och'],related work,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.","( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .","['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .']"
CC154,D12-1037,Discriminative Training for Log-Linear Based SMT,a hierarchical phrasebased model for statistical machine translation,['David Chiang'],experiments,,"We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .","['We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-    (Koehn et al., 2007).', 'Both of these systems are with default setting.', 'All three systems are trained by MERT with 100 best candidates.']",5,"['We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .']"
CC155,D12-1037,Discriminative Training for Log-Linear Based SMT,improved statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",experiments,"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen","We run GIZA + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .","['We run GIZA + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']",5,"['We run GIZA + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']"
CC156,D12-1037,Discriminative Training for Log-Linear Based SMT,tuning as ranking,"['Mark Hopkins', 'Jonathan May']",related work,"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.","( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .","['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']",1,"['( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']"
CC157,D12-1037,Discriminative Training for Log-Linear Based SMT,tuning as ranking,"['Mark Hopkins', 'Jonathan May']",introduction,"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.","Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .","['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']",0,"['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']"
CC158,D13-1038,Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction,the tunareg challenge 2009 overview and evaluation results,"['Albert Gatt', 'Anja Belz', 'Eric Kow']",experiments,,"Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .","['Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .', ""However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to  the situation where the agent's representation of the shared world is problematic and full of mistakes.""]",1,"['Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .']"
CC159,D13-1038,Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction,towards mediating shared perceptual basis in situated dialogue,"['Changsong Liu', 'Rui Fang', 'Joyce Y Chai']",,"To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction.","Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .","['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990).', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .', 'For the referring expression generation task here, we also need a lexicon with grounded semantics.']",2,"['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990).', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .']"
CC160,D13-1038,Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction,towards mediating shared perceptual basis in situated dialogue,"['Changsong Liu', 'Rui Fang', 'Joyce Y Chai']",introduction,"To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction.",How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ) .,"['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ) .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.', 'Robots have much lower perceptual capabilities of the environment than humans.', 'How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?']",2,['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ) .']
CC161,D13-1115,Integrating Theory and Practice: A Daunting Task,combining feature norms and text data with topic models,['Mark Steyvers'],related work,"Many psychological theories of semantic cognition assume that concepts are represented by features. The empirical procedures used to elicit features from humans rely on explicit human judgments which limit the scope of such representations. An alternative computational framework for semantic cognition that does not rely on explicit human judgment is based on the statistical analysis of large text collections. In the topic modeling approach, documents are represented as a mixture of learned topics where each topic is represented as a probability distribution over words. We propose feature-topic models, where each document is represented by a mixture of learned topics as well as predefined topics that are derived from feature norms. Results indicate that this model leads to systematic improvements in generalization tasks. We show that the learned topics in the model play in an important role in the generalization performance by including words that are not part of current feature norms.2009 Elsevier B.V. All rights reserved.","In a similar vein , #AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein , #AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['In a similar vein , #AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"
CC162,D13-1115,Integrating Theory and Practice: A Daunting Task,training a multilingual sportscaster using perceptual context to learn language,"['David L Chen', 'Joohyun Kim', 'Raymond J Mooney']",related work,We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.,"Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat- egory, the two most common representations have been association norms, where subjects are given a cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).']",0,"['Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .']"
CC163,D13-1115,Integrating Theory and Practice: A Daunting Task,distinctive image features from scaleinvariant keypoints,['David G Lowe'],related work,This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ...,They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .']"
CC164,D13-1115,Integrating Theory and Practice: A Daunting Task,describing objects by their attributes,"['Ali Farhadi', 'Ian Endres', 'Derek Hoiem', 'David Forsyth']",related work,"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (""spotty dog"", not just ""dog""); to say something about unfamiliar objects (""hairy and four-legged"", not just ""unknown""); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (""spotty"") or discriminative (""dogs have it but sheep do not""). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. 1","More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG ) , act as excellent substitutes for feature","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG ) , act as excellent substitutes for feature']"
CC165,D13-1115,Integrating Theory and Practice: A Daunting Task,simple supervised document geolocation with geodesic grids,"['Benjamin Wing', 'Jason Baldridge']",introduction,"We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document's raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset.","Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 ) .']",0,"['Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 ) .']"
CC166,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",introduction,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']"
CC167,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",related work,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","That is, we simply take the original mLDA model of #AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.","['That is, we simply take the original mLDA model of #AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', 'At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi.']",2,"['That is, we simply take the original mLDA model of #AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', 'At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi.']"
CC168,D13-1115,Integrating Theory and Practice: A Daunting Task,learning language semantics from ambiguous supervision,"['Rohit J Kate', 'Raymond J Mooney']",introduction,"This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.","Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ) .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a;Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012).']",0,"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ) .']"
CC169,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",method,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.",#AUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .,"['#AUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'Topics consist of multinomial distributions over words, β k , but are extended to also include multinomial distributions over features, ψ k .', 'The generative process is amended to include these feature distributions:']",0,"['#AUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'Topics consist of multinomial distributions over words, b k , but are extended to also include multinomial distributions over features, ps k .', 'The generative process is amended to include these feature distributions:']"
CC170,D13-1115,Integrating Theory and Practice: A Daunting Task,webscale kmeans clustering,['D Sculley'],experiments,,"The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .","['First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (Bay et al., 2008).', 'SURF is a method for selecting points-of-interest within an image.', 'It is faster and more forgiving than the commonly known SIFT algorithm.', 'We compute SURF keypoints for every image in our data set using Sim-pleCV 3 and randomly sample 1% of the keypoints.', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .', 'All images for a given word are summed together to provide an average representation for the word.', 'We refer to this representation as the SURF modality.']",5,"['The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .']"
CC171,D13-1115,Integrating Theory and Practice: A Daunting Task,grounded models of semantic representation,"['Carina Silberer', 'Mirella Lapata']",experiments,"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.","This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .']",1,"['This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .']"
CC172,D13-1115,Integrating Theory and Practice: A Daunting Task,grounding action descriptions in videos,"['Michaela Regneri', 'Marcus Rohrbach', 'Dominikus Wetzel', 'Stefan Thater', 'Bernt Schiele', 'Manfred Pinkal']",related work,"Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information ex-tracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substan-tially when combined with visual information from videos depicting the described actions.","Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG ) .","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG ) .']",0,"['Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG ) .']"
CC173,D13-1115,Integrating Theory and Practice: A Daunting Task,semantic feature production norms for a large set of living and nonliving things,"['Ken McRae', 'George S Cree', 'Mark S Seidenberg', 'Chris McNorgan']",related work,"Semantic features have provided insight into numerous behavioral phenomena concerning concepts, categorization, and semantic memory in adults, children, and neuropsychological populations. Numerous theories and models in these areas are based on representations and computations involving semantic features. Consequently, empirically derived semantic feature production norms have played, and continue to play, a highly useful role in these domains. This article describes a set of feature norms collected from approximately 725 participants for 541 living (dog) and nonliving (chair) basic-level concepts, the largest such set of norms developed to date. This article describes the norms and numerous statistics associated with them. Our aim is to make these norms available to facilitate other research, while obviating the need to repeat the labor-intensive methods involved in collecting and analyzing such norms. The full set of norms may be downloaded from www.psychonomic.org/archive.","Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG ) .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG ) .']",0,"['Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG ) .']"
CC174,D13-1115,Integrating Theory and Practice: A Daunting Task,learning to interpret natural language navigation instructions from observations,"['David L Chen', 'Raymond J Mooney']",introduction,"The ability to understand natural-language instructions is crit-ical to building intelligent agents that interact with humans. We present a system that learns to transform natural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ-ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in-structions for these environments is used for training and test-ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys-tem is able to automatically learn to correctly interpret a rea-sonable fraction of the complex instructions in this corpus.","Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and La- pata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identifica- tion of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012).']",0,"['Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .']"
CC175,D13-1115,Integrating Theory and Practice: A Daunting Task,im2text describing images using 1 million captioned photographs,"['Vicente Ordonez', 'Girish Kulkarni', 'Tamara L Berg']",introduction,"We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset - performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.","Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #AUTHOR_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #AUTHOR_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']",0,"['Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #AUTHOR_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"
CC176,D13-1115,Integrating Theory and Practice: A Daunting Task,models of semantic representation with visual attributes,"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']",introduction,We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.,"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; #AUTHOR_TAG ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; #AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; #AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC177,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",experiments,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","We use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair .","['In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5', 'That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc.', 'The resulting stochastically generated corpus is used in its corresponding experiments.']",5,"['In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5', 'That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc.', 'The resulting stochastically generated corpus is used in its corresponding experiments.']"
CC178,D13-1115,Integrating Theory and Practice: A Daunting Task,relative attributes,"['Devi Parikh', 'Kristen Grauman']",related work,"Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn increasingly complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases. We introduce a deep neural network architecture for the task of relative attribute prediction. A convolutional neural network (ConvNet) is adopted to learn the features by including an additional layer (ranking layer) that learns to rank the images based on these features. We adopt an appropriate ranking loss to train the whole network in an end-to-end fashion. Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various coarse and fine-grained datasets. Our qualitative results along with the visualization of the saliency maps show that the network is able to learn effective features for each specific attribute. Source code of the proposed method is available at https://github.com/yassersouri/ghiaseddin.Comment: ACCV 201","The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"
CC179,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",method,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.",Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .,"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']",5,"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).']"
CC180,D13-1115,Integrating Theory and Practice: A Daunting Task,distributional semantics in technicolor,"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']",experiments,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.",This seems to provide additional evidence of #AUTHOR_TAGb ) 's suggestion that something like a distributional hypothesis of images is plausible .,"['We see that the image modalities are much more useful than they are in compositionality prediction.', 'The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model.', 'Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', ""This seems to provide additional evidence of #AUTHOR_TAGb ) 's suggestion that something like a distributional hypothesis of images is plausible .""]",1,"[""This seems to provide additional evidence of #AUTHOR_TAGb ) 's suggestion that something like a distributional hypothesis of images is plausible .""]"
CC181,D13-1115,Integrating Theory and Practice: A Daunting Task,visual and semantic similarity in imagenet,"['Thomas Deselaers', 'Vittorio Ferrari']",experiments,"Many computer vision approaches take for granted positive answers to questions such as ""Are semantic categories visually separable?"" and ""Is visual similarity correlated to semantic similarity?"". In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances.","It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .","['We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009).', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']",4,"['We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009).', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']"
CC182,D13-1115,Integrating Theory and Practice: A Daunting Task,distributional semantics in technicolor,"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']",related work,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.",#AUTHOR_TAGa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAGa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,['#AUTHOR_TAGa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .']
CC183,D13-1115,Integrating Theory and Practice: A Daunting Task,how many words is a picture worth automatic caption generation for news images,"['Yansong Feng', 'Mirella Lapata']",introduction,"In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.","Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']",0,"['Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"
CC184,D13-1115,Integrating Theory and Practice: A Daunting Task,the wacky wide web a collection of very large linguistically processed webcrawled corpora language resources and evaluation,"['Marco Baroni', 'Silvia Bernardini', 'Adriano Ferraresi', 'Eros Zanchetta']",experiments,,"For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .","['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100.', 'The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens.']",5,"['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .']"
CC185,D13-1115,Integrating Theory and Practice: A Daunting Task,improving video activity recognition using object recognition and text mining,"['Tanvi S Motwani', 'Raymond J Mooney']",related work,"Abstract. Recognizing activities in real-world videos is a chal-lenging AI problem. We present a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling train-ing videos. We cluster verbs used to describe videos to automatically discover classes of activities and produce a labeled training set. This labeled data is then used to train an activity classifier based on spatio-temporal features. Next, text mining is employed to learn the correla-tions between these verbs and related objects. This knowledge is then used together with the outputs of an off-the-shelf object recognizer and the trained activity classifier to produce an improved activity rec-ognizer. Experiments on a corpus of YouTube videos demonstrate the effectiveness of the overall approach.","To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']",0,"['To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']"
CC186,D13-1115,Integrating Theory and Practice: A Daunting Task,perceptual inference through global lexical similarity,"['Brendan T Johns', 'Michael N Jones']",related work,"The literature contains a disconnect between accounts of how humans learn lexical semantic representations for words. Theories generally propose that lexical semantics are learned either through perceptual experience or through exposure to regularities in language. We propose here a model to integrate these two information sources. Specifically, the model uses the global structure of memory to exploit the redundancy between language and perception in order to generate inferred perceptual representations for words with which the model has no perceptual experience. We test the model on a variety of different datasets from grounded cognition experiments and demonstrate that this diverse set of results can be explained as perceptual simulation (cf. Barsalou, Simmons, Barbey, & Wilson, 2003) within a global memory model.Copyright (c) 2012 Cognitive Science Society, Inc.",#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .,"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"
CC187,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",experiments,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","Following #AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .","['Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'Following #AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.', 'We then compute the percentile ranks of similarity for each word pair, e.g., ""cat"" is more similar to ""dog"" than 97.3% of the rest of the vocabulary.', 'We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once.']",5,"['Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'Following #AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.', 'We then compute the percentile ranks of similarity for each word pair, e.g., ""cat"" is more similar to ""dog"" than 97.3% of the rest of the vocabulary.']"
CC188,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",conclusion,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG .","['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.', 'SURF features also provided significant gains over text-only LDA in predicting the compositionality of noun compounds.']",5,"['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.', 'SURF features also provided significant gains over text-only LDA in predicting the compositionality of noun compounds.']"
CC189,D13-1115,Integrating Theory and Practice: A Daunting Task,modeling the shape of the scene a holistic representation of the spatial envelope,"['Aude Oliva', 'Antonio Torralba']",experiments,"In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.","We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .","['We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet.', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']",5,"['We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet.', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']"
CC190,D13-1115,Integrating Theory and Practice: A Daunting Task,supervised textbased geolocation using language models on an adaptive grid,"['Stephen Roller', 'Michael Speriosu', 'Sarat Rallapalli', 'Benjamin Wing', 'Jason Baldridge']",introduction,"The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. One common approach for geolocating texts is rooted in information retrieval. Given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell; then a location for a test document is chosen based on the most similar pseudo-document. Uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. We define an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. We also provide a better way of choosing the locations for pseudo-documents. We evaluate these strategies on existing Wikipedia and Twitter corpora, as well as a new, larger Twitter corpus. The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus. The two grid constructions can also be combined to produce consistently strong results across all training sets.","Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG ) .']",0,"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG ) .']"
CC191,D13-1115,Integrating Theory and Practice: A Daunting Task,topics in semantic representation,"['Thomas L Griffiths', 'Mark Steyvers', 'Joshua B Tenenbaum']",related work,"Accounts of language processing have suggested that it requires retrieving concepts from memory in response to an ongoing stream of information. This can be facilitated by inferring the gist of a sentence, conversation, or document, and using that computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads us to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure. Many aspects of perception and cognition can be understood by considering the computational problem that is addressed by a particular human capacity (Andersion, 1990; Marr, 1982). Perceptual capacities such as identifying shape from shading (Freeman, 1994), motion perceptio","#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', '#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"
CC192,D13-1115,Integrating Theory and Practice: A Daunting Task,zeroshot learning through crossmodal transfer,"['Richard Socher', 'Milind Ganjoo', 'Hamsa Sridhar', 'Osbert Bastani', 'Christopher D Manning', 'Andrew Y Ng']",related work,,"To name a few examples , Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']",0,"['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']"
CC193,D13-1115,Integrating Theory and Practice: A Daunting Task,imagenet a largescale hierarchical image database,"['Jia Deng', 'Wei Dong', 'Richard Socher', 'Li-Jia Li', 'Kai Li', 'Li Fei-Fei']",experiments,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ""ImageNet"", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.","ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG ) .","['BilderNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG ) .', 'Multiple synsets exist for each meaning of a word.', 'For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.', 'This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet.']",5,"['ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG ) .']"
CC194,D13-1115,Integrating Theory and Practice: A Daunting Task,grounded models of semantic representation,"['Carina Silberer', 'Mirella Lapata']",related work,"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.","#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .","['Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']",0,"['Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']"
CC195,D13-1115,Integrating Theory and Practice: A Daunting Task,models of semantic representation with visual attributes,"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']",related work,We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.,"More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,"['More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature']"
CC196,D13-1115,Integrating Theory and Practice: A Daunting Task,online learning for latent dirichlet allocation,"['Matthew Hoffman', 'David M Blei', 'Francis Bach']",experiments,"We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.","The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG .","['In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents.', 'We do not optimize these hyperparameters or vary them over time.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG .']",5,"['The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG .']"
CC197,D13-1115,Integrating Theory and Practice: A Daunting Task,grounded models of semantic representation,"['Carina Silberer', 'Mirella Lapata']",introduction,"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']"
CC198,D13-1115,Integrating Theory and Practice: A Daunting Task,distinctive image features from scaleinvariant keypoints,['David G Lowe'],related work,This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ...,"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"
CC199,D13-1115,Integrating Theory and Practice: A Daunting Task,grounded models of semantic representation,"['Carina Silberer', 'Mirella Lapata']",method,"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.","Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009).', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']",0,"['Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .']"
CC200,D13-1115,Integrating Theory and Practice: A Daunting Task,combining feature norms and text data with topic models,['Mark Steyvers'],introduction,"Many psychological theories of semantic cognition assume that concepts are represented by features. The empirical procedures used to elicit features from humans rely on explicit human judgments which limit the scope of such representations. An alternative computational framework for semantic cognition that does not rely on explicit human judgment is based on the statistical analysis of large text collections. In the topic modeling approach, documents are represented as a mixture of learned topics where each topic is represented as a probability distribution over words. We propose feature-topic models, where each document is represented by a mixture of learned topics as well as predefined topics that are derived from feature norms. Results indicate that this model leads to systematic improvements in generalization tasks. We show that the learned topics in the model play in an important role in the generalization performance by including words that are not part of current feature norms.2009 Elsevier B.V. All rights reserved.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC201,D13-1115,Integrating Theory and Practice: A Daunting Task,learning the abstract motion semantics of verbs from captioned videos,"['Stefan Mathe', 'Afsaneh Fazly', 'Sven Dickinson', 'Suzanne Stevenson']",related work,"We propose an algorithm for learning the semantics of a (motion) verb from videos depicting the action expressed by the verb, paired with sentences describing the action participants and their roles. Acknowledging that commonalities among example videos may not exist at the level of the input features, our approximation algorithm efficiently searches the space of more abstract features for a common solution. We test our algorithm by using it to learn the semantics of a sample set of verbs; results demonstrate the usefulness of the proposed framework, while identifying directions for further improvement.","Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 ) .","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 ) .']",0,"['Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 ) .']"
CC202,D13-1115,Integrating Theory and Practice: A Daunting Task,association norms of german noun compounds,"['Sabine Schulte im Walde', 'Susanne Borgwaldt', 'Ronny Jauch']",experiments,"This paper introduces association norms of German noun compounds as a lexical-semantic resource for cognitive and computational linguistics research on compositionality. Based on an existing database of German noun compounds, we collected human associations to the compounds and their constituents within a web experiment. The current study describes the collection process and a part-of-speech analysis of the association resource. In addition, we demonstrate that the associations provide insight into the semantic properties of the compounds, and perform a case study that predicts the degree of compositionality of the experiment compound nouns, as relying on the norms. Applying a comparatively simple measure of association overlap, we reach a Spearman rank correlation coefficient of rs = 0.5228, p &lt;.000001, when comparing our predictions with human judgements",Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG .,"['Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types.']",5,"['Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.']"
CC203,D13-1115,Integrating Theory and Practice: A Daunting Task,latent dirichlet allocation,"['David M Blei', 'Andrew Y Ng', 'Michael I Jordan']",method,"Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting imagery. However, these models are confined to crisp segmentation, forcing a visual word (i.e., an image patch) to belong to one and only one topic. Yet, there are many images in which some regions cannot be assigned a crisp categorical label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and an associated parameter estimation algorithm. This model can be useful for imagery where a visual word may be a mixture of multiple topics. Experimental results on visual and sonar imagery show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability previous topic modeling methods do not have.Comment: Version 1, Sent for Review. arXiv admin note: substantial text   overlap with arXiv:1511.0282","Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .","['Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated using the fol-lowing generative process:']",0,"['Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .']"
CC204,D13-1115,Integrating Theory and Practice: A Daunting Task,learning to parse natural language commands to a robot control system,"['Cynthia Matuszek', 'Evan Herbst', 'Luke Zettlemoyer', 'Dieter Fox']",related work,"Abstract As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor environment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our formal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.","Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .']"
CC205,D13-1115,Integrating Theory and Practice: A Daunting Task,what helps where–and why semantic relatedness for knowledge transfer,"['Marcus Rohrbach', 'Michael Stark', 'Gy¨orgy Szarvas', 'Iryna Gurevych', 'Bernt Schiele']",related work,"Remarkable performance has been reported to recognize  single object classes. Scalability to large numbers of classes  however remains an important challenge for today's recognition  methods. Several authors have promoted knowledge  transfer between classes as a key ingredient to address this  challenge. However, in previous work the decision, which  knowledge to transfer has required either manual supervision  or at least a few training examples limiting the scalability  of these approaches. In this work we explicitly address  the question of how to automatically decide which information  to transfer between classes without the need of any human  intervention. For this we tap into linguistic knowledge  bases to provide the semantic link between sources (what)  and targets (where) of knowledge transfer. We provide a rigorous  experimental evaluation of different knowledge bases  and state-of-the-art techniques from Natural Language Processing  which goes far beyond the limited use of language  in related work. We also give insights into the applicability  (why) of different knowledge sources and similarity measures  for knowledge transfer","To name a few examples , #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']",0,"['To name a few examples , #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']"
CC206,D13-1115,Integrating Theory and Practice: A Daunting Task,learning to interpret natural language navigation instructions from observations,"['David L Chen', 'Raymond J Mooney']",related work,"The ability to understand natural-language instructions is crit-ical to building intelligent agents that interact with humans. We present a system that learns to transform natural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ-ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in-structions for these environments is used for training and test-ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys-tem is able to automatically learn to correctly interpret a rea-sonable fraction of the complex instructions in this corpus.","Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .']"
CC207,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",related work,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .']"
CC208,D13-1115,Integrating Theory and Practice: A Daunting Task,distributional semantics from text and images,"['Elia Bruni', 'Giang Binh Tran', 'Marco Baroni']",introduction,"We present a distributional semantic model combining text- and image-based features. We evaluate this multimodal semantic model on simulating similarity judgments, concept clus-tering and the BLESS benchmark. When inte-grated with the same core text-based model, image-based features are at least as good as further text-based features, and they capture different qualitative aspects of the tasks, sug-gesting that the two sources of information are complementary.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC209,D13-1115,Integrating Theory and Practice: A Daunting Task,models of semantic representation with visual attributes,"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']",method,We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.,It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ) .,"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009).', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ) .', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']",0,['It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ) .']
CC210,D13-1115,Integrating Theory and Practice: A Daunting Task,the story picturing engine—a system for automatic text illustration,"['Dhiraj Joshi', 'James Z Wang', 'Jia Li']",introduction,"We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented.","Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']",0,"['Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"
CC211,D13-1115,Integrating Theory and Practice: A Daunting Task,learning to parse natural language commands to a robot control system,"['Cynthia Matuszek', 'Evan Herbst', 'Luke Zettlemoyer', 'Dieter Fox']",introduction,"Abstract As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor environment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our formal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.","Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ) .', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a;Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012).']",0,"['Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ) .']"
CC212,D13-1115,Integrating Theory and Practice: A Daunting Task,congruent embodied representations for visually presented actions and linguistic phrases describing actions,"['Lisa Aziz-Zadeh', 'Stephen M Wilson', 'Giacomo Rizzolatti', 'Marco Iacoboni']",related work,"The thesis of embodied semantics holds that conceptual representations accessed during linguistic processing are, in part, equivalent to the sensory-motor representations required for the enactment of the concepts described . Here, using fMRI, we tested the hypothesis that areas in human premotor cortex that respond both to the execution and observation of actions-mirror neuron areas -are key neural structures in these processes. Participants observed actions and read phrases relating to foot, hand, or mouth actions. In the premotor cortex of the left hemisphere, a clear congruence was found between effector-specific activations of visually presented actions and of actions described by literal phrases. These results suggest a key role of mirror neuron areas in the re-enactment of sensory-motor representations during conceptual processing of actions invoked by linguistic stimuli.","The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG ) .']"
CC213,D13-1115,Integrating Theory and Practice: A Daunting Task,stochastic variational inference arxiv eprints,"['Matthew Hoffman', 'David M Blei', 'Chong Wang', 'John Paisley']",method,,"To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .","['To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']",5,"['To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.']"
CC214,D13-1115,Integrating Theory and Practice: A Daunting Task,distributional semantics in technicolor,"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']",introduction,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC215,D13-1115,Integrating Theory and Practice: A Daunting Task,perceptual inference through global lexical similarity,"['Brendan T Johns', 'Michael N Jones']",introduction,"The literature contains a disconnect between accounts of how humans learn lexical semantic representations for words. Theories generally propose that lexical semantics are learned either through perceptual experience or through exposure to regularities in language. We propose here a model to integrate these two information sources. Specifically, the model uses the global structure of memory to exploit the redundancy between language and perception in order to generate inferred perceptual representations for words with which the model has no perceptual experience. We test the model on a variety of different datasets from grounded cognition experiments and demonstrate that this diverse set of results can be explained as perceptual simulation (cf. Barsalou, Simmons, Barbey, & Wilson, 2003) within a global memory model.Copyright (c) 2012 Cognitive Science Society, Inc.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC216,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",experiments,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']",1,"['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']"
CC217,D13-1115,Integrating Theory and Practice: A Daunting Task,modeling the shape of the scene a holistic representation of the spatial envelope,"['Aude Oliva', 'Antonio Torralba']",related work,"In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.","The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"
CC218,D13-1115,Integrating Theory and Practice: A Daunting Task,describing objects by their attributes,"['Ali Farhadi', 'Ian Endres', 'Derek Hoiem', 'David Forsyth']",related work,"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (""spotty dog"", not just ""dog""); to say something about unfamiliar objects (""hairy and four-legged"", not just ""unknown""); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (""spotty"") or discriminative (""dogs have it but sheep do not""). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. 1","The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"
CC219,D13-1115,Integrating Theory and Practice: A Daunting Task,indexing by latent semantic analysis,"['Scott Deerwester', 'Susan T Dumais', 'George W Furnas', 'Thomas K Landauer', 'Richard Harshman']",related work,"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (""semantic structure"") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.","Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"
CC220,D13-1115,Integrating Theory and Practice: A Daunting Task,online learning for latent dirichlet allocation,"['Matthew Hoffman', 'David M Blei', 'Francis Bach']",method,"We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.","To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .","['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']",5,"['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .']"
CC221,D13-1115,Integrating Theory and Practice: A Daunting Task,how many words is a picture worth automatic caption generation for news images,"['Yansong Feng', 'Mirella Lapata']",related work,"In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.",The first work to do this with topic models is #AUTHOR_TAGb ) .,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,['The first work to do this with topic models is #AUTHOR_TAGb ) .']
CC222,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",introduction,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .","['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'We release both our code and data to the community for future research. 1']",2,"['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.']"
CC223,D14-1083,Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates,extralinguistic constraints on stance recognition in ideological debates,"['Kazi Saidul Hasan', 'Vincent Ng']",experiments,,"This choice is motivated by an observation we made previously ( #AUTHOR_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3","['In P2, on the other hand, we recast SC as a se- quence labeling task.', 'In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence.', 'This choice is motivated by an observation we made previously ( #AUTHOR_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3']",2,"['This choice is motivated by an observation we made previously ( #AUTHOR_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3']"
CC224,D14-1083,Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates,extralinguistic constraints on stance recognition in ideological debates,"['Kazi Saidul Hasan', 'Vincent Ng']",experiments,,"Following our previous work on stance classification ( #AUTHOR_TAGc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .","['Frame-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification ( #AUTHOR_TAGc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .', 'Frame-word interaction features encode whether two words appear in different elements of the same frame.', 'Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and']",2,"['Following our previous work on stance classification ( #AUTHOR_TAGc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .']"
CC225,D14-1130,Human Effort and Machine Learnability in Computer Aided Translation,the efficacy of human postediting for language translation,"['S Green', 'J Heer', 'C D Manning']",related work,,Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAGa ) comparing scratch translation to post-edit .,"['The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes.', 'However, he used undergraduate, non-professional subjects, and did not consider re-tuning.', 'Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAGa ) comparing scratch translation to post-edit .', 'Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013).', 'However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature.']",2,['Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAGa ) comparing scratch translation to post-edit .']
CC226,D14-1157,Staying on Topic: An Indicator of Power in Political Debates,power of confidence how poll scores impact topic dynamics in political debates,"['Vinodkumar Prabhakaran', 'Ashima Arora', 'Owen Rambow']",method,"In this paper, we investigate how topic dynamics during the course of an interaction correlate with the power differences between its participants. We perform this study on the US presidential debates and show that a candidate's power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction.",This is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .,"[""Table 1 shows the Pearson's product correlation between each topical feature and candidate's power."", 'We obtain a highly significant (p = 0.002) negative correlation between topic shift tendency of a candidate (PI) and his/her power.', ""In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings."", 'Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'This is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .', 'It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others.', 'On the other hand, we did not obtain any significant correlation for the features proposed by Nguyen et al. (2013).']",1,['This is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .']
CC227,D14-1222,A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents,cascading collective classification for bridging anaphora recognition using a rich linguistic feature set,"['Yufang Hou', 'Katja Markert', 'Michael Strube']",introduction,"Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection fea-tures and integrate these into a cascaded mi-nority preference algorithm that models bridg-ing recognition as a subtask of learning fine-grained information status (IS). We substan-tially improve bridging recognition without impairing performance on other IS classes",We follow our previous work ( #AUTHOR_TAGb ) and restrict bridging to non-coreferential cases .,"['Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993;Löbner, 1998).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference).', 'We follow our previous work ( #AUTHOR_TAGb ) and restrict bridging to non-coreferential cases .', 'We also exclude comparative anaphora (Modjeska et al., 2003).']",2,"['Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993;Lobner, 1998).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference).', 'We follow our previous work ( #AUTHOR_TAGb ) and restrict bridging to non-coreferential cases .', 'We also exclude comparative anaphora (Modjeska et al., 2003).']"
CC228,D14-1222,A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents,cascading collective classification for bridging anaphora recognition using a rich linguistic feature set,"['Yufang Hou', 'Katja Markert', 'Michael Strube']",experiments,"Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection fea-tures and integrate these into a cascaded mi-nority preference algorithm that models bridg-ing recognition as a subtask of learning fine-grained information status (IS). We substan-tially improve bridging recognition without impairing performance on other IS classes","mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .","['7 In ISNotes, 71% of NP antecedents occur in the same or up to two sentences prior to the anaphor.', 'Initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'To deal with data imbalance, the SVM light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system.', 'All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features.', 'mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .', 'Some of these features overlap with the atomic features used in the rule-based system.']",2,"['mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .']"
CC229,E03-1002,Neural network probability estimation for broad coverage parsing,towards historybased grammars using richer models for probabilistic parsing,"['E Black', 'F Jelinek', 'J Lafferty', 'D Magerman', 'R Mercer', 'S Roukos']",method,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.","in history-based models ( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .","['The probability model we use is generative and history-based.', 'Generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'At each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'This sequence of decisions is the derivation of the tree, which we will denote d1,..., dm .', ""Because there is a one-to-one mapping from phrase structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence."", 'The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history-based models ( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.']",5,"['The probability model we use is generative and history-based.', 'Generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'in history-based models ( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .']"
CC230,E03-1002,Neural network probability estimation for broad coverage parsing,a maximumentropyinspired parser,['Eugene Charniak'],,,"The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ) .","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ) .']",1,"['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ) .']"
CC231,E03-1002,Neural network probability estimation for broad coverage parsing,headdriven statistical models for natural language parsing,['Michael Collins'],experiments,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG ) .,['7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG ) .'],5,['7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG ) .']
CC232,E03-1002,Neural network probability estimation for broad coverage parsing,towards historybased grammars using richer models for probabilistic parsing,"['E Black', 'F Jelinek', 'J Lafferty', 'D Magerman', 'R Mercer', 'S Roukos']",introduction,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.","Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .","['Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001;Henderson, 2000).', 'Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'The resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state-of-the-art for parsing the Penn Treebank.']",0,"['Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .']"
CC233,E03-1002,Neural network probability estimation for broad coverage parsing,headdriven statistical models for natural language parsing,['Michael Collins'],,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 ) .","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 ) .']",1,"['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 ) .']"
CC234,E03-1002,Neural network probability estimation for broad coverage parsing,discriminative reranking for natural language parsing,['Michael Collins'],,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG ) .","['The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms.', 'We replace Chomsky adjunction structures (i.e.', 'structures of the form [X [X ...] [Y ...]]) with a special ""modifier"" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]).', 'These transforms are undone before any evaluation is performed on the output trees.', 'We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG ) .""]",0,"[""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG ) .""]"
CC235,E03-1002,Neural network probability estimation for broad coverage parsing,pcfg models of linguistic tree representations,['Mark Johnson'],,"The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.","For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) .","['The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.', 'For this reason, D(top) includes nodes which are structurally local to top,.', ""These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) .', 'Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i � 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.', 'Thus this model is making no a priori hard independence assumptions, just a priori soft biases.']",0,"['The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.', 'For this reason, D(top) includes nodes which are structurally local to top,.', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) .']"
CC236,E03-1002,Neural network probability estimation for broad coverage parsing,new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron,"['Michael Collins', 'Nigel Duffy']",,"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.","#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .","['The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms.', 'We replace Chomsky adjunction structures (i.e.', 'structures of the form [X [X ...] [Y ...]]) with a special ""modifier"" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]).', 'These transforms are undone before any evaluation is performed on the output trees.', 'We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .""]",0,"[""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .""]"
CC237,E03-1002,Neural network probability estimation for broad coverage parsing,a maximum entropy model for partofspeech tagging,['Adwait Ratnaparkhi'],experiments,,We used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system .,"['In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus.', 'We used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system .']",5,"['In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus.', 'We used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system .']"
CC238,E03-1002,Neural network probability estimation for broad coverage parsing,a maximumentropyinspired parser,['Eugene Charniak'],experiments,,"The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']",1,"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.']"
CC239,E03-1002,Neural network probability estimation for broad coverage parsing,headdriven statistical models for natural language parsing,['Michael Collins'],introduction,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .","['Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001;Henderson, 2000).', 'Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'The resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state-of-the-art for parsing the Penn Treebank.']",0,"['Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .']"
CC240,E03-1002,Neural network probability estimation for broad coverage parsing,efficient probabilistic topdown and leftcorner parsing,"['Brian Roark', 'Mark Johnson']",,,"For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #AUTHOR_TAG ) .","['The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.', 'For this reason, D(top) includes nodes which are structurally local to top,.', ""These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #AUTHOR_TAG ) .', 'Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.', 'Thus this model is making no a priori hard independence assumptions, just a priori soft biases.']",0,"['For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #AUTHOR_TAG ) .']"
CC241,E03-1002,Neural network probability estimation for broad coverage parsing,what is the minimal set of fragments that achieves maximal parse accuracy,['Rens Bod'],experiments,"We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precision of 90.8% and a recall of 90.6%). We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy.","The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']",1,"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.']"
CC242,E03-1002,Neural network probability estimation for broad coverage parsing,discriminative reranking for natural language parsing,['Michael Collins'],experiments,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']",1,"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .']"
CC243,E03-1002,Neural network probability estimation for broad coverage parsing,a maximumentropyinspired parser,['Eugene Charniak'],,,"Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .","['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999).', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', 'We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.', 'These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.']",1,"['Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']"
CC244,E03-1005,An efficient implementation of a new DOP model,a maximumentropyinspired parser,['E Charniak'],conclusion,,This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .,"['Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', 'Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.', 'While SL-DOP and LS-DOP have been compared before in']",1,"['This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', 'Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.']"
CC245,E03-1005,An efficient implementation of a new DOP model,new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron,"['M Collins', 'N Duffy']",introduction,"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.","#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ .","['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", 'Goodman (1996Goodman ( , 1998 developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]",0,"[""#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ .""]"
CC246,E03-1005,An efficient implementation of a new DOP model,a new statistical parser based on bigram lexical dependencies,['M Collins'],introduction,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]",0,"[""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]"
CC247,E03-1005,An efficient implementation of a new DOP model,discriminative reranking for natural language parsing,['M Collins'],conclusion,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Compared to the reranking technique in #AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .","['Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', ""This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod's PCFG-reduction reported in Table 1."", 'Compared to the reranking technique in #AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before in']",1,"['Compared to the reranking technique in #AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .']"
CC248,E03-1005,An efficient implementation of a new DOP model,discriminative reranking for natural language parsing,['M Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]",0,"[""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]"
CC249,E03-1005,An efficient implementation of a new DOP model,a dop model for semantic interpretation,"['R Bonnema', 'R Bod', 'R Scha']",,"In data-oriented language processing, an annotated language corpus is used as a stochastic grammar. The most probable analysis of a new sentence is constructed by combining fragments from the corpus in the most probable way. This approach has been successfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Tree-bank. If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence. The present paper explains this semantic interpretation method. A data-oriented semantic interpretation algorithm was tested on two semantically annotated corpora: the English ATIS corpus and the Dutch OVIS corpus. Experiments show an increase in semantic accuracy if larger corpus-fragments are taken into consideration.","Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , #AUTHOR_TAG , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.","[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , #AUTHOR_TAG , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']",0,"[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , #AUTHOR_TAG , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.']"
CC250,E03-1005,An efficient implementation of a new DOP model,a maximumentropyinspired parser,['E Charniak'],experiments,,"Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG ) .","['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG ) .', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'This corresponds to a speedup of over 60 times.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']",1,"['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG ) .', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'This corresponds to a speedup of over 60 times.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']"
CC251,E03-1005,An efficient implementation of a new DOP model,building a large annotated corpus of english the penn treebank,"['M Marcus', 'B Santorini', 'M Marcinkiewicz']",experiments,"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.","For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .","['For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', 'Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing).', 'We employed the same unknown (category) word model as in Bod (2001), based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 -87).', 'We used ""evalb"" 4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999).', 'We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.']",5,"['For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.']"
CC252,E03-1005,An efficient implementation of a new DOP model,a new statistical parser based on bigram lexical dependencies,['M Collins'],experiments,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .","['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'This corresponds to a speedup of over 60 times.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']",1,"['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.']"
CC253,E03-1005,An efficient implementation of a new DOP model,a maximumentropyinspired parser,['E Charniak'],introduction,,"But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]",0,"[""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]"
CC254,E03-1005,An efficient implementation of a new DOP model,discriminative reranking for natural language parsing,['M Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","And #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .","['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]",0,"[""And #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"
CC255,E03-1005,An efficient implementation of a new DOP model,new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron,"['M Collins', 'N Duffy']",,"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.","Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .","[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."", 'We will refer to these models as Likelihood- DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']",0,"[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .""]"
CC256,E03-1005,An efficient implementation of a new DOP model,efficient algorithms for parsing the dop model,['J Goodman'],,"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.","Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.","[""Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']",0,"[""Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.']"
CC257,E03-1005,An efficient implementation of a new DOP model,treegram parsing lexical dependencies and structural relations,"[""K Sima'an""]",,"This paper explores the kinds of probabilistic relations that are important in syntactic disambiguation. It proposes that two widely used kinds of relations, lexical dependencies and structural relations, have complementary disambiguation capabilities. It presents a new model based on structural relations, the Tree-gram model, and reports experiments showing that structural relations should benefit from enrichment by lexical dependencies.","Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #AUTHOR_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .","['Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #AUTHOR_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']",0,"['Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #AUTHOR_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .']"
CC258,E03-1005,An efficient implementation of a new DOP model,efficient algorithms for parsing the dop model,['J Goodman'],introduction,"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.","Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG , 2002 ) .","['Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002).']",0,"['Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG , 2002 ) .']"
CC259,E03-1005,An efficient implementation of a new DOP model,a new statistical parser based on bigram lexical dependencies,['M Collins'],introduction,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .","['Waegner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .']",4,"['This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .']"
CC260,E03-1005,An efficient implementation of a new DOP model,a maximumentropyinspired parser,['E Charniak'],introduction,,The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .,"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include non- lexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .', 'And Collins (2000) argues for ""keeping track of counts of arbitrary fragments within parse trees"", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).']",0,"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include non- lexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .', 'And Collins (2000) argues for ""keeping track of counts of arbitrary fragments within parse trees"", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).']"
CC261,E03-1005,An efficient implementation of a new DOP model,Motivation,new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron,"['M Collins', 'N Duffy']","This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.","And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .","['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]",4,"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"
CC262,E03-1005,An efficient implementation of a new DOP model,efficient algorithms for parsing the dop model,['J Goodman'],introduction,"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.","And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .","['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar.""]",0,"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar.""]"
CC263,E03-1005,An efficient implementation of a new DOP model,efficient algorithms for parsing the dop model,['J Goodman'],introduction,"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.","#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .","['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]",0,"['#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.']"
CC264,E12-1068,Modeling Inflection and Word-Formation in SMT,srilm  an extensible language modeling toolkit,['Andreas Stolcke'],experiments,"SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools. 1",The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG ) .,"['To evaluate our end-to-end system, we perform the well-studied task of news translation, using the Moses SMT package.', 'We use the English/German data released for the 2009 ACL Workshop on Machine Translation shared task on translation. 7', 'There are 82,740 parallel sentences from news-commentary09.de-en and 1,418,115 parallel sentences from europarl-v4.de-en.', 'The monolingual data contains 9.8 M sentences. 8', 'o build the baseline, the data was tokenized using the Moses tokenizer and lowercased.', 'We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the ""grow-diag-final-and"" heuristic.', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG ) .', 'We run MERT separately for each system.', 'The recaser used is the same for all systems.', 'It is the standard recaser supplied with Moses, trained on all German training data.', 'The dev set is wmt-2009-a and the test set is wmt-2009-b, and we report end-to-end case sensitive BLEU scores against the unmodified reference SGML file.', 'The blind test set used is wmt-2009-blind (all lines).']",5,"['The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG ) .', 'We run MERT separately for each system.']"
CC265,E12-1068,Modeling Inflection and Word-Formation in SMT,productive generation of compound words in statistical machine translation,"['Sara Stymne', 'Nicola Cancedda']",experiments,"In this article we investigate statistical machine translation (SMT) into Germanic languages, with a focus on compound processing. Our main goal is to enable the generation of novel compounds that have not been seen in the training data. We adopt a split-merge strategy, where compounds are split before training the SMT system, and merged after the translation step. This approach reduces sparsity in the training data, but runs the risk of placing translations of compound parts in non-consecutive positions. It also requires a postprocessing step of compound merging, where compounds are reconstructed in the translation output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order and show that it can lead to improvements both by direct inspection and in terms of standard translation evaluation metrics. We also propose several new methods for compound merging, based on heuristics and machine learning, which outperform previously suggested algorithms. These methods can produce novel compounds and a translation with at least the same overall quality as the baseline. For all subtasks we show that it is useful to include part-of-speech based information in the translation process, in order to handle compounds. 1","Following the work of #AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .","['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of #AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006).']",5,"['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Following the work of #AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006).']"
CC266,E12-1068,Modeling Inflection and Word-Formation in SMT,how to avoid burning ducks combining linguistic analysis and corpus statistics for german compound processing,"['Fabienne Fritzinger', 'Alexander Fraser']",related work,"Compound splitting is an important problem in many NLP applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance.","For compound splitting , we follow #AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .","['For compound splitting , we follow #AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)).', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']",5,"['For compound splitting , we follow #AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .']"
CC267,E12-1068,Modeling Inflection and Word-Formation in SMT,factored translation models,"['Philipp Koehn', 'Hieu Hoang']",introduction,"We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level -- may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.","#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .","['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']",0,"['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']"
CC268,E12-1068,Modeling Inflection and Word-Formation in SMT,productive generation of compound words in statistical machine translation,"['Sara Stymne', 'Nicola Cancedda']",related work,"In many languages the use of compound words is very productive. A common practice to reduce sparsity consists in splitting compounds in the training data. When this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order. Furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order. We also propose new heuristic methods for merging components that outperform all known methods, and a learning-based method that has similar accuracy as the heuristic method, is better at producing novel compounds, and can operate with no background linguistic resources.","We follow #AUTHOR_TAG , for compound merging .","['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the ge- ometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging ap- proach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow #AUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets.']",5,"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the ge- ometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied.', 'We follow #AUTHOR_TAG , for compound merging .']"
CC269,E12-1068,Modeling Inflection and Word-Formation in SMT,failures in englishczech phrasebased mt,"['Ondˇrej Bojar', 'Kamil Kos']",related work,,#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .,"['Given a stem such as brother, Toutanova et.', 'al\'s system might generate the ""stem and inflection"" corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a ""split in preprocessing and resynthesize in postprocessing"" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.', 'al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']",1,['#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .']
CC270,E12-1068,Modeling Inflection and Word-Formation in SMT,empirical methods for compound splitting,"['Philipp Koehn', 'Kevin Knight']",related work,"Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.Comment: 8 pages, 2 figures. Published at EACL 200","Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .","['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']",1,"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']"
CC271,E12-1068,Modeling Inflection and Word-Formation in SMT,agreement constraints for statistical machine translation into german,"['Philip Williams', 'Philipp Koehn']",related work,"Languages with rich inflectional morphology pose a difficult challenge for statistical machine translation. To address the problem of morphologically inconsistent output, we add unification-based constraints to the target-side of a string-to-tree model. By integrating constraint evaluation into the decoding process, implausible hypotheses can be penalised or filtered out during search. We use a simple heuristic process to extract agreement constraints for German and test our approach on an English-German system trained on WMT data, achieving a small improvement in translation accuracy as measured by BLEU.",#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.,"['Given a stem such as brother, Toutanova et. al�s system might generate the �stem and inflection� corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a map- ping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., ad- jectives) separating his and brother.', 'This required mapping is a significant problem for generaliza- tion.', 'We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection).', 'We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex con- text features.']",1,"['#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex con- text features.']"
CC272,E12-1068,Modeling Inflection and Word-Formation in SMT,efficient parsing of highly ambiguous contextfree grammars with bit vectors,['Helmut Schmid'],introduction,An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one.,"The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .","['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .']",5,"['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .']"
CC273,E12-1068,Modeling Inflection and Word-Formation in SMT,how to avoid burning ducks combining linguistic analysis and corpus statistics for german compound processing,"['Fabienne Fritzinger', 'Alexander Fraser']",experiments,"Compound splitting is an important problem in many NLP applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance.","We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .","['We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training data is then stemmed as described in Section 2.3.', 'The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb.', 'In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization.']",5,"['We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .']"
CC274,E12-1068,Modeling Inflection and Word-Formation in SMT,combining morphemebased machine translation with postprocessing morpheme prediction,"['Ann Clifton', 'Anoop Sarkar']",related work,,"Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .","['We have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step.', 'The direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation.', 'However, it is reasonable to expect that the use of features (and morphological generation) could also be problematic as this requires the use of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation.', 'Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT.', 'This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing.', 'As parsing performance improves, the performance of linguistic-feature-based approaches will increase.', 'Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', 'However, this does not deal directly with linguistic features marked by inflection.', 'In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.', 'So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.']",1,"['Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .']"
CC275,E12-1068,Modeling Inflection and Word-Formation in SMT,experiments in morphosyntactic processing for translating to and from german,['Alexander Fraser'],related work,We describe two shared task systems and associated experiments. The German to English system used reordering rules ap-plied to parses and morphological split-ting and stemming. The English to Ger-man system used an additional translation step which recreated compound words and generated morphological inflection,#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .,"['Given a stem such as brother, Toutanova et.', 'al\'s system might generate the ""stem and inflection"" corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a ""split in preprocessing and resynthesize in postprocessing"" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.', 'al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']",1,['#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .']
CC276,E12-1068,Modeling Inflection and Word-Formation in SMT,factored translation models,"['Philipp Koehn', 'Hieu Hoang']",related work,"We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level -- may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.",#AUTHOR_TAG introduced factored SMT .,"['Given a stem such as brother, Toutanova et. al�s system might generate the �stem and inflection� corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', '#AUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']",1,['#AUTHOR_TAG introduced factored SMT .']
CC277,E12-1068,Modeling Inflection and Word-Formation in SMT,syntaxtomorphology mapping in factored phrasebased statistical machine translation from english to turkish,"['Reyyan Yeniterzi', 'Kemal Oflazer']",related work,"We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.","Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #AUTHOR_TAG and others .","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'Using additional source side information beyond the markup did not produce a gain in performance.']",1,"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #AUTHOR_TAG and others .']"
CC278,E12-1068,Modeling Inflection and Word-Formation in SMT,enriching morphologically poor languages for statistical machine translation,"['Eleftherios Avramidis', 'Philipp Koehn']",related work,"We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For English-Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%.","Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'Using additional source side information beyond the markup did not produce a gain in performance.']",1,"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .']"
CC279,E12-1068,Modeling Inflection and Word-Formation in SMT,german compounds in factored statistical machine translation,['Sara Stymne'],related work,"Abstract. An empirical method for splitting German compounds is explored by varying it in a number of ways to investigate the consequences for factored statistical machine translation between English and German in both directions. Compound splitting is incorporated into translation in a preprocessing step, performed on training data and on German translation input. For translation into German, compounds are merged based on part-of-speech in a postprocessing step. Compound parts are marked, to separate them from ordinary words. Translation quality is improved in both translation directions and the number of untranslated words in the English output is reduced. Different versions of the splitting algorithm performs best in the two different translation directions.","Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .","['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']",1,"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .', 'Compound merging is less well studied.']"
CC280,E14-1023,Frame Semantic Tree Kernels for Social Network Extraction from Text,automatic detection and classification of social events,"['Apoorv Agarwal', 'Owen Rambow']",related work,"In this paper we introduce the new task of social event extraction from text. We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact. We annotate part of Automatic Content Extraction (ACE) data, and perform experiments using Support Vector Machines with Kernel methods. We use a combination of structures derived from phrase structure trees and dependency trees. A characteristic of our events (which distinguishes them from ACE events) is that the participating entities can be spread far across the parse trees. We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classification tasks. We also use three data sampling approaches to solve the problem of data skewness. Sampling methods improve the F1-measure for the task of relation detection by over 20% absolute over the baseline.","Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .","['Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', 'Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003;Culotta and Sorensen, 2004).', 'To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005).', 'Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees.', 'They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees.', 'We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by Harabagiu et al. (2005).']",2,"['Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .']"
CC281,E99-1022,Selective magic HPSG parsing,the logic of typed feature structures  with applications to unification grammars logic programs and constraint resolution,['Bob Carpenter'],,,2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ) .,"['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]",0,"['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]"
CC282,E99-1022,Selective magic HPSG parsing,offline compilation for efficient processing with constraintlogic grammars,['Guido Minnen'],introduction,,I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG ) .,['I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG ) .'],0,['I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG ) .']
CC283,E99-1022,Selective magic HPSG parsing,ale — the attribute logic engine users guide version 202,"['Bob Carpenter', 'Gerald Penn']",,"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ...","Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown .","['Combining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", 'In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1° All types in the type hierarchy can be used as parse types.', 'This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types.', '11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG.', ""The parsing process corresponding to such a parse type specification is represented schematically in figure 8. Starting from the lexical entries, i. e., word word word Figure 8: Schematic representation of the selective magic parsing process the :r~'L definite clauses that specify the word objects in the grammar, phrases are built bottomup by matching the parse type literals of the definite clauses in the grammar against the edges in the table."", 'The non-parse type literals are processed according to the top-down control strategy 1°The notion of a parse type literal is closely related to that of a memo literal as in (Johnson and DSrre, 1995).']",0,"[""Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", 'In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.']"
CC284,E99-1022,Selective magic HPSG parsing,magic for filter optimization in dynamic bottomup processing,['Guido Minnen'],introduction,,"As shown in ( #AUTHOR_TAG ) â¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .","['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See, among others, (Ramakrishnan et al. 1992).', 'As shown in ( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']",0,"['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See, among others, (Ramakrishnan et al. 1992).', 'As shown in ( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']"
CC285,E99-1022,Selective magic HPSG parsing,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG ),"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects.""]",0,['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG )']
CC286,E99-1022,Selective magic HPSG parsing,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",,,Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG ) .,"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG ) .', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into TIT definite clauses which are used to restrict lexical entries.', '(GStz and Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.4', 'Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above men- tioned techniques for compiling an HPSG theory into typed feature grammars.']",0,['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG ) .']
CC287,E99-1022,Selective magic HPSG parsing,efficient bottomup evaluation of logic programs,"['Raghu Ramakrishnan', 'Divesh Srivastava', 'S Sudarshan']",introduction,"In recent years, much work has been directed towards evaluating logic programs and queries on deductive databases by using an iterative bottom-up fixpoint computation. The resulting techniques offer an attractive alternative to Prolog-style top-down evaluation in several situations. They are sound and complete for positive Horn clause programs, are well-suited to applications with large volumes of data (facts), and can support a variety of extensions to the standard logic programming paradigm.","See , among others , ( #AUTHOR_TAG ) .","['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , ( #AUTHOR_TAG ) .', 'As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv�:; GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (GStz and Meurers, 1997a) and (Meurers and Minnen, 1997).', 'Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.']",0,"['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , ( #AUTHOR_TAG ) .', 'As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv:; GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (GStz and Meurers, 1997a) and (Meurers and Minnen, 1997).', 'Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.']"
CC288,E99-1022,Selective magic HPSG parsing,interleaving universal principles and relational constraints over typed feature logic,"['Thilo Gotz', 'Detmar Meurers']",,"We introduce a typed feature logic system providing both universal implicational principles as well as definite clauses over feature terms. We show that such an architecture supports a modular encoding of linguistic theories and allows for a compact representation using underspecification. The system is fully implemented and has been used as a workbench to develop and test large HPSG grammars. The techniques described in this paper are not restricted to a specific implementation, but could be added to many current feature-based grammar development systems.",The ConTroll grammar development system as described in ( #AUTHOR_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .,"['aSee (King, 1994) for a discussion of the appropriateness of T~-£: for HPSG and a comparison with other feature logic approaches designed for HPSG.', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in ( #AUTHOR_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']",0,"['aSee (King, 1994) for a discussion of the appropriateness of T~-PS: for HPSG and a comparison with other feature logic approaches designed for HPSG.', 'The ConTroll grammar development system as described in ( #AUTHOR_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']"
CC289,E99-1022,Selective magic HPSG parsing,prologii manuel de reference et modele theorique,['Alain Colmerauer'],,,"See also ( #AUTHOR_TAG ; Naish , 1986 ) .","['Coroutining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also ( #AUTHOR_TAG ; Naish , 1986 ) .']",0,"['See also ( #AUTHOR_TAG ; Naish , 1986 ) .']"
CC290,E99-1022,Selective magic HPSG parsing,ale — the attribute logic engine users guide version 202,"['Bob Carpenter', 'Gerald Penn']",introduction,"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ...",As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG ) .,"['The proposed parser is related to the so-called Lemma Table deduction system (Johnson and DSrre, 1995) which allows the user to specify whether top-down sub-computations are to be tabled.', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.', 'feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goM-directed interpretation of magic-compiled typed feature grammars.']",1,"['As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.']"
CC291,E99-1022,Selective magic HPSG parsing,interleaving universal principles and relational constraints over typed feature logic,"['Thilo Gotz', 'Detmar Meurers']",introduction,"We introduce a typed feature logic system providing both universal implicational principles as well as definite clauses over feature terms. We show that such an architecture supports a modular encoding of linguistic theories and allows for a compact representation using underspecification. The system is fully implemented and has been used as a workbench to develop and test large HPSG grammars. The techniques described in this paper are not restricted to a specific implementation, but could be added to many current feature-based grammar development systems.","Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAGa ) and ( Meurers and Minnen , 1997 ) .","['magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAGa ) and ( Meurers and Minnen , 1997 ) .', 'Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.']",2,"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAGa ) and ( Meurers and Minnen , 1997 ) .']"
CC292,E99-1022,Selective magic HPSG parsing,typed feature structures as descriptions,['Paul King'],,"A description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable.Comment: COLING 94 reserve paper, 5 pages, LaTeX (no .sty exotica",` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .,"['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars.']",0,['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .']
CC293,J00-1004,Learning Dependency Translation Models as Collections of Finite-State Head Transducers,machine translation divergences a formal description and proposed solution,['B J Dorr'],method,"There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system.","This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .","['It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.', 'Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.', 'For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']",1,"['This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']"
CC294,J00-1004,Learning Dependency Translation Models as Collections of Finite-State Head Transducers,englishtomandarin speech translation with head transducers,"['Hiyan Alshawi', 'Fei Xia']",,"We describe the head transducer model used in an experimental English-toMandarin speech translation system. Head transduction is a translation method in which weighted finite state transducers are associated with sourcetarget word pairs. The method is suitable for speech translation because it allows efficient bottom up processing. The head transducers in the experimental system have a wider range of output positions than input positions. This asymmetry is motivated by a tradeoff between model complexity and search efficiency. 1 I n t r o d u c t i o n In this paper we describe the head transducer model used for translation in an experimental English-to-Mandarin speech translation system. Head transducer models consist of collections of weighted finite state transducers associated with pairs of lexical items in a bilingual lexicon. Head transducers operate ""outwards"" from the heads of phrases; they convert the left and right dependents of a source word into the left and right dependents of a corresponding target word. The transducer model can be characterized as a statistical translation model, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. The underlying linguistic structure of these models is similar to dependency grammar (Hudson 1984), although dependency representations are not explicitly constructed in our approach to translation. The original motivation for the head transducer models was Fei Xia D e p a r t m e n t of C o m p u t e r and I n f o r m a t i o n Science Univers i ty of P e n n s y l v a n i a Ph i l ade lph ia , PA 19104, USA fx i aQc i s .upenn . edu that they are simpler and more amenable to automatic model structure acquisition as compared with earlier transfer models. We first describe the head transduction approach in general in Section 2. In Section 3 we explain properties of the particular head transducers used in the experimental English-to-Mandarin speech translator. In Section 4, we explain how head transducers help satisfy the requirements of the speech translation application, and we conclude in Section 5. 2 B i l i n g u a l H e a d T r a n s d u c t i o n 2.1 Bilingual Head Transducers A head transducer M is a finite state machine associated with a pair of words, a source word w and a target word v. In fact, w is taken from the set V1 consisting of the source language vocabulary augmented by the ""empty word"" e, and v is taken from V~, the target language vocabulary augmented with e. A head transducer reads from a pair of source sequences, a left source sequence L1 and a right source sequence Rt; it writes to a pair of target sequences, a left target sequence L2 and a right target sequence R2 (Figure 1). Head transducers were introduced in Alshawi 1996b, where the symbols in the source and target sequences are source and target words respectively. In the model described in this paper, the symbols written are dependency relation symbols, or the empty symbol e. The use of relation symbols here is a result of the historical development of the system from an earlier transfer model. A conceptually simpler translator can be built using head transducer models with only lexical items, in which case the distinction between different dependents is implicit in the state of a transducer. In head transducer models, the use of relations corresponds to a type of class-based model (cf Je-","In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .","['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']",5,"['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']"
CC295,J00-1004,Learning Dependency Translation Models as Collections of Finite-State Head Transducers,a statistical approach to machine translation,"['P J Brown', 'J Cocke', 'S A Della Pietra', 'V J Della Pietra', 'J Lafferty', 'R L Mercer', 'P Rossin']",conclusion,"Statistical Machine Translation has successfully been used for translation between many language pairs contributing to its popularity in recent years. It has however not been used for the  English/Persian language pair. This paper presents the first such attempt and describes the problems faced in creating a corpus and building a base line system. Our experience with  the construction of a parallel corpus during this ongoing study and the problems encountered especially with the process of alignment are discussed in this paper. The prototype  constructed and its evaluation using the BiLingual Evaluation Understudy (BLEU) is briefly described and results are analyzed. In the final part of the paper, conclusions are drawn  and work planned for the future is discussed","At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .","['At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model.', 'In particular, our search algorithm finds optimal transductions of test sentences in less than ""real time"" on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application.']",1,"['At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .']"
CC296,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,planning text for advisory dialogues,"['Johanna D Moore', 'Cecile Paris']",,"Explanation is an interactive process requiring a dialogue between advice-giver and advice-seeker. In this paper, we argue that in order to participate in a dialogue with its users, a generation system must be capable of reasoning about its own utterances and therefore must maintain a rich representation of the responses it produces. We present a text planner that constructs a detailed text plan, containing the intentional, attentional, and rhetorical structures of the text it generates.","1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG , 203 ) .","['IGEN constructs its plans using a hierarchical planning algorithm (Nilsson 1980).', 'The planner first checks all of its top-level plans to see which have effects that match the goal.', ""Each matching plan's preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan's body."", ""1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987)."", 'In IGEN, the plans can involve any goals or actions that could be achieved via communication.']",0,"[""1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987)."", 'In IGEN, the plans can involve any goals or actions that could be achieved via communication.']"
CC297,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,building another bridge over the generation gap,['Leo Wanner'],,"In this paper, we address one of the central problems in text generation: the missing link (&quot;the generation gap&quot; in Meteer&apos;s terms) between the global discourse organization as often provided by text planning modules and the linguistic realization of this organiza- tion. We argue that the link should be established by the lexical choice process using resources derived from Mel&apos;iuk&apos;s ezicel Functions (LFs). In particular, we demonstrate that sequences of LFs may well serve as lexical discourse structure relations which link up to global discourse relations in the output of a Rhelorical Structure Theory style text planner","McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG ) .","['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']",0,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"
CC298,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,has a consensus nl generation architecture appeared and is it psycholinguistically plausible,['Ehud Reiter'],,"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems","In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ) .","['The opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ) .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', ""Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity.""]",0,"['In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ) .']"
CC299,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating event descriptions with sage a simulation and generation environment,['Marie W Meteer'],,"The SAGE system (Simulation and Generation Environment) was developed to address issues at the interface between conceptual modelling and natural language generation. In this paper, I describe SAGE and its components in the context of event descriptions. I show how kinds of information, such as the Reichenbachian temporal points and event structure, which are usually treated as unified systems, are often best represented at multiple levels in the overall system. SAGE is composed of a knowledge representation language and simulator, which form the underlying model and constitute the ""speaker""; a graphics component, which displays the actions of the simulator and provides an anchor for locative and deictic relations; and the generator SPOKESMAN, which produces a textual narration of events.","McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .","['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']",0,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"
CC300,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating natural language linder pragmatic constraints lawrence erlbaum,['Eduard H Hovy'],,,"Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAGa ) .","['The point here is not just that IGEN can produce different lexical realizations for a particular concept.', 'If that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network (or similar device) to test various features of the information being expressed.', 'The planner could supply whatever information is needed to drive the network.', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAGa ) .']",0,"['The point here is not just that IGEN can produce different lexical realizations for a particular concept.', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAGa ) .']"
CC301,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,an overview of the nigel text generation grammar,['William C Mann'],,"Research on the text generation task has led to creation of a large systemic grammar of English, Nigel, which is embedded in a computer program. The grammar and the systemic framework have been extended by addition of a semantic stratum. The grammar generates sentences and other units under several kinds of experimental control.This paper describes augmentations of various precedents in the systemic framework. The emphasis is on developments which control the text to fulfill a purpose, and on characteristics which make Nigel relatively easy to embed in a larger experimental program.","These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) .","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']",0,"[""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) .""]"
CC302,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,telegram a grammar formalism for language planning,['Douglas E Appelt'],,,"These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) .","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.5']",0,"[""These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) .""]"
CC303,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,intentions structure and expression in multilingual instructions,"['C6cile L Paris', 'Donia R Scott']",,"Instructional tex-ts have been the object of many studies recently, motivated by the increased need to produce manuals (especially multilingual manuals) coupled with the cost of translators and technical writers. Because these studies concentrate on aspects other than the linguistic realismion of instructions for example, the integration of text and graphi c s they all generate a sequence of steps required to achieve a task, using imperatives. Our research so flushows, however, that manuals can iu fact have different styles, i.e., not all instructions are stated using a sequence of imperatives, and that, furthermore, different parts of manuals often use different styles. In this paper, we present our preliminary results from an analysis of over 30 user guides/manuals for consumer appliances and discuss some of the implications.","This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .","['One possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .']",0,"['This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .']"
CC304,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating natural language linder pragmatic constraints lawrence erlbaum,['Eduard H Hovy'],,,"These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) .","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']",0,"[""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) .""]"
CC305,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating natural language linder pragmatic constraints lawrence erlbaum,['Eduard H Hovy'],,,Hovy has described another text planner that builds similar plans ( #AUTHOR_TAGb ) .,"['Hovy has described another text planner that builds similar plans ( #AUTHOR_TAGb ) .', 'This system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern; it is thus not a planner in the sense used here (as Hovy makes clear).', '10 Since text planning was not the primary focus of this work, IGEN is designed to simply assume that any false preconditions are unattainable.', ""IGEN's planner divides the requirements of a plan into two parts: the preconditions, which are not planned for, and those in the plan body, which are."", 'This has no .']",0,['Hovy has described another text planner that builds similar plans ( #AUTHOR_TAGb ) .']
CC306,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,has a consensus nl generation architecture appeared and is it psycholinguistically plausible,['Ehud Reiter'],,"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems",Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG ) .,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG ) .']",0,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG ) .']"
CC307,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating natural language linder pragmatic constraints lawrence erlbaum,['Eduard H Hovy'],,,"The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .","['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']",0,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"
CC308,J00-3002,Incremental Processing and Acceptability,geometry of lexicosyntactic interaction,['Glyn Morrill'],,,"Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and #AUTHOR_TAG .","['Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and #AUTHOR_TAG .', 'Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained.']",0,"['Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and #AUTHOR_TAG .']"
CC309,J00-3002,Incremental Processing and Acceptability,parsing and derivational equivalence,"['Mark Hepple', 'Glyn Morrill']",,"It is a tacit assumption of much linguistic inquiry that all distinct derivations of a string should assign distinct meanings. But despite the tidiness of such derlvational uniqueness, there seems to be no a priori reason to assume that a gramma r must have this property. If a grammar exhibits derivational equivalence, whereby distinct derivations of a string assign the same meanings, naive exhaustive search for all derivations will be redundant, and quite possibly intractable. In this paper we show how notions of derivation-reduction and normal form can be used to avoid unnecessary work while parsing with grammars exhibiting derivational equivalence. With grammar regarded as analogous to logic, derivations are proofs; what we are advocating is proof-reduction, and normal form proof; the invocation of these logical techniques adds a further paragraph to the story of parsing-as-deduction","An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .","['(17) By a result of Zielonka (1981), the Lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of Lambek calculus.', 'Combinatory categorial grammar does not concern itself with the capture of all (or only) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design.', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']",0,"['An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']"
CC310,J00-3002,Incremental Processing and Acceptability,parsing as natural deduction,['Esther Konig'],,"The logic behind parsers for categorial grammars can be formalized in several different ways. Lambek Calculus (LC) constitutes an example for a natural deduction1 style parsing method.In natural language processing, the task of a parser usually consists in finding derivations for all different readings of a sentence. The original Lambek Calculus, when it is used as a parser/theorem prover, has the undesirable property of allowing for the derivation of more than one proof for a reading of a sentence, in the general case.In order to overcome this inconvenience and to turn Lambek Calculus into a reasonable parsing method, we show the existence of ""relative"" normal form proof trees and make use of their properties to constrain the proof procedure in the desired way.","One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .","['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the Lambek calculus iff F ~ A is a theorem of the regulated calculus.', 'However, apart from the issue regarding .L, there is a general cause for dissatisfaction with this approach: it assumes the initial presence of the entire sequent to be proved, i.e., it is in principle nonincremental; on the other hand, allowing incrementality on the basis of Cut would reinstate with a vengeance the problem of spurious ambiguity, for then what are to be the Cut formulas?', 'Consequently, the sequent approach is ill-equipped to address the basic asymmetry of language--the asymmetry of its processing in time---and has never been forwarded in a model of the kind of processing phenomena cited in the introduction.']",0,"['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .']"
CC311,J00-3003,Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,empirical studies on the disambiguation of cue phrases,"['Julia B Hirschberg', 'Diane J Litman']",,"Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse. For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also has one or more alternate uses. While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed. This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech",It is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .,"['DA classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .', 'Similarly, we find distinctive correlations between certain phrases and DA types.', 'For example, 92.4% of the uh-huh\'s occur in BACKCHANNELS, and 88.4% of the trigrams ""<start> do you"" occur in YES-NO-QUESTIONS.', 'To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type.', '5.1.1', 'Classification from True Words.', 'Assuming that the true (hand-transcribed) words of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) in a straightforward way, by building a statistical language model for each of the 42 DAs.', 'All DAs of a particular type found in the training corpus were pooled, and a DA-specific trigram model was estimated using standard techniques (Katz backoff [Katz 1987] with Witten-Bell discounting [Witten and Bell 1991]).']",4,['It is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .']
CC312,J00-3003,Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,automatic grammar induction and parsing free text a transformationbased approach,['Eric Brill'],,"In this paper we describe a new technique for parsing free text: a transformational grammar1 is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.","A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #AUTHOR_TAG ) .","['All the work mentioned so far uses statistical models of various kinds.', 'As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way.', 'However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #AUTHOR_TAG ) .', 'Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed.']",1,"['A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #AUTHOR_TAG ) .']"
CC313,J00-3003,Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,a stochastic parts program and noun phrase parser for unrestricted text,['Kenneth Ward Church'],,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,"The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG ) .","['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG ) .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']",1,"['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG ) .']"
CC314,J00-3003,Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,automatic stochastic tagging of natural language texts,"['Evangelos Dermatas', 'George Kokkinakis']",,"Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English, French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages. The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words. A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer. Measurements of error rate, time response, and memory requirements have shown that the taggers' performance is satisfactory, even though a small training text is available. The error rate is improved when new texts are used to update the stochastic model parameters.","It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .","['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']",0,"['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .']"
CC315,J00-4002,Bidirectional Contextual Resolution,categorial semantics and scoping,['Fernando C N Pereira'],,"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings.",This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG .,"[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.']",1,"[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG .""]"
CC316,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved. 1. INTRODUCTION  The monotonicity property of unification based grammar formalisms is perhaps the most important factor in their widespread use for grammatical description and parsing. Monotonicity guarantees that the grammatical analysis of a sentence can proceed incrementally by combining information from rules and lexical entries in a nondestructive way. By contrast, aspects of semantic interpretation, such as reference and quantifier scope resolution, are often realised by non-mo..","These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .","['What is required is that QLFs are, as here, expressed in a typed higher-order logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, focus, etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .', 'Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context.', '~']",0,"['These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .', 'Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context.']"
CC317,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.","In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.","[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE)."", 'In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']",1,"[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE)."", 'In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']"
CC318,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.",A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG .,"['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', 'The role of resolution rules (for perfectly good presentational reasons) is completely ignored by their treatment.', 'However, it is really the case that in giving the semantics of a QLF, one is interested only in the set of RQLFs that are obtainable from it under closure of the resolution rules.', 'Ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'This is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right.', 'Anyone who has built a wide-coverage system knows that the range of context-dependent phenomena encountered in real life is a lot wider than the preoccupations of many linguists might suggest.', 'In the CLE, for example, contextual resolution forms a larger part of the system than do syntactic and semantic processing.', 'Unfortunately, in the CLE there is no formal theory of resolution rules, and thus no prospect of capturing their role in assigning a semantics to QLFs.']",1,"['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.']"
CC319,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",introduction,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.","We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language  approach of Dalrymple et al. ( 1996 ) .","['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language  approach of Dalrymple et al. ( 1996 ) .']",1,"['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language  approach of Dalrymple et al. ( 1996 ) .']"
CC320,J00-4002,Bidirectional Contextual Resolution,resolving quasi logical forms,['Hiyan Alshawi'],,"The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descripitions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of ""reference resolution rules"" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied.","The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) .","[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", 'In the CLE-QLF approach, as ra-tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994), the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']",1,"[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) .""]"
CC321,J00-4002,Bidirectional Contextual Resolution,resolving quasi logical forms,['Hiyan Alshawi'],,"The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descripitions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of ""reference resolution rules"" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied.","We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.","['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']",0,"['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']"
CC322,J00-4002,Bidirectional Contextual Resolution,training and scaling preference functions for disambiguation,"['Hiyan Alshawi', 'David M Carter']",,"We present an automatic method for weighting the contributions of preference functions used in disambiguation. Initial scaling factors are derived as the solution to a least squares minimization problem, and improvements are then made by hill climbing. The method is applied to disambiguating sentences in the Air Travel Information System corpus, and the performance of the resulting scaling factors is compared with hand-tuned factors. We then focus on one class of preference function, those based on semantic lexical collocations. Experimental results are presented showing that such functions vary considerably in selecting correct analyses. In particular, we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations.","The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG ) , with the resolution process as described here .","['There are several stategies that might be pursued.', 'One is to adopt Pinkal\'s ""radical underspecification"" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG ) , with the resolution process as described here ."", 'Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here.']",3,"[""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG ) , with the resolution process as described here ."", 'Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here.']"
CC323,J00-4002,Bidirectional Contextual Resolution,categorial semantics and scoping,['Fernando C N Pereira'],,"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings.","The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .","['We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .', ""Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin."", 'We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (Alshawi 1990), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '6']",1,"['The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .']"
CC324,J00-4002,Bidirectional Contextual Resolution,categorial semantics and scoping,['Fernando C N Pereira'],,"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings.","It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #AUTHOR_TAG , 1991 ) .","['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #AUTHOR_TAG , 1991 ) .', 'Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a ""free variable"" of type e is introduced in the NP position, with an associated ""quantifier assumption,"" which is added as a kind of premise.', 'At a later stage the quantifier assumption is ""discharged,"" capturing all occurrences of the free variable.', 'Thus their analysis of something like every manager disappeared would proceed as follows:']",1,"['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #AUTHOR_TAG , 1991 ) .', 'Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a ""free variable"" of type e is introduced in the NP position, with an associated ""quantifier assumption,"" which is added as a kind of premise.']"
CC325,J00-4002,Bidirectional Contextual Resolution,an algorithm for generating quantifier scopings,"['Jerry R Hobbs', 'Stuart M Shieber']",,"The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination. But scope dependencies are not so transparent. As a result, many systems for representing the semantics of sentences have ignored scoping or generating scoping mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow. In this paper, we present an algorithm, along with proofs of some of its important properties, that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure. The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy.Engineering and Applied Science","only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â¢ partial scopings are permitted ( see Reyle [ 19961 ) â¢ scoping can be freely interleaved with other types of reference resolution ; â¢ unscoped or partially scoped forms are available for inference or for generation at every stage .","['only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â\x80¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â\x80¢ partial scopings are permitted ( see Reyle [ 19961 ) â\x80¢ scoping can be freely interleaved with other types of reference resolution ; â\x80¢ unscoped or partially scoped forms are available for inference or for generation at every stage .']",0,"['only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â\x80¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â\x80¢ partial scopings are permitted ( see Reyle [ 19961 ) â\x80¢ scoping can be freely interleaved with other types of reference resolution ; â\x80¢ unscoped or partially scoped forms are available for inference or for generation at every stage .']"
CC326,J00-4002,Bidirectional Contextual Resolution,on reasoning with ambiguities,['Uwe Reyle'],,"The paper adresses the problem of reasoning with ambiguities. Semantic representations are presented that leave scope relations between quantifiers and/or other operators unspecified. Truth conditions are provided for these representations and different consequence relations are judged on the basis of intuitive correctness. Finally inference patterns are presented that operate directly on these underspecified structures, i.e. do not rely on any translation into the set of their disambiguations.Comment: EACL 199","But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .","['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .', 'Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'His example is:']",5,"['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .', 'Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'His example is:']"
CC327,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.","#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .","['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']",0,"['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']"
CC328,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,cogniac high precision coreference with limited knowledge and linguistic resources,['Breck Baldwin'],,"This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach.","A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .']"
CC329,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,centeringinthelarge computing referential discourse segments,"['Udo Hahn', 'Michael Strube']",,"We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied.","Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']"
CC330,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,never look back an alternative to centering,['Michael Strube'],,"I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word.","Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG ) .","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']",0,"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG ) .""]"
CC331,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,evaluating anaphora resolution approaches,['Ruslan Mitkov'],,,"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAGa , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAGa , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAGa , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"
CC332,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,anaphora for everyone pronominal anaphora resolution without a parser,"['Christopher Kennedy', 'Branimir Boguraev']",,"We present an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass, 1994). In contrast to that work, our algorithm does not require in-depth, full, syntactic parsing of text. Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from the output of a part of speech tagger, enriched only with annotations of grammatical function of lexical items in the input text stream. Evaluation of the results of our implementation demonstrates that accurate anaphora resolution can be realized within natural language processing frameworks which do not---or cannot--- employ robust and reliable parsing components.","A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .","['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']"
CC333,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,functional centering,"['Michael Strube', 'Udo Hahn']",,"Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering.","Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']",0,"['Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']"
CC334,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,probabilistic coreference in information extraction,['Andrew Kehler'],,"Certain applications require that the out-put of an information extraction system be probabilistic, so that a downstream sys-tem can reliably fuse the output with pos-sibly contradictory information from other sources. In this paper we consider the problem of assigning a probability distri-bution to alternative sets of coreference re-lationships among entity descriptions. We present the results of initial experiments with several approaches to estimating such distributions in an application using SRI's FASTUS information extraction system.","Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']"
CC335,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,multilingual anaphora resolution,['Ruslan Mitkov'],,"This paper presents amultilingual robust, knowledge-poor approach to resolvingpronouns in technical manuals. This approach is a modification of the practicalapproach (Mitkov 1998a) and operates on texts pre-processed by apart-of-speech tagger. Input is checked against agreementand a number of antecedent indicators. Candidates are assigned scores by eachindicator and the candidate with the highest aggregate score isreturned as the antecedent. We propose this approach as aplatform for multilingual pronoun resolution. The robust approach was initiallydeveloped and tested for English, but we have also adaptedand tested it for Polish and Arabic. For bothlanguages, we found that adaptation required minimummodification and that further, even if used unmodified, the approachdelivers acceptable success rates. Preliminary evaluation reports high successrates in the range of over 90%.","Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .']"
CC336,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,robust method of pronoun resolution using fulltext information,['Tetsuya Nasukawa'],,"A consistent text contains rich information for resolving ambiguities within its sentences. Even simple syntactic information such as word occurrence and collocation patterns, which can be extracted from the text without deep discourse analysis, improves the accuracy of sentence analysis. Pronoun resolution is a typical proceeding that utilizes this information. Through the use of this information, along with information on the syntactic position of each candidate, 93.8% of pronoun references were resolved correctly in an experiment on computer manuals.","A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .","['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']"
CC337,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,an architecture for anaphora resolution,"['Elaine Rich', 'Susann LuperFoy']",,"In this paper, we describe the pronominal anaphora resolution module of Lucy, a porta.ble English understanding system. The design.of thi.s module was motivated by the observation that, al- though there exist many theories of anaphora resolution, no one of these theories is complete. Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose .can- didate antecedents and to evaluate each other&apos;s proposals","Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .']"
CC338,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,improving pronoun resolution in two languages by means of bilingual corpora,"['Ruslan Mitkov', 'Catalina Barbu']",,,"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .']"
CC339,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,outstanding issues in anaphora resolution,['Ruslan Mitkov'],,,"The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .","['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .', 'A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.', 'In particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts.', 'Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future.']",3,"['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .']"
CC340,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,analysis of syntaxbased pronoun resolution methods,['Joel Tetreault'],,"This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory (Grosz et al., 1995) and is an alternative to Brennan et al.'s 1987 algorithm. The advantages of this new model, the Left-Right Centering Algorithm (LRC), lie in its incremental processing of utterances and in its low computational overhead. The algorithm is compared with three other pronoun resolution methods: Hobbs' syntax-based algorithm, Strube's S-list approach, and the BFP Centering algorithm. All four methods were implemented in a system and tested on an annotated subset of the Treebank corpus consisting of 2026 pronouns. The noteworthy results were that Hobbs and LRC performed the best.","Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"
CC341,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,anaphora resolution a multistrategy approach,"['Jaime Carbonell', 'Ralf Brown']",,,"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .']"
CC342,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,description of the university of pennsylvania system used for muc6,"['Breck Baldwin', 'Jeff Reynar', 'Mike Collins', 'Jason Eisner', 'Adwait Ratnaparki', 'Joseph Rosenzweig', 'Anoop Sarkar', 'Srivinas Bangalore']",,,"The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .']"
CC343,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,multilingual coreference resolution,"['Sanda Harabagiu', 'Steven Maiorano']",,"The current work investigates the problems that occur when coreference resolution is considered as a multilingual task. We assess the issues that arise when a framework using the mention-pair coreference resolution model and memory-based learning for the resolution process are used. Along the way, we revise three essential subtasks of coreference resolution: mention detection, mention head detection and feature selection. For each of these aspects we propose various multilingual solutions including both heuristic, rule-based and machine learning methods. We carry out a detailed analysis that includes eight different languages (Arabic, Catalan, Chinese, Dutch, English, German, Italian and Spanish) for which datasets were provided by the only two multilingual shared tasks on coreference resolution held so far: SemEval-2 and CoNLL-2012. Our investigation shows that, although complex, the coreference resolution task can be targeted in a multilingual and even language independent way. We proposed machine learning methods for each of the subtasks that are affected by the transition, evaluated and compared them to the performance of rule-based and heuristic approaches. Our results confirmed that machine learning provides the needed flexibility for the multilingual task and that the minimal requirement for a language independent system is a part-of-speech annotation layer provided for each of the approached languages. We also showed that the performance of the system can be improved by introducing other layers of linguistic annotations, such as syntactic parses (in the form of either constituency or dependency parses), named entity information, predicate argument structure, etc. Additionally, we discuss the problems occurring in the proposed approaches and suggest possibilities for their improvement","Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .']"
CC344,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,pronoun resolution the practical alternative presented at the discourse anaphora and anaphor resolution colloquium daarc1,['Ruslan Mitkov'],,,"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG , 1998b ) .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG , 1998b ) .']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG , 1998b ) .']"
CC345,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,resolving pronoun references,['Jerry Hobbs'],,"Two approaches to the problem of resolving pronoun references are presented. The first is a naive algorithm that works by traversing the surface parse trees of the sentences of the text in a particular order looking for noun phrases of the correct gender and number. The algorithm clearly does not work in all cases, but the results of an examination of several hundred examples from published texts show that it performs remarkably well. In the second approach, it is shown how pronoun solution can be handled in a comprehensive system for semantic analysis of English texts. The system is described, and it is shown in a detailed treatment of several examples how semantic analysis locates the antecedents of most pronouns as a by-product. Included are the classic examples of Winograd and Charniak.","Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']",0,"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .""]"
CC346,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,robust reference resolution with limited knowledge high precision genrespecific approach for english and polish,"['Ruslan Mitkov', 'Malgorzata Stys']",,"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labourand time-consuming task. This paper presents a robust, knowledgepoor approach to resolving pronouns in technical manuals in both English and Polish. This approach is a modification of the practical approach reported in [Mitkov 97] and operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and tested for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Preliminary evaluation reports precision of over 90%.","Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.']"
CC347,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,recognizing referential links an information extraction perspective,['Megumi Kameyama'],,"We present an efficient and robust reference resolution algorithm in an end-to-end state-of-the-art information extraction system, which must work with a considerably impoverished syntactic analysis of the input sentences. Considering this disadvantage, the basic setup to collect, filter, then order by salience does remarkably well with third-person pronouns, but needs more semantic and discourse information to improve the treatments of other expression types.","The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"
CC348,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,toward a computational theory of definite anaphora comprehension in english,['Candace Sidner'],,"Abstract : This report investigates the process of focussing as a description and explanation of the comprehension of certain anaphoric expressions in English discourse. The investigation centers on the interpretation of definite anaphora, that is, on the personal pronouns, and noun phrases used with a definite article the, this, or that. Focussing is formalized as a process in which a speaker centers attention on a particular aspect of the discourse. An algorithmic description specifies what the speaker can focus on and how the speaker may change the focus of the discourse as the discourse unfolds. The algorithm allows for a simple focussing mechanism to be constructed: an element in focus, an ordered collection of alternate foci, and a stack of old foci. The data structure for the element in focus is a representation which encodes a limited set of associations between it and other elements from the discourse as well as from general knowledge. This report also establishes other constraints which are needed for the successful comprehension of anaphoric expressions. The focussing mechanism is designed to take advantage of syntactic and semantic information encoded as constraints on the choice of anaphora interpretation. These constraints are due to the work of language researchers; and the focussing mechanism provides a principled means for choosing when to apply the constraints in the comprehension process.","Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .']"
CC349,J02-3002,"Periods, Capitalized Words, etc.",overview of muc7”,['Nancy Chinchor'],,,Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG 1998) of information extraction.,"['Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There the disambiguation of the first word in a sentence (and in other ambiguous positions) is one of the central problems: about 20% of named entities occur in ambiguous positions.', 'For instance, the word Black in the sentenceinitial position can stand for a person�s surname but can also refer to the color.', 'Even in multiword capitalized phrases, the first word can belong to the rest of the phrase or can be just an external modifier.', 'In the sentence Daily, Mason and Partners lost their court case, it is clear that Daily, Mason and Partners is the name of a company.', 'In the sentence Unfortunately, Mason and Partners lost their court case, the name of the company does not include the word Unfortunately, but the word Daily is just as common a word as Unfortunately.']",0,"['Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1']"
CC350,J02-3002,"Periods, Capitalized Words, etc.",robust partofspeech tagging using a hidden markov model computer speech and language,['Julian Kupiec'],,,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']",1,"[""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.']"
CC351,J02-3002,"Periods, Capitalized Words, etc.",a knowledgefree method for capitalized word disambiguation,['Andrei Mikheev'],,,"This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG .","['The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.', 'Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG .']",5,"['The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.', 'Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG .']"
CC352,J02-3002,"Periods, Capitalized Words, etc.",frequency analysis of english usage lexicon and grammar,"['W Nelson Francis', 'Henry Kucera']",,,"There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .","['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens.']",5,"['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.']"
CC353,J02-3002,"Periods, Capitalized Words, etc.",language independent named entity recognition combining morphological and contextual evidence”,"['Silviu Cucerzan', 'David Yarowsky']",,"Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchicaily smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools","Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG ) .","['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']",0,"['Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG ) .']"
CC354,J02-3002,"Periods, Capitalized Words, etc.",some applications of treebased modeling to speech and language indexing”,['Michael D Riley'],,,"Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .","['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']",0,"['Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.']"
CC355,J02-3002,"Periods, Capitalized Words, etc.",adaptive sentence boundary disambiguation”,"['David D Palmer', 'Marti A Hearst']",,"Labeling of sentence boundaries is a necessary prerequisite for many natural language processing tasks, including part-of-speech tagging and sentence alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules. As an alternative, we have developed an efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network. After training for less than one minute, the method correctly labels over 98.5 % of sentence boundaries in a corpus of over 27,000 sentence-boundary marks. We show the method to be efficient and easily adaptable to different text genres, including single-case texts.Comment: This is a Latex version of the previously submitted ps file   (formatted as a uuencoded gz-compressed .tar file created by csh script). The   software from the work described in this paper is available by contacting   dpalmer@cs.berkeley.ed","Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .","['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']",0,"['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.']"
CC356,J02-3002,"Periods, Capitalized Words, etc.",transformationbased errordriven learning and natural language parsing a case study in partofspeech tagging,['Eric Brill'],,,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']",1,"[""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.']"
CC357,J02-3002,"Periods, Capitalized Words, etc.",a maximum entropy model for partofspeech tagging”,['Adwait Ratnaparkhi'],,,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']",1,"[""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.']"
CC358,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.",The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']",1,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .']"
CC359,J02-3002,"Periods, Capitalized Words, etc.",mitre description of the alembic system used for muc6”,"['John S Aberdeen', 'John D Burger', 'David S Day', 'Lynette Hirschman', 'Patricia Robinson', 'Marc Vilain']",,,"For instance , the Alembic workbench ( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .","['There exist two large classes of SBD systems: rule based and machine learning.', 'The rule-based systems use manually built rules that are usually encoded in terms of regular-expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'To put together a few rules is fast and easy, but to develop a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench ( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', 'Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.']",0,"['For instance , the Alembic workbench ( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', 'Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.']"
CC360,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",conclusion,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.",On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .,"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', 'Although these error rates seem to be very small, they are quite significant.', 'Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).', 'This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.', 'On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.']",1,['On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .']
CC361,J02-3002,"Periods, Capitalized Words, etc.",transformationbased errordriven learning and natural language parsing a case study in partofspeech tagging,['Eric Brill'],,,"This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .","['We decided to train the tagger with the minimum of preannotated resources.', 'First, we used 20,000 tagged words to ""bootstrap"" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.', 'We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%.', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'Periods as many other closed-class words cannot be successfully covered by such technique.']",1,"[""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .""]"
CC362,J02-3002,"Periods, Capitalized Words, etc.",tagging sentence boundaries”,['Andrei Mikheev'],,In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.,"In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG .","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG .', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']",1,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG .']"
CC363,J02-3002,"Periods, Capitalized Words, etc.",a stochastic parts program and nounphrase parser for unrestricted text”,['Kenneth Church'],,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,"As #AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .","['Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'As #AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title)."", 'It would be misleading to infer from this evidence that the word \'Acts\' is always a proper noun.""']",1,"['Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'As #AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.']"
CC364,J02-3002,"Periods, Capitalized Words, etc.",statistical inference for probabilistic functions of finite markov chains,"['Leonard E Baum', 'Ted Petrie']",,,"This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .","['We decided to train the tagger with the minimum of preannotated resources.', 'First, we used 20,000 tagged words to ""bootstrap"" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.', 'We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%.', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'Periods as many other closed-class words cannot be successfully covered by such technique.']",1,"[""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.']"
CC365,J02-3002,"Periods, Capitalized Words, etc.",russian morphology an engineering approach,"['Andrei Mikheev', 'Liubov Liubushkina']",experiments,"Morphological analysis, which is at the heart of the processing of natural language requires computationally effective morphological processors. In this paper an approach to the organization of an inflectional morphological model and its application for the Russian language are described. The main objective of our morphological processor is not the classification of word constituents, but rather an efficient computational recognition of morpho-syntactic features of words and the generation of words according to requested morpho-syntactic features. Another major concern that the processor aims to address is the ease of extending the lexicon. The templated word-paradigm model used in the system has an engineering flavour: paradigm formation rules are of a bottom-up (word specific) nature rather than general observations about the language, and word formation units are segments of words rather than proper morphemes. This approach allows us to handle uniformly both general cases and exceptions, and requires extremely simple data structures and control mechanisms which can be easily implemented as a finite-state automata. The morphological processor described in this paper is fully implemented for a substantial subset of Russian (more then 1,500,000 word-tokens - 95,000 word paradigms) and provides an extensive list of morpho-syntactic features together with stress positions for words utilized in its lexicon. Special dictionary management tools were built for browsing, debugging and extension of the lexicon. The actual implementation was done in C and C++, and the system is available for the MS-DOS, MS-Windows and UNIX platforms.","Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .","['Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms.', 'Since the documents in the BBC news corpus were rather short, we applied the cache module, as described in Section 11.1.', 'This allowed us to reuse information across the documents.']",5,"['Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms.']"
CC366,J02-3002,"Periods, Capitalized Words, etc.",one sense per collocation”,['David Yarowsky'],,"Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse. In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation. We test his empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99 % accuracy for binary ambiguities. We utilize this property in a disambiguation algorithm that achieves precision of 92 % using combined models of very local context. 1","This is similar to ""one sense per collocation"" idea of #AUTHOR_TAG .","['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of #AUTHOR_TAG .']",1,"['This is similar to ""one sense per collocation"" idea of #AUTHOR_TAG .']"
CC367,J02-3002,"Periods, Capitalized Words, etc.",celex a guide for users centre for lexical information,['Gavin Burnage'],,,"A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG ) .","['The first list on which our method relies is a list of common words.', 'This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).', 'We do not have to rely on pre-existing resources, however.', 'A list of common words can be easily obtained automatically from a raw (unannotated in any way) text collection by simply collecting and counting lowercased words in it.', 'We generated such list from the NYT collection.', 'To account for potential spelling and capitalization errors, we included in the list of common words only words that occurred lower-cased at least three times in the NYT texts.', 'The list of common words that we developed from the NYT collection contained about 15,000 English words.']",0,"['A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).']"
CC368,J02-3002,"Periods, Capitalized Words, etc.",some applications of treebased modeling to speech and language indexing”,['Michael D Riley'],conclusion,,The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .,"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate).', 'Although these error rates seem to be very small, they are quite significant.', 'Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).', 'This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.', 'On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.']",1,"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate).']"
CC369,J02-3002,"Periods, Capitalized Words, etc.",nymble a high performance learning namefinder”,"['Daniel Bikel', 'Scott Miller', 'Richard Schwartz', 'Ralph Weischedel']",,"This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.",In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ) .,"['In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ) .', 'The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data.', 'Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port.']",0,['In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ) .']
CC370,J02-3002,"Periods, Capitalized Words, etc.",overview of muc7”,['Nancy Chinchor'],,,"For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .","['• abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', 'Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.']",5,"['* abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', 'Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.']"
CC371,J02-3002,"Periods, Capitalized Words, etc.",some applications of treebased modeling to speech and language indexing”,['Michael D Riley'],,,"The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']",1,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']"
CC372,J02-3002,"Periods, Capitalized Words, etc.",what is a word what is a sentence problems of tokenization”,"['Gregory Grefenstette', 'Pasi Tapanainen']",,"Any linguistic treatment of freely occurring text must provide an answer to what is considered as a token. In arti cial languages, the de nition of what is considered as a token can be precisely and unambiguously de ned. Natural languages, on the other hand, display such a rich variety that there are many ways to decide upon what will be considered as a unit for a computational approach to text. Here we will discuss tokenization as a problem for computational lexicography. Our discussion will cover the aspects of what is usually considered preprocessing of text in order to prepare it for some automated treatment. We present the roles of tokenization, methods of tokenizing, grammars for recognizing acronyms, abbreviations, and regular expressions such as numbers and dates. We present the problems encountered and discuss the e ects of seemingly innocent choices.","One of the better-known approaches is described in #AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .","['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', 'Park and Byrd (2001) recently described a hybrid method for finding abbreviations and their definitions.', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']",0,"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.']"
CC373,J02-3002,"Periods, Capitalized Words, etc.",unsupervised word sense disambiguation rivaling supervised methods”,['David Yarowsky'],,"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%","Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .","['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']",0,"['Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .']"
CC374,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",introduction,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.",A detailed introduction to the SBD problem can be found in #AUTHOR_TAG .,"['Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting.', 'Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop).', 'A detailed introduction to the SBD problem can be found in #AUTHOR_TAG .']",0,"['Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting.', 'Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop).', 'A detailed introduction to the SBD problem can be found in #AUTHOR_TAG .']"
CC375,J02-3002,"Periods, Capitalized Words, etc.",tagging sentence boundaries”,['Andrei Mikheev'],conclusion,In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.,"This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .","['With all its strong points, there are a number of restrictions to the proposed approach.', 'First, in its present form it is suitable only for processing of reasonably ""wellbehaved"" texts that consistently use capitalization (mixed case) and do not contain much noisy data.', 'Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']",1,"['This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']"
CC376,J02-3002,"Periods, Capitalized Words, etc.",identifying unknown proper names in newswire text”,"['Inderjeet Mani', 'T Richard MacMillan']",,"The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text. A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction. This paper describes a system which uses text skimming techniques for deriving proper names and their semantic attributes automatically from newswire text, without relying on any listing of name elements. In order to identify new names, the system treats proper names as (potentially) context-dependent linguistic expressions. In addition to using information in the local context, the system exploits a computational model of discourse which identifies individuals based on the way they are described in the text, instead of relying on their description in a pre-existing knowledge base.",#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .,"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']",5,['#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .']
CC377,J02-3002,"Periods, Capitalized Words, etc.",tagging sentence boundaries”,['Andrei Mikheev'],,In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.,"Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries .","['To test our hypothesis that DCA can be used as a complement to a local-context approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger.', 'Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries .']",5,"['Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries .']"
CC378,J02-3002,"Periods, Capitalized Words, etc.",mitre description of the alembic system used for muc6”,"['John S Aberdeen', 'John D Burger', 'David S Day', 'Lynette Hirschman', 'Patricia Robinson', 'Marc Vilain']",,,The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']",1,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.']"
CC379,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",conclusion,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.","This is where robust syntactic systems like SATZ ( #AUTHOR_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .","['With all its strong points, there are a number of restrictions to the proposed approach.', 'First, in its present form it is suitable only for processing of reasonably ""wellbehaved"" texts that consistently use capitalization (mixed case) and do not contain much noisy data.', 'Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( #AUTHOR_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']",1,"['This is where robust syntactic systems like SATZ ( #AUTHOR_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']"
CC380,J02-3002,"Periods, Capitalized Words, etc.",language model adaptation using mixtures and an exponentially decaying cache”,"['Philip Clarkson', 'Anthony J Robinson']",,"Presents two techniques for language model adaptation. The first is based on the use of mixtures of language models: the training text is partitioned according to topic, a language model is constructed for each component and, at recognition time, appropriate weightings are assigned to each component to model the observed style of language. The second technique is based on augmenting the standard trigram model with a cache component in which the words' recurrence probabilities decay exponentially over time. Both techniques yield a significant reduction in perplexity over the baseline trigram language model when faced with a multi-domain test text, the mixture-based model giving a 24% reduction and the cache-based model giving a 14% reduction. The two techniques attack the problem of adaptation at different scales, and as a result can be used in parallel to give a total perplexity reduction of 30%.","#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last","['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']",2,"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last""]"
CC381,J02-3002,"Periods, Capitalized Words, etc.",eagle an extensible architecture for general linguistic engineering”,"['Breck Baldwin', 'Christine Doran', 'Jeffrey Reynar', 'Michael Niv', 'Bangalore Srinivas', 'Mark Wasson']",,"Over the course of two summer projects, we developed a general purpose natural language system which advances the state-of-the-art in several areas. The system contains demonstrated advancements in part-of-speech tagging, end-of-sentence detection, and coreference resolution. In addition, we believe that we have strong maximal noun phrase detection, and subject-verb-object recognition and a pat tern matching language well suited to a range of tasks. Other features of the system include modularity and interchangeability of components, rapid component integration and a debugging environment.",The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .,"['The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', 'The description of the EAGLE case normalization module provided by Baldwin et al. is, however, very brief and provides no performance evaluation or other details.']",1,['The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .']
CC382,J02-3002,"Periods, Capitalized Words, etc.",a maximum entropy approach to identifying sentence boundaries”,"['Jeffrey C Reynar', 'Adwait Ratnaparkhi']",,"We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of ., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.Comment: 4 pages, uses aclap.sty and covingtn.st","Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .","['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']",0,"['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.']"
CC383,J02-3002,"Periods, Capitalized Words, etc.",one term or two” in,['Kenneth Church'],introduction,"Objective SYNTAX score II (SSII) is a long-term mortality prediction model to guide the decision making of the heart-team between coronary artery bypass grafting or percutaneous coronary intervention (PCI) in patients with left main or three-vessel coronary artery disease. This study aims to investigate the long-term predictive value of SSII for all-cause mortality in patients with one- or two-vessel disease undergoing PCI. Methods A total of 628 patients (76% men, mean age: 61+-10 years) undergoing PCI due to stable angina pectoris (43%) or acute coronary syndrome (57%), included between January 2008 and June 2013, were eligible for the current study. SSII was calculated using the original SYNTAX score website (www.syntaxscore.com). Cox regression analysis was used to assess the association between continuous SSII and long-term all-cause mortality. The area under the receiver-operating characteristic curve was used to assess the performance of SSII. Results SSII ranged from 6.6 to 58.2 (median: 20.4, interquartile range: 16.1-26.8). In multivariable analysis, SSII proved to be an independent significant predictor for 4.5-year mortality (hazard ratio per point increase: 1.10; 95% confidence interval: 1.07-1.13; p<0.001). In terms of discrimination, SSII had a concordance index of 0.77. Conclusion In addition to its established value in patients with left main and three-vessel disease, SSII may also predict long-term mortality in PCI-treated patients with one- or two-vessel disease","#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''","['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said . . .', ', or they can be just capitalized common words, as in White elephants are . . . .', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']",0,"[""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''""]"
CC384,J02-3002,"Periods, Capitalized Words, etc.",hybrid text mining for finding abbreviations and their definitions”,"['Youngja Park', 'Roy J Byrd']",,"We present a hybrid text mining method for finding abbreviations and their definitions in free format texts. To deal with the problem, this method employs pattern-based abbreviation rules in addition to text markers and cue words. The pattern-based rules describe how abbreviations are formed from definitions. Rules can be generated automatically and/or manually and can be augmented when the system processes new documents. The proposed method has the advantages of high accuracy, high flexibility, wide coverage, and fast recognition.",#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .,"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']",0,['#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']
CC385,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",conclusion,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.","For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .","['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']",1,"['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"
CC386,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,inducing translation templates for examplebased machine translation,['Michael Carl'],introduction,"This paper describes an example-based machine translation (EBMT) system which relays on various knowledge resources. Morphologic analyses abstract the surface forms of the languages to be translated. A shallow syntactic rule formalism is used to percolate features in derivation trees. Translation examples serve the decomposition of the text to be translated and determine the transfer of lexical values into the target language. Translation templates determine the word order of the target language and the type of phrases (e.g. noun phrase, prepositional phase, ...) to be generated in the target language. An induction mechanism generalizes translation templates from translation examples. The paper outlines the basic idea underlying the EBMT system and investigates the possibilities and limits of the translation template induction process.","Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed .","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed .']",0,"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed .']"
CC387,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,three heads are better than one,"['Robert Frederking', 'Sergei Nirenburg']",conclusion,"With the stress of ongoing budget cuts librarians are tempted to hunker down and focus exclusively on their clients, their college, department or assigned area. But collaboration across campus, within new areas, with different faculty, and different students, can be beneficial to both student and faculty learning. Students often have research needs which cannot be answered by one faculty member or librarian. Cross disciplinary collaboration between multiple librarians and faculty is key to providing the best service to these students. In this case study a team of agribusiness students need help in preparing for a competition on food distribution. During the contest the students play the role of consultants, listen to a client's problem, research the industry and possible solutions, and then present a solution to the client. This competition requires research on commodities, government policies for food safety, food distribution, economics, management, marketing, and merchandising. A team was formed of an agriculture librarian, business librarian, and an agribusiness faculty advisor in order to cover all the elements required for student success. Each person played a specific role in preparing the students for the competition. The business librarian taught a selection of databases and online resources, the agriculture librarian taught agriculture resources and created a LibGuide specific to the contest, and the faculty advisor gave real world examples about the competition and best practices for their presentations. Outcomes of this collaboration included the sharing of knowledge about the research process, building bonds between faculty and librarians, knowledge transfer between the librarians, and successfully preparing the team of students for their competition","These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .","['These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .']",1,"['These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .']"
