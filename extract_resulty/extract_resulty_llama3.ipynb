{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation of token training using LLM2Vec with LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_full_sentence_majo_scopes_2_5e-05_0.1',\n",
       " '_full_sentence_majo_total_2_5e-05_0.1',\n",
       " 'token_llama_scopes_4_0.00012_0.05',\n",
       " 'token_llama_scopes_4_7e-05_0.2',\n",
       " 'token_llama_scopes_2_5e-05_0.15',\n",
       " 'token_llama_scopes_2_7e-05_0.1',\n",
       " 'token_llama_scopes_1_7e-05_0.15',\n",
       " 'token_llama_scopes_1_5e-05_0.2',\n",
       " 'token_llama_scopes_4_0.0001_0.05',\n",
       " 'token_llama_scopes_2_0.00012_0.05',\n",
       " 'token_llama_scopes_1_0.0001_0.15',\n",
       " '_full_sentence_majo_total_2_1e-04_0.0',\n",
       " 'token_llama_scopes_1_0.00012_0.1',\n",
       " 'token_llama_scopes_4_0.00012_0.0',\n",
       " '_full_sentence_majo_scopes_2_1e-04_0.0',\n",
       " 'token_llama_total_2_1e-05_0.0',\n",
       " 'token_llama_scopes_2_1e-05_0.05',\n",
       " 'sentence_majo_scopes_2_1e-05_0.0',\n",
       " 'token_llama_scopes_4_7e-05_0.1',\n",
       " 'token_llama_scopes_2_0.00012_0.1',\n",
       " 'token_llama_scopes_2_0.00012_0.2',\n",
       " 'sentence_prio_scopes_2_1e-05_0.0',\n",
       " 'token_llama_scopes_4_5e-05_0.1',\n",
       " '_full_token_llama_scopes_2_5e-05_0.1',\n",
       " 'token_llama_scopes_2_5e-05_0.2',\n",
       " 'token_llama_scopes_4_0.00012_0.15',\n",
       " 'token_llama_scopes_4_0.00012_0.1',\n",
       " 'token_llama_scopes_4_7e-05_0.05',\n",
       " 'token_llama_scopes_1_5e-05_0.15',\n",
       " 'token_llama_scopes_1_0.00012_0.15',\n",
       " '_full_token_llama_scopes_2_1e-05_0.0',\n",
       " 'token_llama_scopes_1_3e-05_0.0',\n",
       " 'token_llama_scopes_2_0.0001_0.15',\n",
       " 'token_llama_scopes_4_7e-05_0.0',\n",
       " 'sentence_majo_total_2_1e-05_0.0',\n",
       " 'token_llama_scopes_4_5e-05_0.15',\n",
       " 'token_llama_scopes_2_0.0001_0.0',\n",
       " 'token_llama_scopes_1_7e-05_0.0',\n",
       " 'token_llama_scopes_1_0.0001_0.1',\n",
       " 'token_llama_scopes_4_5e-05_0.05',\n",
       " 'token_llama_scopes_1_0.0001_0.0',\n",
       " 'token_llama_scopes_4_0.0001_0.2',\n",
       " '_full_sentence_prio_scopes_2_1e-04_0.0',\n",
       " 'token_llama_scopes_1_0.0001_0.05',\n",
       " 'token_llama_scopes_2_1e-05_0.0',\n",
       " 'token_llama_scopes_4_0.00012_0.2',\n",
       " '_full_sentence_prio_total_2_5e-05_0.1',\n",
       " 'token_llama_scopes_2_0.0001_0.05',\n",
       " 'token_llama_scopes_2_7e-05_0.05',\n",
       " 'token_llama_scopes_2_7e-05_0.15',\n",
       " 'token_llama_scopes_2_5e-05_0.0',\n",
       " 'token_llama_scopes_1_0.00012_0.05',\n",
       " '_full_sentence_prio_scopes_2_5e-05_0.1',\n",
       " 'token_llama_scopes_2_0.00012_0.15',\n",
       " 'token_llama_scopes_2_0.0001_0.1',\n",
       " 'token_llama_scopes_2_5e-05_0.1',\n",
       " 'token_llama_scopes_1_5e-05_0.1',\n",
       " 'token_llama_scopes_2_1e-05_0.2',\n",
       " 'token_llama_scopes_4_0.0001_0.0',\n",
       " '_full_sentence_prio_total_2_1e-04_0.0',\n",
       " '_old',\n",
       " 'token_llama_scopes_2_1e-05_0.1',\n",
       " 'token_llama_scopes_4_7e-05_0.15',\n",
       " 'token_llama_scopes_2_0.00012_0.0',\n",
       " 'token_llama_scopes_2_5e-05_0.05',\n",
       " 'token_llama_scopes_2_7e-05_0.2',\n",
       " 'token_llama_scopes_4_0.0001_0.15',\n",
       " 'token_llama_scopes_2_3e-05_0.0',\n",
       " 'token_llama_scopes_2_1e-05_0.15',\n",
       " 'token_llama_scopes_1_7e-05_0.1',\n",
       " '_full_token_llama_total_2_5e-05_0.1',\n",
       " '_full_token_llama_scopes_2_1e-04_0.0',\n",
       " '_full_token_llama_total_2_1e-04_0.0',\n",
       " 'token_llama_scopes_2_0.0001_0.2',\n",
       " 'token_llama_scopes_1_7e-05_0.05',\n",
       " 'log.txt',\n",
       " 'token_llama_scopes_4_5e-05_0.2',\n",
       " 'token_llama_scopes_1_0.0001_0.2',\n",
       " 'token_llama_scopes_1_0.00012_0.2',\n",
       " 'token_llama_scopes_1_0.00012_0.0',\n",
       " 'token_llama_scopes_4_0.0001_0.1',\n",
       " 'token_llama_scopes_1_7e-05_0.2',\n",
       " 'token_llama_scopes_2_7e-05_0.0']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "file_path = \"../output/llm2vec_llama3/\"\n",
    "\n",
    "os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "token_llama_scopes_4_0.00012_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.46875, 'acc': 0.6360181570053101, 'macro_f1': 0.5756666660308838, 'total_f1': 0.7282339334487915, 'inf_f1': 0.6835871338844299, 'perc_f1': 0.4728950262069702, 'backg_f1': 0.3758142292499542}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}\n",
      "{'loss': 1.46875, 'acc': 0.6360181570053101, 'macro_f1': 0.5756666660308838, 'total_f1': 0.7282339334487915, 'inf_f1': 0.6835871338844299, 'perc_f1': 0.4728950262069702, 'backg_f1': 0.3758142292499542, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_4_7e-05_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 1.0, 'acc': 0.6447223424911499, 'macro_f1': 0.5595176219940186, 'total_f1': 0.7539530992507935, 'inf_f1': 0.6426513195037842, 'perc_f1': 0.44015029072761536, 'backg_f1': 0.3563157021999359}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}\n",
      "{'loss': 1.0, 'acc': 0.6447223424911499, 'macro_f1': 0.5595176219940186, 'total_f1': 0.7539530992507935, 'inf_f1': 0.6426513195037842, 'perc_f1': 0.44015029072761536, 'backg_f1': 0.3563157021999359, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_2_5e-05_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.2734375, 'acc': 0.7006252408027649, 'macro_f1': 0.614546000957489, 'total_f1': 0.7801671624183655, 'inf_f1': 0.6714163422584534, 'perc_f1': 0.4916943609714508, 'backg_f1': 0.4602281451225281}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}\n",
      "{'loss': 1.2734375, 'acc': 0.7006252408027649, 'macro_f1': 0.614546000957489, 'total_f1': 0.7801671624183655, 'inf_f1': 0.6714163422584534, 'perc_f1': 0.4916943609714508, 'backg_f1': 0.4602281451225281, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_2_7e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json']]\n",
      "{'loss': 1.5546875, 'acc': 0.6884270906448364, 'macro_f1': 0.5819263458251953, 'total_f1': 0.7313202619552612, 'inf_f1': 0.6645091772079468, 'perc_f1': 0.48247718811035156, 'backg_f1': 0.3582615554332733}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}\n",
      "{'loss': 1.5546875, 'acc': 0.6884270906448364, 'macro_f1': 0.5819263458251953, 'total_f1': 0.7313202619552612, 'inf_f1': 0.6645091772079468, 'perc_f1': 0.48247718811035156, 'backg_f1': 0.3582615554332733, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_1_7e-05_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.96484375, 'acc': 0.6704058051109314, 'macro_f1': 0.5824297666549683, 'total_f1': 0.7653348445892334, 'inf_f1': 0.6463228464126587, 'perc_f1': 0.4728841483592987, 'backg_f1': 0.3916982412338257}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}\n",
      "{'loss': 0.96484375, 'acc': 0.6704058051109314, 'macro_f1': 0.5824297666549683, 'total_f1': 0.7653348445892334, 'inf_f1': 0.6463228464126587, 'perc_f1': 0.4728841483592987, 'backg_f1': 0.3916982412338257, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_1_5e-05_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.95703125, 'acc': 0.6552041172981262, 'macro_f1': 0.5819942951202393, 'total_f1': 0.7681995034217834, 'inf_f1': 0.6632805466651917, 'perc_f1': 0.4587516486644745, 'backg_f1': 0.40120187401771545}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}\n",
      "{'loss': 0.95703125, 'acc': 0.6552041172981262, 'macro_f1': 0.5819942951202393, 'total_f1': 0.7681995034217834, 'inf_f1': 0.6632805466651917, 'perc_f1': 0.4587516486644745, 'backg_f1': 0.40120187401771545, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_4_0.0001_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.046875, 'acc': 0.6272526383399963, 'macro_f1': 0.4884101450443268, 'total_f1': 0.7430545687675476, 'inf_f1': 0.5701653361320496, 'perc_f1': 0.3720468580722809, 'backg_f1': 0.20897793769836426}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}\n",
      "{'loss': 1.046875, 'acc': 0.6272526383399963, 'macro_f1': 0.4884101450443268, 'total_f1': 0.7430545687675476, 'inf_f1': 0.5701653361320496, 'perc_f1': 0.3720468580722809, 'backg_f1': 0.20897793769836426, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_2_0.00012_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.046875, 'acc': 0.6914306879043579, 'macro_f1': 0.5439516305923462, 'total_f1': 0.7384837865829468, 'inf_f1': 0.6468200087547302, 'perc_f1': 0.38952454924583435, 'backg_f1': 0.300589382648468}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}\n",
      "{'loss': 1.046875, 'acc': 0.6914306879043579, 'macro_f1': 0.5439516305923462, 'total_f1': 0.7384837865829468, 'inf_f1': 0.6468200087547302, 'perc_f1': 0.38952454924583435, 'backg_f1': 0.300589382648468, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_1_0.0001_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.21875, 'acc': 0.5905357599258423, 'macro_f1': 0.4646090865135193, 'total_f1': 0.7329779267311096, 'inf_f1': 0.5077610015869141, 'perc_f1': 0.24457143247127533, 'backg_f1': 0.3282950520515442}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}\n",
      "{'loss': 1.21875, 'acc': 0.5905357599258423, 'macro_f1': 0.4646090865135193, 'total_f1': 0.7329779267311096, 'inf_f1': 0.5077610015869141, 'perc_f1': 0.24457143247127533, 'backg_f1': 0.3282950520515442, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_1_0.00012_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json']]\n",
      "{'loss': 1.0234375, 'acc': 0.6935147643089294, 'macro_f1': 0.5578618049621582, 'total_f1': 0.7502390742301941, 'inf_f1': 0.6368656158447266, 'perc_f1': 0.38685059547424316, 'backg_f1': 0.36380666494369507}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}\n",
      "{'loss': 1.0234375, 'acc': 0.6935147643089294, 'macro_f1': 0.5578618049621582, 'total_f1': 0.7502390742301941, 'inf_f1': 0.6368656158447266, 'perc_f1': 0.38685059547424316, 'backg_f1': 0.36380666494369507, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_4_0.00012_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json']]\n",
      "{'loss': 1.046875, 'acc': 0.6701605916023254, 'macro_f1': 0.5734903812408447, 'total_f1': 0.7415211200714111, 'inf_f1': 0.6934893727302551, 'perc_f1': 0.5, 'backg_f1': 0.2913087010383606}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}\n",
      "{'loss': 1.046875, 'acc': 0.6701605916023254, 'macro_f1': 0.5734903812408447, 'total_f1': 0.7415211200714111, 'inf_f1': 0.6934893727302551, 'perc_f1': 0.5, 'backg_f1': 0.2913087010383606, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_2_1e-05_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.5234375, 'acc': 0.48007845878601074, 'macro_f1': 0.39342713356018066, 'total_f1': 0.628801703453064, 'inf_f1': 0.34548264741897583, 'perc_f1': 0.3187692165374756, 'backg_f1': 0.2614600956439972}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.05}\n",
      "{'loss': 1.5234375, 'acc': 0.48007845878601074, 'macro_f1': 0.39342713356018066, 'total_f1': 0.628801703453064, 'inf_f1': 0.34548264741897583, 'perc_f1': 0.3187692165374756, 'backg_f1': 0.2614600956439972, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_4_7e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 1.046875, 'acc': 0.6737158298492432, 'macro_f1': 0.564685583114624, 'total_f1': 0.7686524987220764, 'inf_f1': 0.6480686664581299, 'perc_f1': 0.35728374123573303, 'backg_f1': 0.42944785952568054}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}\n",
      "{'loss': 1.046875, 'acc': 0.6737158298492432, 'macro_f1': 0.564685583114624, 'total_f1': 0.7686524987220764, 'inf_f1': 0.6480686664581299, 'perc_f1': 0.35728374123573303, 'backg_f1': 0.42944785952568054, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_2_0.00012_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics11.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.3203125, 'acc': 0.6592497229576111, 'macro_f1': 0.5719420313835144, 'total_f1': 0.7491534948348999, 'inf_f1': 0.6715993285179138, 'perc_f1': 0.451886385679245, 'backg_f1': 0.35699865221977234}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}\n",
      "{'loss': 1.3203125, 'acc': 0.6592497229576111, 'macro_f1': 0.5719420313835144, 'total_f1': 0.7491534948348999, 'inf_f1': 0.6715993285179138, 'perc_f1': 0.451886385679245, 'backg_f1': 0.35699865221977234, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_2_0.00012_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json']]\n",
      "{'loss': 1.234375, 'acc': 0.6706509590148926, 'macro_f1': 0.592802882194519, 'total_f1': 0.7598109841346741, 'inf_f1': 0.6779575347900391, 'perc_f1': 0.4900662302970886, 'backg_f1': 0.39772921800613403}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}\n",
      "{'loss': 1.234375, 'acc': 0.6706509590148926, 'macro_f1': 0.592802882194519, 'total_f1': 0.7598109841346741, 'inf_f1': 0.6779575347900391, 'perc_f1': 0.4900662302970886, 'backg_f1': 0.39772921800613403, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_4_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.3046875, 'acc': 0.6642760634422302, 'macro_f1': 0.5927352905273438, 'total_f1': 0.7621768712997437, 'inf_f1': 0.6921787858009338, 'perc_f1': 0.47697219252586365, 'backg_f1': 0.39679238200187683}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 1.3046875, 'acc': 0.6642760634422302, 'macro_f1': 0.5927352905273438, 'total_f1': 0.7621768712997437, 'inf_f1': 0.6921787858009338, 'perc_f1': 0.47697219252586365, 'backg_f1': 0.39679238200187683, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_2_5e-05_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.7109375, 'acc': 0.7131298184394836, 'macro_f1': 0.6056691408157349, 'total_f1': 0.7742427587509155, 'inf_f1': 0.6843079924583435, 'perc_f1': 0.5229460597038269, 'backg_f1': 0.3670026659965515}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}\n",
      "{'loss': 1.7109375, 'acc': 0.7131298184394836, 'macro_f1': 0.6056691408157349, 'total_f1': 0.7742427587509155, 'inf_f1': 0.6843079924583435, 'perc_f1': 0.5229460597038269, 'backg_f1': 0.3670026659965515, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_4_0.00012_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.4296875, 'acc': 0.6896530389785767, 'macro_f1': 0.6029596924781799, 'total_f1': 0.7745049595832825, 'inf_f1': 0.6921266317367554, 'perc_f1': 0.49094757437705994, 'backg_f1': 0.40013304352760315}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}\n",
      "{'loss': 1.4296875, 'acc': 0.6896530389785767, 'macro_f1': 0.6029596924781799, 'total_f1': 0.7745049595832825, 'inf_f1': 0.6921266317367554, 'perc_f1': 0.49094757437705994, 'backg_f1': 0.40013304352760315, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_4_0.00012_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.984375, 'acc': 0.607269823551178, 'macro_f1': 0.5605103969573975, 'total_f1': 0.7467331290245056, 'inf_f1': 0.6421967148780823, 'perc_f1': 0.44124937057495117, 'backg_f1': 0.4162154197692871}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}\n",
      "{'loss': 0.984375, 'acc': 0.607269823551178, 'macro_f1': 0.5605103969573975, 'total_f1': 0.7467331290245056, 'inf_f1': 0.6421967148780823, 'perc_f1': 0.44124937057495117, 'backg_f1': 0.4162154197692871, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_4_7e-05_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.453125, 'acc': 0.6521392464637756, 'macro_f1': 0.5792070627212524, 'total_f1': 0.7414159178733826, 'inf_f1': 0.6940494775772095, 'perc_f1': 0.4710178077220917, 'backg_f1': 0.36094212532043457}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}\n",
      "{'loss': 1.453125, 'acc': 0.6521392464637756, 'macro_f1': 0.5792070627212524, 'total_f1': 0.7414159178733826, 'inf_f1': 0.6940494775772095, 'perc_f1': 0.4710178077220917, 'backg_f1': 0.36094212532043457, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_1_5e-05_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.9140625, 'acc': 0.6555106043815613, 'macro_f1': 0.5893162488937378, 'total_f1': 0.7636242508888245, 'inf_f1': 0.7132169604301453, 'perc_f1': 0.47488951683044434, 'backg_f1': 0.37054336071014404}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}\n",
      "{'loss': 0.9140625, 'acc': 0.6555106043815613, 'macro_f1': 0.5893162488937378, 'total_f1': 0.7636242508888245, 'inf_f1': 0.7132169604301453, 'perc_f1': 0.47488951683044434, 'backg_f1': 0.37054336071014404, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_1_0.00012_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json']]\n",
      "{'loss': 0.984375, 'acc': 0.686710774898529, 'macro_f1': 0.5627118349075317, 'total_f1': 0.7605718970298767, 'inf_f1': 0.6211121678352356, 'perc_f1': 0.4270964562892914, 'backg_f1': 0.3657074272632599}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}\n",
      "{'loss': 0.984375, 'acc': 0.686710774898529, 'macro_f1': 0.5627118349075317, 'total_f1': 0.7605718970298767, 'inf_f1': 0.6211121678352356, 'perc_f1': 0.4270964562892914, 'backg_f1': 0.3657074272632599, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_1_3e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json']]\n",
      "{'loss': 1.515625, 'acc': 0.691369354724884, 'macro_f1': 0.6016454100608826, 'total_f1': 0.7657710313796997, 'inf_f1': 0.6797268986701965, 'perc_f1': 0.47962382435798645, 'backg_f1': 0.4166795015335083}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 3e-05, 'dropout': 0.0}\n",
      "{'loss': 1.515625, 'acc': 0.691369354724884, 'macro_f1': 0.6016454100608826, 'total_f1': 0.7657710313796997, 'inf_f1': 0.6797268986701965, 'perc_f1': 0.47962382435798645, 'backg_f1': 0.4166795015335083, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 3e-05, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_2_0.0001_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics11.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.53125, 'acc': 0.6687507629394531, 'macro_f1': 0.5819576978683472, 'total_f1': 0.7637431621551514, 'inf_f1': 0.7001050710678101, 'perc_f1': 0.46879565715789795, 'backg_f1': 0.3432181179523468}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}\n",
      "{'loss': 1.53125, 'acc': 0.6687507629394531, 'macro_f1': 0.5819576978683472, 'total_f1': 0.7637431621551514, 'inf_f1': 0.7001050710678101, 'perc_f1': 0.46879565715789795, 'backg_f1': 0.3432181179523468, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_4_7e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics11.json', 'eval_metrics12.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.46875, 'acc': 0.6277430653572083, 'macro_f1': 0.5678836703300476, 'total_f1': 0.7387105226516724, 'inf_f1': 0.672203779220581, 'perc_f1': 0.45399051904678345, 'backg_f1': 0.3767320513725281}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}\n",
      "{'loss': 1.46875, 'acc': 0.6277430653572083, 'macro_f1': 0.5678836703300476, 'total_f1': 0.7387105226516724, 'inf_f1': 0.672203779220581, 'perc_f1': 0.45399051904678345, 'backg_f1': 0.3767320513725281, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_4_5e-05_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.7734375, 'acc': 0.7047934532165527, 'macro_f1': 0.589339554309845, 'total_f1': 0.7618679404258728, 'inf_f1': 0.6666666865348816, 'perc_f1': 0.45619744062423706, 'backg_f1': 0.3908868432044983}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}\n",
      "{'loss': 1.7734375, 'acc': 0.7047934532165527, 'macro_f1': 0.589339554309845, 'total_f1': 0.7618679404258728, 'inf_f1': 0.6666666865348816, 'perc_f1': 0.45619744062423706, 'backg_f1': 0.3908868432044983, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_2_0.0001_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.03125, 'acc': 0.6783130764961243, 'macro_f1': 0.548396110534668, 'total_f1': 0.7403780221939087, 'inf_f1': 0.6386197209358215, 'perc_f1': 0.3406466543674469, 'backg_f1': 0.38779306411743164}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}\n",
      "{'loss': 1.03125, 'acc': 0.6783130764961243, 'macro_f1': 0.548396110534668, 'total_f1': 0.7403780221939087, 'inf_f1': 0.6386197209358215, 'perc_f1': 0.3406466543674469, 'backg_f1': 0.38779306411743164, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_1_7e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.9375, 'acc': 0.6631114482879639, 'macro_f1': 0.5866047143936157, 'total_f1': 0.7609068155288696, 'inf_f1': 0.6649845838546753, 'perc_f1': 0.47125867009162903, 'backg_f1': 0.4038296639919281}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}\n",
      "{'loss': 0.9375, 'acc': 0.6631114482879639, 'macro_f1': 0.5866047143936157, 'total_f1': 0.7609068155288696, 'inf_f1': 0.6649845838546753, 'perc_f1': 0.47125867009162903, 'backg_f1': 0.4038296639919281, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_1_0.0001_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json']]\n",
      "{'loss': 0.96484375, 'acc': 0.6892240047454834, 'macro_f1': 0.5760744214057922, 'total_f1': 0.7736729383468628, 'inf_f1': 0.6205461621284485, 'perc_f1': 0.43961864709854126, 'backg_f1': 0.40400952100753784}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}\n",
      "{'loss': 0.96484375, 'acc': 0.6892240047454834, 'macro_f1': 0.5760744214057922, 'total_f1': 0.7736729383468628, 'inf_f1': 0.6205461621284485, 'perc_f1': 0.43961864709854126, 'backg_f1': 0.40400952100753784, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_4_5e-05_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json']]\n",
      "{'loss': 1.3828125, 'acc': 0.6943729519844055, 'macro_f1': 0.5989742279052734, 'total_f1': 0.7601776719093323, 'inf_f1': 0.6886870265007019, 'perc_f1': 0.48208922147750854, 'backg_f1': 0.39459457993507385}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.05}\n",
      "{'loss': 1.3828125, 'acc': 0.6943729519844055, 'macro_f1': 0.5989742279052734, 'total_f1': 0.7601776719093323, 'inf_f1': 0.6886870265007019, 'perc_f1': 0.48208922147750854, 'backg_f1': 0.39459457993507385, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_1_0.0001_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 1.0, 'acc': 0.6353438496589661, 'macro_f1': 0.569516658782959, 'total_f1': 0.7496893405914307, 'inf_f1': 0.6355475783348083, 'perc_f1': 0.4580375850200653, 'backg_f1': 0.40519770979881287}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}\n",
      "{'loss': 1.0, 'acc': 0.6353438496589661, 'macro_f1': 0.569516658782959, 'total_f1': 0.7496893405914307, 'inf_f1': 0.6355475783348083, 'perc_f1': 0.4580375850200653, 'backg_f1': 0.40519770979881287, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_4_0.0001_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.0390625, 'acc': 0.603040337562561, 'macro_f1': 0.49158617854118347, 'total_f1': 0.7359786629676819, 'inf_f1': 0.5713715553283691, 'perc_f1': 0.35727307200431824, 'backg_f1': 0.2622222304344177}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}\n",
      "{'loss': 1.0390625, 'acc': 0.603040337562561, 'macro_f1': 0.49158617854118347, 'total_f1': 0.7359786629676819, 'inf_f1': 0.5713715553283691, 'perc_f1': 0.35727307200431824, 'backg_f1': 0.2622222304344177, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_1_0.0001_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 1.0, 'acc': 0.6659311056137085, 'macro_f1': 0.57143235206604, 'total_f1': 0.7622190713882446, 'inf_f1': 0.5821042060852051, 'perc_f1': 0.46758198738098145, 'backg_f1': 0.42018502950668335}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}\n",
      "{'loss': 1.0, 'acc': 0.6659311056137085, 'macro_f1': 0.57143235206604, 'total_f1': 0.7622190713882446, 'inf_f1': 0.5821042060852051, 'perc_f1': 0.46758198738098145, 'backg_f1': 0.42018502950668335, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_2_1e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.5, 'acc': 0.495341420173645, 'macro_f1': 0.40555888414382935, 'total_f1': 0.6140112280845642, 'inf_f1': 0.3744116425514221, 'perc_f1': 0.29798537492752075, 'backg_f1': 0.29738354682922363}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.0}\n",
      "{'loss': 1.5, 'acc': 0.495341420173645, 'macro_f1': 0.40555888414382935, 'total_f1': 0.6140112280845642, 'inf_f1': 0.3744116425514221, 'perc_f1': 0.29798537492752075, 'backg_f1': 0.29738354682922363, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_4_0.00012_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 1.015625, 'acc': 0.5536349415779114, 'macro_f1': 0.5284326076507568, 'total_f1': 0.722442626953125, 'inf_f1': 0.6391879320144653, 'perc_f1': 0.4313206970691681, 'backg_f1': 0.3600163757801056}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}\n",
      "{'loss': 1.015625, 'acc': 0.5536349415779114, 'macro_f1': 0.5284326076507568, 'total_f1': 0.722442626953125, 'inf_f1': 0.6391879320144653, 'perc_f1': 0.4313206970691681, 'backg_f1': 0.3600163757801056, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_2_0.0001_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.125, 'acc': 0.6617016196250916, 'macro_f1': 0.509577751159668, 'total_f1': 0.7135622501373291, 'inf_f1': 0.5868533849716187, 'perc_f1': 0.37445035576820374, 'backg_f1': 0.25809767842292786}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}\n",
      "{'loss': 1.125, 'acc': 0.6617016196250916, 'macro_f1': 0.509577751159668, 'total_f1': 0.7135622501373291, 'inf_f1': 0.5868533849716187, 'perc_f1': 0.37445035576820374, 'backg_f1': 0.25809767842292786, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_2_7e-05_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.3046875, 'acc': 0.6235748529434204, 'macro_f1': 0.5511486530303955, 'total_f1': 0.7413861155509949, 'inf_f1': 0.6367643475532532, 'perc_f1': 0.4248095750808716, 'backg_f1': 0.3704138398170471}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}\n",
      "{'loss': 1.3046875, 'acc': 0.6235748529434204, 'macro_f1': 0.5511486530303955, 'total_f1': 0.7413861155509949, 'inf_f1': 0.6367643475532532, 'perc_f1': 0.4248095750808716, 'backg_f1': 0.3704138398170471, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_2_7e-05_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json']]\n",
      "{'loss': 1.40625, 'acc': 0.6820522546768188, 'macro_f1': 0.5791991949081421, 'total_f1': 0.7694202661514282, 'inf_f1': 0.6367964148521423, 'perc_f1': 0.42988505959510803, 'backg_f1': 0.4191189110279083}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}\n",
      "{'loss': 1.40625, 'acc': 0.6820522546768188, 'macro_f1': 0.5791991949081421, 'total_f1': 0.7694202661514282, 'inf_f1': 0.6367964148521423, 'perc_f1': 0.42988505959510803, 'backg_f1': 0.4191189110279083, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_2_5e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.3671875, 'acc': 0.6595562100410461, 'macro_f1': 0.5859969854354858, 'total_f1': 0.7609481811523438, 'inf_f1': 0.6832829713821411, 'perc_f1': 0.46144095063209534, 'backg_f1': 0.4010274410247803}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.0}\n",
      "{'loss': 1.3671875, 'acc': 0.6595562100410461, 'macro_f1': 0.5859969854354858, 'total_f1': 0.7609481811523438, 'inf_f1': 0.6832829713821411, 'perc_f1': 0.46144095063209534, 'backg_f1': 0.4010274410247803, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_1_0.00012_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.5, 'acc': 0.6267009973526001, 'macro_f1': 0.4911090135574341, 'total_f1': 0.7158682346343994, 'inf_f1': 0.5789200663566589, 'perc_f1': 0.33887779712677, 'backg_f1': 0.2436487227678299}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}\n",
      "{'loss': 1.5, 'acc': 0.6267009973526001, 'macro_f1': 0.4911090135574341, 'total_f1': 0.7158682346343994, 'inf_f1': 0.5789200663566589, 'perc_f1': 0.33887779712677, 'backg_f1': 0.2436487227678299, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_2_0.00012_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.171875, 'acc': 0.6540395021438599, 'macro_f1': 0.48150521516799927, 'total_f1': 0.6946224570274353, 'inf_f1': 0.5823754668235779, 'perc_f1': 0.3979550004005432, 'backg_f1': 0.13116763532161713}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}\n",
      "{'loss': 1.171875, 'acc': 0.6540395021438599, 'macro_f1': 0.48150521516799927, 'total_f1': 0.6946224570274353, 'inf_f1': 0.5823754668235779, 'perc_f1': 0.3979550004005432, 'backg_f1': 0.13116763532161713, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_2_0.0001_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.1015625, 'acc': 0.6507907509803772, 'macro_f1': 0.5231457948684692, 'total_f1': 0.7223469614982605, 'inf_f1': 0.6018872857093811, 'perc_f1': 0.30528149008750916, 'backg_f1': 0.37798166275024414}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}\n",
      "{'loss': 1.1015625, 'acc': 0.6507907509803772, 'macro_f1': 0.5231457948684692, 'total_f1': 0.7223469614982605, 'inf_f1': 0.6018872857093811, 'perc_f1': 0.30528149008750916, 'backg_f1': 0.37798166275024414, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.296875, 'acc': 0.6472355127334595, 'macro_f1': 0.5808876752853394, 'total_f1': 0.748823344707489, 'inf_f1': 0.6602581739425659, 'perc_f1': 0.4628966748714447, 'backg_f1': 0.4163794219493866}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 1.296875, 'acc': 0.6472355127334595, 'macro_f1': 0.5808876752853394, 'total_f1': 0.748823344707489, 'inf_f1': 0.6602581739425659, 'perc_f1': 0.4628966748714447, 'backg_f1': 0.4163794219493866, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_1_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.95703125, 'acc': 0.6352213025093079, 'macro_f1': 0.5665926933288574, 'total_f1': 0.7544330358505249, 'inf_f1': 0.6366842985153198, 'perc_f1': 0.4549970328807831, 'backg_f1': 0.3910630941390991}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 0.95703125, 'acc': 0.6352213025093079, 'macro_f1': 0.5665926933288574, 'total_f1': 0.7544330358505249, 'inf_f1': 0.6366842985153198, 'perc_f1': 0.4549970328807831, 'backg_f1': 0.3910630941390991, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_2_1e-05_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.359375, 'acc': 0.499877393245697, 'macro_f1': 0.40264666080474854, 'total_f1': 0.6232157945632935, 'inf_f1': 0.32714322209358215, 'perc_f1': 0.31153595447540283, 'backg_f1': 0.3063592314720154}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.2}\n",
      "{'loss': 1.359375, 'acc': 0.499877393245697, 'macro_f1': 0.40264666080474854, 'total_f1': 0.6232157945632935, 'inf_f1': 0.32714322209358215, 'perc_f1': 0.31153595447540283, 'backg_f1': 0.3063592314720154, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_4_0.0001_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.265625, 'acc': 0.6757999062538147, 'macro_f1': 0.5879613161087036, 'total_f1': 0.7702202200889587, 'inf_f1': 0.6925868391990662, 'perc_f1': 0.4373795688152313, 'backg_f1': 0.4012708365917206}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}\n",
      "{'loss': 1.265625, 'acc': 0.6757999062538147, 'macro_f1': 0.5879613161087036, 'total_f1': 0.7702202200889587, 'inf_f1': 0.6925868391990662, 'perc_f1': 0.4373795688152313, 'backg_f1': 0.4012708365917206, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_2_1e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json']]\n",
      "{'loss': 1.5625, 'acc': 0.5351232290267944, 'macro_f1': 0.4025527536869049, 'total_f1': 0.6016413569450378, 'inf_f1': 0.3358449935913086, 'perc_f1': 0.2852657437324524, 'backg_f1': 0.2799428105354309}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.1}\n",
      "{'loss': 1.5625, 'acc': 0.5351232290267944, 'macro_f1': 0.4025527536869049, 'total_f1': 0.6016413569450378, 'inf_f1': 0.3358449935913086, 'perc_f1': 0.2852657437324524, 'backg_f1': 0.2799428105354309, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_4_7e-05_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.34375, 'acc': 0.6533039212226868, 'macro_f1': 0.5767042636871338, 'total_f1': 0.734997570514679, 'inf_f1': 0.6727007031440735, 'perc_f1': 0.4426631033420563, 'backg_f1': 0.40030935406684875}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}\n",
      "{'loss': 1.34375, 'acc': 0.6533039212226868, 'macro_f1': 0.5767042636871338, 'total_f1': 0.734997570514679, 'inf_f1': 0.6727007031440735, 'perc_f1': 0.4426631033420563, 'backg_f1': 0.40030935406684875, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_2_0.00012_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.03125, 'acc': 0.668137788772583, 'macro_f1': 0.5414584875106812, 'total_f1': 0.7504249215126038, 'inf_f1': 0.6222551465034485, 'perc_f1': 0.31300559639930725, 'backg_f1': 0.40740740299224854}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}\n",
      "{'loss': 1.03125, 'acc': 0.668137788772583, 'macro_f1': 0.5414584875106812, 'total_f1': 0.7504249215126038, 'inf_f1': 0.6222551465034485, 'perc_f1': 0.31300559639930725, 'backg_f1': 0.40740740299224854, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_2_5e-05_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.75, 'acc': 0.6892240047454834, 'macro_f1': 0.5770112872123718, 'total_f1': 0.7305857539176941, 'inf_f1': 0.6816561818122864, 'perc_f1': 0.47156399488449097, 'backg_f1': 0.3284552991390228}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.05}\n",
      "{'loss': 1.75, 'acc': 0.6892240047454834, 'macro_f1': 0.5770112872123718, 'total_f1': 0.7305857539176941, 'inf_f1': 0.6816561818122864, 'perc_f1': 0.47156399488449097, 'backg_f1': 0.3284552991390228, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_2_7e-05_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json']]\n",
      "{'loss': 1.171875, 'acc': 0.7017898559570312, 'macro_f1': 0.5876894593238831, 'total_f1': 0.7582689523696899, 'inf_f1': 0.6852484941482544, 'perc_f1': 0.4771196246147156, 'backg_f1': 0.3472682237625122}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}\n",
      "{'loss': 1.171875, 'acc': 0.7017898559570312, 'macro_f1': 0.5876894593238831, 'total_f1': 0.7582689523696899, 'inf_f1': 0.6852484941482544, 'perc_f1': 0.4771196246147156, 'backg_f1': 0.3472682237625122, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_4_0.0001_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.515625, 'acc': 0.6884270906448364, 'macro_f1': 0.5912853479385376, 'total_f1': 0.7707670331001282, 'inf_f1': 0.6793969869613647, 'perc_f1': 0.4650551676750183, 'backg_f1': 0.38912826776504517}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}\n",
      "{'loss': 1.515625, 'acc': 0.6884270906448364, 'macro_f1': 0.5912853479385376, 'total_f1': 0.7707670331001282, 'inf_f1': 0.6793969869613647, 'perc_f1': 0.4650551676750183, 'backg_f1': 0.38912826776504517, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_2_3e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.3125, 'acc': 0.6815618276596069, 'macro_f1': 0.5846582055091858, 'total_f1': 0.759915292263031, 'inf_f1': 0.6372064352035522, 'perc_f1': 0.4912133812904358, 'backg_f1': 0.3837885558605194}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 3e-05, 'dropout': 0.0}\n",
      "{'loss': 1.3125, 'acc': 0.6815618276596069, 'macro_f1': 0.5846582055091858, 'total_f1': 0.759915292263031, 'inf_f1': 0.6372064352035522, 'perc_f1': 0.4912133812904358, 'backg_f1': 0.3837885558605194, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 3e-05, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_2_1e-05_0.15\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.3671875, 'acc': 0.5036778450012207, 'macro_f1': 0.4113457500934601, 'total_f1': 0.6215144395828247, 'inf_f1': 0.348562628030777, 'perc_f1': 0.312178373336792, 'backg_f1': 0.3223443329334259}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.15}\n",
      "{'loss': 1.3671875, 'acc': 0.5036778450012207, 'macro_f1': 0.4113457500934601, 'total_f1': 0.6215144395828247, 'inf_f1': 0.348562628030777, 'perc_f1': 0.312178373336792, 'backg_f1': 0.3223443329334259, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.15}\n",
      "\n",
      "token_llama_scopes_1_7e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.96875, 'acc': 0.6643373966217041, 'macro_f1': 0.5804178714752197, 'total_f1': 0.7518351078033447, 'inf_f1': 0.6699088215827942, 'perc_f1': 0.461667001247406, 'backg_f1': 0.3805614709854126}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}\n",
      "{'loss': 0.96875, 'acc': 0.6643373966217041, 'macro_f1': 0.5804178714752197, 'total_f1': 0.7518351078033447, 'inf_f1': 0.6699088215827942, 'perc_f1': 0.461667001247406, 'backg_f1': 0.3805614709854126, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_2_0.0001_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.4765625, 'acc': 0.6465612649917603, 'macro_f1': 0.5789694786071777, 'total_f1': 0.7702043652534485, 'inf_f1': 0.6837024688720703, 'perc_f1': 0.4588344097137451, 'backg_f1': 0.373020201921463}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}\n",
      "{'loss': 1.4765625, 'acc': 0.6465612649917603, 'macro_f1': 0.5789694786071777, 'total_f1': 0.7702043652534485, 'inf_f1': 0.6837024688720703, 'perc_f1': 0.4588344097137451, 'backg_f1': 0.373020201921463, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_1_7e-05_0.05\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.96875, 'acc': 0.6579625010490417, 'macro_f1': 0.5897548198699951, 'total_f1': 0.7747653126716614, 'inf_f1': 0.6464646458625793, 'perc_f1': 0.48859649896621704, 'backg_f1': 0.42119866609573364}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}\n",
      "{'loss': 0.96875, 'acc': 0.6579625010490417, 'macro_f1': 0.5897548198699951, 'total_f1': 0.7747653126716614, 'inf_f1': 0.6464646458625793, 'perc_f1': 0.48859649896621704, 'backg_f1': 0.42119866609573364, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}\n",
      "\n",
      "token_llama_scopes_4_5e-05_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.9921875, 'acc': 0.6179355382919312, 'macro_f1': 0.5403689742088318, 'total_f1': 0.7527856826782227, 'inf_f1': 0.6450783610343933, 'perc_f1': 0.42051804065704346, 'backg_f1': 0.3178431987762451}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}\n",
      "{'loss': 0.9921875, 'acc': 0.6179355382919312, 'macro_f1': 0.5403689742088318, 'total_f1': 0.7527856826782227, 'inf_f1': 0.6450783610343933, 'perc_f1': 0.42051804065704346, 'backg_f1': 0.3178431987762451, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_1_0.0001_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 1.0234375, 'acc': 0.6417800784111023, 'macro_f1': 0.5516966581344604, 'total_f1': 0.7536470293998718, 'inf_f1': 0.5810810923576355, 'perc_f1': 0.4396825432777405, 'backg_f1': 0.3855571150779724}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}\n",
      "{'loss': 1.0234375, 'acc': 0.6417800784111023, 'macro_f1': 0.5516966581344604, 'total_f1': 0.7536470293998718, 'inf_f1': 0.5810810923576355, 'perc_f1': 0.4396825432777405, 'backg_f1': 0.3855571150779724, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_1_0.00012_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 1.0, 'acc': 0.6439254879951477, 'macro_f1': 0.5782377123832703, 'total_f1': 0.7577762007713318, 'inf_f1': 0.6223933100700378, 'perc_f1': 0.5041384696960449, 'backg_f1': 0.3995320200920105}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}\n",
      "{'loss': 1.0, 'acc': 0.6439254879951477, 'macro_f1': 0.5782377123832703, 'total_f1': 0.7577762007713318, 'inf_f1': 0.6223933100700378, 'perc_f1': 0.5041384696960449, 'backg_f1': 0.3995320200920105, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_1_0.00012_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.99609375, 'acc': 0.6423930525779724, 'macro_f1': 0.5618637800216675, 'total_f1': 0.7605143189430237, 'inf_f1': 0.6071887016296387, 'perc_f1': 0.4449886977672577, 'backg_f1': 0.39735710620880127}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}\n",
      "{'loss': 0.99609375, 'acc': 0.6423930525779724, 'macro_f1': 0.5618637800216675, 'total_f1': 0.7605143189430237, 'inf_f1': 0.6071887016296387, 'perc_f1': 0.4449886977672577, 'backg_f1': 0.39735710620880127, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}\n",
      "\n",
      "token_llama_scopes_4_0.0001_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.3046875, 'acc': 0.6759838461875916, 'macro_f1': 0.5865066647529602, 'total_f1': 0.7578735947608948, 'inf_f1': 0.6537330746650696, 'perc_f1': 0.4782709777355194, 'backg_f1': 0.394610196352005}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}\n",
      "{'loss': 1.3046875, 'acc': 0.6759838461875916, 'macro_f1': 0.5865066647529602, 'total_f1': 0.7578735947608948, 'inf_f1': 0.6537330746650696, 'perc_f1': 0.4782709777355194, 'backg_f1': 0.394610196352005, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}\n",
      "\n",
      "token_llama_scopes_1_7e-05_0.2\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.953125, 'acc': 0.6245555877685547, 'macro_f1': 0.5673010349273682, 'total_f1': 0.753614068031311, 'inf_f1': 0.6539433002471924, 'perc_f1': 0.4435752034187317, 'backg_f1': 0.40418702363967896}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}\n",
      "{'loss': 0.953125, 'acc': 0.6245555877685547, 'macro_f1': 0.5673010349273682, 'total_f1': 0.753614068031311, 'inf_f1': 0.6539433002471924, 'perc_f1': 0.4435752034187317, 'backg_f1': 0.40418702363967896, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}\n",
      "\n",
      "token_llama_scopes_2_7e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.109375, 'acc': 0.6623145937919617, 'macro_f1': 0.5104726552963257, 'total_f1': 0.7011964917182922, 'inf_f1': 0.5990909337997437, 'perc_f1': 0.3486191928386688, 'backg_f1': 0.27861666679382324}\n",
      "{'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}\n",
      "{'loss': 1.109375, 'acc': 0.6623145937919617, 'macro_f1': 0.5104726552963257, 'total_f1': 0.7011964917182922, 'inf_f1': 0.5990909337997437, 'perc_f1': 0.3486191928386688, 'backg_f1': 0.27861666679382324, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}\n",
      "\n",
      "result\n",
      "[{'loss': 1.46875, 'acc': 0.6360181570053101, 'macro_f1': 0.5756666660308838, 'total_f1': 0.7282339334487915, 'inf_f1': 0.6835871338844299, 'perc_f1': 0.4728950262069702, 'backg_f1': 0.3758142292499542, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}, {'loss': 1.0, 'acc': 0.6447223424911499, 'macro_f1': 0.5595176219940186, 'total_f1': 0.7539530992507935, 'inf_f1': 0.6426513195037842, 'perc_f1': 0.44015029072761536, 'backg_f1': 0.3563157021999359, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}, {'loss': 1.2734375, 'acc': 0.7006252408027649, 'macro_f1': 0.614546000957489, 'total_f1': 0.7801671624183655, 'inf_f1': 0.6714163422584534, 'perc_f1': 0.4916943609714508, 'backg_f1': 0.4602281451225281, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}, {'loss': 1.5546875, 'acc': 0.6884270906448364, 'macro_f1': 0.5819263458251953, 'total_f1': 0.7313202619552612, 'inf_f1': 0.6645091772079468, 'perc_f1': 0.48247718811035156, 'backg_f1': 0.3582615554332733, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}, {'loss': 0.96484375, 'acc': 0.6704058051109314, 'macro_f1': 0.5824297666549683, 'total_f1': 0.7653348445892334, 'inf_f1': 0.6463228464126587, 'perc_f1': 0.4728841483592987, 'backg_f1': 0.3916982412338257, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}, {'loss': 0.95703125, 'acc': 0.6552041172981262, 'macro_f1': 0.5819942951202393, 'total_f1': 0.7681995034217834, 'inf_f1': 0.6632805466651917, 'perc_f1': 0.4587516486644745, 'backg_f1': 0.40120187401771545, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}, {'loss': 1.046875, 'acc': 0.6272526383399963, 'macro_f1': 0.4884101450443268, 'total_f1': 0.7430545687675476, 'inf_f1': 0.5701653361320496, 'perc_f1': 0.3720468580722809, 'backg_f1': 0.20897793769836426, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}, {'loss': 1.046875, 'acc': 0.6914306879043579, 'macro_f1': 0.5439516305923462, 'total_f1': 0.7384837865829468, 'inf_f1': 0.6468200087547302, 'perc_f1': 0.38952454924583435, 'backg_f1': 0.300589382648468, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}, {'loss': 1.21875, 'acc': 0.5905357599258423, 'macro_f1': 0.4646090865135193, 'total_f1': 0.7329779267311096, 'inf_f1': 0.5077610015869141, 'perc_f1': 0.24457143247127533, 'backg_f1': 0.3282950520515442, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}, {'loss': 1.0234375, 'acc': 0.6935147643089294, 'macro_f1': 0.5578618049621582, 'total_f1': 0.7502390742301941, 'inf_f1': 0.6368656158447266, 'perc_f1': 0.38685059547424316, 'backg_f1': 0.36380666494369507, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}, {'loss': 1.046875, 'acc': 0.6701605916023254, 'macro_f1': 0.5734903812408447, 'total_f1': 0.7415211200714111, 'inf_f1': 0.6934893727302551, 'perc_f1': 0.5, 'backg_f1': 0.2913087010383606, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}, {'loss': 1.5234375, 'acc': 0.48007845878601074, 'macro_f1': 0.39342713356018066, 'total_f1': 0.628801703453064, 'inf_f1': 0.34548264741897583, 'perc_f1': 0.3187692165374756, 'backg_f1': 0.2614600956439972, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.05}, {'loss': 1.046875, 'acc': 0.6737158298492432, 'macro_f1': 0.564685583114624, 'total_f1': 0.7686524987220764, 'inf_f1': 0.6480686664581299, 'perc_f1': 0.35728374123573303, 'backg_f1': 0.42944785952568054, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}, {'loss': 1.3203125, 'acc': 0.6592497229576111, 'macro_f1': 0.5719420313835144, 'total_f1': 0.7491534948348999, 'inf_f1': 0.6715993285179138, 'perc_f1': 0.451886385679245, 'backg_f1': 0.35699865221977234, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}, {'loss': 1.234375, 'acc': 0.6706509590148926, 'macro_f1': 0.592802882194519, 'total_f1': 0.7598109841346741, 'inf_f1': 0.6779575347900391, 'perc_f1': 0.4900662302970886, 'backg_f1': 0.39772921800613403, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}, {'loss': 1.3046875, 'acc': 0.6642760634422302, 'macro_f1': 0.5927352905273438, 'total_f1': 0.7621768712997437, 'inf_f1': 0.6921787858009338, 'perc_f1': 0.47697219252586365, 'backg_f1': 0.39679238200187683, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 1.7109375, 'acc': 0.7131298184394836, 'macro_f1': 0.6056691408157349, 'total_f1': 0.7742427587509155, 'inf_f1': 0.6843079924583435, 'perc_f1': 0.5229460597038269, 'backg_f1': 0.3670026659965515, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}, {'loss': 1.4296875, 'acc': 0.6896530389785767, 'macro_f1': 0.6029596924781799, 'total_f1': 0.7745049595832825, 'inf_f1': 0.6921266317367554, 'perc_f1': 0.49094757437705994, 'backg_f1': 0.40013304352760315, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}, {'loss': 0.984375, 'acc': 0.607269823551178, 'macro_f1': 0.5605103969573975, 'total_f1': 0.7467331290245056, 'inf_f1': 0.6421967148780823, 'perc_f1': 0.44124937057495117, 'backg_f1': 0.4162154197692871, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.1}, {'loss': 1.453125, 'acc': 0.6521392464637756, 'macro_f1': 0.5792070627212524, 'total_f1': 0.7414159178733826, 'inf_f1': 0.6940494775772095, 'perc_f1': 0.4710178077220917, 'backg_f1': 0.36094212532043457, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}, {'loss': 0.9140625, 'acc': 0.6555106043815613, 'macro_f1': 0.5893162488937378, 'total_f1': 0.7636242508888245, 'inf_f1': 0.7132169604301453, 'perc_f1': 0.47488951683044434, 'backg_f1': 0.37054336071014404, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}, {'loss': 0.984375, 'acc': 0.686710774898529, 'macro_f1': 0.5627118349075317, 'total_f1': 0.7605718970298767, 'inf_f1': 0.6211121678352356, 'perc_f1': 0.4270964562892914, 'backg_f1': 0.3657074272632599, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}, {'loss': 1.515625, 'acc': 0.691369354724884, 'macro_f1': 0.6016454100608826, 'total_f1': 0.7657710313796997, 'inf_f1': 0.6797268986701965, 'perc_f1': 0.47962382435798645, 'backg_f1': 0.4166795015335083, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 3e-05, 'dropout': 0.0}, {'loss': 1.53125, 'acc': 0.6687507629394531, 'macro_f1': 0.5819576978683472, 'total_f1': 0.7637431621551514, 'inf_f1': 0.7001050710678101, 'perc_f1': 0.46879565715789795, 'backg_f1': 0.3432181179523468, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}, {'loss': 1.46875, 'acc': 0.6277430653572083, 'macro_f1': 0.5678836703300476, 'total_f1': 0.7387105226516724, 'inf_f1': 0.672203779220581, 'perc_f1': 0.45399051904678345, 'backg_f1': 0.3767320513725281, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}, {'loss': 1.7734375, 'acc': 0.7047934532165527, 'macro_f1': 0.589339554309845, 'total_f1': 0.7618679404258728, 'inf_f1': 0.6666666865348816, 'perc_f1': 0.45619744062423706, 'backg_f1': 0.3908868432044983, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.15}, {'loss': 1.03125, 'acc': 0.6783130764961243, 'macro_f1': 0.548396110534668, 'total_f1': 0.7403780221939087, 'inf_f1': 0.6386197209358215, 'perc_f1': 0.3406466543674469, 'backg_f1': 0.38779306411743164, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}, {'loss': 0.9375, 'acc': 0.6631114482879639, 'macro_f1': 0.5866047143936157, 'total_f1': 0.7609068155288696, 'inf_f1': 0.6649845838546753, 'perc_f1': 0.47125867009162903, 'backg_f1': 0.4038296639919281, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}, {'loss': 0.96484375, 'acc': 0.6892240047454834, 'macro_f1': 0.5760744214057922, 'total_f1': 0.7736729383468628, 'inf_f1': 0.6205461621284485, 'perc_f1': 0.43961864709854126, 'backg_f1': 0.40400952100753784, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}, {'loss': 1.3828125, 'acc': 0.6943729519844055, 'macro_f1': 0.5989742279052734, 'total_f1': 0.7601776719093323, 'inf_f1': 0.6886870265007019, 'perc_f1': 0.48208922147750854, 'backg_f1': 0.39459457993507385, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.05}, {'loss': 1.0, 'acc': 0.6353438496589661, 'macro_f1': 0.569516658782959, 'total_f1': 0.7496893405914307, 'inf_f1': 0.6355475783348083, 'perc_f1': 0.4580375850200653, 'backg_f1': 0.40519770979881287, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}, {'loss': 1.0390625, 'acc': 0.603040337562561, 'macro_f1': 0.49158617854118347, 'total_f1': 0.7359786629676819, 'inf_f1': 0.5713715553283691, 'perc_f1': 0.35727307200431824, 'backg_f1': 0.2622222304344177, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}, {'loss': 1.0, 'acc': 0.6659311056137085, 'macro_f1': 0.57143235206604, 'total_f1': 0.7622190713882446, 'inf_f1': 0.5821042060852051, 'perc_f1': 0.46758198738098145, 'backg_f1': 0.42018502950668335, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}, {'loss': 1.5, 'acc': 0.495341420173645, 'macro_f1': 0.40555888414382935, 'total_f1': 0.6140112280845642, 'inf_f1': 0.3744116425514221, 'perc_f1': 0.29798537492752075, 'backg_f1': 0.29738354682922363, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.0}, {'loss': 1.015625, 'acc': 0.5536349415779114, 'macro_f1': 0.5284326076507568, 'total_f1': 0.722442626953125, 'inf_f1': 0.6391879320144653, 'perc_f1': 0.4313206970691681, 'backg_f1': 0.3600163757801056, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}, {'loss': 1.125, 'acc': 0.6617016196250916, 'macro_f1': 0.509577751159668, 'total_f1': 0.7135622501373291, 'inf_f1': 0.5868533849716187, 'perc_f1': 0.37445035576820374, 'backg_f1': 0.25809767842292786, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.05}, {'loss': 1.3046875, 'acc': 0.6235748529434204, 'macro_f1': 0.5511486530303955, 'total_f1': 0.7413861155509949, 'inf_f1': 0.6367643475532532, 'perc_f1': 0.4248095750808716, 'backg_f1': 0.3704138398170471, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}, {'loss': 1.40625, 'acc': 0.6820522546768188, 'macro_f1': 0.5791991949081421, 'total_f1': 0.7694202661514282, 'inf_f1': 0.6367964148521423, 'perc_f1': 0.42988505959510803, 'backg_f1': 0.4191189110279083, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}, {'loss': 1.3671875, 'acc': 0.6595562100410461, 'macro_f1': 0.5859969854354858, 'total_f1': 0.7609481811523438, 'inf_f1': 0.6832829713821411, 'perc_f1': 0.46144095063209534, 'backg_f1': 0.4010274410247803, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.0}, {'loss': 1.5, 'acc': 0.6267009973526001, 'macro_f1': 0.4911090135574341, 'total_f1': 0.7158682346343994, 'inf_f1': 0.5789200663566589, 'perc_f1': 0.33887779712677, 'backg_f1': 0.2436487227678299, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.05}, {'loss': 1.171875, 'acc': 0.6540395021438599, 'macro_f1': 0.48150521516799927, 'total_f1': 0.6946224570274353, 'inf_f1': 0.5823754668235779, 'perc_f1': 0.3979550004005432, 'backg_f1': 0.13116763532161713, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.15}, {'loss': 1.1015625, 'acc': 0.6507907509803772, 'macro_f1': 0.5231457948684692, 'total_f1': 0.7223469614982605, 'inf_f1': 0.6018872857093811, 'perc_f1': 0.30528149008750916, 'backg_f1': 0.37798166275024414, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}, {'loss': 1.296875, 'acc': 0.6472355127334595, 'macro_f1': 0.5808876752853394, 'total_f1': 0.748823344707489, 'inf_f1': 0.6602581739425659, 'perc_f1': 0.4628966748714447, 'backg_f1': 0.4163794219493866, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.95703125, 'acc': 0.6352213025093079, 'macro_f1': 0.5665926933288574, 'total_f1': 0.7544330358505249, 'inf_f1': 0.6366842985153198, 'perc_f1': 0.4549970328807831, 'backg_f1': 0.3910630941390991, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 1.359375, 'acc': 0.499877393245697, 'macro_f1': 0.40264666080474854, 'total_f1': 0.6232157945632935, 'inf_f1': 0.32714322209358215, 'perc_f1': 0.31153595447540283, 'backg_f1': 0.3063592314720154, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.2}, {'loss': 1.265625, 'acc': 0.6757999062538147, 'macro_f1': 0.5879613161087036, 'total_f1': 0.7702202200889587, 'inf_f1': 0.6925868391990662, 'perc_f1': 0.4373795688152313, 'backg_f1': 0.4012708365917206, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.0}, {'loss': 1.5625, 'acc': 0.5351232290267944, 'macro_f1': 0.4025527536869049, 'total_f1': 0.6016413569450378, 'inf_f1': 0.3358449935913086, 'perc_f1': 0.2852657437324524, 'backg_f1': 0.2799428105354309, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.1}, {'loss': 1.34375, 'acc': 0.6533039212226868, 'macro_f1': 0.5767042636871338, 'total_f1': 0.734997570514679, 'inf_f1': 0.6727007031440735, 'perc_f1': 0.4426631033420563, 'backg_f1': 0.40030935406684875, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.15}, {'loss': 1.03125, 'acc': 0.668137788772583, 'macro_f1': 0.5414584875106812, 'total_f1': 0.7504249215126038, 'inf_f1': 0.6222551465034485, 'perc_f1': 0.31300559639930725, 'backg_f1': 0.40740740299224854, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}, {'loss': 1.75, 'acc': 0.6892240047454834, 'macro_f1': 0.5770112872123718, 'total_f1': 0.7305857539176941, 'inf_f1': 0.6816561818122864, 'perc_f1': 0.47156399488449097, 'backg_f1': 0.3284552991390228, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.05}, {'loss': 1.171875, 'acc': 0.7017898559570312, 'macro_f1': 0.5876894593238831, 'total_f1': 0.7582689523696899, 'inf_f1': 0.6852484941482544, 'perc_f1': 0.4771196246147156, 'backg_f1': 0.3472682237625122, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}, {'loss': 1.515625, 'acc': 0.6884270906448364, 'macro_f1': 0.5912853479385376, 'total_f1': 0.7707670331001282, 'inf_f1': 0.6793969869613647, 'perc_f1': 0.4650551676750183, 'backg_f1': 0.38912826776504517, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.15}, {'loss': 1.3125, 'acc': 0.6815618276596069, 'macro_f1': 0.5846582055091858, 'total_f1': 0.759915292263031, 'inf_f1': 0.6372064352035522, 'perc_f1': 0.4912133812904358, 'backg_f1': 0.3837885558605194, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 3e-05, 'dropout': 0.0}, {'loss': 1.3671875, 'acc': 0.5036778450012207, 'macro_f1': 0.4113457500934601, 'total_f1': 0.6215144395828247, 'inf_f1': 0.348562628030777, 'perc_f1': 0.312178373336792, 'backg_f1': 0.3223443329334259, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 1e-05, 'dropout': 0.15}, {'loss': 0.96875, 'acc': 0.6643373966217041, 'macro_f1': 0.5804178714752197, 'total_f1': 0.7518351078033447, 'inf_f1': 0.6699088215827942, 'perc_f1': 0.461667001247406, 'backg_f1': 0.3805614709854126, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.1}, {'loss': 1.4765625, 'acc': 0.6465612649917603, 'macro_f1': 0.5789694786071777, 'total_f1': 0.7702043652534485, 'inf_f1': 0.6837024688720703, 'perc_f1': 0.4588344097137451, 'backg_f1': 0.373020201921463, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}, {'loss': 0.96875, 'acc': 0.6579625010490417, 'macro_f1': 0.5897548198699951, 'total_f1': 0.7747653126716614, 'inf_f1': 0.6464646458625793, 'perc_f1': 0.48859649896621704, 'backg_f1': 0.42119866609573364, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.05}, {'loss': 0.9921875, 'acc': 0.6179355382919312, 'macro_f1': 0.5403689742088318, 'total_f1': 0.7527856826782227, 'inf_f1': 0.6450783610343933, 'perc_f1': 0.42051804065704346, 'backg_f1': 0.3178431987762451, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.2}, {'loss': 1.0234375, 'acc': 0.6417800784111023, 'macro_f1': 0.5516966581344604, 'total_f1': 0.7536470293998718, 'inf_f1': 0.5810810923576355, 'perc_f1': 0.4396825432777405, 'backg_f1': 0.3855571150779724, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.2}, {'loss': 1.0, 'acc': 0.6439254879951477, 'macro_f1': 0.5782377123832703, 'total_f1': 0.7577762007713318, 'inf_f1': 0.6223933100700378, 'perc_f1': 0.5041384696960449, 'backg_f1': 0.3995320200920105, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.2}, {'loss': 0.99609375, 'acc': 0.6423930525779724, 'macro_f1': 0.5618637800216675, 'total_f1': 0.7605143189430237, 'inf_f1': 0.6071887016296387, 'perc_f1': 0.4449886977672577, 'backg_f1': 0.39735710620880127, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 0.00012, 'dropout': 0.0}, {'loss': 1.3046875, 'acc': 0.6759838461875916, 'macro_f1': 0.5865066647529602, 'total_f1': 0.7578735947608948, 'inf_f1': 0.6537330746650696, 'perc_f1': 0.4782709777355194, 'backg_f1': 0.394610196352005, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 4, 'max_epoch': 20, 'learning_rate': 0.0001, 'dropout': 0.1}, {'loss': 0.953125, 'acc': 0.6245555877685547, 'macro_f1': 0.5673010349273682, 'total_f1': 0.753614068031311, 'inf_f1': 0.6539433002471924, 'perc_f1': 0.4435752034187317, 'backg_f1': 0.40418702363967896, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 1, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.2}, {'loss': 1.109375, 'acc': 0.6623145937919617, 'macro_f1': 0.5104726552963257, 'total_f1': 0.7011964917182922, 'inf_f1': 0.5990909337997437, 'perc_f1': 0.3486191928386688, 'backg_f1': 0.27861666679382324, 'full_training': False, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 7e-05, 'dropout': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "json_data=[]\n",
    "for fp in os.listdir(file_path):\n",
    "    if fp.split('_')[0:3]==[\"token\",'llama','scopes']:\n",
    "        print()\n",
    "        print(fp)\n",
    "        fp_path=os.path.join(file_path,fp)\n",
    "        #print(fp_path)\n",
    "        json_list=os.listdir(fp_path)\n",
    "        json_list=sorted(json_list)\n",
    "        arr=[[] for _ in range(2)]\n",
    "        for i in range(len(json_list)):\n",
    "            js=json_list[i]\n",
    "            if js.split('.')[0]=='model_setup':\n",
    "                arr[0].append(js)\n",
    "            elif js.split('_')[0]=='eval':\n",
    "                arr[1].append(js)\n",
    "        print(arr)# split json files 3types\n",
    "        tmp_dict={}\n",
    "        for i in range(len(json_list)):\n",
    "            js=json_list[i]\n",
    "            #print(os.path.join(fp_path,js))\n",
    "            json_path=os.path.join(fp_path,js)\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if js==arr[0][0]:#Metadata of model\n",
    "                    tmp_dict.update(data)\n",
    "                    print(data)\n",
    "                if js==arr[1][-3]:#eval_metrics\n",
    "                    tmp_dict.update(data)\n",
    "                    print(data)\n",
    "        print(tmp_dict)\n",
    "        json_data.append(tmp_dict)\n",
    "\n",
    "print()\n",
    "print(\"result\")\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.468750</td>\n",
       "      <td>0.636018</td>\n",
       "      <td>0.575667</td>\n",
       "      <td>0.728234</td>\n",
       "      <td>0.683587</td>\n",
       "      <td>0.472895</td>\n",
       "      <td>0.375814</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.644722</td>\n",
       "      <td>0.559518</td>\n",
       "      <td>0.753953</td>\n",
       "      <td>0.642651</td>\n",
       "      <td>0.440150</td>\n",
       "      <td>0.356316</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.273438</td>\n",
       "      <td>0.700625</td>\n",
       "      <td>0.614546</td>\n",
       "      <td>0.780167</td>\n",
       "      <td>0.671416</td>\n",
       "      <td>0.491694</td>\n",
       "      <td>0.460228</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.554688</td>\n",
       "      <td>0.688427</td>\n",
       "      <td>0.581926</td>\n",
       "      <td>0.731320</td>\n",
       "      <td>0.664509</td>\n",
       "      <td>0.482477</td>\n",
       "      <td>0.358262</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.964844</td>\n",
       "      <td>0.670406</td>\n",
       "      <td>0.582430</td>\n",
       "      <td>0.765335</td>\n",
       "      <td>0.646323</td>\n",
       "      <td>0.472884</td>\n",
       "      <td>0.391698</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.957031</td>\n",
       "      <td>0.655204</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.768200</td>\n",
       "      <td>0.663281</td>\n",
       "      <td>0.458752</td>\n",
       "      <td>0.401202</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.046875</td>\n",
       "      <td>0.627253</td>\n",
       "      <td>0.488410</td>\n",
       "      <td>0.743055</td>\n",
       "      <td>0.570165</td>\n",
       "      <td>0.372047</td>\n",
       "      <td>0.208978</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.046875</td>\n",
       "      <td>0.691431</td>\n",
       "      <td>0.543952</td>\n",
       "      <td>0.738484</td>\n",
       "      <td>0.646820</td>\n",
       "      <td>0.389525</td>\n",
       "      <td>0.300589</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.218750</td>\n",
       "      <td>0.590536</td>\n",
       "      <td>0.464609</td>\n",
       "      <td>0.732978</td>\n",
       "      <td>0.507761</td>\n",
       "      <td>0.244571</td>\n",
       "      <td>0.328295</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.023438</td>\n",
       "      <td>0.693515</td>\n",
       "      <td>0.557862</td>\n",
       "      <td>0.750239</td>\n",
       "      <td>0.636866</td>\n",
       "      <td>0.386851</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  1.468750  0.636018  0.575667  0.728234  0.683587  0.472895  0.375814   \n",
       "1  1.000000  0.644722  0.559518  0.753953  0.642651  0.440150  0.356316   \n",
       "2  1.273438  0.700625  0.614546  0.780167  0.671416  0.491694  0.460228   \n",
       "3  1.554688  0.688427  0.581926  0.731320  0.664509  0.482477  0.358262   \n",
       "4  0.964844  0.670406  0.582430  0.765335  0.646323  0.472884  0.391698   \n",
       "5  0.957031  0.655204  0.581994  0.768200  0.663281  0.458752  0.401202   \n",
       "6  1.046875  0.627253  0.488410  0.743055  0.570165  0.372047  0.208978   \n",
       "7  1.046875  0.691431  0.543952  0.738484  0.646820  0.389525  0.300589   \n",
       "8  1.218750  0.590536  0.464609  0.732978  0.507761  0.244571  0.328295   \n",
       "9  1.023438  0.693515  0.557862  0.750239  0.636866  0.386851  0.363807   \n",
       "\n",
       "   full_training      segment    mode      model_name  batch_size  max_epoch  \\\n",
       "0          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "1          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "2          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "3          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "4          False  token_llama  scopes  llm2vec_llama3           1         20   \n",
       "5          False  token_llama  scopes  llm2vec_llama3           1         20   \n",
       "6          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "7          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "8          False  token_llama  scopes  llm2vec_llama3           1         20   \n",
       "9          False  token_llama  scopes  llm2vec_llama3           1         20   \n",
       "\n",
       "   learning_rate  dropout  \n",
       "0        0.00012     0.05  \n",
       "1        0.00007     0.20  \n",
       "2        0.00005     0.15  \n",
       "3        0.00007     0.10  \n",
       "4        0.00007     0.15  \n",
       "5        0.00005     0.20  \n",
       "6        0.00010     0.05  \n",
       "7        0.00012     0.05  \n",
       "8        0.00010     0.15  \n",
       "9        0.00012     0.10  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_json=pd.DataFrame(json_data).replace(\"None\", np.nan)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f756cf73590>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(data = df_json, x = 'learning_rate', y = 'dropout', c = 'loss', cmap = 'Blues')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('dropout')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of token training using LLM2Vec with LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.273438</td>\n",
       "      <td>0.700625</td>\n",
       "      <td>0.614546</td>\n",
       "      <td>0.780167</td>\n",
       "      <td>0.671416</td>\n",
       "      <td>0.491694</td>\n",
       "      <td>0.460228</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.710938</td>\n",
       "      <td>0.713130</td>\n",
       "      <td>0.605669</td>\n",
       "      <td>0.774243</td>\n",
       "      <td>0.684308</td>\n",
       "      <td>0.522946</td>\n",
       "      <td>0.367003</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.429688</td>\n",
       "      <td>0.689653</td>\n",
       "      <td>0.602960</td>\n",
       "      <td>0.774505</td>\n",
       "      <td>0.692127</td>\n",
       "      <td>0.490948</td>\n",
       "      <td>0.400133</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.515625</td>\n",
       "      <td>0.691369</td>\n",
       "      <td>0.601645</td>\n",
       "      <td>0.765771</td>\n",
       "      <td>0.679727</td>\n",
       "      <td>0.479624</td>\n",
       "      <td>0.416680</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.382812</td>\n",
       "      <td>0.694373</td>\n",
       "      <td>0.598974</td>\n",
       "      <td>0.760178</td>\n",
       "      <td>0.688687</td>\n",
       "      <td>0.482089</td>\n",
       "      <td>0.394595</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.234375</td>\n",
       "      <td>0.670651</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>0.759811</td>\n",
       "      <td>0.677958</td>\n",
       "      <td>0.490066</td>\n",
       "      <td>0.397729</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.664276</td>\n",
       "      <td>0.592735</td>\n",
       "      <td>0.762177</td>\n",
       "      <td>0.692179</td>\n",
       "      <td>0.476972</td>\n",
       "      <td>0.396792</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.515625</td>\n",
       "      <td>0.688427</td>\n",
       "      <td>0.591285</td>\n",
       "      <td>0.770767</td>\n",
       "      <td>0.679397</td>\n",
       "      <td>0.465055</td>\n",
       "      <td>0.389128</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.657963</td>\n",
       "      <td>0.589755</td>\n",
       "      <td>0.774765</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.488596</td>\n",
       "      <td>0.421199</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.773438</td>\n",
       "      <td>0.704793</td>\n",
       "      <td>0.589340</td>\n",
       "      <td>0.761868</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.456197</td>\n",
       "      <td>0.390887</td>\n",
       "      <td>False</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  1.273438  0.700625  0.614546  0.780167  0.671416  0.491694  0.460228   \n",
       "1  1.710938  0.713130  0.605669  0.774243  0.684308  0.522946  0.367003   \n",
       "2  1.429688  0.689653  0.602960  0.774505  0.692127  0.490948  0.400133   \n",
       "3  1.515625  0.691369  0.601645  0.765771  0.679727  0.479624  0.416680   \n",
       "4  1.382812  0.694373  0.598974  0.760178  0.688687  0.482089  0.394595   \n",
       "5  1.234375  0.670651  0.592803  0.759811  0.677958  0.490066  0.397729   \n",
       "6  1.304688  0.664276  0.592735  0.762177  0.692179  0.476972  0.396792   \n",
       "7  1.515625  0.688427  0.591285  0.770767  0.679397  0.465055  0.389128   \n",
       "8  0.968750  0.657963  0.589755  0.774765  0.646465  0.488596  0.421199   \n",
       "9  1.773438  0.704793  0.589340  0.761868  0.666667  0.456197  0.390887   \n",
       "\n",
       "   full_training      segment    mode      model_name  batch_size  max_epoch  \\\n",
       "0          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "1          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "2          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "3          False  token_llama  scopes  llm2vec_llama3           1         20   \n",
       "4          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "5          False  token_llama  scopes  llm2vec_llama3           2         20   \n",
       "6          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "7          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "8          False  token_llama  scopes  llm2vec_llama3           1         20   \n",
       "9          False  token_llama  scopes  llm2vec_llama3           4         20   \n",
       "\n",
       "   learning_rate  dropout  \n",
       "0        0.00005     0.15  \n",
       "1        0.00005     0.20  \n",
       "2        0.00012     0.15  \n",
       "3        0.00003     0.00  \n",
       "4        0.00005     0.05  \n",
       "5        0.00012     0.20  \n",
       "6        0.00005     0.10  \n",
       "7        0.00010     0.15  \n",
       "8        0.00007     0.05  \n",
       "9        0.00005     0.15  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json=df_json.sort_values(['macro_f1','total_f1','acc'],ignore_index=True, ascending=False)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best batch_size and max_epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loss                   1.273438\n",
       "acc                    0.700625\n",
       "macro_f1               0.614546\n",
       "total_f1               0.780167\n",
       "inf_f1                 0.671416\n",
       "perc_f1                0.491694\n",
       "backg_f1               0.460228\n",
       "full_training             False\n",
       "segment             token_llama\n",
       "mode                     scopes\n",
       "model_name       llm2vec_llama3\n",
       "batch_size                    2\n",
       "max_epoch                    20\n",
       "learning_rate           0.00005\n",
       "dropout                    0.15\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best batch_size and max_epochs\")\n",
    "df_json.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation of full training using LLM2Vec with LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_full_sentence_majo_scopes_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.2265625, 'acc': 0.6437612771987915, 'macro_f1': 0.4992097020149231, 'total_f1': 0.711670458316803, 'inf_f1': 0.47863247990608215, 'perc_f1': 0.35424354672431946, 'backg_f1': 0.35230353474617004}\n",
      "{'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 1.2265625, 'acc': 0.6437612771987915, 'macro_f1': 0.4992097020149231, 'total_f1': 0.711670458316803, 'inf_f1': 0.47863247990608215, 'perc_f1': 0.35424354672431946, 'backg_f1': 0.35230353474617004, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_sentence_majo_total_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 0.51953125, 'acc': 0.7622061371803284, 'total_f1': 0.7074527144432068}\n",
      "{'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 0.51953125, 'acc': 0.7622061371803284, 'total_f1': 0.7074527144432068, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_sentence_majo_total_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 0.6640625, 'acc': 0.7784810066223145, 'total_f1': 0.7161065936088562}\n",
      "{'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 0.6640625, 'acc': 0.7784810066223145, 'total_f1': 0.7161065936088562, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_sentence_majo_scopes_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics10.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 2.265625, 'acc': 0.5072332620620728, 'macro_f1': 0.40494704246520996, 'total_f1': 0.6166008114814758, 'inf_f1': 0.3378378450870514, 'perc_f1': 0.3232323229312897, 'backg_f1': 0.28205129504203796}\n",
      "{'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 2.265625, 'acc': 0.5072332620620728, 'macro_f1': 0.40494704246520996, 'total_f1': 0.6166008114814758, 'inf_f1': 0.3378378450870514, 'perc_f1': 0.3232323229312897, 'backg_f1': 0.28205129504203796, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_token_llama_scopes_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json', 'eval_metrics9.json']]\n",
      "{'loss': 1.7578125, 'acc': 0.6620519757270813, 'macro_f1': 0.5815226435661316, 'total_f1': 0.7361472845077515, 'inf_f1': 0.6974146366119385, 'perc_f1': 0.4614795744419098, 'backg_f1': 0.3635459840297699}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 1.7578125, 'acc': 0.6620519757270813, 'macro_f1': 0.5815226435661316, 'total_f1': 0.7361472845077515, 'inf_f1': 0.6974146366119385, 'perc_f1': 0.4614795744419098, 'backg_f1': 0.3635459840297699, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_token_llama_scopes_2_1e-05_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json']]\n",
      "{'loss': 1.25, 'acc': 0.6440873742103577, 'macro_f1': 0.5397127866744995, 'total_f1': 0.7333402633666992, 'inf_f1': 0.6392151713371277, 'perc_f1': 0.37776654958724976, 'backg_f1': 0.3408135771751404}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': 1e-05, 'dropout': 0.0}\n",
      "{'loss': 1.25, 'acc': 0.6440873742103577, 'macro_f1': 0.5397127866744995, 'total_f1': 0.7333402633666992, 'inf_f1': 0.6392151713371277, 'perc_f1': 0.37776654958724976, 'backg_f1': 0.3408135771751404, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': 1e-05, 'dropout': 0.0}\n",
      "\n",
      "_full_sentence_prio_scopes_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json']]\n",
      "{'loss': 1.3046875, 'acc': 0.5913200974464417, 'macro_f1': 0.3104233741760254, 'total_f1': 0.5307692289352417, 'inf_f1': 0.4354243576526642, 'perc_f1': 0.06185567006468773, 'backg_f1': 0.0}\n",
      "{'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 1.3046875, 'acc': 0.5913200974464417, 'macro_f1': 0.3104233741760254, 'total_f1': 0.5307692289352417, 'inf_f1': 0.4354243576526642, 'perc_f1': 0.06185567006468773, 'backg_f1': 0.0, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_sentence_prio_total_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 0.53515625, 'acc': 0.7676311135292053, 'total_f1': 0.6815365552902222}\n",
      "{'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 0.53515625, 'acc': 0.7676311135292053, 'total_f1': 0.6815365552902222, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_sentence_prio_scopes_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 1.1953125, 'acc': 0.6754068732261658, 'macro_f1': 0.45716944336891174, 'total_f1': 0.641697883605957, 'inf_f1': 0.760765552520752, 'perc_f1': 0.1269841343164444, 'backg_f1': 0.14432989060878754}\n",
      "{'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 1.1953125, 'acc': 0.6754068732261658, 'macro_f1': 0.45716944336891174, 'total_f1': 0.641697883605957, 'inf_f1': 0.760765552520752, 'perc_f1': 0.1269841343164444, 'backg_f1': 0.14432989060878754, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_sentence_prio_total_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json', 'eval_metrics6.json', 'eval_metrics7.json', 'eval_metrics8.json']]\n",
      "{'loss': 0.6171875, 'acc': 0.764014482498169, 'total_f1': 0.701714277267456}\n",
      "{'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 0.6171875, 'acc': 0.764014482498169, 'total_f1': 0.701714277267456, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_token_llama_total_2_5e-05_0.1\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.478515625, 'acc': 0.777102530002594, 'total_f1': 0.7239619493484497}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "{'loss': 0.478515625, 'acc': 0.777102530002594, 'total_f1': 0.7239619493484497, 'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}\n",
      "\n",
      "_full_token_llama_scopes_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json', 'eval_metrics5.json']]\n",
      "{'loss': 0.9921875, 'acc': 0.6630598902702332, 'macro_f1': 0.5665122270584106, 'total_f1': 0.7210903763771057, 'inf_f1': 0.6916230916976929, 'perc_f1': 0.4040044844150543, 'backg_f1': 0.36685454845428467}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 0.9921875, 'acc': 0.6630598902702332, 'macro_f1': 0.5665122270584106, 'total_f1': 0.7210903763771057, 'inf_f1': 0.6916230916976929, 'perc_f1': 0.4040044844150543, 'backg_f1': 0.36685454845428467, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "_full_token_llama_total_2_1e-04_0.0\n",
      "[['model_setup.json'], ['eval_metrics1.json', 'eval_metrics2.json', 'eval_metrics3.json', 'eval_metrics4.json']]\n",
      "{'loss': 0.470703125, 'acc': 0.7729819416999817, 'total_f1': 0.7468429803848267}\n",
      "{'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "{'loss': 0.470703125, 'acc': 0.7729819416999817, 'total_f1': 0.7468429803848267, 'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}\n",
      "\n",
      "result\n",
      "[{'loss': 1.2265625, 'acc': 0.6437612771987915, 'macro_f1': 0.4992097020149231, 'total_f1': 0.711670458316803, 'inf_f1': 0.47863247990608215, 'perc_f1': 0.35424354672431946, 'backg_f1': 0.35230353474617004, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.51953125, 'acc': 0.7622061371803284, 'total_f1': 0.7074527144432068, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.6640625, 'acc': 0.7784810066223145, 'total_f1': 0.7161065936088562, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 2.265625, 'acc': 0.5072332620620728, 'macro_f1': 0.40494704246520996, 'total_f1': 0.6166008114814758, 'inf_f1': 0.3378378450870514, 'perc_f1': 0.3232323229312897, 'backg_f1': 0.28205129504203796, 'full_training': True, 'segment': 'sentence_majo', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 1.7578125, 'acc': 0.6620519757270813, 'macro_f1': 0.5815226435661316, 'total_f1': 0.7361472845077515, 'inf_f1': 0.6974146366119385, 'perc_f1': 0.4614795744419098, 'backg_f1': 0.3635459840297699, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 1.25, 'acc': 0.6440873742103577, 'macro_f1': 0.5397127866744995, 'total_f1': 0.7333402633666992, 'inf_f1': 0.6392151713371277, 'perc_f1': 0.37776654958724976, 'backg_f1': 0.3408135771751404, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': 1e-05, 'dropout': 0.0}, {'loss': 1.3046875, 'acc': 0.5913200974464417, 'macro_f1': 0.3104233741760254, 'total_f1': 0.5307692289352417, 'inf_f1': 0.4354243576526642, 'perc_f1': 0.06185567006468773, 'backg_f1': 0.0, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 0.53515625, 'acc': 0.7676311135292053, 'total_f1': 0.6815365552902222, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 1.1953125, 'acc': 0.6754068732261658, 'macro_f1': 0.45716944336891174, 'total_f1': 0.641697883605957, 'inf_f1': 0.760765552520752, 'perc_f1': 0.1269841343164444, 'backg_f1': 0.14432989060878754, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.6171875, 'acc': 0.764014482498169, 'total_f1': 0.701714277267456, 'full_training': True, 'segment': 'sentence_prio', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 0.478515625, 'acc': 0.777102530002594, 'total_f1': 0.7239619493484497, 'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 20, 'learning_rate': 5e-05, 'dropout': 0.1}, {'loss': 0.9921875, 'acc': 0.6630598902702332, 'macro_f1': 0.5665122270584106, 'total_f1': 0.7210903763771057, 'inf_f1': 0.6916230916976929, 'perc_f1': 0.4040044844150543, 'backg_f1': 0.36685454845428467, 'full_training': True, 'segment': 'token_llama', 'mode': 'scopes', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}, {'loss': 0.470703125, 'acc': 0.7729819416999817, 'total_f1': 0.7468429803848267, 'full_training': True, 'segment': 'token_llama', 'mode': 'total', 'model_name': 'llm2vec_llama3', 'batch_size': 2, 'max_epoch': 10, 'learning_rate': '1e-04', 'dropout': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "json_data=[]\n",
    "for fp in os.listdir(file_path):\n",
    "    if fp.split('_')[1:2]==['full']:\n",
    "        print()\n",
    "        print(fp)\n",
    "        fp_path=os.path.join(file_path,fp)\n",
    "        #print(fp_path)\n",
    "        json_list=os.listdir(fp_path)\n",
    "        json_list=sorted(json_list)\n",
    "        arr=[[] for _ in range(2)]\n",
    "        for i in range(len(json_list)):\n",
    "            js=json_list[i]\n",
    "            if js.split('.')[0]=='model_setup':\n",
    "                arr[0].append(js)\n",
    "            elif js.split('_')[0]=='eval':\n",
    "                arr[1].append(js)\n",
    "        print(arr)# split json files 3types\n",
    "        tmp_dict={}\n",
    "        for i in range(len(json_list)):\n",
    "            js=json_list[i]\n",
    "            #print(os.path.join(fp_path,js))\n",
    "            json_path=os.path.join(fp_path,js)\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if js==arr[0][0]:#Metadata of model\n",
    "                    tmp_dict.update(data)\n",
    "                    print(data)\n",
    "                if js==arr[1][-3]:#eval_metrics\n",
    "                    tmp_dict.update(data)\n",
    "                    print(data)\n",
    "        print(tmp_dict)\n",
    "        json_data.append(tmp_dict)\n",
    "\n",
    "print()\n",
    "print(\"result\")\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.226562</td>\n",
       "      <td>0.643761</td>\n",
       "      <td>0.499210</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>0.354244</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.762206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.707453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.265625</td>\n",
       "      <td>0.507233</td>\n",
       "      <td>0.404947</td>\n",
       "      <td>0.616601</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.757812</td>\n",
       "      <td>0.662052</td>\n",
       "      <td>0.581523</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>0.697415</td>\n",
       "      <td>0.461480</td>\n",
       "      <td>0.363546</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.644087</td>\n",
       "      <td>0.539713</td>\n",
       "      <td>0.733340</td>\n",
       "      <td>0.639215</td>\n",
       "      <td>0.377767</td>\n",
       "      <td>0.340814</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.591320</td>\n",
       "      <td>0.310423</td>\n",
       "      <td>0.530769</td>\n",
       "      <td>0.435424</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.767631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.195312</td>\n",
       "      <td>0.675407</td>\n",
       "      <td>0.457169</td>\n",
       "      <td>0.641698</td>\n",
       "      <td>0.760766</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.617188</td>\n",
       "      <td>0.764014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  1.226562  0.643761  0.499210  0.711670  0.478632  0.354244  0.352304   \n",
       "1  0.519531  0.762206       NaN  0.707453       NaN       NaN       NaN   \n",
       "2  0.664062  0.778481       NaN  0.716107       NaN       NaN       NaN   \n",
       "3  2.265625  0.507233  0.404947  0.616601  0.337838  0.323232  0.282051   \n",
       "4  1.757812  0.662052  0.581523  0.736147  0.697415  0.461480  0.363546   \n",
       "5  1.250000  0.644087  0.539713  0.733340  0.639215  0.377767  0.340814   \n",
       "6  1.304688  0.591320  0.310423  0.530769  0.435424  0.061856  0.000000   \n",
       "7  0.535156  0.767631       NaN  0.681537       NaN       NaN       NaN   \n",
       "8  1.195312  0.675407  0.457169  0.641698  0.760766  0.126984  0.144330   \n",
       "9  0.617188  0.764014       NaN  0.701714       NaN       NaN       NaN   \n",
       "\n",
       "   full_training        segment    mode      model_name  batch_size  \\\n",
       "0           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "1           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "2           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "3           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "4           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "5           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "6           True  sentence_prio  scopes  llm2vec_llama3           2   \n",
       "7           True  sentence_prio   total  llm2vec_llama3           2   \n",
       "8           True  sentence_prio  scopes  llm2vec_llama3           2   \n",
       "9           True  sentence_prio   total  llm2vec_llama3           2   \n",
       "\n",
       "   max_epoch learning_rate  dropout  \n",
       "0         20       0.00005      0.1  \n",
       "1         20       0.00005      0.1  \n",
       "2         10         1e-04      0.0  \n",
       "3         10         1e-04      0.0  \n",
       "4         20       0.00005      0.1  \n",
       "5         10       0.00001      0.0  \n",
       "6         10         1e-04      0.0  \n",
       "7         20       0.00005      0.1  \n",
       "8         20       0.00005      0.1  \n",
       "9         10         1e-04      0.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json=pd.DataFrame(json_data).replace(\"None\", np.nan)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of full training using LLM2Vec with LLaMA-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.757812</td>\n",
       "      <td>0.662052</td>\n",
       "      <td>0.581523</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>0.697415</td>\n",
       "      <td>0.461480</td>\n",
       "      <td>0.363546</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.663060</td>\n",
       "      <td>0.566512</td>\n",
       "      <td>0.721090</td>\n",
       "      <td>0.691623</td>\n",
       "      <td>0.404004</td>\n",
       "      <td>0.366855</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.644087</td>\n",
       "      <td>0.539713</td>\n",
       "      <td>0.733340</td>\n",
       "      <td>0.639215</td>\n",
       "      <td>0.377767</td>\n",
       "      <td>0.340814</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.226562</td>\n",
       "      <td>0.643761</td>\n",
       "      <td>0.499210</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>0.354244</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.195312</td>\n",
       "      <td>0.675407</td>\n",
       "      <td>0.457169</td>\n",
       "      <td>0.641698</td>\n",
       "      <td>0.760766</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.265625</td>\n",
       "      <td>0.507233</td>\n",
       "      <td>0.404947</td>\n",
       "      <td>0.616601</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.304688</td>\n",
       "      <td>0.591320</td>\n",
       "      <td>0.310423</td>\n",
       "      <td>0.530769</td>\n",
       "      <td>0.435424</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.470703</td>\n",
       "      <td>0.772982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.478516</td>\n",
       "      <td>0.777103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.723962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  1.757812  0.662052  0.581523  0.736147  0.697415  0.461480  0.363546   \n",
       "1  0.992188  0.663060  0.566512  0.721090  0.691623  0.404004  0.366855   \n",
       "2  1.250000  0.644087  0.539713  0.733340  0.639215  0.377767  0.340814   \n",
       "3  1.226562  0.643761  0.499210  0.711670  0.478632  0.354244  0.352304   \n",
       "4  1.195312  0.675407  0.457169  0.641698  0.760766  0.126984  0.144330   \n",
       "5  2.265625  0.507233  0.404947  0.616601  0.337838  0.323232  0.282051   \n",
       "6  1.304688  0.591320  0.310423  0.530769  0.435424  0.061856  0.000000   \n",
       "7  0.470703  0.772982       NaN  0.746843       NaN       NaN       NaN   \n",
       "8  0.478516  0.777103       NaN  0.723962       NaN       NaN       NaN   \n",
       "9  0.664062  0.778481       NaN  0.716107       NaN       NaN       NaN   \n",
       "\n",
       "   full_training        segment    mode      model_name  batch_size  \\\n",
       "0           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "1           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "2           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "3           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "4           True  sentence_prio  scopes  llm2vec_llama3           2   \n",
       "5           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "6           True  sentence_prio  scopes  llm2vec_llama3           2   \n",
       "7           True    token_llama   total  llm2vec_llama3           2   \n",
       "8           True    token_llama   total  llm2vec_llama3           2   \n",
       "9           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "\n",
       "   max_epoch learning_rate  dropout  \n",
       "0         20       0.00005      0.1  \n",
       "1         10         1e-04      0.0  \n",
       "2         10       0.00001      0.0  \n",
       "3         20       0.00005      0.1  \n",
       "4         20       0.00005      0.1  \n",
       "5         10         1e-04      0.0  \n",
       "6         10         1e-04      0.0  \n",
       "7         10         1e-04      0.0  \n",
       "8         20       0.00005      0.1  \n",
       "9         10         1e-04      0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json=df_json.sort_values(['macro_f1','total_f1','acc'],ignore_index=True, ascending=False)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>total_f1</th>\n",
       "      <th>inf_f1</th>\n",
       "      <th>perc_f1</th>\n",
       "      <th>backg_f1</th>\n",
       "      <th>full_training</th>\n",
       "      <th>segment</th>\n",
       "      <th>mode</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>max_epoch</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.470703</td>\n",
       "      <td>0.772982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.757812</td>\n",
       "      <td>0.662052</td>\n",
       "      <td>0.581523</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>0.697415</td>\n",
       "      <td>0.461480</td>\n",
       "      <td>0.363546</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.644087</td>\n",
       "      <td>0.539713</td>\n",
       "      <td>0.733340</td>\n",
       "      <td>0.639215</td>\n",
       "      <td>0.377767</td>\n",
       "      <td>0.340814</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.478516</td>\n",
       "      <td>0.777103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.723962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.663060</td>\n",
       "      <td>0.566512</td>\n",
       "      <td>0.721090</td>\n",
       "      <td>0.691623</td>\n",
       "      <td>0.404004</td>\n",
       "      <td>0.366855</td>\n",
       "      <td>True</td>\n",
       "      <td>token_llama</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.226562</td>\n",
       "      <td>0.643761</td>\n",
       "      <td>0.499210</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>0.354244</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>scopes</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.762206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.707453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_majo</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.617188</td>\n",
       "      <td>0.764014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.767631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>sentence_prio</td>\n",
       "      <td>total</td>\n",
       "      <td>llm2vec_llama3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  macro_f1  total_f1    inf_f1   perc_f1  backg_f1  \\\n",
       "0  0.470703  0.772982       NaN  0.746843       NaN       NaN       NaN   \n",
       "1  1.757812  0.662052  0.581523  0.736147  0.697415  0.461480  0.363546   \n",
       "2  1.250000  0.644087  0.539713  0.733340  0.639215  0.377767  0.340814   \n",
       "3  0.478516  0.777103       NaN  0.723962       NaN       NaN       NaN   \n",
       "4  0.992188  0.663060  0.566512  0.721090  0.691623  0.404004  0.366855   \n",
       "5  0.664062  0.778481       NaN  0.716107       NaN       NaN       NaN   \n",
       "6  1.226562  0.643761  0.499210  0.711670  0.478632  0.354244  0.352304   \n",
       "7  0.519531  0.762206       NaN  0.707453       NaN       NaN       NaN   \n",
       "8  0.617188  0.764014       NaN  0.701714       NaN       NaN       NaN   \n",
       "9  0.535156  0.767631       NaN  0.681537       NaN       NaN       NaN   \n",
       "\n",
       "   full_training        segment    mode      model_name  batch_size  \\\n",
       "0           True    token_llama   total  llm2vec_llama3           2   \n",
       "1           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "2           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "3           True    token_llama   total  llm2vec_llama3           2   \n",
       "4           True    token_llama  scopes  llm2vec_llama3           2   \n",
       "5           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "6           True  sentence_majo  scopes  llm2vec_llama3           2   \n",
       "7           True  sentence_majo   total  llm2vec_llama3           2   \n",
       "8           True  sentence_prio   total  llm2vec_llama3           2   \n",
       "9           True  sentence_prio   total  llm2vec_llama3           2   \n",
       "\n",
       "   max_epoch learning_rate  dropout  \n",
       "0         10         1e-04      0.0  \n",
       "1         20       0.00005      0.1  \n",
       "2         10       0.00001      0.0  \n",
       "3         20       0.00005      0.1  \n",
       "4         10         1e-04      0.0  \n",
       "5         10         1e-04      0.0  \n",
       "6         20       0.00005      0.1  \n",
       "7         20       0.00005      0.1  \n",
       "8         10         1e-04      0.0  \n",
       "9         20       0.00005      0.1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json=df_json.sort_values(['total_f1','acc'],ignore_index=True, ascending=False)\n",
    "df_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for full training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loss                   0.470703\n",
       "acc                    0.772982\n",
       "macro_f1                    NaN\n",
       "total_f1               0.746843\n",
       "inf_f1                      NaN\n",
       "perc_f1                     NaN\n",
       "backg_f1                    NaN\n",
       "full_training              True\n",
       "segment             token_llama\n",
       "mode                      total\n",
       "model_name       llm2vec_llama3\n",
       "batch_size                    2\n",
       "max_epoch                    10\n",
       "learning_rate             1e-04\n",
       "dropout                     0.0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best parameters for full training\")\n",
    "df_json.iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "djk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
