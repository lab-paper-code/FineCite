token_context,word_context,seg_context,sent_cotext,label
"['These translations gave rise to a number of automatically constructed linguistic resources (  Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .']","['These translations gave rise to a number of automatically constructed linguistic resources ( 1 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .']","['These translations gave rise to a number of automatically constructed linguistic resources (  are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .']","['These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .']",1
"['ised', '', 'Kay and Rscheisen (1993) attempt to extract a bilingual dictionary a hybrid method of sentence and word alignment assumption the a', ' #TAUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', 'replicates the work of Fung and McKeown with different language pairs using the simpler metric Levenshtein', '', '', '', '', '']","['premised', '', 'Kay and Röscheisen (1993) attempt to extract a bilingual dictionary a hybrid method of sentence and word alignment assumption the a', ' #TAUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', 'replicates the work of Fung and McKeown with different language pairs using the simpler metric Levenshtein', '', '', '', '', '']","['', '', 'Kay and Rscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment the assumption', ' #TAUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', 'replicates the work of Fung and McKeown with different language pairs using the simpler metric Levenshte', '', '', '', '', '']","['', '', 'Kay and Röscheisen (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution.', ' #TAUTHOR_TAG attempt to translate technical terms using word relation matrices , although the resource from which such relations are derived is a pair of nonparallel corpora .', ' #AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', '', '', '', '', '']",0
"['â\x80¢ Learnability #TAUTHOR_TAG â\x80¢ Text generation ( Hovy 1988 ;osljic , Tulloch , and Dale 1996 )¢ Speech generation Ray Localization (¨er']","['â\x80¢ Learnability #TAUTHOR_TAG â\x80¢ Text generation ( Hovy 1988 ; , Tulloch , and Dale 1996 ) Speech generation Rayner Localization ( aler']","['â\x80¢ Learnability #TAUTHOR_TAG â\x80¢ Text generation ( Hovy 1988 ;osljic , Tulloch , and Dale 1996 )¢ Speech generation Ray Localization (er']","['â\x80¢ Learnability #TAUTHOR_TAG â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0
"['', 'are derived from, patterns are computed from the derived chunks means followingfor each pairs Using algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of, many other researchers to general', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG , inter alia .']","['', 'are derived from patterns are computed from the derived chunks means following ""for each pairs Using algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers to generalized', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG , inter alia .']","['', 'are derived from are computed from the derived chunks means each pair chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of, many other researchers to general', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG , inter alia .']","['', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG , inter alia .']",0
"['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation #TAUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation Localization (¨er ']","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation #TAUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation Localization ( aler']","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation #TAUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â¢ Speech generation Localization (er ']","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation #TAUTHOR_TAG ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation ( Rayner and Carter 1997 ) â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0
"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In TL sentence we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #TAUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In TL sentence we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #TAUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #TAUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX #TAUTHOR_TAG ; Veale and Way 1997 ; Carl 1999 ) .7']",0
"['translation memory (TM systems have become a very useful tool in the translators arm.', 'systems store a set of', 'a new a for (zy', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one #TAUTHOR_TAG .']","[""translation memory (TM) systems have become a very useful tool in the translator's armory."", 'systems store a set of', 'a new a for (or', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one #TAUTHOR_TAG .']","["", translation memory (TM) systems have become a very useful tool in the translator's armory."", 'M systems store a set of', 'a new input string a search for (', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one #TAUTHOR_TAG .']","[""In quite a short space of time, translation memory (TM) systems have become a very useful tool in the translator's armory."", '', '', 'From this description , it should be clear that TM systems do not translate : Indeed , some researchers consider them to be little more than a search-and-replace engine , albeit a rather sophisticated one #TAUTHOR_TAG .']",0
"['', '', '', '', ' #AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', ' #TAUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', 'lengths of', '', '', '']","['', '', '', '', ' #AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', ' #TAUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', 'lengths of', '', '', '']","['', '', '', '', ' #AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', ' #TAUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', 'of', '', '', '']","['', '', '', '', ' #AUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance.', ' #TAUTHOR_TAG use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities .', '', '', '', '']",0
"['¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 )¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #TAUTHOR_TAG ; Gough , Way , and Hearne 2002 )']","['language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #TAUTHOR_TAG ; Gough , Way , and Hearne 2002 )']","['¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 )¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #TAUTHOR_TAG ; Gough , Way , and Hearne 2002 )']","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; #TAUTHOR_TAG ; Gough , Way , and Hearne 2002 )']",0
"['â\x80¢ language learning #TAUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 )\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Ju 994 )¢ into universal grammar ( Ju']","['â\x80¢ language learning #TAUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) into universal grammar ( Juola']","['â\x80¢ language learning #TAUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 )\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )¢ into universal grammar ( Ju']","['â\x80¢ language learning #TAUTHOR_TAG ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0
"['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , #TAUTHOR_TAG , and #AUTHOR_TAG .']","['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , #TAUTHOR_TAG , and #AUTHOR_TAG .']","['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , #TAUTHOR_TAG , and #AUTHOR_TAG .']","['Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , #TAUTHOR_TAG , and #AUTHOR_TAG .']",1
"['More recently , #TAUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and #AUTHOR_TAG , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those in3']","['More recently , #TAUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and #AUTHOR_TAG , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those in']","['More recently , #TAUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and #AUTHOR_TAG , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those in']","['More recently , #TAUTHOR_TAG have proposed the exploitation of TMs at a subsentential level , while Carl , Way , and Sch Â¨ aler ( 2002 ) and Sch Â¨ aler , Way , and #AUTHOR_TAG , pages 108 -- 109 ) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment .', 'This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from _source, target_ phrasal segments, and from there they suggest that �it is a reasonably short step to enabling an automated solution via the recombination element of EBMT systems such as those described in [Carl and Way 2003].�']",0
"['', '', '', 'attempt to relation matrices from such are derived a pair of nonparallel corpora.', ' #TAUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', '', '', '', '', '']","['', '', '', 'attempt to relation matrices, from such are derived a pair of nonparallel corpora.', ' #TAUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', '', '', '', '', '']","['', '', '', ' #AUTHOR_TAG attempt to word relation matrices, from such relations are derived is a pair of nonparallel corpora.', ' #TAUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', '', '', '', '', '']","['', '', '', '', ' #TAUTHOR_TAG replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance .', '', '', '', '', '']",0
"[' #TAUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides', 'the, a present particip', '', '']","[' #TAUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.', 'the is, a present', '', '']","[' #TAUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.', ', a present participle.', '', '']","[' #TAUTHOR_TAG , 1997 ) assumes that words ending in - ed are verbs .', 'However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category.', 'Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides.', '', '', '']",1
"['', '', '', '', '', '', '', '', '', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm #TAUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm #TAUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm #TAUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', 'We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm #TAUTHOR_TAG .']",3
"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', '', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #TAUTHOR_TAG ; Carl 1999 ) .7']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', '', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #TAUTHOR_TAG ; Carl 1999 ) .7']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', '', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #TAUTHOR_TAG ; Carl 1999 ) .7']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', '', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; #TAUTHOR_TAG ; Carl 1999 ) .7']",0
"['The problem of boundary friction is clearly here We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TAUTHOR_TAG .', 'Grefen the Web can be used', '', '', '', '', '', '', '']","['The problem of boundary friction is clearly here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TAUTHOR_TAG .', 'Grefenstette the Web can be used', '', '', '', '', '', '', '']","['The problem of boundary friction is clearly We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TAUTHOR_TAG .', 'Grefen the Web can be used', '', '', '', '', '', '', '']","['The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP.', 'However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on #TAUTHOR_TAG .', '', '', '', '', '', '', '', '']",5
"['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', ' #TAUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', '→ language pair, Ju', '', '', '', '']","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', ' #TAUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', '−→ language pair, Juola', '', '', '', '']","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', ' #TAUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', 'French language pair, Juola', '', '', '', '']","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', ' #TAUTHOR_TAG , 1997 ) conducts some small experiments using his METLA system to show the viability of this approach for English â\x88\x92 > French and English â\x88\x92 > Urdu .', '', '', '', '', '']",0
"['¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization #TAUTHOR_TAG â\x80¢ insights into universal grammar ( Juola 1998 )\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 7 ; Gough , , and Hearne 2 )']","['language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization #TAUTHOR_TAG â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way ; Gough , , and Hearne )']","['¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization #TAUTHOR_TAG â\x80¢ insights into universal grammar ( Juola 1998 )\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , , and Hearne 2002 )']","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization #TAUTHOR_TAG â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0
"['In a final processing stage , we generalize over the marker lexicon following a process found in #TAUTHOR_TAG .', ""In's approach, word alignments are assigned probabilities by means of a statistical word alignment."", 'chunk extracted']","['In a final processing stage , we generalize over the marker lexicon following a process found in #TAUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'chunk extracted,']","['In a final processing stage , we generalize over the marker lexicon following a process found in #TAUTHOR_TAG .', ""In's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", 'extracted']","['In a final processing stage , we generalize over the marker lexicon following a process found in #TAUTHOR_TAG .', ""In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool."", '']",5
"['', '', '', '', '', '', 'to in the to be', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TAUTHOR_TAG to permit a limited form of insertion in the translation process .', 'we a ""- le', '']","['', '', '', '', '', '', 'to in the to be', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TAUTHOR_TAG to permit a limited form of insertion in the translation process .', 'we a', '']","['', '', '', '', '', '', 'to in the languages to be', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TAUTHOR_TAG to permit a limited form of insertion in the translation process .', 'we', '']","['', '', '', '', '', '', '', 'Following construction of the marker lexicon , the ( source , target ) chunks are generalized further using a methodology based on #TAUTHOR_TAG to permit a limited form of insertion in the translation process .', '', '']",5
"['the isably universal is clear that benefits may accue to facilitate subsential of source, target chunks.', '', '', '', '', '', 'In their Gaijin system , #TAUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']","['the is universal, is clear that benefits may accrue to facilitate subsentential of source, target chunks.', '', '', '', '', '', 'In their Gaijin system , #TAUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']","['is arguably universal it is clear that benefits may accrue to facilitate subsential alignment of source, target chunks.', '', '', '', '', '', 'In their Gaijin system , #TAUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']","['Given that the marker hypothesis is arguably universal, it is clear that benefits may accrue by using it to facilitate subsentential alignment of source, target chunks.', '', '', '', '', '', 'In their Gaijin system , #TAUTHOR_TAG give a result of 63 % accurate translations obtained for English â\x88\x92 > German on a test set of 791 sentences from CorelDRAW manuals .']",1
"['â\x80¢ language learning ( Green 1979 ; #TAUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )¢ insights into universal grammar ( Ju  )¢']","['â\x80¢ language learning ( Green 1979 ; #TAUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) insights into universal grammar ( Juola )']","['â\x80¢ language learning ( Green 1979 ; #TAUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 )¢ insights into universal grammar ( Ju )¢']","['â\x80¢ language learning ( Green 1979 ; #TAUTHOR_TAG ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction ( Juola 1998 ) â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0
"['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction #TAUTHOR_TAG â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 )¢ machine translation Ju , ; Gough ,']","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction #TAUTHOR_TAG â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) machine translation Juola , ; Gough ,']","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction #TAUTHOR_TAG â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 )¢ machine translation Ju , ; Gough ,']","['â\x80¢ language learning ( Green 1979 ; Mori and Moeser 1983 ; Morgan , Meier , and Newport 1989 ) â\x80¢ monolingual grammar induction #TAUTHOR_TAG â\x80¢ grammar optimization ( Juola 1994 ) â\x80¢ insights into universal grammar ( Juola 1998 ) â\x80¢ machine translation ( Juola 1994 , 1997 ; Veale and Way 1997 ; Gough , Way , and Hearne 2002 )']",0
"['the is may accue to facilitate subsential _ chunks', '', 'For the English __ French language pair, Ju gives results of61% correct when the system is tested on the training cor, and36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu , #TAUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', 'Gaij #AUTHOR_TAG6 transl obtained']","['the is may accrue to facilitate subsentential chunks.', '', 'For the English __ French language pair, Juola gives results of 61% correct when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu , #TAUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', 'Gaijin #AUTHOR_TAG translations obtained']","['it is may accrue to facilitate subsential alignment _', '', 'For the English __ French language pair, Juola gives results of61% correct translation when the system is tested on the training corpus, and36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu , #TAUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', 'Gaij #AUTHOR_TAG6 obtained']","['', '', 'For the English __ French language pair, Juola gives results of 61% correct translation when the system is tested on the training corpus, and 36% accuracy when it is evaluated with test data.', 'For English â\x88\x92 > Urdu , #TAUTHOR_TAG , page 213 ) notes that ""the system learned the original training corpus ... perfectly and could reproduce it without errors ; that is , it scored 100 % accuracy when tested against the training corpus .', 'On novel test sentences, he gives results of 72% correct translation.', '']",0
"['', 'are derived from, patterns are computed from the derived chunks means following Using described above, the patterns in ( 26) are derived from the chunks in ( 2): Of, researchers to', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , Mc #AUTHOR_TAG , #TAUTHOR_TAG , and #AUTHOR_TAG , inter alia .']","['', 'are derived from patterns are computed from the derived chunks means following Using described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, researchers to', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , Mc #AUTHOR_TAG , #TAUTHOR_TAG , and #AUTHOR_TAG , inter alia .']","['', 'are derived from are computed from the derived chunks means described above, the patterns in ( 26) are derived from the chunks in ( 2): Of, to', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , Mc #AUTHOR_TAG , #TAUTHOR_TAG , and #AUTHOR_TAG , inter alia .']","['', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , Mc #AUTHOR_TAG , #TAUTHOR_TAG , and #AUTHOR_TAG , inter alia .']",0
"['', '', 'First, the phrasalxicon is segmented using the marker hypothesis to produce a marker lexicon.', 'This is then generalized , following a methodology based on #TAUTHOR_TAG , to generate the `` generalized marker lexicon .', '', '']","['', '', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', 'This is then generalized , following a methodology based on #TAUTHOR_TAG , to generate the `` generalized marker lexicon .', '', '']","['', '', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', 'This is then generalized , following a methodology based on #TAUTHOR_TAG , to generate the `` generalized marker lexicon .', '', '']","['', '', 'First, the phrasal lexicon is segmented using the marker hypothesis to produce a marker lexicon.', ""This is then generalized , following a methodology based on #TAUTHOR_TAG , to generate the `` generalized marker lexicon . ''"", '', '']",5
"['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ;osavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation #TAUTHOR_TAG â\x80¢ Localization ( Sch Â¨ aler 1996 )']","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation #TAUTHOR_TAG â\x80¢ Localization ( Sch Â¨ aler 1996 )']","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ;osavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation #TAUTHOR_TAG â\x80¢ Localization ( Sch Â¨ aler 1996 )']","['â\x80¢ Learnability ( Zernik and Dyer 1987 ) â\x80¢ Text generation ( Hovy 1988 ; Milosavljevic , Tulloch , and Dale 1996 ) â\x80¢ Speech generation #TAUTHOR_TAG â\x80¢ Localization ( Sch Â¨ aler 1996 )']",0
"[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' #TAUTHOR_TAG is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes', '']","[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' #TAUTHOR_TAG is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.', '']","[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' #TAUTHOR_TAG is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes', '']","[""Each set of translations is stored separately , and for each set the `` marker hypothesis '' #TAUTHOR_TAG is used to segment the phrasal lexicon into a `` marker lexicon . ''"", 'The marker hypothesis is a universal psycholinguistic constraint which states that natural languages are ""marked"" for complex syntactic structure at surface form by a closed set of specific lexemes and morphemes.', '']",5
"['That is, where #TAUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the to form the correct translation un bon.', '']","['That is, where #TAUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the to form the correct translation un bon homme.', '']","['That is, where #TAUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by to form the correct translation un bon homme.', '']","['That is, where #TAUTHOR_TAG substitutes variables for various words in his templates, we replace certain lexical items with their marker tag.', 'Given that examples such as ��<DET> a : un� are likely to exist in the word-level lexicon, they may be inserted at the point indicated by the marker tag to form the correct translation un bon homme.', '']",1
"['', '', '', 'Jus ( , 8 work on grammar optimization andction shows that context-free grars can be converted to ""marker-normal form.""', ', marker- grammars cannot capture the of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless , #TAUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is straightforward to map an Englishiner-noun onto a Japanese noun- the of marker in the to be', '', '', '']","['', '', '', 'Juola\'s ( , work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal grammars cannot capture the of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless , #TAUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is straightforward to map an English determiner-noun onto a Japanese noun-case the of marker in the to be', '', '', '']","['', '', '', '( , 8 work on grammar optimization andction shows that context-free grammars can be converted to ""marker-normal form.""', ', marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless , #TAUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, the sets of marker tags in the languages to be', '', '', '']","['', '', '', ' #AUTHOR_TAG Juola\'s ( , 1998 work on grammar optimization and induction shows that context-free grammars can be converted to ""marker-normal form.""', 'However, marker-normal form grammars cannot capture the sorts of regularities demonstrated for languages that do not have a oneto-one mapping between a terminal symbol and a word.', ""Nevertheless , #TAUTHOR_TAG , page 23 ) observes that `` a slightly more general mapping , where two adjacent terminal symbols can be merged into a single lexical item ( for example , a word and its case-marking ) , can capture this sort of result quite handily . ''"", 'Work using the marker hypothesis for MT adapts this monolingual mapping for pairs of languages: It is reasonably straightforward to map an English determiner-noun sequence onto a Japanese noun-case marker segment, once one has identified the sets of marker tags in the languages to be translated.', '', '', '']",0
"['', 'are derived from, patterns are computed from the derived chunks means following each Using described above, the patterns in ( 26) are derived from the chunks in ( 25): Of, many researchers', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , Mc #AUTHOR_TAG , #AUTHOR_TAG , and #TAUTHOR_TAG , inter alia .']","['', 'are derived from patterns are computed from the derived chunks means following each Using described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many researchers', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , Mc #AUTHOR_TAG , #AUTHOR_TAG , and #TAUTHOR_TAG , inter alia .']","['', 'are derived from are computed from the derived chunks means each pair described above, the patterns in ( 26) are derived from the chunks in ( 25): Of, many other researchers', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , Mc #AUTHOR_TAG , #AUTHOR_TAG , and #TAUTHOR_TAG , inter alia .']","['', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', '', '', 'Other similar approaches include those of Cicekli and G Â¨ uvenir ( 1996 ) , Mc #AUTHOR_TAG , #AUTHOR_TAG , and #TAUTHOR_TAG , inter alia .']",0
"['', '', '', '', '', '', '', '', '', 'More specifically , the notion of the phrasal lexicon ( used first by #TAUTHOR_TAG has been used successfully in a number of areas :']","['', '', '', '', '', '', '', '', '', 'More specifically , the notion of the phrasal lexicon ( used first by #TAUTHOR_TAG has been used successfully in a number of areas :']","['', '', '', '', '', '', '', '', '', 'More specifically , the notion of the phrasal lexicon ( used first by #TAUTHOR_TAG has been used successfully in a number of areas :']","['', '', '', '', '', '', '', '', '', 'More specifically , the notion of the phrasal lexicon ( used first by #TAUTHOR_TAG has been used successfully in a number of areas :']",0
"['All EBMT systems , from the initial proposal by #TAUTHOR_TAG to the recent collection of #AUTHOR_TAG , are premised on the availability of subsentential alignments derived from the input bitext .', 'is a of literature on trying to establish subsentential translations bual cor', '', '', '', '', '', '', '', '']","['All EBMT systems , from the initial proposal by #TAUTHOR_TAG to the recent collection of #AUTHOR_TAG , are premised on the availability of subsentential alignments derived from the input bitext .', 'is a of literature on trying to establish subsentential translations bilingual', '', '', '', '', '', '', '', '']","['All EBMT systems , from the initial proposal by #TAUTHOR_TAG to the recent collection of #AUTHOR_TAG , are premised on the availability of subsentential alignments derived from the input bitext .', 'is a wealth of literature on trying to establish subsentential translations', '', '', '', '', '', '', '', '']","['All EBMT systems , from the initial proposal by #TAUTHOR_TAG to the recent collection of #AUTHOR_TAG , are premised on the availability of subsentential alignments derived from the input bitext .', 'There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus. 3', '', '', '', '', '', '', '', '']",0
"['', 'derived are computed derived following each Using described above, the patterns in ( 26) are derived from the chunks in ( 2): Of course, many other researchers to extract generalized templates.', 'suchents with to generate a set of translation patterns.', ' #TAUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include Cicek and Güven (1996), Mc #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG ,']","['', 'derived are computed derived following each Using described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers to extract generalized templates.', 'such equivalents with to generate a set of translation patterns.', ' #TAUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include Cicekli and Güvenir (1996), Mc #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG ,']","['', 'are derived are computed derived each pair described above, the patterns in ( 26) are derived from the chunks in ( 2): Of course, many other researchers to extract generalized templates.', 'such equivalents with to generate a set of translation patterns.', ' #TAUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include Cicek and Güven (1996), Mc #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG ,']","['', 'Once chunks are derived from source, target alignments, patterns are computed from the derived chunks by means of the following algorithm: ""for each pair of chunk pairs Using the algorithm described above, the patterns in ( 26) are derived from the chunks in ( 25): Of course, many other researchers also try to extract generalized templates.', '', ' #TAUTHOR_TAG combines lexical and dependency mappings to form his generalizations .', 'Other similar approaches include those of Cicekli and Güvenir (1996), Mc #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG , inter alia.']",0
"['where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TAUTHOR_TAG .', 'parse', 'direct application', '', '', '', '', '']","['where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TAUTHOR_TAG .', 'parse', 'direct application', '', '', '', '', '']","['where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TAUTHOR_TAG .', '', 'a direct application', '', '', '', '', '']","['where V is a random variable that can take any possible outcome in set V, and p(v) = Pr(V = v) is the density function.', 'Further details about the properties of entropy can be found in textbooks on information theory ( e.g. , #TAUTHOR_TAG .', '', '', '', '', '', '', '']",0
"['applied tonotationleneck', '', '', '', '', 'The work of #TAUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', '', '']","['applied to annotation bottleneck', '', '', '', '', 'The work of #TAUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', '', '']","['have applied to', '', '', '', '', 'The work of #TAUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', '', '']","['', '', '', '', '', 'The work of #TAUTHOR_TAG and Steedman , Osborne , et al. ( 2003 ) suggests that co-training can be helpful for statistical parsing .', '', '']",0
"['benefits problems in which the costiring raw data is cheap but thenotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization #TAUTHOR_TAG , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill  ) , and word sense disambiguation ( Fii et.1998 ) .']","['benefits problems in which the cost acquiring raw data is cheap but the annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization #TAUTHOR_TAG , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill ) , and word sense disambiguation ( Fujii et al. 1998 ) .']","['Sample selection benefits problems in which the costiring raw data is cheap but the costnotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization #TAUTHOR_TAG , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Br ) , and word sense disambiguation ( Fii et.1998 ) .']","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization #TAUTHOR_TAG , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']",0
"['from active, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing', '', '', '', 'Another technique for making better use of unlabeled data is cotraining #TAUTHOR_TAG , in which two sufficiently different learners help each other learn by labeling training data for one another .', '', '', '']","['from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', '', '', '', 'Another technique for making better use of unlabeled data is cotraining #TAUTHOR_TAG , in which two sufficiently different learners help each other learn by labeling training data for one another .', '', '', '']","['from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing', '', '', '', 'Another technique for making better use of unlabeled data is cotraining #TAUTHOR_TAG , in which two sufficiently different learners help each other learn by labeling training data for one another .', '', '', '']","['Aside from active learning, researchers have applied other learning techniques to combat the annotation bottleneck problem in parsing.', '', '', '', 'Another technique for making better use of unlabeled data is cotraining #TAUTHOR_TAG , in which two sufficiently different learners help each other learn by labeling training data for one another .', '', '', '']",0
"['', 'shar', '', ""predict domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism #TAUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although models arexicalized', '', '']","['', '', '', ""predictive domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism #TAUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although models are lexicalized,', '', '']","['', '', '', "", we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism #TAUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models arexicalized', '', '']","['', '', '', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism #TAUTHOR_TAG ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) ."", '', '', '']",5
"['of structuraluities ar fromactic in which apositional equally theoun.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models #TAUTHOR_TAG , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '', '']","['of structural ambiguities arises from syntactic in which a prepositional equally the noun it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models #TAUTHOR_TAG , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '', '']","['of structural ambiguities ar from in which a prepositional phrase', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models #TAUTHOR_TAG , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '', '']","['', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models #TAUTHOR_TAG , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '', '']",0
"['is cheap is as is certainly the case for many supervised learning tasks in natural processing.', '', 'includeization and Catlett 9 base noun phraseing ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation #TAUTHOR_TAG , and word sense disambiguation ( Fujii et al. 1998 ) .']","['is cheap is as is certainly the case for many supervised learning tasks in natural processing.', '', 'include categorization and Catlett base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation #TAUTHOR_TAG , and word sense disambiguation ( Fujii et al. 1998 ) .']","['is cheap them is as is certainly the case for many supervised learning tasks in natural language processing.', '', 'include and Catlett 1994 base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation #TAUTHOR_TAG , and word sense disambiguation ( Fujii et al. 1998 ) .']","['', '', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking ( Ngai and Yarowsky 2000 ) , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation #TAUTHOR_TAG , and word sense disambiguation ( Fujii et al. 1998 ) .']",0
"['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities #TAUTHOR_TAG , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the quantity']","['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities #TAUTHOR_TAG , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the quantity']","['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities #TAUTHOR_TAG , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the quantity']","['Using the bottom-up , dynamic programming technique ( see the appendix for details ) of computing inside probabilities #TAUTHOR_TAG , we can efficiently compute the probability of the sentence , P ( w | G ) .', 'Similarly, the algorithm can be modified to compute the quantity']",5
"['', '', 'vised', '', '', '', 'Our algorithm is similar to the approach taken by #TAUTHOR_TAG for inducing PCFG parsers .']","['', '', '', '', '', '', 'Our algorithm is similar to the approach taken by #TAUTHOR_TAG for inducing PCFG parsers .']","['', '', 'vised', '', '', '', 'Our algorithm is similar to the approach taken by #TAUTHOR_TAG for inducing PCFG parsers .']","['', '', '', '', '', '', 'Our algorithm is similar to the approach taken by #TAUTHOR_TAG for inducing PCFG parsers .']",1
"['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data #TAUTHOR_TAG .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santor Marcinkicz 3', '', '', '', '']","['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data #TAUTHOR_TAG .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, Marcinkiewicz', '', '', '', '']","['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data #TAUTHOR_TAG .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, Marcink', '', '', '', '']","['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example , while it is difficult to induce a grammar with raw text alone , the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data #TAUTHOR_TAG .', 'Current state-of-the-art statistical parsers (Collins 1999;Charniak 2000) are all trained on large annotated corpora such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).', '', '', '', '']",0
"['benefits problems in which the cost of acquiring raw data is cheap but the cost annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'to-attachment, as in this, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking #TAUTHOR_TAG , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 200 ) , and word sense disambiguation ( Fii et.1998 ) .']","['benefits problems in which the cost of acquiring raw data is cheap but the cost annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'to PP-attachment, as in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking #TAUTHOR_TAG , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'to-attachment, as in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking #TAUTHOR_TAG , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fii et.1998 ) .']","['Sample selection benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing.', 'In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification applications.', 'Some examples include text categorization ( Lewis and Catlett 1994 ) , base noun phrase chunking #TAUTHOR_TAG , part-of-speech tagging ( Engelson Dagan 1996 ) , spelling confusion set disambiguation ( Banko and Brill 2001 ) , and word sense disambiguation ( Fujii et al. 1998 ) .']",0
"['', 'shar', '', ""to domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #TAUTHOR_TAG , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models arexicalized, statisticals, learning algorithms are', '', '']","['', '', '', ""to domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #TAUTHOR_TAG , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, learning algorithms are', '', '']","['', '', '', ""to , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #TAUTHOR_TAG , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models arexicalized, statisticals, learning algorithms are', '', '']","['', '', '', ""Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; #TAUTHOR_TAG , and Collins 's Model 2 parser ( 1997 ) ."", 'Although both models are lexicalized, statistical parsers, their learning algorithms are different.', '', '']",5
"['have applied other techniques to annotation bottleneck in', '', '', '', '', '', ' #TAUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', '3']","['have applied other techniques to annotation bottleneck in', '', '', '', '', '', ' #TAUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', '']","['have applied other learning techniques to the annotation bottleneck problem in', '', '', '', '', '', ' #TAUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', '3']","['', '', '', '', '', '', ' #TAUTHOR_TAG have shown , in the context of base noun identification , that combining sample selection and cotraining can be an effective learning framework for large-scale training .', '']",0
"['Many learning tasks for natural language processing require supervised training; the system successfullys a concept only if it has been given annotated training data.', 'it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 199).', 'Current state-of-the-art statistical parsers #TAUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', '', '', '', '']","['Many learning tasks for natural language processing require supervised training; the system successfully learns a concept only if it has been given annotated training data.', 'it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers #TAUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', '', '', '', '']","['Many learning tasks for natural language processing require supervised training; the system successfullys a concept only if it has been given annotated training data.', 'it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers #TAUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', '', '', '', '']","['Many learning tasks for natural language processing require supervised training; that is, the system successfully learns a concept only if it has been given annotated training data.', 'For example, while it is difficult to induce a grammar with raw text alone, the task is tractable when the syntactic analysis for each sentence is provided as a part of the training data (Pereira and Schabes 1992).', 'Current state-of-the-art statistical parsers #TAUTHOR_TAG ; Charniak 2000 ) are all trained on large annotated corpora such as the Penn Treebank ( Marcus , Santorini , and Marcinkiewicz 1993 ) .', '', '', '', '']",0
"['One source of structural ambiguities arises from syntactic constructs in which apositional phrase be equally to modify the or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models #TAUTHOR_TAG , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '', 'use theBrooks model as learning algorithm']","['One source of structural ambiguities arises from syntactic constructs in which a prepositional phrase be equally to modify the or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models #TAUTHOR_TAG , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '', 'use the Collins-Brooks model as learning algorithm']","['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models #TAUTHOR_TAG , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '', 'We use the Collins-Brooks model as the basic learning algorithm']","['One common source of structural ambiguities arises from syntactic constructs in which a prepositional phrase might be equally likely to modify the verb or the noun preceding it.', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models #TAUTHOR_TAG , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .', '', '']",0
"['In the first experiment , we use an induction algorithm #TAUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', 'The heistic through re procedure to local optima values for the grammarizes the gram generating the training data', 'unvised learning; too', '', '', '', '']","['In the first experiment , we use an induction algorithm #TAUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', 'The heuristic through procedure to local optima: values for the grammar maximizes the generating the training data.', 'unsupervised learning; too', '', '', '', '']","['In the first experiment , we use an induction algorithm #TAUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', 'The algorithm heistic search through to local optima: sets values for the grammar parametersizes generating the training data.', 'unvised learning has too', '', '', '', '']","['In the first experiment , we use an induction algorithm #TAUTHOR_TAG a ) based on the expectation-maximization ( EM ) principle that induces parsers for PLTIGs .', '', '', '', '', '', '']",5
"['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example #TAUTHOR_TAG .', 'The underlying assumption is that an uncertain output is likely to be wrong.']","['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example #TAUTHOR_TAG .', 'The underlying assumption is that an uncertain output is likely to be wrong.']","['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example #TAUTHOR_TAG .', 'The underlying assumption is that an uncertain output is likely to be wrong.']","['Performance of the hypothesis: Testing the candidates on the current working hypothesis shows the type of input data on which the hypothesis may perform weakly.', 'That is , if the current hypothesis is unable to label a candidate or is uncertain about it , then the candidate might be a good training example #TAUTHOR_TAG .', 'The underlying assumption is that an uncertain output is likely to be wrong.']",0
"['have applied other techniques to annotation bottleneck', '', '', '', '', '', '', 'Similar approaches are being explored for parsing ( Steedman , #TAUTHOR_TAG ; Hwa et al. 2003 ) .']","['have applied other techniques to annotation bottleneck', '', '', '', '', '', '', 'Similar approaches are being explored for parsing ( Steedman , #TAUTHOR_TAG ; Hwa et al. 2003 ) .']","['have applied other learning techniques to the annotation bottleneck problem', '', '', '', '', '', '', 'Similar approaches are being explored for parsing ( Steedman , #TAUTHOR_TAG ; Hwa et al. 2003 ) .']","['', '', '', '', '', '', '', 'Similar approaches are being explored for parsing ( Steedman , #TAUTHOR_TAG ; Hwa et al. 2003 ) .']",0
"['The-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by #TAUTHOR_TAG .', 'bit', 'the builds attachment statistics for', '', '']","['The Collins-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by #TAUTHOR_TAG .', '', 'the builds attachment statistics for', '', '']","['The Collins-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by #TAUTHOR_TAG .', '', 'the system builds attachment statistics for', '', '']","['The Collins-Brooks PP-attachment classification algorithm.', 'preposition, and the prepositional noun phrase, respectively, and a specifies the attachment classification.', 'For example, (wrote a book in three days, attach-verb) would be annotated as (wrote, book, in, days, verb).', 'The head words can be automatically extracted using a heuristic table lookup in the manner described by #TAUTHOR_TAG .', '', '', '', '']",5
"['ities ar fromactic in which apositional equally theoun', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model #TAUTHOR_TAG .', '', 'use theBro']","['arises from syntactic in which a prepositional equally the noun', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model #TAUTHOR_TAG .', '', 'use the']","['ar from in which a prepositional phrase', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model #TAUTHOR_TAG .', '', 'We use']","['', 'Researchers have proposed many computational models for resolving PPattachment ambiguities.', 'Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model #TAUTHOR_TAG .', '', '']",0
"['', '', 'b', 'We follow the notation convention of #TAUTHOR_TAG .']","['', '', '', 'We follow the notation convention of #TAUTHOR_TAG .']","['', '', '', 'We follow the notation convention of #TAUTHOR_TAG .']","['', '', '', 'We follow the notation convention of #TAUTHOR_TAG .']",5
"['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and singlener.', 'A committee-based works with multiple learn maintaining a hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #TAUTHOR_TAG .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']","['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based works with multiple maintaining a hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #TAUTHOR_TAG .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']","['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #TAUTHOR_TAG .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']","['Unlike traditional learning systems that receive training examples indiscriminately, a sample selection learning system actively influences its own progress by choosing new examples to incorporate into its training set.', 'There are two types of selection algorithms: committee-based and single learner.', 'A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).', 'The candidate examples that lead to the most disagreements among the different learners are considered to have the highest TUV ( Cohn , Atlas , and Ladner 1994 ; #TAUTHOR_TAG .', 'For computationally intensive problems, such as parsing, keeping multiple learners may be impractical.']",0
"['', '', '', '', 'The largest lexical evaluation we know of is that of Schulte im #AUTHOR_TAG b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden #TAUTHOR_TAG .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']","['', '', '', '', 'The largest lexical evaluation we know of is that of Schulte im #AUTHOR_TAG b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden #TAUTHOR_TAG .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']","['', '', '', '', 'The largest lexical evaluation we know of is that of Schulte im #AUTHOR_TAG b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden #TAUTHOR_TAG .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']","['', '', '', '', 'The largest lexical evaluation we know of is that of Schulte im #AUTHOR_TAG b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden #TAUTHOR_TAG .', 'We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.']",0
"['drawback to using an existing external gold standard such', '', '', '', '', '', '', '', ' #TAUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', '(95%), (84%).', 'on recall of against COM', '', '', '']","['drawback to using an existing external gold standard such', '', '', '', '', '', '', '', ' #TAUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', '(95%), (84%).', 'on recall of against', '', '', '']","['Another drawback to using an existing external gold standard such', '', '', '', '', '', '', '', ' #TAUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', '(95%), (84%).', 'on recall scores of against COM', '', '', '']","['', '', '', '', '', '', '', '', ' #TAUTHOR_TAG report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .', 'Precision was quite high (95%), but recall was low (84%).', '', '', '', '']",1
"['', '', '', '', '', '', '', 'extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #TAUTHOR_TAG and #AUTHOR_TAG', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', '', 'extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #TAUTHOR_TAG and #AUTHOR_TAG', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', '', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #TAUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', '', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #TAUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '']",0
"['In syntactic ( exical-functional grammar [ LFG ] [ Kaplan and Bresnan2 ; Bresnan  ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #TAUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and 8 the central']","['In syntactic ( lexical-functional grammar [ LFG ] [ Kaplan and Bresnan ; Bresnan ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #TAUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and the central']","['In modern syntactic theories ( exical-functional grammar [ LFG ] [ Kaplan and Bres ; Bres ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #TAUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ #TAUTHOR_TAG ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0
"['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #TAUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'L that subcategorization requirements are stated at thestructure level, in functional rather than phal terms.', 'This because of the assumption that abstract grammatical functions are primitive concepts as opposed to of position.', '']","['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #TAUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'that subcategorization requirements are stated at the level, in functional rather than phrasal terms.', 'This because of the assumption that abstract grammatical functions are primitive concepts as opposed to of position.', '']","['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #TAUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'L that subcategorization requirements are best stated at the f-structure level, in functional rather than phal terms.', 'This because of the assumption that abstract grammatical functions are primitive concepts as opposed to of phrase structural position.', '']","['While many linguistic theories state subcategorization requirements in terms of phrase structure ( CFG categories ) , #TAUTHOR_TAG questions the viability and universality of such an approach because of the variety of ways in which grammatical functions may be realized at the language-specific constituent structure level .', 'LFG argues that subcategorization requirements are best stated at the f-structure level, in functional rather than phrasal terms.', 'This is because of the assumption that abstract grammatical functions are primitive concepts as opposed to derivatives of phrase structural position.', '']",0
"['approaches', '', '', '', '', '', 'adj', 'frames', ' #TAUTHOR_TAG attempts to improve on the approach of #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted', '', '', '']","['approaches', '', '', '', '', '', '', 'frames.', ' #TAUTHOR_TAG attempts to improve on the approach of #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted', '', '', '']","['general approaches', '', '', '', '', '', 'adj', '', ' #TAUTHOR_TAG attempts to improve on the approach of #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', 'The extracted', '', '', '']","['', '', '', '', '', '', '', '', ' #TAUTHOR_TAG attempts to improve on the approach of #AUTHOR_TAG by passing raw text through a stochastic tagger and a finite-state parser ( which includes a set of simple rules for subcategorization frame recognition ) in order to extract verbs and the constituents with which they co-occur .', 'He assumes 19 different subcategorization frame definitions, and the extracted frames include details of specific prepositions.', '', '', '', '']",0
"['Applying an absolute threshold of five occurrences, we still generate 162 frame types from Penn-II and 221 from Penn-III.', ' #TAUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']","['Applying an absolute threshold of five occurrences, we still generate 162 frame types from Penn-II and 221 from Penn-III.', ' #TAUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']","['Applying an absolute threshold of five occurrences, we still generate 162 frame types from Penn-II and 221 from Penn-III.', ' #TAUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']","['Applying an absolute threshold of five occurrences, we still generate 162 frame types from Penn-II and 221 from Penn-III.', ' #TAUTHOR_TAG , by comparison , employ 163 distinct predefined frames .']",0
"['-', 'extr essentially to a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Unlike our approach , those of #TAUTHOR_TAG and Hockenmaier , Bierner , and #AUTHOR_TAG include a substantial initial correction and clean-up of the Penn-II trees .', ' #AUTHOR_TAG']","['', 'extraction essentially to a', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Unlike our approach , those of #TAUTHOR_TAG and Hockenmaier , Bierner , and #AUTHOR_TAG include a substantial initial correction and clean-up of the Penn-II trees .', ' #AUTHOR_TAG']","['-', 'essentially to', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Unlike our approach , those of #TAUTHOR_TAG and Hockenmaier , Bierner , and #AUTHOR_TAG include a substantial initial correction and clean-up of the Penn-II trees .', ' #AUTHOR_TAG']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Unlike our approach , those of #TAUTHOR_TAG and Hockenmaier , Bierner , and #AUTHOR_TAG include a substantial initial correction and clean-up of the Penn-II trees .', ' #AUTHOR_TAG']",1
"['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG #TAUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'our approach generalizes to CFG category-based approaches.', '', '', '', '', '', '', '', '']","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG #TAUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'our approach generalizes to CFG category-based approaches.', '', '', '', '', '', '', '', '']","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG #TAUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'our approach generalizes to CFG category-based approaches.', '', '', '', '', '', '', '', '']","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG #TAUTHOR_TAG ; Xia 1999 ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', '', '', '', '', '', '', '', '']",0
"['will divide-general approaches toization frame acquisition into two', '', '', '', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate']","['will divide more-general approaches to frame acquisition into two', '', '', '', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate']","['We will divide-general approaches toization frame acquisition into', '', '', '', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate']","['', '', '', '', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT ( Boguraev et al. 1987 ) dictionaries and adding around 30 frames found by manual inspection .', 'The frames incorporate']",0
"['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank #TAUTHOR_TAG and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'Thex']","['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank #TAUTHOR_TAG and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', 'The']","['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank #TAUTHOR_TAG and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', '']","['We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank #TAUTHOR_TAG and Penn Chinese Treebank (Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke, Lam, et al. 2004).', '']",5
['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure #TAUTHOR_TAG : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .'],['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure #TAUTHOR_TAG : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .'],['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure #TAUTHOR_TAG : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .'],['The subcategorization requirements expressed by semantic forms are enforced at f-structure level through completeness and coherence well-formedness conditions on f-structure #TAUTHOR_TAG : An f-structure is locally complete iff it contains all the governable grammatical functions that its predicate governs .'],0
"['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #TAUTHOR_TAG is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C', 'produced functional nodes', 'judge (']","['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #TAUTHOR_TAG is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '', 'produced functional nodes', 'judge']","['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #TAUTHOR_TAG is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C', 'is produced the nodes', 'the judge (']","['Lexical functional grammar ( Kaplan and Bresnan 1982 ; Bresnan 2001 ; #TAUTHOR_TAG is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '', '', '']",0
"['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #TAUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It positsally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C', 'produced functional', 'judge (']","['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #TAUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '', 'produced functional', 'judge']","['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #TAUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C', 'is produced', 'the judge (']","['Lexical functional grammar ( Kaplan and Bresnan 1982 ; #TAUTHOR_TAG ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '', '', '']",0
"['RED', '', '', '', 'verbUS object introduced', 'judge', 'According to #TAUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list grammat functions is), summar in Table1.']","['PRED', '', '', '', 'verb object introduced', 'judge', 'According to #TAUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list grammatical functions is summarized in Table 1.']","['', '', '', '', 'introduced', 'judge', 'According to #TAUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', 'This list grammatical functions is divided), summar in Table1.']","['', '', '', '', '', '', 'According to #TAUTHOR_TAG , LFG assumes the following universally available inventory of grammatical functions : SUBJ ( ect ) , OBJ ( ect ) , OBJe , COMP , XCOMP , OBL ( ique ) e , ADJ ( unct ) , XADJ .', 'OBJ θ and OBL θ represent families of grammatical functions indexed by their semantic role, represented by the theta subscript.', '']",0
"['', '', ', to date this is the largest number of verbs used in any of the evaluations of the systems for English described in .', ' #TAUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that ofte im #AUTHOR_TAG b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 201', 'We will refer to this work and the methods and results presented by im Wal']","['', '', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in 3.', ' #TAUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that of Schulte im #AUTHOR_TAG b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).', 'We will refer to this work and the methods and results presented by im Walde']","['', '', ', to date this is the largest number of verbs used in any of the evaluations of the systems for English described in', ' #TAUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that ofte im #AUTHOR_TAG b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).', 'We will refer to this work and the methods and results presented byte im Walde']","['', '', 'Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3.', ' #TAUTHOR_TAG evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88 % .', 'However, their evaluation does not examine the extracted subcatego- rization frames but rather the argument�adjunct distinctions posited by their sys- tem.', 'The largest lexical evaluation we know of is that of Schulte im #AUTHOR_TAG b) for German.', 'She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001).', '']",1
"['syntactic functionsCOMP refer to clausal complements with predicate control as in 2. neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause #TAUTHOR_TAG .', 'extract', '']","['syntactic functions XCOMP refer to clausal complements with predicate control as in 2. neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause #TAUTHOR_TAG .', 'extract', '']","['The syntactic functions COMPCOMP refer to clausal complements with different predicate control patterns as in Section 2. neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause #TAUTHOR_TAG .', 'extract', '']","['The syntactic functions COMP and XCOMP refer to clausal complements with different predicate control patterns as described in Section 2. However, as it stands, neither of these functions betrays anything about the syntactic nature of the constructs in question.', 'Many lexicons , both automatically acquired and manually created , are more fine grained in their approaches to subcategorized clausal arguments , differentiating , for example , between a that-clause and a to + infinitive clause #TAUTHOR_TAG .', '', '']",0
"['', 'x to ofmar', 'number of related approaches extrx.', ""The extraction procedure utilizes a head percolation table as introduced by #TAUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a of lex anch elementary', '', '', '', '', '', '', '', '', '', '', '', '', '']","['', 'to of', 'number of related approaches extraction', ""The extraction procedure utilizes a head percolation table as introduced by #TAUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a of lexically anchored elementary', '', '', '', '', '', '', '', '', '', '', '', '', '']","['', 'to of', 'a number of related approaches.', ""The extraction procedure utilizes a head percolation table as introduced by #TAUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", 'This results in the construction of a set of lex anch elementary trees', '', '', '', '', '', '', '', '', '', '', '', '', '']","['', '', '', ""The extraction procedure utilizes a head percolation table as introduced by #TAUTHOR_TAG in combination with a variation of Collins 's ( 1997 ) approach to the differentiation between complement and adjunct ."", '', '', '', '', '', '', '', '', '', '', '', '', '', '']",0
"['approaches toization frame acquisition two', '', '', '', '', '', '', '', 'och a finite-state constituents', 'frame extracted frames detailspos', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TAUTHOR_TAG .', '', '', '']","['approaches to frame acquisition two', '', '', '', '', '', '', '', 'a finite-state constituents', 'frame extracted frames details', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TAUTHOR_TAG .', '', '', '']","['general approaches toization frame acquisition', '', '', '', '', '', '', '', 'a finite-state parser the constituents', 'the extracted frames details', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TAUTHOR_TAG .', '', '', '']","['', '', '', '', '', '', '', '', '', '', 'The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory ( BHT ) , following #TAUTHOR_TAG .', '', '', '']",0
"['We applied lexical-redundancy rules #TAUTHOR_TAG to automatically convert the active COMLEX frames to their passive counterparts:lique and objects become', 'resulting precision (7 to .']","['We applied lexical-redundancy rules #TAUTHOR_TAG to automatically convert the active COMLEX frames to their passive counterparts: oblique and objects become', 'resulting precision (from to']","['We applied lexical-redundancy rules #TAUTHOR_TAG to automatically convert the active COMLEX frames to their passive counterparts: and direct objects become', 'The resulting precision ( to .']","['We applied lexical-redundancy rules #TAUTHOR_TAG to automatically convert the active COMLEX frames to their passive counterparts: For example, subjects are demoted to optional by oblique agents, and direct objects become subjects.', '']",5
"['to ensure the of the forms our we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #TAUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']","['to ensure the of the forms our we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #TAUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']","['to ensure the quality of the semantic forms our we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #TAUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']","['In order to ensure the quality of the semantic forms extracted by our method, we must first ensure the quality of the f-structure annotations.', 'The results of two different evaluations of the automatically generated f-structures are presented in Table 2.', 'Both use the evaluation software and triple encoding presented in #TAUTHOR_TAG .', 'The first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures']",5
"['', '', '', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank #TAUTHOR_TAG , a set of 700 randomly selected sentences from Section23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature feature geometry', 'all these differences are captured automatic conversion software', '', '', '']","['', '', '', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank #TAUTHOR_TAG , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, feature geometry.', 'all, these differences are captured automatic conversion software.', '', '', '']","['', '', '', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank #TAUTHOR_TAG , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, feature geometry.', 'these differences are captured automatic conversion software.', '', '', '']","['', '', '', 'More recently , Burke , Cahill , et al. ( 2004a ) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank #TAUTHOR_TAG , a set of 700 randomly selected sentences from Section 23 which have been parsed , converted to dependency format , and manually corrected and extended by human validators .', 'They report precision of over 88.5% and recall of over 86% (Table 2).', 'The PARC 700 Dependency Bank differs substantially from both the DCU 105 f-structure bank and the automatically generated f-structures in regard to the style of linguistic analysis, feature nomenclature, and feature geometry.', 'Some, but not all, of these differences are captured by automatic conversion software.', '', '', '']",0
"['', 'definedur are used to automatically annotate each tree in the treebank with specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TAUTHOR_TAG and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct errors in', '']","['', 'defined are used to automatically annotate each tree in the treebank with specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TAUTHOR_TAG and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct errors in', '']","['', 'definedur are used to automatically annotate each tree in the treebank with specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TAUTHOR_TAG and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in', '']","['', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #TAUTHOR_TAG and #AUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', '']",5
"['acticfunctional Kapresnan Dal head-driven phrase structure and , tree-ing gram [AG ] [ Joshi 1 , and combinatory categorial grammar [ CCG ] [ #TAUTHOR_TAG ] ) , thexicon is the central for much morphological , syntactic , and semantic information']","['syntactic Kaplan Bresnan head-driven phrase structure and , tree-adjoining grammar [ TAG ] [ Joshi , and combinatory categorial grammar [ CCG ] [ #TAUTHOR_TAG ] ) , the lexicon is the central for much morphological , syntactic , and semantic information']","['functionres Dal head- and , tree-ing grammar [ TAG ] [ Joshi 1988 , and combinatory categorial grammar [ CCG ] [ #TAUTHOR_TAG ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",[''],0
"['', '', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #TAUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', '']","['', '', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #TAUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', '']","['', '', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #TAUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', '']","['', '', 'Most of the early work on automatic f-structure annotation ( e.g. , van Genabith , Way , and Sadler 1999 ; #TAUTHOR_TAG ; Sadler , van Genabith , and Way 2000 ) was applied only to small data sets ( fewer than 200 sentences ) and was largely proof of concept .', '']",0
"['pus sixpositional are treated as adjuncts', '', '', '', '', '', '', '', '', '', 'induced', 'Recent work by #TAUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection .', '', '']","['sixpositional phrases are treated as adjuncts', '', '', '', '', '', '', '', '', '', 'induced', '. Recent work by #TAUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . #AUTHOR_TAG', '', '']","['pus to sixpositional are treated as adjuncts', '', '', '', '', '', '', '', '', '', 'induced frames', 'Recent work by #TAUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . #AUTHOR_TAG', '', '']","['', '', '', '', '', '', '', '', '', '', '', '. Recent work by #TAUTHOR_TAG on the filtering phase of this approach uses linguistic verb classes ( based on Levin [ 1993 ] ) for obtaining more accurate back-off estimates for hypothesis selection . #AUTHOR_TAG use a handwritten head-lexicalized, context', '', '']",0
"['The rate of accession may also be represented graphically.', ' #AUTHOR_TAG and #TAUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition ofxical material from the data displayed a similar prop', 'Figurection sem CF combined', '', '']","['The rate of accession may also be represented graphically.', ' #AUTHOR_TAG and #TAUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the data displayed a similar', 'Figure induction semantic CFG combined).', '', '']","['The rate of accession may also be represented graphically.', ' #AUTHOR_TAG and #TAUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition ofxical material from the same data displayed a similar propensity.', 'ction sem CF', '', '']","['The rate of accession may also be represented graphically.', ' #AUTHOR_TAG and #TAUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were interested in discovering whether the acquisition of lexical material from the same data displayed a similar propensity.', '', '', '']",0
"['describe a methodology for acquiring an English HPS the-', 'ually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and #TAUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and an H to', '']","['describe a methodology for acquiring an English HPSG the', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and #TAUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and an to', '']","['describe a methodology for acquiring an English HPSG', 'ually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and #TAUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and to', '']","[' #AUTHOR_TAG and #AUTHOR_TAG describe a methodology for acquiring an English HPSG from the Penn-II Treebank.', 'Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees : Head/argument/modifier distinctions are made for each node in the tree based on #AUTHOR_TAG and #TAUTHOR_TAG ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the tree- bank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category.', '']",5
"['', ' #TAUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arg were mapped to syntactic functions.', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['', ' #TAUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were mapped to syntactic functions.', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['', ' #TAUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arg were then mapped to traditional syntactic functions.', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['', ' #TAUTHOR_TAG describe a simple tool which uses fine-grained rules to identify the arguments of verb occurrences in the Penn-II Treebank .', 'This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank.', 'Each of these sequences was categorized as a modifier or argument.', 'Arguments were then mapped to traditional syntactic functions.', '', '', '', '', '', '', '', '', '', '', '', '', '', '']",0
"['', '', '', '', '', '', '', '', '', '', '', '', '', 'verbs or more).', ' #TAUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', '', '', '', '']","['', '', '', '', '', '', '', '', '', '', '', '', '', 'verbs or more).', ' #TAUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', '', '', '', '']","['', '', '', '', '', '', '', '', '', '', '', '', '', '4 verbs or more).', ' #TAUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', '', '', '', '']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG present preliminary work on the automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank ( Simov , Popova , and Osenova 2002 ) .', '', '', '', '']",0
"['', '', '', '', '', '', ' #TAUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and #AUTHOR_TAG .', 'the elementary are read in a.', 'any invalid produced as a of annotation in the tree are out using linguistic', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', ' #TAUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and #AUTHOR_TAG .', 'the elementary are read in a manner.', 'any invalid produced as a of annotation in the treebank are out using linguistic', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', ' #TAUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and #AUTHOR_TAG .', 'Then the elementary trees are read off in', 'produced as a result of annotation errors in the treebank are filtered out', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', ' #TAUTHOR_TAG also presents a similar method for the extraction of a TAG from the Penn Treebank .', 'The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and #AUTHOR_TAG .', 'Then the elementary trees are read off in a quite straightforward manner.', 'Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics.', '', '', '', '', '', '', '', '']",0
"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TAUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CC [ Ad and  central']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TAUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG [ Ades and central']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TAUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ #TAUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0
"['', '', 'science fiction, mystery and.', 'It has been shown #TAUTHOR_TAG that the subcategorization tendencies of verbs vary across linguistic domains .', 'le', '', '', '', '', 'functional -)']","['', '', 'science fiction, mystery and humor.', 'It has been shown #TAUTHOR_TAG that the subcategorization tendencies of verbs vary across linguistic domains .', '', '', '', '', '', 'functional -TMP']","['', '', 'fiction, mystery and.', 'It has been shown #TAUTHOR_TAG that the subcategorization tendencies of verbs vary across linguistic domains .', '', '', '', '', '', '-)']","['', '', '', 'It has been shown #TAUTHOR_TAG that the subcategorization tendencies of verbs vary across linguistic domains .', '', '', '', '', '', '']",4
"['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #TAUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'our approach to CFG category-based approaches', '', '', '', '', '', '', '', '']","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #TAUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'our approach to CFG category-based approaches.', '', '', '', '', '', '', '', '']","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #TAUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'our approach to CFG category-based approaches.', '', '', '', '', '', '', '', '']","['Aside from the extraction of theory-neutral subcategorization lexicons , there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG , CCG , and HPSG ( Chen and Vijay-Shanker 2000 ; #TAUTHOR_TAG ; Hockenmaier , Bierner , and Baldridge 2004 ; Nakanishi , Miyao , and Tsujii 2004 ) .', 'In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems).', 'However, our approach also generalizes to CFG category-based approaches.', '', '', '', '', '', '', '', '']",0
"['will divide-general approaches toization frame acquisition into two', '', '', '', '', '', '', '', '', '', '', '', 'pre 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT #TAUTHOR_TAG dictionaries and adding around 30 frames found by manual inspection .', 'The frames', '']","['will divide more-general approaches to frame acquisition into two', '', '', '', '', '', '', '', '', '', '', '', 'predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT #TAUTHOR_TAG dictionaries and adding around 30 frames found by manual inspection .', 'The frames', '']","['We will divide-general approaches toization frame acquisition into', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT #TAUTHOR_TAG dictionaries and adding around 30 frames found by manual inspection .', 'The frames', '']","['', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG predefine 163 verbal subcategorization frames , obtained by manually merging the classes exemplified in the COMLEX ( MacLeod , Grishman , and Meyers 1994 ) and ANLT #TAUTHOR_TAG dictionaries and adding around 30 frames found by manual inspection .', '', '']",0
"['We utilize the automatic annotation algorithm of #AUTHOR_TAG and Cahill, Mc #AUTHOR_TAG to derive a version of Penn-II in which node is annot with LFG functional annotations in the form of-value structure.', '', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #TAUTHOR_TAG is used .', 'head is annotated with the LG equation ↑↓', '', '', '']","['We utilize the automatic annotation algorithm of #AUTHOR_TAG and Cahill, Mc #AUTHOR_TAG to derive a version of Penn-II in which node is annotated with LFG functional annotations in the form of attribute-value structure equations.', '', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #TAUTHOR_TAG is used .', 'head is annotated with the LFG equation ↑=↓.', '', '', '']","['We utilize the automatic annotation algorithm of #AUTHOR_TAG and Cahill, Mc #AUTHOR_TAG to derive a version of Penn-II in which each node is annotated with LFG functional annotations in the form of-value structure equations.', '', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #TAUTHOR_TAG is used .', 'The head is annotated with the LFG equation ↑=↓.', '', '', '']","['We utilize the automatic annotation algorithm of #AUTHOR_TAG and Cahill, Mc #AUTHOR_TAG to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations.', '', 'The annotation procedure is dependent on locating the head daughter , for which an amended version of #TAUTHOR_TAG is used .', 'The head is annotated with the LFG equation ↑=↓.', '', '', '']",5
"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #TAUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combatory categ gram']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #TAUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #TAUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and comb']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; #TAUTHOR_TAG ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0
"['', '', '', 'more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank #TAUTHOR_TAG , containing more than 1,000,000 words and 49,000 sentences .']","['', '', '', 'more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank #TAUTHOR_TAG , containing more than 1,000,000 words and 49,000 sentences .']","['', '', '', 'more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank #TAUTHOR_TAG , containing more than 1,000,000 words and 49,000 sentences .']","['', '', '', 'However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank #TAUTHOR_TAG , containing more than 1,000,000 words and 49,000 sentences .']",0
"['In modern syntactic theories ( e.. ,xical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #TAUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CC central']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #TAUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG central']","['In modern syntactic theories ( e.. ,xical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #TAUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ #TAUTHOR_TAG ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0
"['to the of our extracted sem we also examined at induced', 'This of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and #AUTHOR_TAG , #TAUTHOR_TAG , and Miyao , Ninomiya , and #AUTHOR_TAG , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lex', '', '', '', '', '', '']","['to the of our extracted semantic we also examined at induced.', 'This of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and #AUTHOR_TAG , #TAUTHOR_TAG , and Miyao , Ninomiya , and #AUTHOR_TAG , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon', '', '', '', '', '', '']","['to of our we also examined at induced', 'This of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and #AUTHOR_TAG , #TAUTHOR_TAG , and Miyao , Ninomiya , and #AUTHOR_TAG , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon', '', '', '', '', '', '']","['', 'This can be expressed as a measure of the coverage of the induced lexicon on new data.', 'Following Hockenmaier , Bierner , and #AUTHOR_TAG , #TAUTHOR_TAG , and Miyao , Ninomiya , and #AUTHOR_TAG , we extract a reference lexicon from Sections 02 -- 21 of the WSJ .', 'We then compare this to a test lexicon from Section 23.', '', '', '', '', '', '']",5
"['will divide-general approaches to subcization frame acquisition into two groups', '', ' #TAUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'sixategorization verb', 'pos treated as adj', '', '', '', '', '', '', '']","['will divide more-general approaches to subcategorization frame acquisition into two groups:', '', ' #TAUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', 'six subcategorization verb', 'treated as', '', '', '', '', '', '', '']","['We will divide-general approaches to subcization frame acquisition into two groups:', '', ' #TAUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '', 'are treated as adj', '', '', '', '', '', '', '']","['', '', ' #TAUTHOR_TAG relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames .', 'The frames do not include details of specific prepositions.', 'Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames.', '', '', '', '', '', '', '', '', '']",0
"['Lexical functional grammar #TAUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It positsally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C', 'is produced from functionalnotations on the nodes', 'judge']","['Lexical functional grammar #TAUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '', 'is produced from functional annotations on the nodes', 'judge']","['Lexical functional grammar #TAUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', 'C', 'is produced from functional annotations on the nodes', 'the judge']","['Lexical functional grammar #TAUTHOR_TAG ; Bresnan 2001 ; Dalrymple 2001 ) is a member of the family of constraint-based grammars .', 'It posits minimally two levels of syntactic representation: 2 c(onstituent)-structure encodes details of surface syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic information about predicate-argument-modifier relations and certain morphosyntactic properties such as tense, aspect, and case.', '', '', '']",0
"['', '', '', '', '', '', '', '', ' #TAUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame', 'elihood t', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs ( or', '', '', '', '', '']","['', '', '', '', '', '', '', '', ' #TAUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', '', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs or', '', '', '', '', '']","['', '', '', '', '', '', '', '', ' #TAUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', 't', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs ( or', '', '', '', '', '']","['', '', '', '', '', '', '', '', ' #TAUTHOR_TAG present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank ( Hajic', 'Czech is a language with a freer word order than English and so configurational information cannot be relied upon.', 'In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame.', 'Finding subcategorization frames involves filtering adjuncts from the observed frame.', '', 'The system learns 137 subcategorization frames from 19,126 sentences for 914 verbs (those which occurred five times or more).', '', '', '', '', '']",0
"['', '', '', '', '', '', 'extr Treebank', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and #TAUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', 'extraction Treebank.', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and #TAUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', '', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and #TAUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', '', 'The extraction procedure consists of three steps : First , the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of #AUTHOR_TAG and #TAUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '']",0
"['order to capture CF- categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn- Treebanks', '', '', '', '', 'details', ' #TAUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', '']","['order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks.', '', '', '', '', 'details.', ' #TAUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', '']","['order to capture CF-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks.', '', '', '', '', '', ' #TAUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', '']","['In order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks.', '', '', '', '', '', ' #TAUTHOR_TAG argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .', '']",4
"['banks, annot with Lstruct', '', '', '', '', '', '', '', '', '', '', '', '', '', 'analysis revealed interesting issues associated with using an external standard such as', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank #TAUTHOR_TAG .']","['Treebanks, annotated with', '', '', '', '', '', '', '', '', '', '', '', '', '', 'analysis revealed interesting issues associated with using an external standard such as', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank #TAUTHOR_TAG .']","['III Treebanks, annot with L', '', '', '', '', '', '', '', '', '', '', '', '', '', 'error analysis revealed some interesting issues associated with using an external standard such as', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank #TAUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX.', 'In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank #TAUTHOR_TAG .']",3
"['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented #TAUTHOR_TAG that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX using examples', '', '', '', '', '', '', '', '', '', '', '']","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented #TAUTHOR_TAG that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX using examples', '', '', '', '', '', '', '', '', '', '', '']","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented #TAUTHOR_TAG that subcategorization frames ( and their frequencies ) vary across domains .', 'We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX examples', '', '', '', '', '', '', '', '', '', '', '']","['Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data.', 'As noted above , it is well documented #TAUTHOR_TAG that subcategorization frames ( and their frequencies ) vary across domains .', '', '', '', '', '', '', '', '', '', '', '', '']",4
"['been carried extractionxical resources frombank', 'fullyxical with an ( CCHPS the extraction of axicon essentially amounts to the creation of a grammar', ' #TAUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'The extraction procedure utilizes a head percolation table as introduced by in combination with a variation of #AUTHOR_TAG approach to the differentiation between complement and adjunct.', 'a le', 'le', '', '', '', '', '', '', '', '', '', '', '', '']","['been carried extraction lexical resources from', 'fully lexicalized with an the extraction of a lexicon essentially amounts to the creation of a grammar.', ' #TAUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'The extraction procedure utilizes a head percolation table as introduced by in combination with a variation of #AUTHOR_TAG approach to the differentiation between complement and adjunct.', 'a', '', '', '', '', '', '', '', '', '', '', '', '', '']","['has been carried out the extractionspecific lexical resources from', 'are fullyxical with an invariant ( CCHPS the extraction of a lexicon essentially amounts to the creation of a grammar.', ' #TAUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'The extraction procedure utilizes a head percolation table as introduced by in combination with a variation of #AUTHOR_TAG approach to the differentiation between complement and adjunct.', 'a set le', '', '', '', '', '', '', '', '', '', '', '', '', '']","['', '', ' #TAUTHOR_TAG explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing .', 'The extraction procedure utilizes a head percolation table as introduced by #AUTHOR_TAG in combination with a variation of #AUTHOR_TAG approach to the differentiation between complement and adjunct.', '', '', '', '', '', '', '', '', '', '', '', '', '', '']",0
"['The rate of accession may also be represented graphically.', 'In #TAUTHOR_TAG and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were discoverxical a similar', '', '', '']","['The rate of accession may also be represented graphically.', 'In #TAUTHOR_TAG and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were discovering lexical a similar', '', '', '']","['The rate of accession may also be represented graphically.', 'In #TAUTHOR_TAG and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', 'We were discoverxical material', '', '', '']","['The rate of accession may also be represented graphically.', 'In #TAUTHOR_TAG and #AUTHOR_TAG , it was observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank .', '', '', '', '']",0
"['information is theization of entry the a pred take in form a', 'Lex includingategorization details,ally produced hand', 'bott', 'ization may genre (roll and Rooth 199', ' #TAUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']","['information is the of entry the a predicate take in form a', 'including subcategorization details, traditionally produced hand.', '', 'may genre (Carroll and Rooth 1998).', ' #TAUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']","['ical information is of an entry the arguments a predicate must take in form', 'Lex, includingategorization details, produced hand', 'bott', 'may vary genre (roll and Rooth 1998).', ' #TAUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']","['', 'Lexicons, including subcategorization details, were traditionally produced by hand.', '', '', ' #TAUTHOR_TAG argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .', 'Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.']",4
"['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #TAUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ central']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #TAUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ central']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #TAUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades']","['In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; #TAUTHOR_TAG ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ Ades and Steedman 1982 ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .']",0
"['approaches to frame', '', '', '', '', ' #TAUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'positional treated as adjuncts', '', '', '', '', '', '', '']","['approaches to frame', '', '', '', '', ' #TAUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'prepositional treated as adjuncts.', '', '', '', '', '', '', '']","['general approaches to frame acquisition', '', '', '', '', ' #TAUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', 'are treated as adjuncts', '', '', '', '', '', '', '']","['', '', '', '', '', ' #TAUTHOR_TAG run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .', '', '', '', '', '', '', '', '']",0
"['drawback to using an existing external gold standard such', '', '', '', '', '', '', 'As a generalization , #TAUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', '', '', '', '', '', '']","['drawback to using an existing external gold standard such', '', '', '', '', '', '', 'As a generalization , #TAUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', '', '', '', '', '', '']","['Another drawback to using an existing external gold standard such', '', '', '', '', '', '', 'As a generalization , #TAUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', '', '', '', '', '', '']","['', '', '', '', '', '', '', 'As a generalization , #TAUTHOR_TAG notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .', '', '', '', '', '', '']",0
"['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #TAUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', '', 'le', '', '', '', '']","['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #TAUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', '', '', '', '', '', '']","['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #TAUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', '', 'le', '', '', '', '']","['We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.', 'We propose a unified approach that solves both problems simultaneously.', 'A previous work along this line is #TAUTHOR_TAG , which is based on weighted finite-state transducers ( FSTs ) .', '', '', '', '', '', '']",1
"['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', ' #TAUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995).', '', '', '', '', '']","['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', ' #TAUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995).', '', '', '', '', '']","['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', ' #TAUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6', '', '', '', '', '']","['Common sense (as well as the Gricean maxims; Grice 1975) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication.', 'We are not aware of any empirical validation of this idea, but the fact that vague descriptions are frequent is fairly well documented.', ' #TAUTHOR_TAG , for example , discussed the transcripts of a dialogue between people who assemble a piece of garden furniture ( originally recorded by Candy Sidner ) .', 'They found that, while instructional texts tended to use numerical descriptions like the 3 1 4 "" bolt, human assemblers ""unless they were reading or discussing the written instructions, in all cases used relative modifiers, such as the long bolt"" (Dale and Reiter 1995). 6 Our own experiments (van Deemter 2004) point in the same direction.', '', '', '', '', '']",0
"['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" #TAUTHOR_TAG : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']","['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" #TAUTHOR_TAG : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']","['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" #TAUTHOR_TAG : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']","['Viewed in this way , gradable adjectives are an extreme example of the ""efficiency of language"" #TAUTHOR_TAG : Far from meaning something concrete like ""larger than 8 cm"" -- a concept that would have very limited applicability -- or even something more general like ""larger than the average N , ""a word like large is applicable across a wide range of different situations .']",1
"['Clearly , what it takes for the adjective to be applicable has not been cast in , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #TAUTHOR_TAG .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about standards employed (Kyburg and Morreau 2000;DeVault and 2004): (3),  cm and', '', '', '']","['Clearly , what it takes for the adjective to be applicable has not been cast in , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #TAUTHOR_TAG .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about standards employed (Kyburg and Morreau 2000;DeVault and 2004): (3), cm and', '', '', '']","['Clearly , what it takes for the adjective to be applicable has not been cast in , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #TAUTHOR_TAG .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), 0 cm and', '', '', '']","['Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , #TAUTHOR_TAG .', 'The numeral (whether it is implicit, as in (3), or explicit) can be construed as allowing the reader to draw inferences about the standards employed (Kyburg and Morreau 2000;DeVault and Stone 2004): (3), for example, implies a standard that counts 10 cm as large and 8 cm as not large.', '', '', '']",0
"['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #TAUTHOR_TAG was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']","['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #TAUTHOR_TAG was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']","['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #TAUTHOR_TAG was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']","['Cases like this would be covered if the decision-theoretic property of Pareto optimality ( e.g. , #TAUTHOR_TAG was used as the sole criterion : Formally , an object r E C has a Pareto-optimal combination of Values V iff there is no other x E C such that']",0
"['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TAUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '', '', '', '', '', '', '']","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TAUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '', '', '', '', '', '', '']","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TAUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '', '', '', '', '', '', '']","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , #TAUTHOR_TAG ; Malouf 2000 ) .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '', '', '', '', '', '', '']",0
"['3 The degree of precision of the measurement #TAUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']","['3 The degree of precision of the measurement #TAUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']","['3 The degree of precision of the measurement #TAUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']","['3 The degree of precision of the measurement #TAUTHOR_TAG , Section 1.5 ) determines which objects can be described by the GRE algorithm , since it determines which objects count as having the same size .']",0
"['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #TAUTHOR_TAG .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '', '', '']","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #TAUTHOR_TAG .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '', '', '']","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #TAUTHOR_TAG .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '', '', '']","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( Stone and Webber 1998 ; #TAUTHOR_TAG .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '', '', '']",1
"['more', 'observ', 'a answer prohibit binary are and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. , #TAUTHOR_TAG .']","['more', '', 'a answer prohibit binary are and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. , #TAUTHOR_TAG .']","['more', 'observ', 'a negative answer would prohibit are and objects that are not, given that one can always construct objects x and y, one of which falls just below the divide while the other falls just above it.', 'This is the strongest version of the sorites paradox ( e.g. , #TAUTHOR_TAG .']","['', '', '', 'This is the strongest version of the sorites paradox ( e.g. , #TAUTHOR_TAG .']",0
"['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #TAUTHOR_TAG .', 'this tract', '', '', 'the we on the2 4, and.']","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #TAUTHOR_TAG .', 'this, tractability', '', '', 'the we on the 4, and']","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #TAUTHOR_TAG .', '', '', '', 'we focus on the2 4, and.']","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , #TAUTHOR_TAG .', '', '', '', '']",0
"['', '', 'We shall focus on more challenging case where output less, as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people #TAUTHOR_TAG , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', '', '', '', '', '', '', '']","['', '', 'We shall focus on more challenging case where output less input, as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people #TAUTHOR_TAG , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', '', '', '', '', '', '', '']","['', '', 'We shall focus on more challenging case where the output is less as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people #TAUTHOR_TAG , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', '', '', '', '', '', '', '']","['', '', 'We shall focus on the more challenging case where the output of the generator is less precise than the input, as is the case in FOG and DYD.', 'This can be a hazardous affair , since vague expressions tend to be interpreted in different ways by different people #TAUTHOR_TAG , sometimes in stark contrast with the intention of the speaker/writer ( Berry , Knapp , and Raynor 2002 ) .', '', '', '', '', '', '', '']",0
"['compar properties at the, while the fixed', 'the KB contains () height > < x,c) width >, and () width < x.', 'Which of these should come first?', ' #TAUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', 'refer tallended', '', '', '', '', '']","['comparative properties at the order, while the fixed', 'the KB contains (a) height > < x, (c) width > x, and (d) width < x.', 'Which of these should come first?', ' #TAUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', 'referent tallest tended', '', '', '', '', '']","['comparative properties at the bottom while the order fixed', 'the KB contains () height > x, < x,c) width > x, and () width < x.', 'Which of these should come first?', ' #TAUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', 'ended', '', '', '', '', '']","['', '', 'Which of these should come first?', ' #TAUTHOR_TAG ; also reported in Levelt 1989 ) show that greater differences are most likely to be chosen , presumably because they are more striking .', '', '', '', '', '', '']",0
"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #TAUTHOR_TAG , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '(W redu', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #TAUTHOR_TAG , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '(Wildly', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #TAUTHOR_TAG , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '(W', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; Pechmann 1989 ; #TAUTHOR_TAG , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '', '', '', '', '', '', '', '', '']",0
"[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TAUTHOR_TAG .', 'IA Pl refer to target']","[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TAUTHOR_TAG .', 'IA Plur refer to target']","[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TAUTHOR_TAG .', 'IA Plur can refer to a target']","[""FindBestValue selects the 'best value' from among the Values of a given Attribute, assuming that these are linearly ordered in terms of specificity."", 'The function selects the Value that removes most distractors , but in case of a tie , the least specific contestant is chosen , as long as it is not less specific than the basic-level Value ( i.e. , the most commonly occurring and psychologically most fundamental level , #TAUTHOR_TAG .', '']",0
"['', 'will will', 'A New Perspective on Salience.', "" #TAUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'effect this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', '', '', '', '', '', '', '']","['', 'will will', 'A New Perspective on Salience.', "" #TAUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', '', '', '', '', '', '', '']","['', 'will allow', '1 A New Perspective on Salience.', "" #TAUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'effect this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', '', '', '', '', '', '', '']","['', '', '9.4.1 A New Perspective on Salience.', "" #TAUTHOR_TAG have argued that Dale and Reiter 's ( 1995 ) dichotomy between salient and nonsalient objects ( where the objects in the domain are the salient ones ) should be replaced by an account that takes degrees of salience into account : No object can be too unsalient to be referred to , as long as the right properties are available ."", 'In effect, this proposal (which measured salience numerically) analyzes the black mouse as denoting the unique most salient object in the domain that is both black and a mouse.', '', '', '', '', '', '', '']",0
"['Gradability is especially widespread in adjectives.', 'British Cor), ten most adjectives (', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month #TAUTHOR_TAG .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when hats of different sizes are']","['Gradability is especially widespread in adjectives.', 'British Corpus ten most adjectives', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month #TAUTHOR_TAG .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when hats of different sizes are']","['Gradability is especially widespread in adjectives.', '), the ten most frequent adjectives (', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month #TAUTHOR_TAG .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are']","['Gradability is especially widespread in adjectives.', '', 'Children use vague adjectives among their first dozens of words ( Peccei 1994 ) and understand some of their intricacies as early as their 24th month #TAUTHOR_TAG .', 'These intricacies include what Ebeling and Gelman call perceptual context dependence, as when a set of objects is perceptually available and the adjective is applied to an element or subset of the set (e.g., Is this hat big or is it little?, when two hats of different sizes are visible).']",0
"['for the adject to be applicable has not been cast open fiat decide 8 enough, or (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed #TAUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '', '', '']","['for the adjective to be applicable has not been cast open fiat: decide 8 enough, or (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed #TAUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '', '', '']","['for the adject to be applicable has not been cast open fiat may decide 8 cm enough, or (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed #TAUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '', '', '']","['Clearly, what it takes for the adjective to be applicable has not been cast in stone, but is open to fiat: the speaker may decide that 8 cm is enough, or the speaker may set the standards higher (cf., Kennedy 1999).', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed #TAUTHOR_TAG ; DeVault and Stone 2004 ) : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '', '', '']",0
"['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects #TAUTHOR_TAG , Krahmer and Theune 2002 ) .', 'we do this, consider tract', '', '', 'we on the2  and']","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects #TAUTHOR_TAG , Krahmer and Theune 2002 ) .', 'we do this, consider tractability', '', '', 'we on the and']","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects #TAUTHOR_TAG , Krahmer and Theune 2002 ) .', 'we do this, consider', '', '', 'we focus on the2  and']","['We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects #TAUTHOR_TAG , Krahmer and Theune 2002 ) .', '', '', '', '']",0
"['', '', '', '; also reported in #TAUTHOR_TAG show that greater differences are most likely to be chosen , presumably because they are more striking .', 'ights width the refer the tall and theended tall', '', '', '', '', '']","['', '', '', '; also reported in #TAUTHOR_TAG show that greater differences are most likely to be chosen , presumably because they are more striking .', 'heights the referent the tallest and the tended tall', '', '', '', '', '']","['', '', '', '; also reported in #TAUTHOR_TAG show that greater differences are most likely to be chosen , presumably because they are more striking .', 'width the referent both the tallest andended', '', '', '', '', '']","['', '', '', ' #AUTHOR_TAG ; also reported in #TAUTHOR_TAG show that greater differences are most likely to be chosen , presumably because they are more striking .', '', '', '', '', '', '']",0
"['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #TAUTHOR_TAG .', '', '']","['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #TAUTHOR_TAG .', '', '']","['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #TAUTHOR_TAG .', '', '']","['The account sketched in Section 4 was superimposed on an incremental GRE algorithm , partly because incrementality is well established in this area ( Appelt 1985 ; #TAUTHOR_TAG .', '', '']",0
"['Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.', 'degree we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following #TAUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as or form (large)', '', '']","['Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.', 'degree we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following #TAUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as or form (large)', '', '']","['Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.', 'we shall explore how referring expressions containing gradable adjectives can be produced by a Natural Language Generation (NLG) program.', 'Following #TAUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as or base form (large)', '', '']","['Vague or gradable expressions pose problems to models of language, caused by their context dependence, and by the fact that they are applicable to different degrees.', '', 'Following #TAUTHOR_TAG , such expressions will be called vague descriptions even though , as we shall see , the vagueness of the adjective does not extend to the description as a whole .', 'It will be useful to generalize over different forms of the adjective, covering the superlative form (e.g., largest) and the comparative form (larger), as well as the positive or base form (large) of the adjective.', '', '']",5
"['', '', '', '', '', '', '', 'A similar problem is discussed in the psycholinguistics of interpretation #TAUTHOR_TAG : Interpretation is widely assumed to proceed incrementally , but vague strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known', '', '']","['', '', '', '', '', '', '', 'A similar problem is discussed in the psycholinguistics of interpretation #TAUTHOR_TAG : Interpretation is widely assumed to proceed incrementally , but vague strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known', '', '']","['', '', '', '', '', '', '', 'A similar problem is discussed in the psycholinguistics of interpretation #TAUTHOR_TAG : Interpretation is widely assumed to proceed incrementally , but vague descriptions strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .', '', '']","['', '', '', '', '', '', '', 'A similar problem is discussed in the psycholinguistics of interpretation #TAUTHOR_TAG : Interpretation is widely assumed to proceed incrementally , but vague descriptions resist strict incrementality , since an adjective in a vague description can only be fully interpreted when its comparison set is known .', '', '']",1
"['', 'the property added to the is context', 'the set with a form size ( involvesing the distractors as size may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #TAUTHOR_TAG ] Chapter8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since is the for all referring']","['', 'the property added to the is context', 'the set with a form size ( involves sorting the distractors as size may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #TAUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since is the for all referring']","['', 'the property added to the description is', 'with the form size ( involves sorting the distractors as size may amount to O ( n2d ) or O ( nd log nd ) calculations ( depending on the sorting algorithm : cfXXX [ #TAUTHOR_TAG ] Chapter 8 ) .', 'Once again, the most time-consuming part of the calculation can be performed off-line, since it is the for all referring expressions.']","['', '', '', 'Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions.']",0
"['Common sense ( as well as the Gricean maxims ; #TAUTHOR_TAG suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', 'the fact that vague descriptions are frequent is fairly well documented.', '', '', '', '', '', '', '']","['Common sense ( as well as the Gricean maxims ; #TAUTHOR_TAG suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', 'the fact that vague descriptions are frequent is fairly well documented.', '', '', '', '', '', '', '']","['Common sense ( as well as the Gricean maxims ; #TAUTHOR_TAG suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', 'the fact that vague descriptions are frequent is fairly well documented.', '', '', '', '', '', '', '']","['Common sense ( as well as the Gricean maxims ; #TAUTHOR_TAG suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .', '', '', '', '', '', '', '', '']",0
"['ity the', 'shapes', 'there exists a formula for mapping three into (g height the dimension (over-size), and the of 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence #TAUTHOR_TAG .']","['the', 'shapes.', 'there exists a formula for mapping three into height) the dimension (overall-size), and the of 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence #TAUTHOR_TAG .']","['ity', '', 'there exists a formula for mapping three dimensions into (g one dimension (over-size), and the algorithm of Section 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence #TAUTHOR_TAG .']","['', '', 'If there exists a formula for mapping three dimensions into one (e.g., length × width × height) then the result is one dimension (overall-size), and the algorithm of Section 4 can be applied verbatim.', 'But if big is applied to a person then it is far from clear that there is one canonical formula for mapping the different dimensions of your body into one overall dimension, and this complicates the situation.', 'Similar things hold for multifaceted properties like intelligence #TAUTHOR_TAG .']",0
"['of the ""global', 'For some adjectives , including the ones that #TAUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989),']","['of the ""global""', 'For some adjectives , including the ones that #TAUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989),']","['of the ""global""', 'For some adjectives , including the ones that #TAUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989),']","['', 'For some adjectives , including the ones that #TAUTHOR_TAG called evaluative ( as opposed to dimensional ) , this is clearly inadequate .', 'He argued that evaluative adjectives (such as beautiful and its antonym ugly; smart and its antonym stupid, etc.) can be recognized by the way in which they compare with antonyms.', 'For example (after Bierwisch 1989),']",0
"['Minimality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles #TAUTHOR_TAG .']","['Minimality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles #TAUTHOR_TAG .']","['Minimality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles #TAUTHOR_TAG .']","['Minimality.', 'Unless Small Gaps and Dichotomy forbid it, we expected that preference should be given to the base form.', 'In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles #TAUTHOR_TAG .']",0
"['If the usefulness ofG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available #TAUTHOR_TAG .', 'hook', '', '', '']","['If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available #TAUTHOR_TAG .', 'hooked', '', '', '']","['If the usefulness ofG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available #TAUTHOR_TAG .', '', '', '', '']","['If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail.', 'In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available #TAUTHOR_TAG .', '', '', '', '']",4
"['description (., Daleock', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm #TAUTHOR_TAG , Section 8.6.2 )', '']","['description (cf., Dale', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm #TAUTHOR_TAG , Section 8.6.2 )', '']","['(., Dale', 'CD for this type of descriptions along the lines of Section 4 is not difficult once relational descriptions are integrated with a standard GRE algorithm #TAUTHOR_TAG , Section 8.6.2 )', '']","['', '', '']",0
"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #TAUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates', '(', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #TAUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates', '', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #TAUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '(', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; Levelt 1989 ; #TAUTHOR_TAG ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '', '', '', '', '', '', '', '', '']",0
"['to complex Boolean involving neg andunction ( 2 to marked', 'For the have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TAUTHOR_TAG .', 'on to refer', '']","['to complex Boolean involving negation and (van to', 'For the have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TAUTHOR_TAG .', 'on to referent', '']","['to complex Boolean descriptions involving neg andunction ( to marked', 'For will have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TAUTHOR_TAG .', 'on to', '']","['', 'For example, the generator will have to decide whether to say the patients that are old or the patients that are not young.', '9.3 Multidimensionality 9.3.1 Combinations of Adjectives.', 'When objects are compared in terms of several dimensions , these dimensions can be weighed in different ways ( e.g. , #TAUTHOR_TAG .', '', '']",0
"['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #TAUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it', '', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #TAUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it', '', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #TAUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which', '', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production ( Hermann and Deutsch 1976 ; #TAUTHOR_TAG ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '', '', '', '', '', '', '', '', '']",0
"['', 'Fritz to be the stupid man, it is not enough for him to be the least male in the local context; he also has to be a specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TAUTHOR_TAG 1999, discussed in Section 7.2).', '']","['', 'Fritz to be the stupid man, it is not enough for him to be the least male in the local context; he also has to be a specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TAUTHOR_TAG 1999, discussed in Section 7.2).', '']","['', 'For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TAUTHOR_TAG 1999, discussed in Section 7.2).', '']","['', '(For Fritz to be the stupid man, it is not enough for him to be the least intelligent male in the local context; he also has to be a fairly stupid specimen in his own right.) If this is done, it is not evident that dimensional adjectives should be treated differently: If Hans�s and Fritz�s heights are 210 and 205 cm, respectively, then it seems questionable to describe Fritz as the short man, even if Hans is the only other man in the local context (but see #TAUTHOR_TAG 1999, discussed in Section 7.2).', '']",0
"['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TAUTHOR_TAG : The selected expression should also be felicitous .', 'observ', '', '']","['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TAUTHOR_TAG : The selected expression should also be felicitous .', '', '', '']","['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TAUTHOR_TAG : The selected expression should also be felicitous .', 'observ', '', '']","['NLG has to do more than select a distinguishing description ( i.e. , one that unambiguously denotes its referent ; #TAUTHOR_TAG : The selected expression should also be felicitous .', '', '', '']",0
"['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #TAUTHOR_TAG .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others', '', '', '', '', '', '', '']","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #TAUTHOR_TAG .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '', '', '', '', '', '', '']","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #TAUTHOR_TAG .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others', '', '', '', '', '', '', '']","['One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , Shaw and Hatzivassiloglou 1999 ; #TAUTHOR_TAG .', ""Work in this area is often based on assigning adjectives to a small number of categories (e.g., Precentral, Central, Postcentral, and Prehead), which predict adjectives' relative position."", 'Interestingly, vague properties tend to be realized before others.', '', '', '', '', '', '', '']",0
"['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #TAUTHOR_TAG .', 'VAGUE uses both of these devices.']","['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #TAUTHOR_TAG .', 'VAGUE uses both of these devices.']","['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #TAUTHOR_TAG .', 'VAGUE uses both of these devices.']","['4 To turn this likelihood into a certainty , one can add a test at the end of the algorithm , which adds a type-related property if none is present yet ( cfXXX , #TAUTHOR_TAG .', 'VAGUE uses both of these devices.']",0
"[', is the only object that has a Pareto-optimal combination of Values, predicting correctly can be called the tall giraffe', '', '', 'alternative', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #TAUTHOR_TAG ; Thorisson 1994 , for other plans ) .']","['example, is the only object that has a Pareto-optimal combination of Values, predicting correctly can be called the tall giraffe.', '', '', 'alternative', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #TAUTHOR_TAG ; Thorisson 1994 , for other plans ) .']","[', b is the only object that has a Pareto-optimal combination of Values, predicting correctly can be called the tall fat giraffe.', '', '', '', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #TAUTHOR_TAG ; Thorisson 1994 , for other plans ) .']","['In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.', '', '', '', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors ( Nash 1950 ; cfXXX #TAUTHOR_TAG ; Thorisson 1994 , for other plans ) .']",0
"['for cast9', 'numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #TAUTHOR_TAG : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '', '', '']","['for cast', 'numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #TAUTHOR_TAG : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '', '', '']","['for been cast', 'numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #TAUTHOR_TAG : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '', '', '']","['', 'The numeral ( whether it is implicit , as in ( 3 ) , or explicit ) can be construed as allowing the reader to draw inferences about the standards employed ( Kyburg and Morreau 2000 ; #TAUTHOR_TAG : ( 3 ) , for example , implies a standard that counts 10 cm as large and 8 cm as not large .', '', '', '']",0
"['', '', '', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #TAUTHOR_TAG .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", '', '']","['', '', '', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #TAUTHOR_TAG .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", '', '']","['', '', '', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #TAUTHOR_TAG .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", '', '']","['', '', '', 'Each of these Values has equal status , so the notion of a basic-level Value can not play a role ( cfXXX , #TAUTHOR_TAG .', ""If we abstract away from the role of basic-level Values, then Dale and Reiter's FindBestValue chooses the most general Value that removes the maximal number of distractors, as we have seen."", '', '']",0
"['Some generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX , #TAUTHOR_TAG involving a gradable adjective , as in the dog in the large shed .', 'of of4 difficult relational are integrated with a standard GRE algorithm (Krahmer and Theune , Section', '', '', '']","['Some generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX , #TAUTHOR_TAG involving a gradable adjective , as in the dog in the large shed .', 'of of 4 difficult relational are integrated with a standard GRE algorithm (Krahmer and Theune Section', '', '', '']","['Some generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX , #TAUTHOR_TAG involving a gradable adjective , as in the dog in the large shed .', 'of of difficult once relational descriptions are integrated with a standard GRE algorithm (Krahmer and The', '', '', '']","['Some generalizations of our method are fairly straightforward.', 'For example , consider a relational description ( cfXXX , #TAUTHOR_TAG involving a gradable adjective , as in the dog in the large shed .', '', '', '', '']",0
"[', is the only object that has a Pareto-optimal combination of Values, predicting correctly b can be called the tall giraffe', '', '', '', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors #TAUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']","['example, is the only object that has a Pareto-optimal combination of Values, predicting correctly b can be called the tall giraffe.', '', '', '', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors #TAUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']","[', b is the only object that has a Pareto-optimal combination of Values, predicting correctly b can be called the tall fat giraffe.', '', '', '', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors #TAUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']","['In our example, b is the only object that has a Pareto-optimal combination of Values, predicting correctly that b can be called the tall fat giraffe.', '', '', '', 'The Nash arbitration plan , for example , would allow a doubly graded description whenever the product of the Values for the referent r exceeds that of all distractors #TAUTHOR_TAG ; cfXXX Gorniak and Roy 2003 ; Thorisson 1994 , for other plans ) .']",0
"['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #TAUTHOR_TAG .', 'An example of such an inference rule is the one that transforms a list of the form >0 cm into one of the', '']","['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #TAUTHOR_TAG .', 'An example of such an inference rule is the one that transforms a list of the form >10 cm into one of the', '']","['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #TAUTHOR_TAG .', 'An example of such an inference rule is the one that transforms a list of the form mouse, >10 cm into one of', '']","['The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , #TAUTHOR_TAG .', '', '']",0
"['systems produce gradable adjectives TheG weather-fore, for, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG not to have generic rules governing the of not: it does not compute the meaning of a term on the context, but fixed boundary values instead', 'A more flexible approach is used by #TAUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', '', '', '']","['systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG not to have generic rules governing the of notions: it does not compute the meaning of a term on the context, but fixed boundary values instead.', 'A more flexible approach is used by #TAUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', '', '', '']","['produce gradable adjectives The FOG weather-fore for, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG does not to have generic rules governing the use ofable notions: it does not compute the meaning of a vague term on the context, but fixed boundary values instead', 'A more flexible approach is used by #TAUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', '', '', '']","['Some NLG systems produce gradable adjectives. The FOG weather-forecast system, for example, uses numerical input (Rain[Tuesday] = 45 mm) to generate vague output (Heavy rain fell on Tuesday, Goldberg, Driedger, and Kitteridge 1994).', 'FOG does not appear to have generic rules governing the use of gradable notions: it does not compute the meaning of a vague term based on the context, but uses fixed boundary values instead.', 'A more flexible approach is used by #TAUTHOR_TAG , where users can specify boundary values for attributes like rainfall , specifying , for example , rain counts as moderate above 7 mm/h , as heavy above 20 mm/h , and so on .', '', '', '']",0
"['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead #TAUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '', '', '']","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead #TAUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '', '', '']","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead #TAUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '', '', '']","['Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead #TAUTHOR_TAG ; Krahmer and Theune 2002 ) .', 'We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult.', '', '', '']",1
"['While IA is generally thought to be consistent with findings on human language production #TAUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates', '', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production #TAUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates', '', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production #TAUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '', '', '', '', '', '', '', '', '']","['While IA is generally thought to be consistent with findings on human language production #TAUTHOR_TAG ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .', '', '', '', '', '', '', '', '', '']",0
"['', 'modeled in saturation', '', 'chair chairs refer', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension #TAUTHOR_TAG , pages 10 -- 12 ) .', '( G', '', '', '', '', '', '', '', '']","['', 'modeled in saturation,', '', 'chair, chairs, refer', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension #TAUTHOR_TAG , pages 10 -- 12 ) .', 'GRE', '', '', '', '', '', '', '', '']","['', 'is modeled in saturation', '', 'would refer', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension #TAUTHOR_TAG , pages 10 -- 12 ) .', '(', '', '', '', '', '', '', '', '']","['', '', '', '', 'A further complication is that different speakers can regard very different values as prototypical , making it difficult to assess which of two objects is greener even on one dimension #TAUTHOR_TAG , pages 10 -- 12 ) .', '', '', '', '', '', '', '', '', '']",0
"['', 'it is worth noting that the inequalities computed as step  the of4 might be psychologically more plausible, they are essentially no more than compar between.', ' #TAUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene contain three distractors: () a less tall object of the height a to ad', 'object', '', '']","['', 'it is worth noting that the inequalities computed as step 2 the of 4 might be psychologically more plausible, they are essentially no more than comparisons between objects.', ' #TAUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene contain three distractors: (1) a less tall object of the height a to', 'object', '', '']","['', 'it is worth noting that the inequalities computed as step 2 the algorithm of might be psychologically more plausible, they are essentially no more than compar between.', ' #TAUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', 'The relevant scene would contain three distractors: () a less tall object of the target height to', 'object', '', '']","['', '', ' #TAUTHOR_TAG asked subjects to identify the target of a vague description in a visual scene .', 'Consider the tall cup.', '', '', '', '']",0
"['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process #TAUTHOR_TAG .', '4 interpretation vague descriptions a', 'grad properties a low might cause the algorithm to underuse them to', '', '']","['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process #TAUTHOR_TAG .', '4.3 interpretation vague descriptions a', 'gradable properties a low might cause the algorithm to underuse them, to', '', '']","['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process #TAUTHOR_TAG .', 'interpretation vague descriptions', 'giving gradable properties a low ranking, we might cause the algorithm to underuse them to', '', '']","['It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process #TAUTHOR_TAG .', '', '', '', '']",1
"['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions #TAUTHOR_TAG ; also Section 8.1 of the present article ) .']","['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions #TAUTHOR_TAG ; also Section 8.1 of the present article ) .']","['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions #TAUTHOR_TAG ; also Section 8.1 of the present article ) .']","['2 The reader is asked to focus on any reasonable size measurement , for example , the maximal horizontal or vertical distance , or some combination of dimensions #TAUTHOR_TAG ; also Section 8.1 of the present article ) .']",0
"['treated regardless', '', '', '', '', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account #TAUTHOR_TAG ; see our Section 2 ) .""]","['treated regardless', '', '', '', '', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account #TAUTHOR_TAG ; see our Section 2 ) .""]","['are treated', '', '', '', '', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account #TAUTHOR_TAG ; see our Section 2 ) .""]","['', '', '', '', '', ""In other words , existing treatments of gradables in GRE fail to take the `` efficiency of language '' into account #TAUTHOR_TAG ; see our Section 2 ) .""]",0
"['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care #TAUTHOR_TAG :']","['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care #TAUTHOR_TAG :']","['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care #TAUTHOR_TAG :']","['The second facet is independent of the clinical task and pertains to the structure of a well-built clinical question.', 'The following four components have been identified as the key elements of a question related to patient care #TAUTHOR_TAG :']",0
"['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved #TAUTHOR_TAG .', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']","['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved #TAUTHOR_TAG .', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']","['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved #TAUTHOR_TAG .', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']","['Mean Average Precision (MAP) is the average of precision values after each relevant document is retrieved #TAUTHOR_TAG .', 'It is the most widely accepted single-value metric in information retrieval, and is seen to balance the need for both precision and recall.']",5
"['be sensitive to is not new', 'Mend have studiedSH associated basic clinical tasks therapyosisology', 'cit', '', 'Although originally developed as a tool to assist in query formulation , #TAUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'ICObased querying in information retriev', '']","['be sensitive to is not new.', 'have studied MeSH associated basic clinical tasks therapy,', 'citations', '', 'Although originally developed as a tool to assist in query formulation , #TAUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'PICO-based querying in information retrieval', '']","['should be sensitive to is not new', 'have studiedSH terms associated the four basic clinical tasks therapyology', 'cit', '', 'Although originally developed as a tool to assist in query formulation , #TAUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', 'ICObased querying in information retrieval', '']","['', '', '', '', 'Although originally developed as a tool to assist in query formulation , #TAUTHOR_TAG pointed out that PICO frames can be employed to structure IR results for improving precision .', '', '']",0
"['- is na Bayes class it.', 'We first identified the most informative unigrams and bigrams using the information gain measure #TAUTHOR_TAG , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', '', '', '']","['is naive Bayes it features.', 'We first identified the most informative unigrams and bigrams using the information gain measure #TAUTHOR_TAG , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', '', '', '']","['- is also a naive Bayes classifier, it operates.', 'We first identified the most informative unigrams and bigrams using the information gain measure #TAUTHOR_TAG , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', '', '', '']","['', 'We first identified the most informative unigrams and bigrams using the information gain measure #TAUTHOR_TAG , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .', '', '', '']",5
"['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #TAUTHOR_TAG ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the of evidence-based medicine.', 'doubt']","['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #TAUTHOR_TAG ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the of evidence-based medicine.', 'doubt']","['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #TAUTHOR_TAG ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.', 'no doubt']","['In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs.', ""The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; #TAUTHOR_TAG ."", 'Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine.', '']",1
"['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , Mc #AUTHOR_TAG describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #TAUTHOR_TAG .', '', '', '']","['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , Mc #AUTHOR_TAG describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #TAUTHOR_TAG .', '', '', '']","['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , Mc #AUTHOR_TAG describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #TAUTHOR_TAG .', '', '', '']","['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', 'For example , Mc #AUTHOR_TAG describe a machine learning approach to automatically label sentences as belonging to introduction , methods , results , or conclusion using structured abstracts as training data ( see also #TAUTHOR_TAG .', '', '', '']",0
"['most important phys', '', '', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization #TAUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems .']","['most important', '', '', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization #TAUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems .']","['most important characteristic', '', '', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization #TAUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems .']","['', '', '', 'We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization #TAUTHOR_TAG b ) , but these features are also beyond the capabilities of current summarization systems .']",1
"['The literature contains work on sentence-level classification ofEDLINE abstracts', '', ' #TAUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', '', '']","['The literature contains work on sentence-level classification of MEDLINE abstracts', '', ' #TAUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', '', '']","['The literature contains work on sentence-level classification ofEDLINE abstracts', '', ' #TAUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', '', '']","['The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes.', '', ' #TAUTHOR_TAG have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance .', '', '']",0
"['', 'Dorsch', 'have shown that existing systems for searching MEDLINE ( such as , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #TAUTHOR_TAG .', 'ins', '']","['', 'Dorsch', 'have shown that existing systems for searching MEDLINE ( such as , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #TAUTHOR_TAG .', 'insufficient', '']","['', 'D', 'have shown that existing systems for searching MEDLINE ( such as , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #TAUTHOR_TAG .', 'ins', '']","['', '', 'However , studies have shown that existing systems for searching MEDLINE ( such as PubMed , the search service provided by the National Library of Medicine ) are often inadequate and unable to supply clinically relevant answers in a timely manner ( Gorman , Ash , and Wykoff 1994 ; #TAUTHOR_TAG .', '', '']",0
"['Additional are associated with eachED citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', 'NLMSH', '.', 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content #TAUTHOR_TAG .', ', the indexing process remains firmly human-centered.']","['Additional are associated with each citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', ""NLM's"", '', 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content #TAUTHOR_TAG .', 'Nevertheless, the indexing process remains firmly human-centered.']","['Additional metadata are associated with each MEDLINE citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', 'NLMSH', '.', 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content #TAUTHOR_TAG .', ', the indexing process remains firmly human-centered.']","['Additional metadata are associated with each MEDLINE citation.', 'The most important of these is the controlled vocabulary terms assigned by human indexers.', '', '', 'Since mid-2002 , the Library has been employing software that automatically suggests MeSH headings based on content #TAUTHOR_TAG .', 'Nevertheless, the indexing process remains firmly human-centered.']",0
"['', 'Previously , a user study #TAUTHOR_TAG has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', '', '', '', '', '']","['', 'Previously , a user study #TAUTHOR_TAG has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', '', '', '', '', '']","['', 'Previously , a user study #TAUTHOR_TAG has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', '', '', '', '', '']","['', 'Previously , a user study #TAUTHOR_TAG has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .', '', '', '', '', '']",1
"['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #TAUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #TAUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #TAUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , #TAUTHOR_TAG , Rinaldi et al. 2004 ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']",0
"['', '', 'cod', ', software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep #TAUTHOR_TAG extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALISTxicon.', '', '', '', '', '']","['', '', 'codified', ', software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep #TAUTHOR_TAG extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '', '', '', '', '']","['', '', '', ', software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep #TAUTHOR_TAG extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '', '', '', '', '']","['', '', '', 'Second , software for utilizing this ontology already exists : MetaMap ( Aronson 2001 ) identifies concepts in free text , and SemRep #TAUTHOR_TAG extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '', '', '', '', '']",0
"['We attempted two approaches for assigning these weights. The first method rel ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification #TAUTHOR_TAG , which can be described by the following equation:']","['We attempted two approaches for assigning these weights. The first method relied ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification #TAUTHOR_TAG , which can be described by the following equation:']","['We attempted two approaches for assigning these weights. The first method rel ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification #TAUTHOR_TAG , which can be described by the following equation:']","['We attempted two approaches for assigning these weights. The first method relied on ad hoc weight selection based on intuition.', 'The second involved a more principled method using confidence values generated by the base classifiers and least squares linear regression adapted for classification #TAUTHOR_TAG , which can be described by the following equation:']",5
"['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TAUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope', '', '']","['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TAUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope).', '', '']","['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TAUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope', '', '']","['The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by #TAUTHOR_TAG .', 'Their study also illustrates the importance of semantic classes and relations.', 'However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope).', '', '']",1
"['', '', 'cod H', 'Second , software for utilizing this ontology already exists : MetaMap #TAUTHOR_TAG identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALISTxicon.', '', '', '', '', '']","['', '', 'codified', 'Second , software for utilizing this ontology already exists : MetaMap #TAUTHOR_TAG identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '', '', '', '', '']","['', '', '', 'Second , software for utilizing this ontology already exists : MetaMap #TAUTHOR_TAG identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '', '', '', '', '']","['', '', '', 'Second , software for utilizing this ontology already exists : MetaMap #TAUTHOR_TAG identifies concepts in free text , and SemRep ( Rindflesch and Fiszman 2003 ) extracts relations between the concepts .', 'Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon.', '', '', '', '', '']",0
"['to not', 'Mend have studiedSH clinical tasks therapyosisology', 'cit', '', '', '', 'The work of #TAUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']","['to not', 'have studied MeSH clinical tasks therapy,', 'citations', '', '', '', 'The work of #TAUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']","['to is not', 'have studiedSH terms therapyology', 'cit', '', '', '', 'The work of #TAUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']","['', '', '', '', '', '', 'The work of #TAUTHOR_TAG demonstrates that faceted queries can be converted into simple filtering constraints to boost precision .']",0
"['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine #TAUTHOR_TAG .', 'is implemented as an ensemble of class trained using supervised machine learning', '', '', '', '', '', '', '', '']","['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine #TAUTHOR_TAG .', 'is implemented as an ensemble of classifiers trained using supervised machine learning', '', '', '', '', '', '', '', '']","['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine #TAUTHOR_TAG .', 'is implemented as an ensemble of class trained using supervised machine learning', '', '', '', '', '', '', '', '']","['The extraction of each PICO element relies to a different extent on an annotated corpus of MEDLINE abstracts , created through an effort led by the first author at the National Library of Medicine #TAUTHOR_TAG .', '', '', '', '', '', '', '', '', '']",5
"['Gan , Ash , Wykoff 994 ; #TAUTHOR_TAG , 2005', '', '', '', '']","['Gorman , Ash , Wykoff 1994 ; #TAUTHOR_TAG , 2005', '', '', '', '']","['Gan , Ash , Wykoff 1994 ; #TAUTHOR_TAG , 2005', '', '', '', '']","['', '', '', '', '']",0
"['further have to first define what a answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #TAUTHOR_TAG and #AUTHOR_TAG', '', '']","['further have to first define what a answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #TAUTHOR_TAG and #AUTHOR_TAG', '', '']","['s further to first define what a good answer should be.', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #TAUTHOR_TAG and #AUTHOR_TAG .', '', '']","['', 'We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see #TAUTHOR_TAG and #AUTHOR_TAG .', '', '']",0
"['After much exploration , #TAUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment:']","['After much exploration , #TAUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment:']","['After much exploration , #TAUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment:']","['After much exploration , #TAUTHOR_TAG discovered that it was not practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues .', 'Consider the following segment:']",0
"['', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio #TAUTHOR_TAG .', 'ases such as rhe', '', '']","['', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio #TAUTHOR_TAG .', 'such as', '', '']","['', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio #TAUTHOR_TAG .', 'ases such as rhe', '', '']","['', 'We first identified the most informative unigrams and bigrams using the information gain measure ( Yang and Pedersen 1997 ) , and then selected only the positive outcome predictors using odds ratio #TAUTHOR_TAG .', '', '', '']",5
"['', '', '', 'present more effectively the; for example #AUTHOR_TAG', 'multilevelleted lists , appropri integrated with interface elements for expanding and items , might provide physicians a better overview of the information landscape ; see , for example , #TAUTHOR_TAG .']","['', '', '', 'present more effectively the information; for example, #AUTHOR_TAG', 'multi-level bulleted lists , appropriately integrated with interface elements for expanding and items , might provide physicians a better overview of the information landscape ; see , for example , #TAUTHOR_TAG .']","['', '', '', 'present more effectively for example #AUTHOR_TAG .', 'multilevelleted lists , appropri integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #TAUTHOR_TAG .']","['', '', '', '', 'Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , #TAUTHOR_TAG .']",3
"['the evaluation of answers to complex questions remains an open research problem.', 'are not agreed methodology comparisons results', '', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions #TAUTHOR_TAG ."", 'A of (eg., Hildet, Katz, 2004) have pointed out shortcom nug number these have been Dem', '']","['the evaluation of answers to complex questions remains an open research problem.', 'are not agreed methodology comparisons results', '', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions #TAUTHOR_TAG ."", 'A of (e.g., Hildebrandt, Katz, 2004) have pointed out shortcomings nugget number these have been', '']","['the evaluation of answers to complex questions remains an open research problem.', 'are not agreed a methodology meaningful comparisons results', '', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions #TAUTHOR_TAG ."", 'A number of (eg., Hildebrandt, Katz, Lin 2004) have pointed out shortcom a number these issues have been recently Dem', '']","['Finally, the evaluation of answers to complex questions remains an open research problem.', '', '', 'Recently, there is a growing consensus that an evaluation methodology based on the notion of ""information nuggets"" may provide an appropriate framework for assessing the quality of answers to complex questions.', ""Nugget F-score has been employed as a metric in the TREC question-answering track since 2003 , to evaluate so-called definition and `` other '' questions #TAUTHOR_TAG ."", '', '']",0
"['', '', '', '', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TAUTHOR_TAG a ) for a brief overview .']","['', '', '', '', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TAUTHOR_TAG a ) for a brief overview .']","['', '', '', '', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TAUTHOR_TAG a ) for a brief overview .']","['', '', '', '', 'In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see #TAUTHOR_TAG a ) for a brief overview .']",0
"['the evaluation of answers to remains an open', '', '', 'method', '', 'A of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed #TAUTHOR_TAG a , 2006b ) .', 'ende']","['the evaluation of answers to remains an open', '', '', 'methodology', '', 'A of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed #TAUTHOR_TAG a , 2006b ) .', 'endeavor']","['the evaluation of answers to remains', '', '', '', '', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed #TAUTHOR_TAG a , 2006b ) .', '']","['Finally, the evaluation of answers to complex questions remains an open research problem.', '', '', '', '', 'A number of studies ( e.g. , Hildebrandt , Katz , and Lin 2004 ) have pointed out shortcomings of the original nugget scoring model , although a number of these issues have been recently addressed #TAUTHOR_TAG a , 2006b ) .', '']",0
"['The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy #TAUTHOR_TAG .']","['The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy #TAUTHOR_TAG .']","['The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy #TAUTHOR_TAG .']","['The potential highest level of the strength of evidence for a given citation can be identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study.', 'Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy #TAUTHOR_TAG .']",5
"['', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity #TAUTHOR_TAG ; Groote and Dorsch 2003 ) ."", '', '', '']","['', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity #TAUTHOR_TAG ; Groote and Dorsch 2003 ) ."", '', '', '']","['', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity #TAUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", '', '', '']","['', ""MEDLINE , the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine , provides the clinically relevant sources for answering physicians ' questions , and is commonly used in that capacity #TAUTHOR_TAG ; De Groote and Dorsch 2003 ) ."", '', '', '']",0
"['', '', '-', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start #TAUTHOR_TAG .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner- #AUTHOR_TAG a) for a brief overview.']","['', '', '', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start #TAUTHOR_TAG .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner- #AUTHOR_TAG a) for a brief overview.']","['', '', '', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start #TAUTHOR_TAG .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner- #AUTHOR_TAG a) for a brief overview.']","['', '', '', 'For an overview of systems designed to answer open-domain factoid questions , the TREC QA track overview papers are a good place to start #TAUTHOR_TAG .', 'In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner- #AUTHOR_TAG a) for a brief overview.']",0
"['', '', '', '', '', '', 'The Semantic Network a consistent categorization of all concepts represented in the UMLS Metathesaurus', 'Third the paradigm of evidence-based medicine #TAUTHOR_TAG provides a task-based model of the clinical information-seeking process .', 'The PRichardson. 95 well- queries that br the needsical', '']","['', '', '', '', '', '', 'The Semantic Network a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third the paradigm of evidence-based medicine #TAUTHOR_TAG provides a task-based model of the clinical information-seeking process .', 'The (Richardson al. well-formulated queries that bridges the needs analytical', '']","['', '', '', '', '', '', 'The Semantic Network a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third the paradigm of evidence-based medicine #TAUTHOR_TAG provides a task-based model of the clinical information-seeking process .', 'Richardson. 95 that br the needsical capabilities', '']","['', '', '', '', '', '', 'The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus.', 'Third , the paradigm of evidence-based medicine #TAUTHOR_TAG provides a task-based model of the clinical information-seeking process .', '', '']",0
"['structured M cittakinglevant', '', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #TAUTHOR_TAG .']","['structured citations (taking', '', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #TAUTHOR_TAG .']","['structured representations Mtaking', '', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #TAUTHOR_TAG .']","['', '', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; #TAUTHOR_TAG .']",1
"['citlevant', '', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems #TAUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']","['citations', '', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems #TAUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']","['', '', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems #TAUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']","['', '', 'As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems #TAUTHOR_TAG ; Hirschman and Gaizauskas 2001 ) .']",1
"['clinical', ""SH head of the human indexers' tasks assigning terms is mainsometimes a"", 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TAUTHOR_TAG .']","['clinical', ""MeSH headings of the human indexers' tasks assigning terms is main (sometimes a"", 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TAUTHOR_TAG .']","['', ""of the human indexers' tasks assigning terms issometimes"", 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TAUTHOR_TAG .']","['', '', 'For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in #TAUTHOR_TAG .']",5
"['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #TAUTHOR_TAG , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #TAUTHOR_TAG , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #TAUTHOR_TAG , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']","['The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , #TAUTHOR_TAG , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .', '']",0
"['Our knowledge extractors rely extensively on MetaMap #TAUTHOR_TAG , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'algorithmssergrantic types Semanticray Burg', '', '', '', '', '', '', '']","['Our knowledge extractors rely extensively on MetaMap #TAUTHOR_TAG , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'algorithms semantic types Semantic', '', '', '', '', '', '', '']","['Our knowledge extractors rely extensively on MetaMap #TAUTHOR_TAG , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', 'algorithms operatesergrained semantic types Semray', '', '', '', '', '', '', '']","['Our knowledge extractors rely extensively on MetaMap #TAUTHOR_TAG , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .', '', '', '', '', '', '', '', '']",5
"['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #TAUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an completely annotated, see Figure .', '']","['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #TAUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an completely annotated abstract, see Figure 2.', '']","['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #TAUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example a completely annotated abstract, see Figure .', '']","['The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering.', 'This section , which elaborates on preliminary results reported in #TAUTHOR_TAG , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .', 'For an example of a completely annotated abstract, see Figure 2.', '']",2
"['The function  maps a MeSH term to a positive score if the term is a positive indicator for that task a score if the term is a clinical task', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed #TAUTHOR_TAG .']","['The function maps a MeSH term to a positive score if the term is a positive indicator for that task a score if the term is a clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed #TAUTHOR_TAG .']","['The function  maps a MeSH term to a positive score if the term is a positive indicator for a negative score if the term is the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed #TAUTHOR_TAG .']","['The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task.', 'Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed #TAUTHOR_TAG .']",3
"['', '', '', '', '', '', '', '', 'The PICO framework #TAUTHOR_TAG for capturing well-formulated clinical queries ( described in Section2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', '']","['', '', '', '', '', '', '', '', 'The PICO framework #TAUTHOR_TAG for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', '']","['', '', '', '', '', '', '', '', 'The PICO framework #TAUTHOR_TAG for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', '']","['', '', '', '', '', '', '', '', 'The PICO framework #TAUTHOR_TAG for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .', '']",0
"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al.2003 ; Preiss 2003 ; #TAUTHOR_TAG ; Miyao and Tsujii 2004 ) .', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #TAUTHOR_TAG ; Miyao and Tsujii 2004 ) .', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al.2003 ; Preiss 2003 ; #TAUTHOR_TAG ; Miyao and Tsujii 2004 ) .', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; #TAUTHOR_TAG ; Miyao and Tsujii 2004 ) .', '']",4
"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #TAUTHOR_TAG ; Preiss 2003 ; Kaplan et al.004 ; M4', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #TAUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ;', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #TAUTHOR_TAG ; Preiss 2003 ; Kaplan et al.004 ; M', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; #TAUTHOR_TAG ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', '']",4
"['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al.2003 ; #TAUTHOR_TAG ; Kaplan et al.2004 ; M and 4', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #TAUTHOR_TAG ; Kaplan et al. 2004 ; and', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al.2003 ; #TAUTHOR_TAG ; Kaplan et al.2004 ; M and', '']","['Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; #TAUTHOR_TAG ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .', '']",4
"['graph dependencies-one is that the computational of training the model and searching for the most likely labeling given the tree can be prohibitive, and', '', 'efficiency adopt re-ranking algorithms', '', 'Our re-ranking approach , like the approach to parse re-ranking of #TAUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', '']","['graphical dependencies dangers-one is that the computational of training the model and searching for the most likely labeling given the tree can be prohibitive, and', '', 'efficiency adopt re-ranking algorithms.', '', 'Our re-ranking approach , like the approach to parse re-ranking of #TAUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', '']","['many dependencies is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and', '', 'we adopt re-ranking algorithms.', '', 'Our re-ranking approach , like the approach to parse re-ranking of #TAUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', '']","['', '', '', '', 'Our re-ranking approach , like the approach to parse re-ranking of #TAUTHOR_TAG , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .', '']",1
"['', '', '', '', 'classes due strong independence assumptions', 'in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach #TAUTHOR_TAG , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n of local semantic role label S to', 'our re-ranking approach does not present a serious bottleneck to performance', '= ', '', '', '']","['', '', '', '', 'classes due strong independence assumptions.', 'in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach #TAUTHOR_TAG , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n of local semantic role labeling SRL to', 'our re-ranking approach does not present a serious bottleneck to performance.', '=', '', '', '']","['', '', '', '', 'classes due strong independence assumptions.', 'in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach #TAUTHOR_TAG , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of local semantic role labeling to', 'our re-ranking approach does not present a serious bottleneck to performance', '= ', '', '', '']","['', '', '', '', '', 'Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach #TAUTHOR_TAG , which selects from likely assignments generated by a model which makes stronger independence assumptions .', 'We utilize the top n assignments of our local semantic role labeling model P SRL to generate likely assignments.', '', '', '', '', '']",5
"['Following our previous work #TAUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the to information an un of represented lists', '', '', '', '', '', '', '', '']","['Following our previous work #TAUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the to information an unordered of represented lists', '', '', '', '', '', '', '', '']","['Following our previous work #TAUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering an unordered set of represented', '', '', '', '', '', '', '', '']","['Following our previous work #TAUTHOR_TAG ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .', '', '', '', '', '', '', '', '']",2
"['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TAUTHOR_TAG .', 'We use the words to', '', '']","['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TAUTHOR_TAG .', 'We use the words to', '', '']","['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TAUTHOR_TAG .', 'We use the words to', '', '']","['Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by #TAUTHOR_TAG .', '', '', '']",4
"['a classification method to predict clusters from the values of the.', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs #TAUTHOR_TAG .']","['a classification method to predict clusters from the values of the cases.', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs #TAUTHOR_TAG .']","['a classification method to predict clusters from the values of', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs #TAUTHOR_TAG .']","['', 'We posit that this would not have a significant effect on the results , in particular for MML-based classification techniques , such as Decision Graphs #TAUTHOR_TAG .']",0
"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain #TAUTHOR_TAG .', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain #TAUTHOR_TAG .', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain #TAUTHOR_TAG .', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997).', 'Such technologies require significant human input , and are difficult to create and maintain #TAUTHOR_TAG .', '', '']",0
"['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #TAUTHOR_TAG .']","['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #TAUTHOR_TAG .']","['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #TAUTHOR_TAG .']","['â\x80¢ Only an automatic evaluation was performed , which relied on having model responses ( Berger and Mittal 2000 ; #TAUTHOR_TAG .']",1
"['As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm #TAUTHOR_TAG , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', '']","['As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm #TAUTHOR_TAG , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', '']","['As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm #TAUTHOR_TAG , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', '']","['As stated herein, we studied two document-based methods: Document Retrieval and Document Prediction.', '(Doc-Ret).', 'This method follows a traditional Information Retrieval paradigm #TAUTHOR_TAG , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .', '']",5
"['', '', '', '', '', '', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #TAUTHOR_TAG to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2', '', '', '', '']","['', '', '', '', '', '', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #TAUTHOR_TAG to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2', '', '', '', '']","['', '', '', '', '', '', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #TAUTHOR_TAG to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2', '', '', '', '']","['', '', '', '', '', '', 'For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; #TAUTHOR_TAG to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .', '', '', '', '']",5
"['The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm #TAUTHOR_TAG , where a new request is matched with existing response documents ( e-mails ) .', 'specific words in the do match a, and sometimes do not match a as the examples in Figures 1(a) and 1(c),.']","['The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm #TAUTHOR_TAG , where a new request is matched with existing response documents ( e-mails ) .', 'specific words in the do match a well, and sometimes do not match a as the examples in Figures 1(a) and 1(c), respectively.']","['The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm #TAUTHOR_TAG , where a new request is matched with existing response documents ( e-mails ) .', 'specific words in the request match a response well, and sometimes do not match a response as the examples in Figures 1(a) and 1(c), respectively.']","['The example in Figure 1(b) illustrates a situation where specific words in the request (docking station and install) are also mentioned in the response.', 'This situation suggests a response-automation approach that follows the document retrieval paradigm #TAUTHOR_TAG , where a new request is matched with existing response documents ( e-mails ) .', 'However, specific words in the request do not always match a response well, and sometimes do not match a response at all, as demonstrated by the examples in Figures 1(a) and 1(c), respectively.']",0
"['even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique and hence care must be taken to compose a response that does not confuse, irritate, or mislead the.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven #TAUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven #TAUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven #TAUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven #TAUTHOR_TAG ; Watson 1997 ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0
"['of (ations (  and ', 'We then use the program Snob ( Wallace and Boulton 1968 ; #TAUTHOR_TAG to cluster these experiences .', 'Figure 8(a)ids clusters produced Snob into three most Pr Analysis (', 'project', '']","['of (Equations ( and', 'We then use the program Snob ( Wallace and Boulton 1968 ; #TAUTHOR_TAG to cluster these experiences .', 'Figure 8(a) centroids clusters produced Snob into three most Principal Analysis', '', '']","['of (ations (  and ', 'We then use the program Snob ( Wallace and Boulton 1968 ; #TAUTHOR_TAG to cluster these experiences .', 'Figure 8(a) the clusters produced Snob into three most Pr (', 'project', '']","['', 'We then use the program Snob ( Wallace and Boulton 1968 ; #TAUTHOR_TAG to cluster these experiences .', '', '', '']",5
"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems #TAUTHOR_TAG and case-based reasoning ( Watson 1997 ) .', 'Such require significant human difficult to create and maintainDel', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems #TAUTHOR_TAG and case-based reasoning ( Watson 1997 ) .', 'Such require significant human difficult to create and maintain', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems #TAUTHOR_TAG and case-based reasoning ( Watson 1997 ) .', 'Such technologies require significant human input, difficult to create and maintainDel', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems #TAUTHOR_TAG and case-based reasoning ( Watson 1997 ) .', '', '', '']",1
"['is Function kernel', 'trainedas as', 'binary bag--lemed results.', 'We employed the LIBSVM package #TAUTHOR_TAG .', 'the SVMs predict zero or more SCs for each, as shown in Figure 3.', 'following steps']","['Basis Function kernel', 'trained as', 'binary bag-of-lemmas yielded results.', 'We employed the LIBSVM package #TAUTHOR_TAG .', 'the SVMs predict zero or more SCs for each request, as shown in Figure 3.', 'following steps.']","['', 'is trained as', 'the simple binary bag-of-lemmas representationed similar results.', 'We employed the LIBSVM package #TAUTHOR_TAG .', 'the SVMs predict zero or more SCs for each request, as shown in Figure 3.', 'the following steps.']","['', '', '', '7 We employed the LIBSVM package #TAUTHOR_TAG .', 'prediction stage, the SVMs predict zero or more SCs for each request, as shown in Figure 3.', '']",5
"['', 'our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #TAUTHOR_TAG .', 'We this the in and aabilistic for its', '', '', '', '', '', '', 'Table']","['', 'our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #TAUTHOR_TAG .', 'We this the in and a probabilistic for its', '', '', '', '', '', '', '(Table']","['', 'our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #TAUTHOR_TAG .', 'We chose this program the number in and a probabilistic interpretation for its', '', '', '', '', '', '', 'Table']","['', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; #TAUTHOR_TAG .', '', '', '', '', '', '', '', '']",5
"['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #TAUTHOR_TAG .', 'Prec measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overallity between the automatically generated response and the model response.', 'harm precision', '', '', '']","['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #TAUTHOR_TAG .', 'Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response.', 'harmonic precision', '', '', '']","['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #TAUTHOR_TAG .', 'Prec measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response.', 'precision', '', '', '']","['We use two measures from Information Retrieval to determine the quality of an automatically generated response : precision and F-score ( van Rijsbergen 1979 ; #TAUTHOR_TAG .', 'Precision measures how much of the information in an automatically generated response is correct (i.e., appears in the model response), and F-score measures the overall similarity between the automatically generated response and the model response.', '', '', '', '']",5
"['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning #TAUTHOR_TAG .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 199).', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning #TAUTHOR_TAG .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning #TAUTHOR_TAG .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', '', '']","['The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms , such as expert systems ( Barr and Tessler 1995 ) and case-based reasoning #TAUTHOR_TAG .', 'Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998).', '', '']",1
"['', '', '', '', '', '', '', '', '', '', '', '', '', '.', ' #TAUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-mails']","['', '', '', '', '', '', '', '', '', '', '', '', '', 'templates.', ' #TAUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-mails']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-mails']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG investigated three approaches to the automatic generation of response e-mails : text classification , case-based reasoning , and question answering .', 'Text classification was used to group request e-mails']",1
"['5 Significant bigrams are obtained using the n-gram statistics package NSP #TAUTHOR_TAG , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']","['5 Significant bigrams are obtained using the n-gram statistics package NSP #TAUTHOR_TAG , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']","['5 Significant bigrams are obtained using the n-gram statistics package NSP #TAUTHOR_TAG , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']","['5 Significant bigrams are obtained using the n-gram statistics package NSP #TAUTHOR_TAG , which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram ( that it is not a collocation ) .']",5
"['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', '', '', 'been little on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #TAUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'de of corpor-help-', '', '', '', '', '']","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', '', '', 'been little on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #TAUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'dearth of corpora-help-desk', '', '', '', '', '']","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', '', '', 'has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #TAUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', 'of corpor-help-', '', '', '', '', '']","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', '', '', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; #TAUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '']",0
"['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005; #TAUTHOR_TAG .', '', '', '']","['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005; #TAUTHOR_TAG .', '', '', '']","['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005; #TAUTHOR_TAG .', '', '', '']","['Two applications that, like help-desk, deal with question�answer pairs are: sum- marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000; Berger et al. 2000; Jijkoun and de Rijke 2005; #TAUTHOR_TAG .', '', '', '']",1
['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) #TAUTHOR_TAG ; Roy and Subramaniam 2006 ) .'],['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) #TAUTHOR_TAG ; Roy and Subramaniam 2006 ) .'],['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) #TAUTHOR_TAG ; Roy and Subramaniam 2006 ) .'],['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) #TAUTHOR_TAG ; Roy and Subramaniam 2006 ) .'],1
"['', '', '', ""we tried to ensure that the sets of cases shown to the judges were of quality, so that the judges' assessments would be."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures #TAUTHOR_TAG .', '']","['', '', '', ""we tried to ensure that the sets of cases shown to the judges were of quality, so that the judges' assessments would be comparable."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures #TAUTHOR_TAG .', '']","['', '', '', ""we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures #TAUTHOR_TAG .', '']","['', '', '', ""In addition, we tried to ensure that the sets of cases shown to the judges were of similar quality, so that the judges' assessments would be comparable."", 'Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures #TAUTHOR_TAG .', '']",5
"['are be a requests only or match portions of', '', '', '', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization #TAUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']","['are be a requests only or match portions of', '', '', '', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization #TAUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']","['are be addressed requests only or match portions of', '', '', '', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization #TAUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']","['', '', '', '', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization #TAUTHOR_TAG ; Barzilay , Elhadad , and McKeown 2001 ; Barzilay and McKeown 2005 ) .']",0
"['A common way to combine different models consists of selecting the model that is most confident regarding its decision #TAUTHOR_TAG .', '', 'selecting method', '', '', '']","['A common way to combine different models consists of selecting the model that is most confident regarding its decision #TAUTHOR_TAG .', '', 'selecting method,', '', '', '']","['A common way to combine different models consists of selecting the model that is most confident regarding its decision #TAUTHOR_TAG .', '', 'selecting', '', '', '']","['A common way to combine different models consists of selecting the model that is most confident regarding its decision #TAUTHOR_TAG .', '', '', '', '', '']",1
"['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) #TAUTHOR_TAG ; Soricut and Brill 2006).', '', '', '']","['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) #TAUTHOR_TAG ; Soricut and Brill 2006).', '', '', '']","['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) #TAUTHOR_TAG ; Soricut and Brill 2006).', '', '', '']","['Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004;Shrestha and McKeown 2004), and answer extraction in FAQs (Frequently Asked Questions) #TAUTHOR_TAG ; Soricut and Brill 2006).', '', '', '']",1
"['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #TAUTHOR_TAG .', 'The representativeness of the sample size was not discussed in any of these studies.']","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #TAUTHOR_TAG .', 'The representativeness of the sample size was not discussed in any of these studies.']","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #TAUTHOR_TAG .', 'The representativeness of the sample size was not discussed in any of these studies.']","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours ( Feng et al. 2006 ; #TAUTHOR_TAG .', 'The representativeness of the sample size was not discussed in any of these studies.']",1
"[' #AUTHOR_TAG a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses #TAUTHOR_TAG ; Berger et al. 2000 ) .']","[' #AUTHOR_TAG a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses #TAUTHOR_TAG ; Berger et al. 2000 ) .']","[' #AUTHOR_TAG a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses #TAUTHOR_TAG ; Berger et al. 2000 ) .']","[' #AUTHOR_TAG a) we identified several systems that resemble ours in that they provide answers to queries.', 'These systems addressed the evaluation issue as follows.', 'â\x80¢ Only an automatic evaluation was performed , which relied on having model responses #TAUTHOR_TAG ; Berger et al. 2000 ) .']",1
"['In #TAUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses .']","['In #TAUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses .']","['In #TAUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses .']","['In #TAUTHOR_TAG a ) we identified several systems that resemble ours in that they provide answers to queries .', 'These systems addressed the evaluation issue as follows.', 'r Only an automatic evaluation was performed, which relied on having model responses .']",0
"['clusters basis', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion #TAUTHOR_TAG ; Wallace 2005 ) .', '', '', '', '', '', '', '', 'Table']","['clusters, basis', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion #TAUTHOR_TAG ; Wallace 2005 ) .', '', '', '', '', '', '', '', '(Table']","['clusters the basis', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion #TAUTHOR_TAG ; Wallace 2005 ) .', '', '', '', '', '', '', '', 'Table']","['', 'In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion #TAUTHOR_TAG ; Wallace 2005 ) .', '', '', '', '', '', '', '', '']",5
"['In FAQs , #TAUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijk', '', 'the.']","['In FAQs , #TAUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijkoun', '', 'the following.']","['In FAQs , #TAUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', 'Jijk', '', 'the following.']","['In FAQs , #TAUTHOR_TAG employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence , and the questions and answers are embedded in an FAQ document .', 'They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models.', 'compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models).', '', '', '']",0
"['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours #TAUTHOR_TAG ; Leuski et al. 2006 )', 'The representat of the sample size was not discussed in any of these studies.']","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours #TAUTHOR_TAG ; Leuski et al. 2006 )', 'The representativeness of the sample size was not discussed in any of these studies.']","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours #TAUTHOR_TAG ; Leuski et al. 2006 )', 'The representativeness of the sample size was not discussed in any of these studies.']","['â\x80¢ A user study was performed , but it was either very small compared to the corpus ( Carmel , Shtalhaim , and Soffer 2000 ; Jijkoun and de Rijke 2005 ) , or the corpus itself was significantly smaller than ours #TAUTHOR_TAG ; Leuski et al. 2006 ) .', 'The representativeness of the sample size was not discussed in any of these studies.']",1
"['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', '', '', '( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #TAUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '']","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', '', '', '( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #TAUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '']","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', '', '', '( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #TAUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '']","['In recent times, such knowledge-intensive approaches to content delivery have been largely superseded by data-intensive, statistical approaches.', '', '', 'Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #TAUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '']",0
"['performance', '', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs #TAUTHOR_TAG for Sent-Pred .', '', '']","['performance', '', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs #TAUTHOR_TAG for Sent-Pred', '', '']","['', '', 'Specifically , we used Decision Graphs ( Oliver 1993 ) for Doc-Pred , and SVMs #TAUTHOR_TAG for Sent-', '', '']","['', '', '', '', '']",5
"['.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TAUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e its is not decrement', '', '']","['sentences.', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TAUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its is not decremented).', '', '']","['', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TAUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its is not decrement', '', '']","['', 'After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by #TAUTHOR_TAG to penalize redundant sentences in cohesive clusters .', 'This is done by decrementing the score of a sentence that belongs to an SC for which there is a higher or equal scoring sentence (if there are several highest-scoring sentences, we retain one sentence as a reference sentence-i.e., its score is not decremented).', '', '']",5
"['', '', '', '', '', 'The predictive model is a Decision Graph #TAUTHOR_TAG , which , like Snob , is based on the MML principle', 'mm input the identifier of the response cluster that contains the actual response for the request as the target feature', 'predicts which response is most for a re- quest and returns the this prediction is correct', '', 'Table']","['', '', '', '', '', 'The predictive model is a Decision Graph #TAUTHOR_TAG , which , like Snob , is based on the MML principle', 'input the identifier of the response cluster that contains the actual response for the request as the target feature.', 'predicts which response is most for a re- quest, and returns the this prediction is correct.', '', '(Table']","['', '', '', '', '', 'The predictive model is a Decision Graph #TAUTHOR_TAG , which , like Snob , is based on the MML principle .', 'the identifier of the response cluster that contains the actual response for the request as the target feature.', 'predicts which response cluster is most for a re- quest, and returns the probability this prediction is correct', '', 'Table']","['', '', '', '', '', 'The predictive model is a Decision Graph #TAUTHOR_TAG , which , like Snob , is based on the MML principle .', '', 'The model predicts which response cluster is most suitable for a given re- quest, and returns the probability that this prediction is correct.', '', '']",5
"['even the automation of responses to the ""easy"" problems is a difficult task.', 'such inquiries revolve around a relatively small set of issues specific circumstancesquiry unique and hence care be taken compose a response notuse.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #TAUTHOR_TAG .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['even the automation of responses to the ""easy"" problems is a difficult task.', 'such inquiries revolve around a relatively small set of issues, specific circumstances inquiry unique, and hence care be taken compose a response not customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #TAUTHOR_TAG .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['even the automation of responses to the ""easy"" problems is a difficult task.', 'such inquiries revolve around a relatively small set of issues specific circumstances and hence care must be taken compose a response does notuse', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #TAUTHOR_TAG .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['However, even the automation of responses to the ""easy"" problems is a difficult task.', 'Although such inquiries revolve around a relatively small set of issues, specific circumstances can make each inquiry unique, and hence care must be taken to compose a response that does not confuse, irritate, or mislead the customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; #TAUTHOR_TAG .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0
"['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #TAUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #TAUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #TAUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; #TAUTHOR_TAG ; Bickel and Scheffer 2004 ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']",1
"['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #TAUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #TAUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #TAUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']","['There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; #TAUTHOR_TAG ; Malik , Subramaniam , and Kaushik 2007 ) .', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']",1
"['', '', '', '', ' #TAUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', 'Two the.']","['', '', '', '', ' #TAUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', 'Two the following.']","['', '', '', '', ' #TAUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', 'the following.']","['', '', '', '', ' #TAUTHOR_TAG compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .', '']",1
"['with kernel', 'trainedas as a target feature', 'For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person #TAUTHOR_TAG , but the simple binary bag-of-lemmas representation yielded similar results .', '', '', '']","['with kernel', 'trained as a target feature', 'For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person #TAUTHOR_TAG , but the simple binary bag-of-lemmas representation yielded similar results .', '7', '', '']","['with', 'is trained as a binary target feature specifying', 'For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person #TAUTHOR_TAG , but the simple binary bag-of-lemmas representation yielded similar results .', '', '', '']","['', '', '6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person #TAUTHOR_TAG , but the simple binary bag-of-lemmas representation yielded similar results .', '', '', '']",5
"['', '', '', 'proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TAUTHOR_TAG as follows .', 'merging category to techniques where the individual methods affect each other in ways (this category encompasses Burs feature', 'prediction']","['', '', '', 'proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TAUTHOR_TAG as follows .', ""merging category to techniques where the individual methods affect each other in ways (this category encompasses Burke's feature"", 'prediction']","['', '', '', 'proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TAUTHOR_TAG as follows .', 'The merging category corresponds to techniques where the individual methods affect each other in different ways (this', '']","['', '', '', 'They also proposed two major categories of meta-learning approaches for recommender systems , merging and ensemble , each subdivided into the more specific subclasses suggested by #TAUTHOR_TAG as follows .', ""The merging category corresponds to techniques where the individual methods affect each other in different ways (this category encompasses Burke's feature combination, cascade, feature augmentation, and meta-level sub-categories)."", '']",0
"['', '', '', '', 'we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system #TAUTHOR_TAG b ) .', '']","['', '', '', '', 'we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system #TAUTHOR_TAG b ) .', '']","['', '', '', '', 'we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system #TAUTHOR_TAG b ) .', '']","['', '', '', '', 'In Section 5 , we discuss the difficulties associated with such user studies , and describe a human-based evaluation we conducted for a small subset of the responses generated by our system #TAUTHOR_TAG b ) .', '']",5
"['even the automation of responses to the ""easy"" problems is a difficult task.', 'such inquiries revolve around a relatively small set of issues specific circumstancesquiry hence care taken comp response notuse.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #TAUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['even the automation of responses to the ""easy"" problems is a difficult task.', 'such inquiries revolve around a relatively small set of issues, specific circumstances inquiry hence care taken compose response not customer.', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #TAUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['even the automation of responses to the ""easy"" problems is a difficult task.', 'such inquiries revolve around a relatively small set of issues specific circumstances hence care comp a response does notuse', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #TAUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']","['However, even the automation of responses to the ""easy"" problems is a difficult task.', '', 'It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; #TAUTHOR_TAG ; Delic and Lahaix 1998 ) .', 'These systems were carefully designed to produce relevant and correct responses, but required significant human input and maintenance (Delic and Lahaix 1998).']",0
"['', '', ' #TAUTHOR_TAG two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models )', 'ijk', '', '']","['', '', ' #TAUTHOR_TAG two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models )', '', '', '']","['', '', ' #TAUTHOR_TAG two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models )', 'ijk', '', '']","['', '', ' #TAUTHOR_TAG compared two retrieval approaches ( TF.IDF and query expansion ) and two predictive approaches ( statistical translation and latent variable models ) .', '', '', '']",0
"['Following #TAUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', '', 'an', '', '', '']","['Following #TAUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', '', 'an', '', '', '']","['Following #TAUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', '', '', '', '', '']","['Following #TAUTHOR_TAG , one approach for achieving this objective consists of applying supervised learning , where a winning method is selected for each case in the training set , all the training cases are labeled accordingly , and then the system is trained to predict a winner for unseen cases .', '', '', '', '', '']",1
['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #TAUTHOR_TAG .'],['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #TAUTHOR_TAG .'],['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #TAUTHOR_TAG .'],['â\x80¢ Only qualitative observations of the responses were reported ( no formal evaluation was performed ) ( Lapalme and Kosseim 2003 ; #TAUTHOR_TAG .'],1
"['are a requests or match portions of', '', '', '', 'our we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #TAUTHOR_TAG .']","['are a requests or match portions of', '', '', '', 'our we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #TAUTHOR_TAG .']","['are requests or match portions of', '', '', '', 'our we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #TAUTHOR_TAG .']","['', '', '', '', 'In our work , we gather sets of sentences , and assume ( but do not employ ) existing approaches for their organization ( Goldstein et al. 2000 ; Barzilay , Elhadad , and McKeown 2001 ; #TAUTHOR_TAG .']",0
"['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #TAUTHOR_TAG , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', '', '', '']","['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #TAUTHOR_TAG , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', '', '', '']","['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #TAUTHOR_TAG , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', '', '', '']","['Two applications that , like help-desk , deal with question -- answer pairs are : summarization of e-mail threads ( Dalli , Xia , and Wilks 2004 ; #TAUTHOR_TAG , and answer extraction in FAQs ( Frequently Asked Questions ) ( Berger and Mittal 2000;Soricut and Brill 2006).', '', '', '']",1
"['performance techniques', '', 'Specifically , we used Decision Graphs #TAUTHOR_TAG for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .', '', '']","['performance techniques.', '', 'Specifically , we used Decision Graphs #TAUTHOR_TAG for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred', '', '']","['', '', 'Specifically , we used Decision Graphs #TAUTHOR_TAG for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-', '', '']","['', '', '', '', '']",5
"[' We also employed sequence-based measures using the ROUGE tool set #TAUTHOR_TAG , with similar results to those obtained with the word-by-word measures .']","['We also employed sequence-based measures using the ROUGE tool set #TAUTHOR_TAG , with similar results to those obtained with the word-by-word measures .']","[' We also employed sequence-based measures using the ROUGE tool set #TAUTHOR_TAG , with similar results to those obtained with the word-by-word measures .']","['13 We also employed sequence-based measures using the ROUGE tool set #TAUTHOR_TAG , with similar results to those obtained with the word-by-word measures .']",5
"['of (ations (  and', 'We then use the program Snob #TAUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8a)ids clusters produced by Snob into the three most significant dimensions discovered by Pr Component Analysis (', 'project', '']","['of (Equations ( and', 'We then use the program Snob #TAUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8(a) centroids clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis', '', '']","['of (ations (  and', 'We then use the program Snob #TAUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', 'Figure 8a) the clusters produced by Snob into the three most significant dimensions discovered by Principal Component Analysis (', 'project', '']","['', 'We then use the program Snob #TAUTHOR_TAG ; Wallace 2005 ) to cluster these experiences .', '', '', '']",5
"['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system #TAUTHOR_TAG a ) .', '', '']","['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system #TAUTHOR_TAG a ) .', '', '']","['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system #TAUTHOR_TAG a ) .', '', '']","['In order to address these limitations in a practical way , we conducted a small user study where we asked four judges ( graduate students from the Faculty of Information Technology at Monash University ) to assess the responses generated by our system #TAUTHOR_TAG a ) .', '', '']",5
"['a', '', '', '', ""The question answering system developed by #TAUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", '', 'twoys methods that not use', '']","['a', '', '', '', ""The question answering system developed by #TAUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", '', 'two methods that not use', '']","['', '', '', '', ""The question answering system developed by #TAUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", '', 'twoys methods that are not they use', '']","['', '', '', '', ""The question answering system developed by #TAUTHOR_TAG belongs to the merging category of approaches , where the output of an individual method can be used as input to a different method ( this corresponds to Burke 's cascade sub-category ) ."", '', '', '']",1
"['M', '', '', 'has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al.005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases #TAUTHOR_TAG ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']","['', '', '', 'has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases #TAUTHOR_TAG ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']","['M', '', '', 'has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases #TAUTHOR_TAG ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']","['', '', '', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases #TAUTHOR_TAG ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4
"['the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Al', ""η precision  step chosen with strongfe'sNocedal and Wright 99)."", 'PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient #TAUTHOR_TAG ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', '']","['the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm', ""η precision, α step chosen with strong Wolfe's (Nocedal and Wright 1999)."", 'PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient #TAUTHOR_TAG ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', '']","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Al', ""Here η chosen with the strong Wolfe's ruleNocedal and Wright 1999)."", 'PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient #TAUTHOR_TAG ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', '']","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here η is an optimization precision, α is a step size chosen with the strong Wolfe's rule (Nocedal and Wright 1999)."", 'Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient #TAUTHOR_TAG ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .', '']",5
"['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'abilities See #TAUTHOR_TAG for further discussion .']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'abilities See #TAUTHOR_TAG for further discussion .']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'phrase probabilities. See #TAUTHOR_TAG for further discussion .']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']",0
"['', '3 4 and 5 attempt to (the tendency each word generate several target), resulting in probabilistically deficient, intractable models that heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package #TAUTHOR_TAG as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency', '', '', '']","['', '3, 4, and 5 attempt to (the tendency each word generate several target words), resulting in probabilistically deficient, intractable models that heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package #TAUTHOR_TAG as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency', '', '', '']","['', 'and 5 attempt to (the tendency each source word generate several target), resulting in probabilistically deficient, intractable models that local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package #TAUTHOR_TAG as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', '', '', '']","['', 'IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend.', 'Many researchers use the GIZA + + software package #TAUTHOR_TAG as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .', '', '', '']",0
"['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule #TAUTHOR_TAG ."", 'Here, λ) represents an as direction chosen as: gradient', '']","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule #TAUTHOR_TAG ."", 'Here, represents an ascent direction chosen as follows: gradient', '']","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule #TAUTHOR_TAG ."", 'Here, λ) represents an ascent direction chosen as:', '']","['Hence the projection step uses the same inference algorithm (forward-backward for HMMs) to compute the gradient, only modifying the local factors using the current setting of λ.', 'We optimize the dual objective using the gradient based methods shown in Algorithm 1.', ""Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule #TAUTHOR_TAG ."", '', '']",5
"['alignment models in general in grossimpl of and the optimal likelihood parameters learned often do not correspond to alignments', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model #TAUTHOR_TAG .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '', '', '', '', '', '', '', '', '']","['alignment models in general in gross of and the optimal likelihood parameters learned often do not correspond to alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model #TAUTHOR_TAG .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '', '', '', '', '', '', '', '', '']","['Word alignment models in general in of and the optimal likelihood parameters learned often do not correspond to alignments', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model #TAUTHOR_TAG .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '', '', '', '', '', '', '', '', '']","['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; Och and Ney 2003 ) , and more recently by the LEAF model #TAUTHOR_TAG .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '', '', '', '', '', '', '', '', '']",1
['5 The open source Moses #TAUTHOR_TAG toolkit from www.statmt.org/moses/ .'],['5 The open source Moses #TAUTHOR_TAG toolkit from www.statmt.org/moses/ .'],['5 The open source Moses #TAUTHOR_TAG toolkit from www.statmt.org/moses/ .'],['5 The open source Moses #TAUTHOR_TAG toolkit from www.statmt.org/moses/ .'],5
"['', '', '', '', '', '', '', '', 'bank and parses forian', 'Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles #TAUTHOR_TAG , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', '']","['', '', '', '', '', '', '', '', 'Treebank and parses for Bulgarian', 'Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles #TAUTHOR_TAG , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', '']","['', '', '', '', '', '', '', '', 'and parses forian', 'shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles #TAUTHOR_TAG , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', '']","['', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG , we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles #TAUTHOR_TAG , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings ( Koehn 2005 ) .', '']",5
"['M', '', ', [ K , andcu 3 ] and rules [ Galley et al.04 ; #TAUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'importance machine translation transferring annotations betweensky and Ng1;5;Gancheasesny']","['', '', ', [ , and Marcu ] and rules [ Galley et al. ; #TAUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'importance machine translation: transferring annotations between and Ngai']","['M', '', ', [ Koehn , andcu 2003 ] and rules [ Galley et al.04 ; #TAUTHOR_TAG ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'importance machine translation:, transferring annotations betweensky and Ngasesny']","['', '', '', '']",0
"['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding #TAUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'this we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'accumulation of probability from several low-scoring align that agree on one alignment link', '', '', '', '', '', '', '']","['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding #TAUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'this we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'accumulation of probability from several low-scoring alignments that agree on one alignment link.', '', '', '', '', '', '', '']","['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding #TAUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'the accumulation of probability from several low-scoring alignments that agree on one alignment link.', '', '', '', '', '', '', '']","['Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding #TAUTHOR_TAG ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .', 'Using this decoding we include an alignment link i − j if the posterior probability that word i aligns to word j is above some threshold.', 'This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link.', '', '', '', '', '', '', '']",5
"['', '', '', '', 'We used a standard implementation of IBM Model 4 #TAUTHOR_TAG and the existing code is not we could not use the same stopping criterion to avoid overfitting and we not to produce precision/recall curves', '']","['', '', '', '', 'We used a standard implementation of IBM Model 4 #TAUTHOR_TAG and the existing code is not we could not use the same stopping criterion to avoid overfitting and we not to produce precision/recall curves', '']","['', '', '', '', 'We used a standard implementation of IBM Model 4 #TAUTHOR_TAG and changing the existing code is not we could not use the same stopping criterion to avoid overfitting and we are not to produce precision/recall curves .', '']","['', '', '', '', 'We used a standard implementation of IBM Model 4 #TAUTHOR_TAG and because changing the existing code is not trivial , we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves .', '']",5
"['M', '', '', '; ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages #TAUTHOR_TAG .']","['', '', '', '; ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages #TAUTHOR_TAG .']","['M', '', '', '; ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages #TAUTHOR_TAG .']","['', '', '', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages #TAUTHOR_TAG .']",4
"['al nature of the gener models used to recover word alignments conflicts with interpretation', '', 'The standard approach is to train two models independently and then intersect their predictions #TAUTHOR_TAG .', 'that much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source', '', '', '']","['directional nature of the generative models used to recover word alignments conflicts with interpretation', '', 'The standard approach is to train two models independently and then intersect their predictions #TAUTHOR_TAG .', 'that much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source).', '', '←', '']","['The directional nature of the generative models used to recover word alignments conflicts with interpretation', '', 'The standard approach is to train two models independently and then intersect their predictions #TAUTHOR_TAG .', 'that better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source', '', '', '']","['The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations.', '', 'The standard approach is to train two models independently and then intersect their predictions #TAUTHOR_TAG .', 'However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree.', 'Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source).', '', '', '']",1
"['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) #TAUTHOR_TAG :']","['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) #TAUTHOR_TAG :']","['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) #TAUTHOR_TAG :']","['Because of the latent alignment variables z, the log-likelihood function for the HMM model is not concave, and the model is fit using the Expectation Maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).', 'EM maximizes G ( 0 ) via block-coordinate ascent on a lower bound F ( q , 0 ) using an auxiliary distribution over the latent variables q ( z | x , y ) #TAUTHOR_TAG :']",5
"['alignment models in general and the in grossimpl of and the optimal likelihood parameters learned often do not correspond to alignments', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #TAUTHOR_TAG , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors', '', '', '', '', '', '', '', '', '']","['alignment models in general and the in gross of and the optimal likelihood parameters learned often do not correspond to alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #TAUTHOR_TAG , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '', '', '', '', '', '', '', '', '']","['Word alignment models in general and in of and the optimal likelihood parameters learned often do not correspond to alignments', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #TAUTHOR_TAG , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors', '', '', '', '', '', '', '', '', '']","['Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments.', 'One solution to this problem is to add more complexity to the model to better reflect the translation process.', 'This is the approach taken by IBM Models 4 + ( Brown et al. 1993b ; #TAUTHOR_TAG , and more recently by the LEAF model ( Fraser and Marcu 2007 ) .', 'Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors.', '', '', '', '', '', '', '', '', '']",1
"['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #TAUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every probability η > 1', 'transitions', '', '', '', '', '', '', '', '', '']","['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #TAUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every probability η > 1.', 'transitions', '', '', '', '', '', '', '', '', '']","['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #TAUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability a constant η > 1', 'more transitions', '', '', '', '', '', '', '', '', '']","['The idea of introducing constraints over a model to better guide the learning process has appeared before.', 'In the context of word alignment , #TAUTHOR_TAG use a state-duration HMM in order to model word-to-phrase translations .', 'The fertility of each source word is implicitly encoded in the durations of the HMM states.', 'Without any restrictions, likelihood prefers to always use longer phrases and the authors try to control this behavior by multiplying every transition probability by a constant η > 1.', '', '', '', '', '', '', '', '', '', '']",0
"['', '', '', '', '', '', '', '', 'and', 'Figure 10 shows our results for transferring from Englishian (En→Bg) and English Spanish (En→).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings #TAUTHOR_TAG .', 'in Figure  models trained using posterior']","['', '', '', '', '', '', '', '', 'and', 'Figure 10 shows our results for transferring from English Bulgarian (En→Bg) and English Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings #TAUTHOR_TAG .', 'in Figure models trained using posterior']","['', '', '', '', '', '', '', '', 'and', 'shows our results for transferring from Englishian (En→Bg) and English Spanish (En→).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings #TAUTHOR_TAG .', 'in Figure 10 the models trained using']","['', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG , we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En→Bg) and from English to Spanish (En→Es).', 'results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En â\x86\x92 Es results are based on a corpus of parliamentary proceedings #TAUTHOR_TAG .', '']",5
"['are whereas applications a', '', '', '', '', '', '', '', 'this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union #TAUTHOR_TAG .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, the', '', '']","['are whereas applications a', '', '', '', '', '', '', '', 'this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union #TAUTHOR_TAG .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, the', '', '']","['are whereas most applications', '', '', '', '', '', '', '', 'this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union #TAUTHOR_TAG .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, the differences', '', '']","['', '', '', '', '', '', '', '', 'In this work we followed a more principled approach that uses the knowledge about the posterior distributions of each directional model.', 'We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold.', 'This heuristic is called soft union #TAUTHOR_TAG .', 'Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.', 'The posterior regularization-trained models still performed better, but the differences get smaller after doing the symmetrization.', '', '']",5
"['M', '', '', 'their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #TAUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 8 )']","['', '', '', 'their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #TAUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay )']","['M', '', '', 'their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #TAUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 )']","['', '', '', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; #TAUTHOR_TAG ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4
"['for a pair the between words in a and a (.3', 'ato-word (to-) correspondence is not every aux in., English He walked and French Il est allé), in), English German Massenichtungen), and expressions indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations #TAUTHOR_TAG .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', '', '', '']","['for a pair the between words in a and a al.', 'a word-to-word (1-to-1) correspondence is not every auxiliary in English He walked and French Il est allé), in English German Massenvernichtungswaffen), and expressions indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations #TAUTHOR_TAG .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', '', '', '']","['for a parallel sentence pair the correspondence between words in and (.3', 'a simple word-to-word (to-) correspondence is not in., English He walked and French Il est allé), articles in English weapons German Massenvernichtungswaffen), and expressions indirectly.', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations #TAUTHOR_TAG .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', '', '', '']","['', '', 'Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations #TAUTHOR_TAG .', 'The top row of Figure 1 shows two word alignments between an English-French sentence pair.', '', '', '']",0
"['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final #TAUTHOR_TAG .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'theuristic is used, to to transfer', '', '', '', '', '', '', '', '', '']","['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final #TAUTHOR_TAG .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'the heuristic is used, to to transfer', '', '', '', '', '', '', '', '', '']","['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final #TAUTHOR_TAG .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', 'the intersection heuristic is normally used, to to transfer', '', '', '', '', '', '', '', '', '']","['As discussed earlier, the word alignment models are asymmetric, whereas most applications require a single alignment for each sentence pair.', 'Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment.', 'For MT the most commonly used heuristic is called grow diagonal final #TAUTHOR_TAG .', 'This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.', 'The alignment produced has high recall relative to the intersection and only slightly lower recall than the union.', '', '', '', '', '', '', '', '', '', '']",1
"['PR is closely related to the work of #TAUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', ', the posteriors of the model on unlabeled data are regular directly', 'data and an ""expectation regularization"" penalty term on the unlabeled data']","['PR is closely related to the work of #TAUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', 'framework, the posteriors of the model on unlabeled data are regularized directly.', 'data and an ""expectation regularization"" penalty term on the unlabeled data:']","['PR is closely related to the work of #TAUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', ', the posteriors of the model on unlabeled data are regularized directly', 'data and an ""expectation regularization"" penalty term on the unlabeled data:']","['PR is closely related to the work of #TAUTHOR_TAG , 2008 ) , who concurrently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning .', 'They call their method generalized expectation (GE) constraints or alternatively expectation regularization.', 'In the original GE framework, the posteriors of the model on unlabeled data are regularized directly.', 'They train a discriminative model, using conditional likelihood on labeled data and an ""expectation regularization"" penalty term on the unlabeled data:']",0
"['M', '', '', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages #TAUTHOR_TAG ; Hwa et al. 005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay']","['', '', '', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages #TAUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay']","['M', '', '', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages #TAUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzil']","['', '', '', 'But their importance has grown far beyond machine translation : for instance , transferring annotations between languages #TAUTHOR_TAG ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .']",4
"['sem work b) introduced a seriesabilistic modelsM ) machine', '', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #TAUTHOR_TAG ; Chiang et al.2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'transferasesny']","['seminal work b) introduced a series probabilistic models machine', '', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #TAUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'transferring']","['The seminal work #AUTHOR_TAG b) introduced a seriesabilistic modelsM', '', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #TAUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', 'asesny']","['', '', 'Word alignments are used primarily for extracting minimal translation units for machine translation ( MT ) ( e.g. , phrases [ Koehn , Och , and Marcu 2003 ] and rules [ #TAUTHOR_TAG ; Chiang et al. 2005 ] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006).', '']",0
"['of introducing constraints over a model to better guide the learning process has appeared before.', '', '', '', '', ""For the task of unsupervised dependency parsing , #TAUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", '', 'two the of controlling un and', '', '', '', '', '']","['of introducing constraints over a model to better guide the learning process has appeared before.', '', '', '', '', ""For the task of unsupervised dependency parsing , #TAUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", 'δ', 'two the of controlling and', '', '', '', '', '']","['of introducing constraints over a model to better guide the learning process has appeared before.', '', '', '', '', ""For the task of unsupervised dependency parsing , #TAUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", '', 'the goal of and', '', '', '', '', '']","['The idea of introducing constraints over a model to better guide the learning process has appeared before.', '', '', '', '', ""For the task of unsupervised dependency parsing , #TAUTHOR_TAG add a constraint of the form `` the average length of dependencies should be X '' to capture the locality of syntax ( at least half of the dependencies are between adjacent words ) , using a scheme they call structural annealing ."", ""They modify the model's distribution over trees p θ (y) by a penalty term as: p θ (y) ∝ p θ (y)e (δ e∈y length(e)) , where length(e) is the surface length of edge e."", '', '', '', '', '', '', '']",0
"['', '', 'We use the agreement checker code developed by #TAUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-ser using', '', '', '', '', '', '', '', '', '', '', '', '']","['', '', 'We use the agreement checker code developed by #TAUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using', '', '', '', '', '', '', '', '', '', '', '', '']","['', '', 'We use the agreement checker code developed by #TAUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using', '', '', '', '', '', '', '', '', '', '', '', '']","['', '', 'We use the agreement checker code developed by #TAUTHOR_TAG and evaluate our baseline ( MaltParser using only CORE12 ) , best performing model ( Easy-First Parser using CORE12 + DET+LMM+PERSON+FN * NGR g + p ) , and the gold reference .', '', '', '', '', '', '', '', '', '', '', '', '']",5
"['', '', 'In comparison, the tag set of the Buckwalter Morphological Analyzer #TAUTHOR_TAG used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006;, Das, and McDonald 2', '', '', '']","['', '', 'In comparison, the tag set of the Buckwalter Morphological Analyzer #TAUTHOR_TAG used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006; Das, and McDonald', '', '', '']","['', '', 'In comparison, the tag set of the Buckwalter Morphological Analyzer #TAUTHOR_TAG used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006; Petrov, Das, and', '', '', '']","['', '', 'In comparison, the tag set of the Buckwalter Morphological Analyzer #TAUTHOR_TAG used in the PATB has a core POS set of 44 tags (CORE44) before mor- phological extension.8', 'Cross-linguistically, a core set containing around 12 tags is often assumed as a �universal tag set� (Rambow et al. 2006; Petrov, Das, and McDonald 2012).', '', '', '']",1
"['For better comparison with work of others , we adopt the suggestion made by #TAUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', 'in Table 14', '', '', '', '']","['For better comparison with work of others , we adopt the suggestion made by #TAUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', 'in Table 14.', '', '', '', '']","['For better comparison with work of others , we adopt the suggestion made by #TAUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', 'in Table 14', '', '', '', '']","['For better comparison with work of others , we adopt the suggestion made by #TAUTHOR_TAG to evaluate the parsing quality on sentences up to 70 tokens long .', '', '', '', '', '']",5
"['As for work onic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and M 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #TAUTHOR_TAG and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #TAUTHOR_TAG and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work onic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #TAUTHOR_TAG and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; #TAUTHOR_TAG and the CATiB ( Habash and Roth 2009 ) .', '', '', '']",0
"['We use the Columbia Arabic Treebank ( CATiB ) #TAUTHOR_TAG .', 'use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper noun), VRB (active-), VRB-PASSpass- ver prepos or conj', 'CATiB a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object pre); for', '']","['We use the Columbia Arabic Treebank ( CATiB ) #TAUTHOR_TAG .', 'use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice VRB-PASS prepositions or', 'CATiB a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, for', '']","['We use the Columbia Arabic Treebank ( CATiB ) #TAUTHOR_TAG .', 'we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper noun), VRB (active-voice verbs), VRB-PASSpass- prepos or conj', 'CATiB a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, pre for', '']","['We use the Columbia Arabic Treebank ( CATiB ) #TAUTHOR_TAG .', 'Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information.', ""CATiB's dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations."", 'It has a reduced POS tag set consisting of six tags only (henceforth CATIB6).', 'The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX (punctuation).', '', '']",5
"['Different languages vary with respect to which features may be most helpful given various among three.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing #TAUTHOR_TAG : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivuslavsky Idin']","['Different languages vary with respect to which features may be most helpful given various among three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing #TAUTHOR_TAG : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, Iomdin']","['Different languages vary with respect to which features may be most helpful given among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing #TAUTHOR_TAG : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre,uslavsky Id']","['Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors.', 'In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy.', 'For example , modeling CASE in Czech improves Czech parsing #TAUTHOR_TAG : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .', 'It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008;Nivre, Boguslavsky, and Iomdin 2008;.']",4
"['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 #TAUTHOR_TAG , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', '', '', '']","['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 #TAUTHOR_TAG , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', '', '', '']","['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 #TAUTHOR_TAG , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', '', '', '']","['For all the experiments reported in this article , we used the training portion of PATB Part 3 v3 .1 #TAUTHOR_TAG , converted to the CATiB Treebank format , as mentioned in Section 2.5 .', '', '', '']",5
"['18 In this article , we use a newer version of the corpus by #TAUTHOR_TAG than the one we used in Marton , Habash , and #AUTHOR_TAG .']","['18 In this article , we use a newer version of the corpus by #TAUTHOR_TAG than the one we used in Marton , Habash , and #AUTHOR_TAG .']","['18 In this article , we use a newer version of the corpus by #TAUTHOR_TAG than the one we used in Marton , Habash , and #AUTHOR_TAG .']","['18 In this article , we use a newer version of the corpus by #TAUTHOR_TAG than the one we used in Marton , Habash , and #AUTHOR_TAG .']",5
"['', '', '', '', '', '', '', '', '', '', 'functional In this article , we use an in-house system which provides functional gender , number , and rationality features #TAUTHOR_TAG . ']","['', '', '', '', '', '', '', '', '', '', 'the functional In this article , we use an in-house system which provides functional gender , number , and rationality features #TAUTHOR_TAG . See Section']","['', '', '', '', '', '', '', '', '', '', 'functional inf In this article , we use an in-house system which provides functional gender , number , and rationality features #TAUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', '', '']",5
"['As for work onic ( MSA , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 26 ; Diab 2 ; Green and M 20 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB #TAUTHOR_TAG .', 'the PATB for annotation consistency an enhanced- gram for', '', '']","['As for work on Arabic ( MSA , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB #TAUTHOR_TAG .', 'the PATB for annotation consistency, an enhanced for', '', '']","['As for work onic ( MSA , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB #TAUTHOR_TAG .', 'the PATB for annotation consistency, for', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB #TAUTHOR_TAG .', '', '', '']",0
"['', '', '', 'the to use functional', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser #TAUTHOR_TAG ( Section 6 ) .', '21 certain attributes equal node', '', '']","['', '', '', 'the to use functional', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser #TAUTHOR_TAG ( Section 6 ) .', '21 certain attributes equal node', '', '']","['', '', '', 'the to use functional features', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser #TAUTHOR_TAG ( Section 6 ) .', '2 certain attributes equal a node', '', '']","['', '', '', '', 'Furthermore , we demonstrate that our results carry over successfully to another parser , the Easy-First Parser #TAUTHOR_TAG ( Section 6 ) .', '', '', '']",5
"['The Elixirxical resource used previously provided functionalBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality #TAUTHOR_TAG . This the resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender )', '', '', '', '', '', '', '']","['The ElixirFM lexical resource used previously provided functional feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality #TAUTHOR_TAG This the resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender )', '', '', '', '', '', '', '']","['The ElixirFMxical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality #TAUTHOR_TAG . This the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender )', '', '', '', '', '', '', '']","['The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values.', 'To address this issue , we use a version of the PATB3 training and dev sets manually annotated with functional gender , number , and rationality #TAUTHOR_TAG .18 This is the first resource providing all three features ( ElixirFm only provides functional number , and to some extent functional gender ) .', '', '', '', '', '', '', '']",5
"['', '', '', 'the trade between relevance and accu- r.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit #TAUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It B', '', '', '']","['', '', '', 'the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit #TAUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'It', '', '', '']","['', '', '', 'the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit #TAUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', 'B', '', '', '']","['', '', '', 'Put differently, we are interested in the tradeoff between relevance and accu- racy.Therefore, we repeated the experiments with POS tags predicted by the MADA toolkit #TAUTHOR_TAG ; Habash, Rambow, and Roth 2012)15 (see Table 2, columns 5�7).', '', '', '', '']",5
"['All results are reported mainly in terms of labeled attachment accuracy (the parent word and the type of dependency relation to it, abbreviated as LAS), is also used for greedy (hill-climbing) decisions for feature combination', 'ed attachment accuracyAS) and label accuracy (dependency relation regardless) also given', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TAUTHOR_TAG ."", 'We p < 0.05 and p < 0.01 with + ++ respectively.']","['All results are reported mainly in terms of labeled attachment accuracy (the parent word and the type of dependency relation to it, abbreviated as LAS), is also used for greedy (hill-climbing) decisions for feature combination.', 'attachment accuracy (UAS) and label accuracy (dependency relation regardless also given.', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TAUTHOR_TAG ."", 'We p < 0.05 and p < 0.01 with + ++ respectively.']","['All results are reported mainly in terms of labeled attachment accuracy (the parent word and the type of dependency relation to it, abbreviated as LAS), is also used for greedy (hill-climbing) decisions for feature combination.', 'ed attachment accuracy scoreAS) and label accuracy (dependency relation regardless are also given', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TAUTHOR_TAG ."", 'We denote p < 0.05 and p < 0.01 with + ++ respectively.']","['All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as LAS), which is also used for greedy (hill-climbing) decisions for feature combination.', 'Unlabeled attachment accuracy score (UAS) and label accuracy (dependency relation regardless of parent, LS) are also given.', ""For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and #TAUTHOR_TAG ."", 'We denote p < 0.05 and p < 0.01 with + and ++ , respectively.']",5
"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #TAUTHOR_TAG , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use ""dev set', '', '', '', '', '', '', '', '', '', '', '']","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #TAUTHOR_TAG , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use ""dev set""', '', '', '', '', '', '', '', '', '', '', '']","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #TAUTHOR_TAG , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set""', '', '', '', '', '', '', '', '', '', '', '']","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; KÃ¼bler , McDonald , and #TAUTHOR_TAG , a transition-based parser with an input buffer and a stack , which uses SVM classifiers We use the term ""dev set"" to denote a non-blind test set, used for model development (feature selection and feature engineering).', '', '', '', '', '', '', '', '', '', '', '']",5
"['ixir used functional GENDationality', '', 'the resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource #TAUTHOR_TAG .', 'Table', '', '', '', '', '']","['ElixirFM used functional GENDER', '', 'the resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource #TAUTHOR_TAG', 'Table', '', '', '', '', '']","['used previouslyationality', '', 'the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource #TAUTHOR_TAG .', '', '', '', '', '', '']","['', '', 'This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender).', 'We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource #TAUTHOR_TAG .19', '', '', '', '', '', '']",5
"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) #TAUTHOR_TAG , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) #TAUTHOR_TAG , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) #TAUTHOR_TAG , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) #TAUTHOR_TAG , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', '', '', '']",1
"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional, and not mark rational; this includes the Pennic TreebankPATB) (), the Buckter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit #TAUTHOR_TAG ; Habash, Rambow, and Roth 2012).', 'The Elixir', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and not mark rationality; this includes the Penn Arabic Treebank (PATB) the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit #TAUTHOR_TAG ; Habash, Rambow, and Roth 2012).', 'The Elixir-FM', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rational; this includes the Penn Arabic TreebankPATB) (), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit #TAUTHOR_TAG ; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit #TAUTHOR_TAG ; Habash, Rambow, and Roth 2012).', '', '', '']",1
"[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) #TAUTHOR_TAG and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the-performing tag set for Arabic on predicted in Section4 ; ( c Buck""]","[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) #TAUTHOR_TAG and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted in Section 4 ; ( c""]","[""The following are the various tag sets we use in this article : ( a ) the core POS tag sets CORE44 and the newly introduced CORE12 ; ( b ) CATiB Treebank tag set ( CATIB6 ) #TAUTHOR_TAG and its newly introduced extension of CATIBEX created using simple regular expressions on word form , indicating particular morphemes such as the prefix JI Al + or the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values in Section 4 ; ( c Buck""]",[''],5
"['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer #TAUTHOR_TAG , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer #TAUTHOR_TAG , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer #TAUTHOR_TAG , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', 'The Elixir-FM analyzer', '', '']","['Most available Arabic NLP tools and resources model morphology using form- based (�surface�) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer #TAUTHOR_TAG , and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012).', '', '', '']",1
"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) #TAUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) #TAUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) #TAUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) #TAUTHOR_TAG ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']",0
"['most', '', '', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #TAUTHOR_TAG .']","['most', '', '', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #TAUTHOR_TAG .']","['', '', '', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #TAUTHOR_TAG .']","['', '', '', 'It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #TAUTHOR_TAG .']",4
"['', '', 'SA having a rich agreement system both verb-subject and noun-adject relations', 'The result holds for both the MaltParser #TAUTHOR_TAG and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']","['', '', 'MSA having a rich agreement system, both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser #TAUTHOR_TAG and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']","['', '', 'SA having a rich agreement system, covering both verb-subject and noun-adject', 'The result holds for both the MaltParser #TAUTHOR_TAG and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']","['', '', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser #TAUTHOR_TAG and the Easy-First Parser ( Goldberg and Elhadad 2010 ) .']",5
"[""OS setsRE and the b ) CATiB Treebank tag set ( CATIB6 ) ( Hab and Roth 2009 ) and its newly introduced of CATIBEX created using on form indicating morhe the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) #TAUTHOR_TAG ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both""]","[""POS sets and the b ) CATiB Treebank tag set ( CATIB6 ) ( Habash and Roth 2009 ) and its newly introduced of CATIBEX created using on form indicating morphemes the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) #TAUTHOR_TAG ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both""]","[""and the b ) CATiB Treebank tag set ( CATIB6 ) ( Hab and Roth 2009 ) and its newly introduced of CATIBEX created using on word form indicating particular morphemes the suffix v ' + wn ; this tag set is the best-performing tag set for Arabic on predicted values as reported in Section 4 ; ( c ) the PATB full tag set with complete morphological tag ( BW ) #TAUTHOR_TAG ; and two extensions of the PATB reduced tag set ( PENN POS , a.k.a. RTS , size 24 [ Diab , Hacioglu , and Jurafsky 2004 ] ) , both""]",[''],5
"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', ' #TAUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'results for', '', '', '', '', '', '']","['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', ' #TAUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'results for', '', '', '', '', '', '']","['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', ' #TAUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', 'The results for', '', '', '', '', '', '']","['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', ' #TAUTHOR_TAG report that an optimal tag set for parsing Czech consists of a basic POS tag plus a CASE feature ( when applicable ) .', 'This tag set (size 58) outperforms the basic Czech POS tag set (size 13) and the complete tag set (size ≈3000+).', 'They also report that the use of gender, number, and person features did not yield any improvements.', '', '', '', '', '', '', '']",0
"['9 We do not relate to specific results in their study because it has been brought to our attention that #TAUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']","['9 We do not relate to specific results in their study because it has been brought to our attention that #TAUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']","['9 We do not relate to specific results in their study because it has been brought to our attention that #TAUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']","['9 We do not relate to specific results in their study because it has been brought to our attention that #TAUTHOR_TAG are in the process of rechecking their code for errors , and rerunning their experiments ( personal communication ) .']",1
"['', '', 'SA having a rich agreement system, both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser #TAUTHOR_TAG .']","['', '', 'MSA having a rich agreement system, both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser #TAUTHOR_TAG .']","['', '', 'SA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser #TAUTHOR_TAG .']","['', '', 'This is likely a result of MSA having a rich agreement system, covering both verb-subject and noun-adjective relations.', 'The result holds for both the MaltParser ( Nivre 2008 ) and the Easy-First Parser #TAUTHOR_TAG .']",5
"['', '', '', '', 'select P tag with only certain', 'A more detailed discussion of the various available Arabic tag sets can be found in #TAUTHOR_TAG .']","['', '', '', '', 'selectively POS tag with only certain', 'A more detailed discussion of the various available Arabic tag sets can be found in #TAUTHOR_TAG .']","['', '', '', '', 'select with', 'A more detailed discussion of the various available Arabic tag sets can be found in #TAUTHOR_TAG .']","['', '', '', '', '', 'A more detailed discussion of the various available Arabic tag sets can be found in #TAUTHOR_TAG .']",0
"['', '', '', '', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #TAUTHOR_TAG and Habash , Faraj , and #AUTHOR_TAG .']","['', '', '', '', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #TAUTHOR_TAG and Habash , Faraj , and #AUTHOR_TAG .']","['', '', '', '', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #TAUTHOR_TAG and Habash , Faraj , and #AUTHOR_TAG .']","['', '', '', '', 'An example CATiB dependency tree is shown in Figure 1.', 'For the corpus statistics, see Table 1.', 'For more information on CATiB , see #TAUTHOR_TAG and Habash , Faraj , and #AUTHOR_TAG .']",0
"['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #TAUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', '', '']","['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #TAUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', '', '']","['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #TAUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', '', '']","['So far, we have only evaluated models trained on gold POS tag set and morphological feature values.', 'Some researchers , however , including #TAUTHOR_TAG , train on predicted feature values instead .', 'It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test.', '', '']",1
"['19 The paper by #TAUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']","['19 The paper by #TAUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']","['19 The paper by #TAUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']","['19 The paper by #TAUTHOR_TAG presents additional , more sophisticated models that we do not use in this article .']",1
"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #TAUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #TAUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #TAUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; #TAUTHOR_TAG ; Green and Manning 2010 ) , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']",0
"['11 #TAUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Niv �standard� algorithm is also reported there to do better on Arabic']","['11 #TAUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Nivre �standard� algorithm is also reported there to do better on Arabic,']","['11 #TAUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', 'The Nivre �standard� algorithm is also reported there to do better on Arabic']","['11 #TAUTHOR_TAG reports that non-projective and pseudo-projective algorithms outperform the `` eager projective algorithm in MaltParser , but our training data did not contain any non-projective dependencies .', '']",1
"['', '', '', 'alt definedk at buf[0], and so on K�bler, McDonald, and #TAUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.', '', '', '', '', '', '']","['', '', '', 'defined at buf[0], and so on. K�bler, McDonald, and #TAUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.13', '', '', '', '', '', '']","['', '', '', 'are definedk at buf[0], and so on. K�bler, McDonald, and #TAUTHOR_TAG describe a �typical� MaltParser model configuration of attributes and features.', '', '', '', '', '', '']","['', '', '', '', '', '', '', '', '', '']",5
"['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #TAUTHOR_TAG , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #TAUTHOR_TAG , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #TAUTHOR_TAG , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']","['As for work on Arabic ( MSA ) , results have been reported on the PATB ( Kulick , Gabbard , and Marcus 2006 ; Diab 2007 ; #TAUTHOR_TAG , the Prague Dependency Treebank ( PADT ) ( Buchholz and Marsi 2006 ; Nivre 2008 ) and the CATiB ( Habash and Roth 2009 ) .', '', '', '']",0
"['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 #TAUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , Nivre  , buffer', '', '', '', '', '', '', '', '', '', '', '']","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 #TAUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , Nivre , buffer', '', '', '', '', '', '', '', '', '', '', '']","['For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 #TAUTHOR_TAG , 2008 ; KÃ¼bler , McDonald , Niv ,', '', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', '', '', '', '', '', '', '', '']",5
"['Greenend Mars', ' #AUTHOR_TAG analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on Arabic parsing using his MaltParser #TAUTHOR_TAG , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']","['Marsi', ' #AUTHOR_TAG analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on Arabic parsing using his MaltParser #TAUTHOR_TAG , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']","['Mars', ' #AUTHOR_TAG analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on Arabic parsing using his MaltParser #TAUTHOR_TAG , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']","['', ' #AUTHOR_TAG analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.', ' #AUTHOR_TAG reports experiments on Arabic parsing using his MaltParser #TAUTHOR_TAG , trained on the PADT .', 'His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.']",0
"['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', '', '', '', '', 'Table3', 'Similarly , #TAUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We', '']","['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', '', '', '', '', 'Table', 'Similarly , #TAUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We', '']","['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', '', '', '', '', 'Table3', 'Similarly , #TAUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', 'We', '']","['Much work has been done on the use of morphological features for parsing of morphologically rich languages.', '', '', '', '', '', 'Similarly , #TAUTHOR_TAG report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .', 'Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features.', '', '']",0
"['7 We ignore the rare ""false idafa"" construction #TAUTHOR_TAG , p. 102 ) .']","['7 We ignore the rare ""false idafa"" construction #TAUTHOR_TAG , p. 102 ) .']","['7 We ignore the rare ""false idafa"" construction #TAUTHOR_TAG , p. 102 ) .']","['7 We ignore the rare ""false idafa"" construction #TAUTHOR_TAG , p. 102 ) .']",0
"['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser #TAUTHOR_TAG .', 'Section 4 all models evaluated on nongoldmachine-icted feature values', 'Easy-First Parser is a shift-reduce parser MaltParser', '', '', '']","['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser #TAUTHOR_TAG .', 'Section 4, all models evaluated on non-gold (machine-predicted) feature values.', 'Easy-First Parser is a shift-reduce parser MaltParser).', '', '', '']","['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser #TAUTHOR_TAG .', 'Section 4, all models are evaluated on nongoldmachine-icted) feature values.', 'The Easy-First Parser is a shift-reduce parser MaltParser', '', '', '']","['In this section , we validate the contribution of key tag sets and morphological features -- and combinations thereof -- using a different parser : the Easy-First Parser #TAUTHOR_TAG .', 'As in Section 4, all models are evaluated on both gold and non-gold (machine-predicted) feature values.', 'The Easy-First Parser is a shift-reduce parser (as is MaltParser).', '', '', '']",5
"['Our with on and in the definite is for', '', 'Previous work with MaltParser in , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #TAUTHOR_TAG .', 'Our work is the first to usingalt and in Arab the', '', '', '', '']","['Our with on and in the definite is for', '', 'Previous work with MaltParser in , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #TAUTHOR_TAG .', 'Our work is the first to using and in Arabic the', '', '', '', '']","['Our with on and in is for', '', 'Previous work with MaltParser in , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #TAUTHOR_TAG .', 'Our work is the first to gains usingalt and in Arab the', '', '', '', '']","['', '', 'Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; #TAUTHOR_TAG .', '', '', '', '', '']",1
"['', '', '', '', 'Gold 6', ' #TAUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs', 'We expect this kind of feature to yield lower gains for Arabic r one functional feature values']","['', '', '', '', '6).', ' #TAUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', 'We expect this kind of feature to yield lower gains for Arabic, r one functional feature values']","['', '', '', '', 'GoldSection 6).', ' #TAUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', 'We expect this kind of feature to yield lower gains for Arabic: r one functional feature values']","['', '', '', '', '', ' #TAUTHOR_TAG have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .', 'These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.', '']",0
"['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #TAUTHOR_TAG .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in un way.', '', '', '', '', '']","['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #TAUTHOR_TAG .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in way.', '', '', '', '', '']","['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #TAUTHOR_TAG .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', '', '', '', '', '']","['This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; #TAUTHOR_TAG .', 'In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way.', '', '', '', '', '']",2
"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et. 1980 , Dixon and 1979 , Erman et al.1980 , Haton and Pierrel 1976 , Lea 1980 , #TAUTHOR_TAG , Medress 1980 , Reddy 1976 , 1978 , and ', 'these', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #TAUTHOR_TAG , Medress 1980 , Reddy 1976 , 1978 , and', 'these', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et. 1980 , Dixon and Martin 1979 , Erman et al.1980 , Haton and Pierrel 1976 , Lea 1980 , #TAUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and', 'these efforts', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , #TAUTHOR_TAG , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '', '', '']",1
"['Though the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #TAUTHOR_TAG .""]","['Though the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #TAUTHOR_TAG .""]","['Though the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #TAUTHOR_TAG .""]","['Though the implemented system is limited to matrix-oriented problems, the theoretical system is capable of learning a wide range of problem types.', 'The only requirement on the problem or situation is that it can be entered into the expectation system in the form of examples.', ""Thus , for example , it can acquire a `` script '' such as the one for going to a restaurant as defined in #TAUTHOR_TAG .""]",0
"['', '', '.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of #TAUTHOR_TAG .']","['', '', 'structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of #TAUTHOR_TAG .']","['', '', '', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of #TAUTHOR_TAG .']","['', '', '', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #AUTHOR_TAG and the PROLOG synthesis method of #TAUTHOR_TAG .']",1
"[""¢ use of low level knowledge from the speech recognition phase ,¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG ,\x80¢ a `` conditioning '' facility as described by #TAUTHOR_TAG , â¢ implementation of new""]","[""use of low level knowledge from the speech recognition phase , use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by #TAUTHOR_TAG , â\x80¢ implementation of new""]","[""¢ use of low level knowledge from the speech recognition phase ,¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG ,\x80¢ a `` conditioning '' facility as described by #TAUTHOR_TAG , â¢ implementation of new types""]","[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #AUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by #TAUTHOR_TAG , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]",3
"['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']",1
"['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']",1
"['overview approach', 'details', '', 'useful method', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #TAUTHOR_TAG .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCEink 983).', '']","['overview approach', 'details', '', 'usefulness methodology', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #TAUTHOR_TAG .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']","['an overview the approach', 'The details', '', '', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #TAUTHOR_TAG .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']","['', '', '', '', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) ( Ballard 1979 , #TAUTHOR_TAG .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']",0
"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in #AUTHOR_TAG , or semantic nets as in #TAUTHOR_TAG .', '.', 'is PRO']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in #AUTHOR_TAG , or semantic nets as in #TAUTHOR_TAG .', 'structures.', 'is PROLOG']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in #AUTHOR_TAG , or semantic nets as in #TAUTHOR_TAG .', '', 'is']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in #AUTHOR_TAG , or semantic nets as in #TAUTHOR_TAG .', '', '']",1
"['', '', '', '', '', '', '', '', 'the ""flowcharts in the current project are probabilistic in nature and the problems sentences nodes', 'Another dialogue acquisition system has been developed by #TAUTHOR_TAG .', 'that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user', 'It self activates to bias toward historically observed patterns but is not otherwise observable.']","['', '', '', '', '', '', '', '', 'the ""flowcharts"" in the current project are probabilistic in nature and the problems sentences nodes', 'Another dialogue acquisition system has been developed by #TAUTHOR_TAG .', 'that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias toward historically observed patterns but is not otherwise observable.']","['', '', '', '', '', '', '', '', 'the ""flowcharts"" in the current project are probabilistic in nature and the problems existing nodes', 'Another dialogue acquisition system has been developed by #TAUTHOR_TAG .', 'that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias toward historically observed patterns but is not otherwise observable.']","['', '', '', '', '', '', '', '', 'However, the ""flowcharts"" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed.', 'Another dialogue acquisition system has been developed by #TAUTHOR_TAG .', 'However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction.', 'The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user.', 'It self activates to bias recognition toward historically observed patterns but is not otherwise observable.']",1
"['of speech understanding systems have been developed during fifteen years ( Barnett.ixon Er et.  Haton and Pierrel 1976 , Le  , Lower anddy 980 ,ress 80 ,dy 976 , Walker 1978 , and #TAUTHOR_TAG .', 'Most of these efforts concentrated on a', '', '']","['of speech understanding systems have been developed during fifteen years ( Barnett al. Dixon Erman et al. Haton and Pierrel 1976 , Lea , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #TAUTHOR_TAG .', 'Most of these efforts concentrated on a', '', '']","['of speech understanding systems have been developed during the past fifteen years ( Barnett.ixon Er et.  Haton and Pierrel 1976 , Le , Lower anddy 1980 ,ress 1980 ,dy 1976 , Walker 1978 , and #TAUTHOR_TAG .', 'Most of these efforts concentrated on', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and #TAUTHOR_TAG .', '', '', '']",1
"['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']",1
"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in #TAUTHOR_TAG , or semantic nets as in #AUTHOR_TAG .', '.', 'is']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in #TAUTHOR_TAG , or semantic nets as in #AUTHOR_TAG .', 'structures.', 'is']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in #TAUTHOR_TAG , or semantic nets as in #AUTHOR_TAG .', '', 'is']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #AUTHOR_TAG , assertional statements as in #TAUTHOR_TAG , or semantic nets as in #AUTHOR_TAG .', '', '']",1
"['', '', '', '', '', '', '', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TAUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', '', '', '', '', '']","['', '', '', '', '', '', '', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TAUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', '', '', '', '', '']","['', '', '', '', '', '', '', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TAUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', '', '', '', '', '']","['', '', '', '', '', '', '', 'The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by #TAUTHOR_TAG where program flowcharts were constructed from traces of their behaviors .', '', '', '', '', '']",1
"['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph #TAUTHOR_TAG , a deep parse of Si , or some other representation .', 'A user behavior represented a directed', '']","['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph #TAUTHOR_TAG , a deep parse of Si , or some other representation .', 'A user behavior represented a directed', '']","['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph #TAUTHOR_TAG , a deep parse of Si , or some other representation .', 'A user behavior is represented directed', '']","['We denote the meaning of each sentence Si with the notation M(Si).', 'The exact form of M ( Si ) need not be discussed at this point ; it could be a conceptual dependence graph #TAUTHOR_TAG , a deep parse of Si , or some other representation .', '', '']",0
"['A of speech understanding systems have been developed during the past fifteen years ( Barnett et.9 ,ixon 19 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 198 , Lower and Reddy 1980 , #TAUTHOR_TAG , Reddy 1976 , Walker 978 , and and ', '', '', '']","['A of speech understanding systems have been developed during the past fifteen years ( Barnett et al. , Dixon 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #TAUTHOR_TAG , Reddy 1976 , Walker 1978 , and and', '', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et.9 ,ixon Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lower and Reddy 1980 , #TAUTHOR_TAG , Reddy 1976 , Walker 1978 , and and', '', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , #TAUTHOR_TAG , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '', '', '']",1
"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #TAUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 980 ,ress ,6 ,', '', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #TAUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress , ,', '', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #TAUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , , ,', '', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , #TAUTHOR_TAG , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '', '', '']",1
"['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']",1
"['', '', '', '', '', '', '', 'current system should be distinguished from an earlier voice system ( VNLC , #TAUTHOR_TAG , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']","['', '', '', '', '', '', '', 'current system should be distinguished from an earlier voice system ( VNLC , #TAUTHOR_TAG , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']","['', '', '', '', '', '', '', 'The current system should be distinguished from an earlier voice system ( VNLC , #TAUTHOR_TAG , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']","['', '', '', '', '', '', '', '[ The current system should be distinguished from an earlier voice system ( VNLC , #TAUTHOR_TAG , which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word . ]']",1
"['The expectation parser uses an ATN-like representation for its grammar #TAUTHOR_TAG .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjctions (Bard 9).', '', 'tasks', '']","['The expectation parser uses an ATN-like representation for its grammar #TAUTHOR_TAG .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).', '', 'tasks', '']","['The expectation parser uses an ATN-like representation for its grammar #TAUTHOR_TAG .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjctions (Bard 9).', '', '', '']","['The expectation parser uses an ATN-like representation for its grammar #TAUTHOR_TAG .', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar, imperative sentences with nested noun groups and conjunctions (Ballard 1979).', '', '', '']",5
"['', 'details', '', '', '-the- speech recognition device a Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, B andBallard 19).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #TAUTHOR_TAG .', '']","['', 'details', '', '', 'speech recognition device, a Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #TAUTHOR_TAG .', '']","['', 'The details', '', '', 'An off-the-shelf speech recognition device a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, B andBallard 19).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #TAUTHOR_TAG .', '']","['', '', '', '', 'An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann andBallard 1980).', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation ( VNLCE , #TAUTHOR_TAG .', '']",0
"['A number of speech understanding systems have been developed during the past fifteen years #TAUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 ,ress 980 , 976 , Walker 78 , and and 98 )', 'Most of these efforts concentrated on the level from a a', '', '']","['A number of speech understanding systems have been developed during the past fifteen years #TAUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , 1976 , Walker 1978 , and and 1980 )', 'Most of these efforts concentrated on the level from a a', '', '']","['A number of speech understanding systems have been developed during the past fifteen years #TAUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 ,ress 1980 ,dy 1976 , Walker 1978 , and and )', 'Most of these efforts concentrated on the interaction from', '', '']","['A number of speech understanding systems have been developed during the past fifteen years #TAUTHOR_TAG , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '', '', '']",1
"['', '', 'some of these systems did exhibit expectation capabilities at the sentence level described here for the sake level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #TAUTHOR_TAG .']","['', '', 'some of these systems did exhibit expectation capabilities at the sentence level, described here for the sake level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #TAUTHOR_TAG .']","['', '', 'some of these systems did exhibit expectation capabilities at the sentence level, none described here for the sake dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #TAUTHOR_TAG .']","['', '', 'While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction.', 'A detailed description of the kinds of expectation mechanisms appearing in these systems appears in #TAUTHOR_TAG .']",0
"[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TAUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â¢ implementation of new""]","[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TAUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â\x80¢ implementation of new""]","[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TAUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â¢ implementation of new types""]","[""â\x80¢ use of low level knowledge from the speech recognition phase , â\x80¢ use of high level knowledge about the domain in particular and the dialogue task in general , â\x80¢ a `` continue '' facility and an `` auto-loop '' facility as described by #TAUTHOR_TAG , â\x80¢ a `` conditioning '' facility as described by #AUTHOR_TAG , â\x80¢ implementation of new types of paraphrasing , â\x80¢ checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and â\x80¢ examining inter-speaker dialogue patterns .""]",3
"['The problem of handling ill-formed input has been studied by #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'used', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'used', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', 'used', '', '', '', '', '', '', '', '', '', '']","['The problem of handling ill-formed input has been studied by #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level.', '', '', '', '', '', '', '', '', '', '', '']",1
"['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TAUTHOR_TAG , assertional statements as in #AUTHOR_TAG , or semantic nets as in #AUTHOR_TAG .', '.', '']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TAUTHOR_TAG , assertional statements as in #AUTHOR_TAG , or semantic nets as in #AUTHOR_TAG .', 'structures.', '']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TAUTHOR_TAG , assertional statements as in #AUTHOR_TAG , or semantic nets as in #AUTHOR_TAG .', '', '']","['The VNLCE processor may be considered to be a learning system of the tradition described, for example, in #AUTHOR_TAG .', 'The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in #TAUTHOR_TAG , assertional statements as in #AUTHOR_TAG , or semantic nets as in #AUTHOR_TAG .', '', '']",1
"['approach', 'details', '', 'usefulness of the methodology described above was the of a connected.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) #TAUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (NLCEink 93', '']","['approach', 'details', '', 'usefulness of the methodology described above was the of a connected system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) #TAUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']","['the approach', 'The details', '', 'The usefulness of the methodology described above was tested the implementation of a connected speech understanding.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) #TAUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (NLCE', '']","['', '', '', 'The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system.', 'An off-the-shelf speech recognition device , a Nippon Electric Corporation DP-200 , was added to an existing natural language processing system , the Natural Language Computer ( NLC ) #TAUTHOR_TAG , Biermann and Ballard 1980 ) .', 'The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC.', 'The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983).', '']",0
"['of speech understanding systems have been developed during fifteen ( Barnett.ixon Er et. Haton and Pierrel 1976 , Le  , anddy 9ress980 , #TAUTHOR_TAG , Walker 1978 , and and 9', '', '', '']","['of speech understanding systems have been developed during fifteen ( Barnett al. Dixon Erman et al. Haton and Pierrel 1976 , Lea , and Reddy Medress 1980 , #TAUTHOR_TAG , Walker 1978 , and and', '', '', '']","['of speech understanding systems have been developed during ( Barnett.ixon Er et. Haton and Pierrel 1976 , Le , anddy 1980ress 1980 , #TAUTHOR_TAG , Walker 1978 , and and', '', '', '']","['', '', '', '']",1
"['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions #TAUTHOR_TAG .', 'An attempt has been made to build as deep a parse as that sentences with the meaning identical.', '""meaningresult in identical tasks being performed', '']","['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions #TAUTHOR_TAG .', 'An attempt has been made to build as deep a parse as that sentences with the meaning identical', '""meaning"" ""result in identical tasks being performed.', '']","['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions #TAUTHOR_TAG .', 'An attempt has been made to build as deep a parse as that sentences with the same meaning result identical parses.', 'the same ""meaning"" in identical tasks being performed', '']","['The expectation parser uses an ATN-like representation for its grammar (Woods 1970).', 'Its strategy is top-down.', 'The types of sentences accepted are essentially those accepted by the original NLC grammar , imperative sentences with nested noun groups and conjunctions #TAUTHOR_TAG .', 'An attempt has been made to build as deep a parse as possible so that sentences with the same meaning result in identical parses.', '', '']",0
"['reflected', '', 'unable', 'backs', 'should the the expectation', '', 'that be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will by using the.', 'How it is done is beyond the scope of this paper but is explained in detail in #TAUTHOR_TAG .']","['reflected', '', 'unable', '', 'should the the expectation', '', 'that be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in #TAUTHOR_TAG .']","['', '', 'unable', 'backs', 'should the expectation parser', '', 'that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in #TAUTHOR_TAG .']","['', '', '', '', '', '', 'The comparison that must be made to determine which option is better in a given situation is how often the first will error correct incorrectly as opposed to how much error correcting power we will lose by using the second.', 'How it is done is beyond the scope of this paper but is explained in detail in #TAUTHOR_TAG .']",0
"['', '', '.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #TAUTHOR_TAG and the PROLOG synthesis method of #AUTHOR_TAG .']","['', '', 'structures.', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #TAUTHOR_TAG and the PROLOG synthesis method of #AUTHOR_TAG .']","['', '', '', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #TAUTHOR_TAG and the PROLOG synthesis method of #AUTHOR_TAG .']","['', '', '', 'There is some literature on procedure acquisition such as the LISP synthesis work described in #TAUTHOR_TAG and the PROLOG synthesis method of #AUTHOR_TAG .']",1
"['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al.1980 , #TAUTHOR_TAG , Lea 1980 , Lower and Reddy 980 ,ress 8 , 96 ,', '', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #TAUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress , 1976 ,', '', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al.1980 , #TAUTHOR_TAG , Lea 1980 , Lower and Reddy 1980 ,ress 1980 ,dy 1976 ,', '', '', '']","['A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , #TAUTHOR_TAG , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .', '', '', '']",1
"['We to employ LDO as the machine readable source to aid the development of ax this dictionary several properties which make it uniquely appropriate for use as the knowledge base of a natural language processing system.', ""amount of information concerningras, noun compounds and idioms, the individual, collocational and semantic codes for the entries and the consistent of a 'core' voc in the words."", '( #AUTHOR_TAG contains further description and of L', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and #TAUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']","['We to employ LDOCE as the machine readable source to aid the development of a this dictionary several properties which make it uniquely appropriate for use as the knowledge base of a natural language processing system.', ""amount of information concerning noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent of a 'core' vocabulary in the words dictionary."", '( #AUTHOR_TAG contains further description and of', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and #TAUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']","['We chose to employ LDO as the machine readable source to aid the development of this dictionary several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""of information concerningras, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a 'core' voc in defining the words"", '( #AUTHOR_TAG contains further description and of L', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and #TAUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']","['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #AUTHOR_TAG and #TAUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']",0
"['to relations', '', ' #TAUTHOR_TAG and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', ' #AUTHOR_TAG comprehensively compares different approaches to complementation grammatical theory a touch against which the L scheme be evaluated.']","['to relations', '', ' #TAUTHOR_TAG and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', ' #AUTHOR_TAG comprehensively compares different approaches to complementation grammatical theory a touchstone against which the scheme be evaluated.']","['toantic relations', '', ' #TAUTHOR_TAG and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', ' #AUTHOR_TAG comprehensively compares different approaches to complementation grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']","['', '', ' #TAUTHOR_TAG and #AUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']",0
"['Recent developments in lingu and especially on grammatical theory', '', '', 'research described below is taking place in the context of three collaborative projects #TAUTHOR_TAG to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['Recent developments in and especially on grammatical theory', '', '', 'research described below is taking place in the context of three collaborative projects #TAUTHOR_TAG to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['Recent developments in lingu and especially on grammat', '', '', 'The research described below is taking place in the context of three collaborative projects #TAUTHOR_TAG to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['', '', '', 'The research described below is taking place in the context of three collaborative projects #TAUTHOR_TAG to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']",0
"['lispified L retains the structure the typeset tape', 'each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TAUTHOR_TAG for further discussion ) .', 'the form on the typesetting tape are crucial since they provide clues to the correct structure of this information', 'word are largely defined in terms of word core voc, words ( defined terms this', '', 'the definition of rivet as verb includes the nIVET', 'homograph', '', '', '']","['lispified retains the structure the typesetting tape', 'each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TAUTHOR_TAG for further discussion ) .', 'the formatting on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'word are largely defined in terms of word core vocabulary, words defined terms this', '', 'the definition of rivet as verb includes the noun ""RIVET', 'homograph;', '', '', '']","['The lispified LDOCE file retains the broad structure the typesetting tape', 'each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TAUTHOR_TAG for further discussion ) .', 'the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', 'are largely defined in terms of other words ( defined elsewhere terms', '', 'the definition of rivet as verb includes the noun definitionIVET', 'homograph', '', '', '']","['', 'However , each of these fields requires further decoding and restructuring to provide client programs with easy access to the information they require ( see #TAUTHOR_TAG for further discussion ) .', 'For this purpose the formatting codes on the typesetting tape are crucial since they provide clues to the correct structure of this information.', '', '', '', '', '', '', '']",0
"['capital to sem relations', '', '', ' #TAUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']","['capital to semantic relations', '', '', ' #TAUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']","['capital letters to semantic relations', '', '', ' #TAUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']","['', '', '', ' #TAUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated .']",1
"['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #TAUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system.']","['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #TAUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system.']","['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #TAUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system.']","['Clearly, there are other syntactic and semantic tests for this distinction, (see eg. #TAUTHOR_TAG :472 ), but these are the only ones which are explicit in the LDOCE coding system.']",0
"['occurred wide', '', '', '', '', 'x even those employ very comprehensive grammars.', 'consult relatively smallxicons, typically generated by hand', 'Two exceptions to this generalisation are the Linguistic String Project #AUTHOR_TAG and the IBM CRITIQUE ( formerly EPISTLE ) Project #TAUTHOR_TAG ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'to substantialx from machine read', '', '']","['occurred wide', '', '', '', '', 'even those employ very comprehensive grammars', 'consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project #AUTHOR_TAG and the IBM CRITIQUE ( formerly EPISTLE ) Project #TAUTHOR_TAG ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'to substantial from machine readable', '', '']","['has occurred', '', '', '', '', 'even those employ very comprehensive grammars.', ' #AUTHOR_TAG consult relatively small lexicons, typically generated by hand', 'Two exceptions to this generalisation are the Linguistic String Project #AUTHOR_TAG and the IBM CRITIQUE ( formerly EPISTLE ) Project #TAUTHOR_TAG ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', 'to substantial lexicons from machine read', '', '']","['', '', '', '', '', '', ' #AUTHOR_TAG consult relatively small lexicons, typically generated by hand.', 'Two exceptions to this generalisation are the Linguistic String Project #AUTHOR_TAG and the IBM CRITIQUE ( formerly EPISTLE ) Project #TAUTHOR_TAG ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .', '', '', '']",1
"['gram a a relatively theory sense this further a parsing', '', '', '', '', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #TAUTHOR_TAG .']","['grammar a a relatively theory sense this further a parsing', '', '', '', '', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #TAUTHOR_TAG .']","['a parser a relatively theory neutral representation the sense this representation a format', '', '', '', '', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #TAUTHOR_TAG .']","['', '', '', '', '', 'No attempt has been made to map any closed class entries from LDOCE , as a 3,000 word lexicon containing most closed class items has been developed independently by one of the groups collaborating with us to develop the general purpose morphological and syntactic analyser ( see the Introduction and #TAUTHOR_TAG .']",0
"['We to employ LDO as the machine readable to aid the development of ax this properties make it uniquely for use as the base of a natural language processing system', ""the large amount of information concerning phrasalbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDO.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #TAUTHOR_TAG and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']","['We to employ LDOCE as the machine readable to aid the development of a this properties make it uniquely for use as the base of a natural language processing system.', ""the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #TAUTHOR_TAG and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']","['We chose to employ LDO as the machine readable source to aid the development of this dictionary several properties make it uniquely for use as the core knowledge base of a natural language processing system.', ""the large amount of information concerning phrasalbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDO.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #TAUTHOR_TAG and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']","['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", '( #AUTHOR_TAG contains further description and discussion of LDOCE.)', 'In this paper we focus on the exploitation of the LDOCE grammar coding system ; #TAUTHOR_TAG and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE .']",0
"['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see #AUTHOR_TAG for further comment).', 'One approach to this problem is that taken by the ASCOT project #TAUTHOR_TAG .', ', a new lexicon is being manually derived from LDOCE.', 'The for thex a', 'the of codes for problematic or erroneously labelled words is being corrected tox', '']","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see #AUTHOR_TAG for further comment).', 'One approach to this problem is that taken by the ASCOT project #TAUTHOR_TAG .', 'project, a new lexicon is being manually derived from LDOCE.', 'The for the a', 'the of codes for problematic or erroneously labelled words is being corrected to', '']","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see #AUTHOR_TAG for further comment).', 'One approach to this problem is that taken by the ASCOT project #TAUTHOR_TAG .', 'a new lexicon is being manually derived from LDOCE.', 'for a', 'of codes for problematic or erroneously labelled words is being corrected to', '']","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see #AUTHOR_TAG for further comment).', 'One approach to this problem is that taken by the ASCOT project #TAUTHOR_TAG .', 'In this project, a new lexicon is being manually derived from LDOCE.', '', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', '']",0
"['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system #TAUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', '']","['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system #TAUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', '']","['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system #TAUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', '']","['The output of the transformation program can be used to derive entries which are appropriate for particular grammatical formalisms.', 'To demonstrate that this is possible we have implemented a system which constructs dictionary entries for the PATR-II system #TAUTHOR_TAG and references therein ) .', 'PATR-II was chosen because it has been reimplemented in Cambridge and was therefore, available; however, the task would be nearly identical if we were constructing entries for a system based on GPSG, FUG or LFG.', '']",5
"['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see #AUTHOR_TAG for further comment).', 'One approach to this problem is that taken by the ASCOT project #TAUTHOR_TAG .', ', a new lexicon is being manually derived from LDOCE.', 'The for thex a', 'the of codes for problematic or erroneously labelled words is being corrected tox', '']","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see #AUTHOR_TAG for further comment).', 'One approach to this problem is that taken by the ASCOT project #TAUTHOR_TAG .', 'project, a new lexicon is being manually derived from LDOCE.', 'The for the a', 'the of codes for problematic or erroneously labelled words is being corrected to', '']","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see #AUTHOR_TAG for further comment).', 'One approach to this problem is that taken by the ASCOT project #TAUTHOR_TAG .', 'a new lexicon is being manually derived from LDOCE.', 'for a', 'of codes for problematic or erroneously labelled words is being corrected to', '']","['This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see #AUTHOR_TAG for further comment).', 'One approach to this problem is that taken by the ASCOT project #TAUTHOR_TAG .', 'In this project, a new lexicon is being manually derived from LDOCE.', '', 'More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis.', '']",0
"['occurred wide', '', '', '', '', 'x those employ very comprehensive grammars.', 'consult relatively smallxicons, typically generated hand', 'Two exceptions to this generalisation are the Linguistic String Project #TAUTHOR_TAG and the IBM CRITIQUE ( formerly EPISTLE ) Project #AUTHOR_TAG ; the former emplo a dictionary of 1 , most medical', '', '', '']","['occurred wide', '', '', '', '', 'those employ very comprehensive grammars', 'consult relatively small lexicons, typically generated hand.', 'Two exceptions to this generalisation are the Linguistic String Project #TAUTHOR_TAG and the IBM CRITIQUE ( formerly EPISTLE ) Project #AUTHOR_TAG ; the former employs a dictionary of , most medical', '', '', '']","['has occurred', '', '', '', '', 'those employ very comprehensive grammars.', ' #AUTHOR_TAG consult relatively small lexicons, typically generated hand', 'Two exceptions to this generalisation are the Linguistic String Project #TAUTHOR_TAG and the IBM CRITIQUE ( formerly EPISTLE ) Project #AUTHOR_TAG ; the former emplo a dictionary of , most', '', '', '']","['', '', '', '', '', '', ' #AUTHOR_TAG consult relatively small lexicons, typically generated by hand.', '', '', '', '']",1
"['ational read in of are', '', 'marked', '', '', 'is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TAUTHOR_TAG ;']","['computational readable in of are', '', 'marked', '', '', 'is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TAUTHOR_TAG ;']","['read in of are', '', 'marked', '', '', 'is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TAUTHOR_TAG ;']","['', '', '', '', '', 'Lisp is not particularly well suited for interfacing to complex , structured objects , and it was not our intention to embark on a major effort involving the development of a formal model of a dictionary ( of the style described in , eg. , #TAUTHOR_TAG ; on the other hand a method of access was clearly required , which was flexible enough to support a range of applications intending to make use of the LDOCE tape .']",0
"['The four verbs which are misclassified as Object Equi and which do not have T5 anywhere in their entries are elect, love, represent and require.', 'None of these verbs sents and therefore to beex to our Object Raising', 'In addition , #TAUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb.']","['The four verbs which are misclassified as Object Equi and which do not have T5 anywhere in their entries are elect, love, represent and require.', 'None of these verbs sentential complements and therefore to be to our Object Raising', 'In addition , #TAUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb.']","['The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require.', 'None of these verbs sent and therefore to beex to our Object Raising rule.', 'In addition , #TAUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb.']","['The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require.', 'None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule.', 'In addition , #TAUTHOR_TAG note that our Object Raising rule would assign mean to this category incorrectly .', 'Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e.', '""intend""), however, when it is used in this sense it must be treated as an Object Equi verb.']",1
"['occurred wideaurus', '', '', '', '', '', '', '', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TAUTHOR_TAG for details ) .', 'xicons', '']","['occurred wide', '', '', '', '', '', '', '', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TAUTHOR_TAG for details ) .', '', '']","['has occurred', '', '', '', '', '', '', '', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TAUTHOR_TAG for details ) .', '', '']","['', '', '', '', '', '', '', '', 'In addition , there are a number of projects under way to develop substantial lexicons from machine readable sources ( see #TAUTHOR_TAG for details ) .', '', '']",0
"['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) #AUTHOR_TAG , Lexical Functional Grammar ( LFG ) #TAUTHOR_TAG -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) #AUTHOR_TAG a ) , PATR-II #AUTHOR_TAG -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individualxical items elegantly and efficiently, then the lexicon must be a central component of the parsing system', '', '', '']","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) #AUTHOR_TAG , Lexical Functional Grammar ( LFG ) #TAUTHOR_TAG -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) #AUTHOR_TAG a ) , PATR-II #AUTHOR_TAG -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', '', '', '']","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) #AUTHOR_TAG , Lexical Functional Grammar ( LFG ) #TAUTHOR_TAG -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) #AUTHOR_TAG a ) , PATR-II #AUTHOR_TAG -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', '', '', '']","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) #AUTHOR_TAG , Lexical Functional Grammar ( LFG ) #TAUTHOR_TAG -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) #AUTHOR_TAG a ) , PATR-II #AUTHOR_TAG -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', '', '', '']",0
"['contains', 'highlights a deficiency in the LCE coding since prefer occurs much more naturally with a sentential if it collocates with a modal such as ""w', 'This deficiency is rectified in the verb classification system employed by #TAUTHOR_TAG in the Brandeis verb catalogue .']","['contains', 'highlights a deficiency in the LDOCE coding since prefer occurs much more naturally with a sentential if it collocates with a modal such as ""would"".', 'This deficiency is rectified in the verb classification system employed by #TAUTHOR_TAG in the Brandeis verb catalogue .']","['contains', 'highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as ""w', 'This deficiency is rectified in the verb classification system employed by #TAUTHOR_TAG in the Brandeis verb catalogue .']","['', '', 'This deficiency is rectified in the verb classification system employed by #TAUTHOR_TAG in the Brandeis verb catalogue .']",1
"['Recent developments in lingu and especially on grammatical theory', '', '', 'research described below is taking place in the context of three collaborative projects #TAUTHOR_TAG to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['Recent developments in and especially on grammatical theory', '', '', 'research described below is taking place in the context of three collaborative projects #TAUTHOR_TAG to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['Recent developments in lingu and especially on grammat', '', '', 'The research described below is taking place in the context of three collaborative projects #TAUTHOR_TAG to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']","['', '', '', 'The research described below is taking place in the context of three collaborative projects #TAUTHOR_TAG to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', '']",0
"['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TAUTHOR_TAG , for further details ) .', 'exploiting this information to the be a non-t task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']","['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TAUTHOR_TAG , for further details ) .', 'exploiting this information to the be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']","['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TAUTHOR_TAG , for further details ) .', ', exploiting this information to the would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']","['There are many more distinctions which are conveyed by the conjunction of grammar codes and word qualifiers ( see #TAUTHOR_TAG , for further details ) .', 'However, exploiting this information to the full would be a non-trivial task, because it would require accessing the relevant knowledge about the words contained in the qualifier fields from their LDOCE entries.']",0
"['', '', '', '', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TAUTHOR_TAG .', 'in this sophisticated dictionary']","['', '', '', '', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TAUTHOR_TAG .', 'in this sophisticated dictionary']","['', '', '', '', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TAUTHOR_TAG .', 'in this paper sophisticated']","['', '', '', '', 'In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. #TAUTHOR_TAG .', '']",3
"['occurred wide,', '', '', '', '', 'Few established parsing systems have substantialxicons and even those which employ very comprehensive grammars (eg.', ' #TAUTHOR_TAG consult relatively small lexicons , typically generated by hand .', 'isation', '', '', '']","['occurred wide researchers,', '', '', '', '', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', ' #TAUTHOR_TAG consult relatively small lexicons , typically generated by hand .', 'generalisation', '', '', '']","['has occurred,', '', '', '', '', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', ' #TAUTHOR_TAG consult relatively small lexicons , typically generated by hand .', '', '', '', '']","['', '', '', '', '', 'Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg.', ' #TAUTHOR_TAG consult relatively small lexicons , typically generated by hand .', '', '', '', '']",1
"['', '', '', '', '', '', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #TAUTHOR_TAG , for further discussion ) .']","['', '', '', '', '', '', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #TAUTHOR_TAG , for further discussion ) .']","['', '', '', '', '', '', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #TAUTHOR_TAG , for further discussion ) .']","['', '', '', '', '', '', 'Michiels proposed rules for doing this for infinitive complement codes ; however there seems to be no principled reason not to extend this approach to computing the underlying relations in other types of VP as well as in cases of NP , AP and PP predication ( see #TAUTHOR_TAG , for further discussion ) .']",0
"['We chose to employ LDO as the machine readable source to aid the development of a substantialxicon because this dictionary has several properties which make it uniquely appropriate for use as the knowledge base of a natural language processing system.', ""prominent these the rich gramical subcategorisations of the 6000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", ' #TAUTHOR_TAG contains further description and discussion of LDOCE .', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; #AUTHOR_TAG in utilising different types of information available in LDO']","['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the knowledge base of a natural language processing system.', ""prominent these the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", ' #TAUTHOR_TAG contains further description and discussion of LDOCE .', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; #AUTHOR_TAG in utilising different types of information available in LDOCE.']","['We chose to employ LDO as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""prominent these the rich grammatical subcategorisations of the 6000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", ' #TAUTHOR_TAG contains further description and discussion of LDOCE .', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; #AUTHOR_TAG in utilising different types of information available in LDO']","['We chose to employ LDOCE as the machine readable source to aid the development of a substantial lexicon because this dictionary has several properties which make it uniquely appropriate for use as the core knowledge base of a natural language processing system.', ""Most prominent among these are the rich grammatical subcategorisations of the 60,000 entries, the large amount of information concerning phrasal verbs, noun compounds and idioms, the individual subject, collocational and semantic codes for the entries and the consistent use of a controlled 'core' vocabulary in defining the words throughout the dictionary."", ' #TAUTHOR_TAG contains further description and discussion of LDOCE . )', 'In this paper we focus on the exploitation of the LDOCE grammar coding system; #AUTHOR_TAG and #AUTHOR_TAG describe further research in Cambridge utilising different types of information available in LDOCE.']",0
"['', '', '', 'Expanding on a suggestion of #TAUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction', '', '']","['', '', '', 'Expanding on a suggestion of #TAUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction;', '', '']","['', '', '', 'Expanding on a suggestion of #TAUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', 'These terms, which derive from Transformational Grammar, are used as convenient labels for what we regard as a semantic distinction;', '', '']","['', '', '', 'Expanding on a suggestion of #TAUTHOR_TAG , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .', '', '', '']",2
"['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; #TAUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure #AUTHOR_TAG .', '', '']","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; #TAUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure #AUTHOR_TAG .', '', '']","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; #TAUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure #AUTHOR_TAG .', '', '']","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; #TAUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure #AUTHOR_TAG .', '', '']",5
"['Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TAUTHOR_TAG , 1985 ) .', 'The are doub artic relations frames', '', '']","['Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TAUTHOR_TAG , 1985 ) .', 'The are doubly relations frames', '', '']","['Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TAUTHOR_TAG , 1985 ) .', 'The codes are doubly articisation frames', '', '']","['Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing.', 'The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of #TAUTHOR_TAG , 1985 ) .', '', '', '']",2
"['we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English #TAUTHOR_TAG .']","['we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp,', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English #TAUTHOR_TAG .']","['we were targeting all envisaged access routes from LDOCE to systems implemented in Lisp', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English #TAUTHOR_TAG .']","['', 'Our task was made possible by the fact that while far from being a database in the accepted sense of the word , the LDOCE typesetting tape is the only truly computerised dictionary of English #TAUTHOR_TAG .']",0
"['', '', ' #AUTHOR_TAG and #TAUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the L scheme can be evaluated.']","['', '', ' #AUTHOR_TAG and #TAUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the scheme can be evaluated.']","['', '', ' #AUTHOR_TAG and #TAUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']","['', '', ' #AUTHOR_TAG and #TAUTHOR_TAG provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description .', ' #AUTHOR_TAG comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.']",0
"['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) #AUTHOR_TAG , Lexical Functional Grammar ( LFG ) #AUTHOR_TAG -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) #AUTHOR_TAG a ) , PATR-II #TAUTHOR_TAG -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle thematical and semantic idyncrac eleg', '', '', '']","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) #AUTHOR_TAG , Lexical Functional Grammar ( LFG ) #AUTHOR_TAG -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) #AUTHOR_TAG a ) , PATR-II #TAUTHOR_TAG -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies elegantly', '', '', '']","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) #AUTHOR_TAG , Lexical Functional Grammar ( LFG ) #AUTHOR_TAG -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) #AUTHOR_TAG a ) , PATR-II #TAUTHOR_TAG -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', 'These developments also emphasise that if natural language processing systems are to be able to handle thematical and semantic idiosyncracies eleg', '', '', '']","['Recent developments in linguistics , and especially on grammatical theory -- for example , Generalised Phrase Structure Grammar ( GPSG ) #AUTHOR_TAG , Lexical Functional Grammar ( LFG ) #AUTHOR_TAG -- and on natural language parsing frameworks -- for example , Functional Unification Grammar ( FUG ) #AUTHOR_TAG a ) , PATR-II #TAUTHOR_TAG -- make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language .', '', '', '', '']",0
"['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes', 'dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure #TAUTHOR_TAG .', '', '']","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure #TAUTHOR_TAG .', '', '']","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure #TAUTHOR_TAG .', '', '']","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords , dictionary search through the pronunciation field is available ; #AUTHOR_TAG has merged information from the pronunciation and hyphenation fields , creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure #TAUTHOR_TAG .', '', '']",0
"['', '4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 L provides considerably syntactic.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #TAUTHOR_TAG .', 'gram behaviour', '', '']","['', '4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 provides considerably syntactic dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #TAUTHOR_TAG .', 'grammar behaviour', '', '']","['', 'illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 L provides considerably more syntactic information', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #TAUTHOR_TAG .', 'behaviour', '', '']","['', 'Figure 4 illustrates the grammar code field for the third word sense of the verb believe as it appears in the published dictionary, on the typesetting tape and after restructuring.', 'Figure 4 LDOCE provides considerably more syntactic information than a traditional dictionary.', 'The Longman lexicographers have developed a grammar coding system capable of representing in compact form a nontrivial amount of information , usually to be found only in large descriptive grammars of English ( such as #TAUTHOR_TAG .', '', '', '']",0
