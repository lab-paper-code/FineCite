token_context,word_context,seg_context,sent_cotext,label
"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS #AUTHOR_TAG , and JOYCE #AUTHOR_TAG .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG #TAUTHOR_TAG .']","['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS #AUTHOR_TAG , and JOYCE #AUTHOR_TAG .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG #TAUTHOR_TAG .']","['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS #AUTHOR_TAG , and JOYCE #AUTHOR_TAG .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG #TAUTHOR_TAG .']","['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS #AUTHOR_TAG , and JOYCE #AUTHOR_TAG .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG #TAUTHOR_TAG .']",0
"['these a dependency structure a very useful for since gives linguists a representation that allows them to abstract over numerous crossuistic divergences due language specificPol', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent #AUTHOR_TAG , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework #TAUTHOR_TAG .']","['these a dependency structure a very useful for since gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due language specific', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent #AUTHOR_TAG , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework #TAUTHOR_TAG .']","['these characteristics a dependency tree structure a very useful representation for since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due language specificPol', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent #AUTHOR_TAG , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework #TAUTHOR_TAG .']","['', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent #AUTHOR_TAG , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework #TAUTHOR_TAG .']",2
"['--o II a failli pleuvoir.', 'More details on how the structural divergences described in #TAUTHOR_TAG can be accounted for using our formalism can be found in #AUTHOR_TAG .']","['--o II a failli pleuvoir.', 'More details on how the structural divergences described in #TAUTHOR_TAG can be accounted for using our formalism can be found in #AUTHOR_TAG .']","['--o II a failli pleuvoir.', 'More details on how the structural divergences described in #TAUTHOR_TAG can be accounted for using our formalism can be found in #AUTHOR_TAG .']","['--o II a failli pleuvoir.', 'More details on how the structural divergences described in #TAUTHOR_TAG can be accounted for using our formalism can be found in #AUTHOR_TAG .']",0
"['', 'transfer', 'Our work extends directions taken in systems such as Ariane #AUTHOR_TAG FoG #AUTHOR_TAG , JOYCE ( Rambow and #TAUTHOR_TAG , and LFS #AUTHOR_TAG .', 'Although adopts the general in the abovementioned systems practical integrate better with emerging-based approaches']","['', 'transfer', 'Our work extends directions taken in systems such as Ariane #AUTHOR_TAG FoG #AUTHOR_TAG , JOYCE ( Rambow and #TAUTHOR_TAG , and LFS #AUTHOR_TAG .', 'Although adopts the general in the abovementioned systems, practical, integrate better with emerging statistics-based approaches']","['', 'transfer', 'Our work extends directions taken in systems such as Ariane #AUTHOR_TAG FoG #AUTHOR_TAG , JOYCE ( Rambow and #TAUTHOR_TAG , and LFS #AUTHOR_TAG .', 'Although it adopts the general principles in the abovementioned systems, practical integrate better with emerging statistics-based approaches']","['', '', 'Our work extends directions taken in systems such as Ariane #AUTHOR_TAG , FoG #AUTHOR_TAG , JOYCE ( Rambow and #TAUTHOR_TAG , and LFS #AUTHOR_TAG .', '']",2
"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG #AUTHOR_TAG , LFS #AUTHOR_TAG , and JOYCE ( Rambow and #TAUTHOR_TAG .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG .']","['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG #AUTHOR_TAG , LFS #AUTHOR_TAG , and JOYCE ( Rambow and #TAUTHOR_TAG .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG .']","['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG #AUTHOR_TAG , LFS #AUTHOR_TAG , and JOYCE ( Rambow and #TAUTHOR_TAG .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG .']","['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG #AUTHOR_TAG , LFS #AUTHOR_TAG , and JOYCE ( Rambow and #TAUTHOR_TAG .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG .']",2
"['text AS format ation is not is sentence boundary information', '', ""of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger #TAUTHOR_TAG ) and parsers generally aim to produce a tree spanning each sentence .""]","['text ASR format ation is not is sentence boundary information.', '', ""of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger #TAUTHOR_TAG ) and parsers generally aim to produce a tree spanning each sentence .""]","['Example text AS format ation is not is sentence boundary information.', '', ""of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger #TAUTHOR_TAG ) and parsers generally aim to produce a tree spanning each sentence .""]","['', '', ""Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger #TAUTHOR_TAG ) and parsers generally aim to produce a tree spanning each sentence .""]",0
"['resolved coreference usingIZZLE,our implementation a bual coreference res', 'is a multilingual enhancement of COCKTAIL #TAUTHOR_TAG , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information .', 'COCKTAIL was applied separately on the and the Ro- manian texts coreferring links were identified for each English and Romanian document respectively', 'referential expressions corefer with non-aligned anaphors, SWZLE derived newis for', '']","['resolved coreference using SWIZZLE,our implementation a bilingual coreference', 'is a multilingual enhancement of COCKTAIL #TAUTHOR_TAG , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information .', 'COCKTAIL was applied separately on the and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.', 'referential expressions corefer with non-aligned anaphors, SWIZZLE derived new for', '']","['we resolved coreference using SWIZZLE,our implementation a bilingual coreference resolver.', 'is a multilingual enhancement of COCKTAIL #TAUTHOR_TAG , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information .', 'COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.', 'referential expressions corefer with non-aligned anaphors, SWIZZLE derived for', '']","['', 'SWIZZLE is a multilingual enhancement of COCKTAIL #TAUTHOR_TAG , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information .', 'When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.', 'When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.', '']",2
"['some the-performing and most robust coreference resolution systems employ knowledge-', '', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX #TAUTHOR_TAG , #AUTHOR_TAG #AUTHOR_TAG ) .', '', '', '', '', '']","['some the best-performing and most robust coreference resolution systems employ knowledge-based', '', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX #TAUTHOR_TAG , #AUTHOR_TAG #AUTHOR_TAG ) .', '', '', '', '', '']","['some the best-performing and most robust coreference resolution systems employ knowledge-', '', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX #TAUTHOR_TAG , #AUTHOR_TAG #AUTHOR_TAG ) .', '', '', '', '', '']","['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', '', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX #TAUTHOR_TAG , #AUTHOR_TAG #AUTHOR_TAG ) .', '', '', '', '', '']",0
"['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'other, coerc are as paths of meronyms.-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjs must be identical or the adj of theent be less specific than the antecedent.', 'Table 1 lists the top performing heuristics ofCKILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table2.', 'Details of the top performing heuristics of COCKTAIL were reported in #TAUTHOR_TAG .']","['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'other occasions, coercions are as paths of meronyms is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in #TAUTHOR_TAG .']","['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'other occasions, coercions are obtained as paths of meronyms. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics ofCKILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table2.', 'Details of the top performing heuristics of COCKTAIL were reported in #TAUTHOR_TAG .']","['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'On other occasions, coercions are obtained as paths of meronyms (e.g. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in #TAUTHOR_TAG .']",0
"['and', '', 'timeconsum', ', recent results show that knowledge-poor methods perform with amazing accuracy (cf.', ' #AUTHOR_TAG , #AUTHOR_TAG #AUTHOR_TAG ).', 'AC #AUTHOR_TAG a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases ofonominal', 'For this research , we used a coreference resolution system ( #TAUTHOR_TAG ) that implements different sets of heuristics corresponding to various forms of coreference .', 'CK', '', '', '']","['and', '', '', 'Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf.', ' #AUTHOR_TAG , #AUTHOR_TAG #AUTHOR_TAG ).', ' #AUTHOR_TAG a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal', 'For this research , we used a coreference resolution system ( #TAUTHOR_TAG ) that implements different sets of heuristics corresponding to various forms of coreference .', '', '', '', '']","['and', '', 'timeconsum', ', recent results show that knowledge-poor methods perform with amazing accuracy (cf.', ' #AUTHOR_TAG , #AUTHOR_TAG #AUTHOR_TAG ).', ' #AUTHOR_TAG a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases ofonom', 'For this research , we used a coreference resolution system ( #TAUTHOR_TAG ) that implements different sets of heuristics corresponding to various forms of coreference .', 'CK', '', '', '']","['', '', '', 'Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf.', ' #AUTHOR_TAG , #AUTHOR_TAG #AUTHOR_TAG ).', 'For example, CogNIAC #AUTHOR_TAG , a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'For this research , we used a coreference resolution system ( #TAUTHOR_TAG ) that implements different sets of heuristics corresponding to various forms of coreference .', '', '', '', '']",5
"['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying particular type of unknown word.', 'For, the missp will specialize in identifying misspell, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. #TAUTHOR_TAG .', 'The results from each component are evaluated to determine the final category of the word.']","['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying particular type of unknown word.', 'For example, the misspelling will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. #TAUTHOR_TAG .', 'The results from each component are evaluated to determine the final category of the word.']","['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. #TAUTHOR_TAG .', 'The results from each component are evaluated to determine the final category of the word.']","['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. #TAUTHOR_TAG .', 'The results from each component are evaluated to determine the final category of the word.']",4
"['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on #TAUTHOR_TAG ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns.']","['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on #TAUTHOR_TAG ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns.']","['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on #TAUTHOR_TAG ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns.']","['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on #TAUTHOR_TAG ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns.']",5
"['Research that is more similar in goal to that outlined in this paper is Vosse #TAUTHOR_TAG .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', 'his system in we', 'anMyze', '']","['Research that is more similar in goal to that outlined in this paper is Vosse #TAUTHOR_TAG .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', 'his system ineffective we', 'anMyze', '']","['Research that is more similar in goal to that outlined in this paper is Vosse #TAUTHOR_TAG .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', 'his system in we are', 'anMyze', '']","['Research that is more similar in goal to that outlined in this paper is Vosse #TAUTHOR_TAG .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', '', '', '']",1
"['Corpus frequency : #TAUTHOR_TAG differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', 'ions']","['Corpus frequency : #TAUTHOR_TAG differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', '']","['Corpus frequency : #TAUTHOR_TAG differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', 'ions']","['Corpus frequency : #TAUTHOR_TAG differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', '']",5
"['The the first that we of to analyze a corpus of logs a dialogue system for the purpose of learning to predict situations', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance #TAUTHOR_TAG .', '', '']","['The the first that we of to analyze a corpus of logs a dialogue system for the purpose of learning to predict situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance #TAUTHOR_TAG .', '', '']","['The research the first that we know of to analyze a corpus of logs a dialogue system for the purpose of learning to predict', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance #TAUTHOR_TAG .', '', '']","['The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance #TAUTHOR_TAG .', '', '']",2
"['M as running percentages (percentreprompts,ir percentsubdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors #TAUTHOR_TAG .']","['as running percentages (percent-reprompts, percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors #TAUTHOR_TAG .']","['as running percentages (percentreprompts, percentconfirms, percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors #TAUTHOR_TAG .']","['', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors #TAUTHOR_TAG .']",4
"['where f (•) extracts a feature vector from a classified, θ are the corresponding weights of those features, and Z θ (x) def = y ux is a normalizer', 'We use the same set of binary features as in previous work on this dataset #TAUTHOR_TAG .', 'let { 1 ... v 17744 } be the set of word types with count ≥ 4 in the 2000-document corpus.', '', '', '', '']","['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, is a normalizer.', 'We use the same set of binary features as in previous work on this dataset #TAUTHOR_TAG .', 'let {v 1 ..., v 17744 } be the set of word types with count ≥ 4 in the 2000-document corpus.', '', '', '', '']","['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y ux is a normalizer.', 'We use the same set of binary features as in previous work on this dataset #TAUTHOR_TAG .', 'let V = {v 1 ... v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', '', '', '', '']","['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset #TAUTHOR_TAG .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', '', '', '', '']",5
"['this approach', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams #AUTHOR_TAG , class n-grams #TAUTHOR_TAG , grammatical features #AUTHOR_TAG , etc ' .""]","['this approach.', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams #AUTHOR_TAG , class n-grams #TAUTHOR_TAG , grammatical features #AUTHOR_TAG , etc ' .""]","['this work approach', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams #AUTHOR_TAG , class n-grams #TAUTHOR_TAG , grammatical features #AUTHOR_TAG , etc ' .""]","['', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams #AUTHOR_TAG , class n-grams #TAUTHOR_TAG , grammatical features #AUTHOR_TAG , etc ' .""]",3
"['Self-supervised boosting was presented as a general method for estimation, and was not tested in the context of language.', '', '', '', 'Unfortunately , as shown in #TAUTHOR_TAG , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', '', '', '', '']","['Self-supervised boosting was presented as a general method for estimation, and was not tested in the context of language modeling.', '', '', '', 'Unfortunately , as shown in #TAUTHOR_TAG , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', '', '', '', '']","['Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', '', '', '', 'Unfortunately , as shown in #TAUTHOR_TAG , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', '', '', '', '']","['Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', '', '', '', 'Unfortunately , as shown in #TAUTHOR_TAG , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', '', '', '', '']",4
"['For our features we used-ifiers', 'The code for theifier was generously provided Daisuanara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in #TAUTHOR_TAG , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', '']","['For our features we used classifiers', 'The code for the classifier was generously provided Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in #TAUTHOR_TAG , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', '']","['For our features we used-', 'The code for the classifier was generously provided Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in #TAUTHOR_TAG , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', '']","['', 'The code for the classifier was generously provided by Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in #TAUTHOR_TAG , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', '']",4
"['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TAUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '', '', '', '']","['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TAUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '', '', '', '']","['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TAUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '', '', '', '']","['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #TAUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '', '', '', '']",5
"['The features can be easily obtained by modifying the TAT extraction algorithm described in #TAUTHOR_TAG .', 'a TAT is extracted from a word-aligned, source-parsed parallel sentence, context features and', '', '']","['The features can be easily obtained by modifying the TAT extraction algorithm described in #TAUTHOR_TAG .', 'a TAT is extracted from a word-aligned, source-parsed parallel sentence, contextual features and', '', '']","['The features can be easily obtained by modifying the TAT extraction algorithm described in #TAUTHOR_TAG .', 'a TAT is extracted from a word-aligned, source-parsed parallel sentence, the contextual features and', '', '']","['The features can be easily obtained by modifying the TAT extraction algorithm described in #TAUTHOR_TAG .', '', '', '']",2
"['Belief propagation improves non-projective dependency parsing with features that would make exact inference in', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #TAUTHOR_TAG and history-based parsing ( Nivre and Mc #AUTHOR_TAG .', 'We could also introduce']","['Belief propagation improves non-projective dependency parsing with features that would make exact inference', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #TAUTHOR_TAG and history-based parsing ( Nivre and Mc #AUTHOR_TAG .', 'We could also introduce']","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #TAUTHOR_TAG and history-based parsing ( Nivre and Mc #AUTHOR_TAG .', 'We could also introduce']","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #TAUTHOR_TAG and history-based parsing ( Nivre and Mc #AUTHOR_TAG .', '']",3
"['Belief propagation improves non-projective dependency parsing with features that would make exact inference in', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #TAUTHOR_TAG and history-based parsing ( Nivre and Mc #AUTHOR_TAG .', 'We could also introduce']","['Belief propagation improves non-projective dependency parsing with features that would make exact inference', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #TAUTHOR_TAG and history-based parsing ( Nivre and Mc #AUTHOR_TAG .', 'We could also introduce']","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #TAUTHOR_TAG and history-based parsing ( Nivre and Mc #AUTHOR_TAG .', 'We could also introduce']","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #TAUTHOR_TAG and history-based parsing ( Nivre and Mc #AUTHOR_TAG .', '']",3
"['Belief propagation improves non-projective dependency parsing with features that would make exactference.', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #AUTHOR_TAG and history-based parsing #TAUTHOR_TAG .', 'We could also']","['Belief propagation improves non-projective dependency parsing with features that would make exact inference', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #AUTHOR_TAG and history-based parsing #TAUTHOR_TAG .', 'We could also']","['Belief propagation improves non-projective dependency parsing with features that would make', 'We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #AUTHOR_TAG and history-based parsing #TAUTHOR_TAG .', 'We could also']","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking #AUTHOR_TAG and history-based parsing #TAUTHOR_TAG .', '']",3
"['', '', 'We could also introduce new variables , e.g. , nonterminal refinements #TAUTHOR_TAG , orREE ) that augment the parse with representations of control , binding , etc. #AUTHOR_TAG Buch .']","['', '', 'We could also introduce new variables , e.g. , nonterminal refinements #TAUTHOR_TAG , or ) that augment the parse with representations of control , binding , etc. #AUTHOR_TAG .']","['', '', 'We could also introduce new variables , e.g. , nonterminal refinements #TAUTHOR_TAG , orREE ) that augment the parse with representations of control , binding , etc. #AUTHOR_TAG Buch #AUTHOR_TAG .']","['', '', 'We could also introduce new variables , e.g. , nonterminal refinements #TAUTHOR_TAG , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. #AUTHOR_TAG Buch- #AUTHOR_TAG .']",3
"['(Section 3', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system #TAUTHOR_TAG , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', '']","['(Section 3),', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system #TAUTHOR_TAG , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', '']","['(Section 3', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system #TAUTHOR_TAG , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', '']","['', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system #TAUTHOR_TAG , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', '']",2
"['The is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models #AUTHOR_TAG', 'The first direct application of parse forest in translation is our previous work #TAUTHOR_TAG which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', '']","['The is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models #AUTHOR_TAG', 'The first direct application of parse forest in translation is our previous work #TAUTHOR_TAG which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', '']","['is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models #AUTHOR_TAG .', 'The first direct application of parse forest in translation is our previous work #TAUTHOR_TAG which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', '']","['The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models #AUTHOR_TAG .', 'The first direct application of parse forest in translation is our previous work #TAUTHOR_TAG which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', '']",2
"['', '', '', '', 'the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction #TAUTHOR_TAG , and Machine Translation ( Boas 2002 ) .', 'efforts many researchers (Ceras and Mque2']","['', '', '', '', 'the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction #TAUTHOR_TAG , and Machine Translation ( Boas 2002 ) .', 'efforts many researchers (Carreras and']","['', '', '', '', 'the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction #TAUTHOR_TAG , and Machine Translation ( Boas 2002 ) .', 'the efforts many researchers (Ceras and Mque']","['', '', '', '', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction #TAUTHOR_TAG , and Machine Translation ( Boas 2002 ) .', '']",0
"['To prove that our method is effective , we also make a comparison between the performances of our system and #TAUTHOR_TAG , #AUTHOR_TAG .', ' #AUTHOR_TAG is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table6', '', '']","['To prove that our method is effective , we also make a comparison between the performances of our system and #TAUTHOR_TAG , #AUTHOR_TAG .', ' #AUTHOR_TAG is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6', '', '']","['To prove that our method is effective , we also make a comparison between the performances of our system and #TAUTHOR_TAG , #AUTHOR_TAG .', ' #AUTHOR_TAG is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table6', '', '']","['To prove that our method is effective , we also make a comparison between the performances of our system and #TAUTHOR_TAG , #AUTHOR_TAG .', ' #AUTHOR_TAG is the best SRL system until now and it has the same data setting with ours.', '', '', '']",1
"['was defined', '', '', 'Agent.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering #TAUTHOR_TAG , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'the efforts many (Ceras and Mrque2 and this']","['was defined', '', '', 'Agent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering #TAUTHOR_TAG , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'the efforts many (Carreras and Màrquez and this']","['was first defined', '', '', 'Agent', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering #TAUTHOR_TAG , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'the efforts many researchers (Ceras and Mrque2 and']","['', '', '', '', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering #TAUTHOR_TAG , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', '']",0
"['sem pred.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese #TAUTHOR_TAG .']","['predicate word.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese #TAUTHOR_TAG .']","['sem pred.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese #TAUTHOR_TAG .']","['', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese #TAUTHOR_TAG .']",5
"['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']","['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']","['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']","['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0
"['still', 'transplant machine', '', '', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank #TAUTHOR_TAG was built , #AUTHOR_TAG and #AUTHOR_TAG have produced more complete and systematic research on Chinese SRL .', 'attempt idea', '', '', '', '', '', '']","['still', 'transplant machine', '', '', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank #TAUTHOR_TAG was built , #AUTHOR_TAG and #AUTHOR_TAG have produced more complete and systematic research on Chinese SRL .', 'attempt idea', '', '', '', '', '', '']","['is still', 'transplant', '', '', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank #TAUTHOR_TAG was built , #AUTHOR_TAG and #AUTHOR_TAG have produced more complete and systematic research on Chinese SRL .', 'the idea', '', '', '', '', '', '']","['', '', '', '', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank #TAUTHOR_TAG was built , #AUTHOR_TAG and #AUTHOR_TAG have produced more complete and systematic research on Chinese SRL .', '', '', '', '', '', '', '']",0
"['', '', '', '', '', '', '', '', '', '', 'sub tasks and.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL #TAUTHOR_TAG , Xue 2008 ) reassured these findings .']","['', '', '', '', '', '', '', '', '', '', 'sub tasks and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL #TAUTHOR_TAG , Xue 2008 ) reassured these findings .']","['', '', '', '', '', '', '', '', '', '', 'different sub tasks and.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL #TAUTHOR_TAG , Xue 2008 ) reassured these findings .']","['', '', '', '', '', '', '', '', '', '', '', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL #TAUTHOR_TAG , Xue 2008 ) reassured these findings .']",4
"[' #TAUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', '', '', '', '', '', '']","[' #TAUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', '', '', '', '', '', '']","[' #TAUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', '', '', '', '', '', '']","[' #TAUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', '', '', '', '', '', '']",5
"['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', ' #TAUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', 'limited the amount of the research target ARG2 is few in Prop', '', '']","['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', ' #TAUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', 'limited the amount of the research target, ARG2, is few in', '', '']","['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', ' #TAUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', 'limited the amount of the research target, ARG2, is few in Prop', '', '']","['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', ' #TAUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', '', '', '']",1
"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #TAUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #TAUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #TAUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #TAUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '', '']",0
"['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , #TAUTHOR_TAG .', ' #AUTHOR_TAG is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6 three Gold', '', '']","['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , #TAUTHOR_TAG .', ' #AUTHOR_TAG is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6 three Gold', '', '']","['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , #TAUTHOR_TAG .', ' #AUTHOR_TAG is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6', '', '']","['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , #TAUTHOR_TAG .', ' #AUTHOR_TAG is the best SRL system until now and it has the same data setting with ours.', '', '', '']",1
"['Previous semantic role classifiers always did the classification problem in one-step.', 'this we SRC in two', 'The architectures of hierarch semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in #TAUTHOR_TAG .']","['Previous semantic role classifiers always did the classification problem in one-step.', 'this we SRC in two', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in #TAUTHOR_TAG .']","['Previous semantic role classifiers always did the classification problem in one-step.', 'we did SRC in', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in #TAUTHOR_TAG .']","['Previous semantic role classifiers always did the classification problem in one-step.', '', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in #TAUTHOR_TAG .']",1
"['frame first , last , and its P , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TAUTHOR_TAG .']","['frame first , last , and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TAUTHOR_TAG .']","['cat frame first word , last word , and its P , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TAUTHOR_TAG .']","['Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #TAUTHOR_TAG .']",5
"['', '', '', '', 'arguments can useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation #TAUTHOR_TAG .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, et al 22 different machine learning methods and linguistics resources are applied in this task, which has madeRL task progress']","['', '', '', '', 'arguments can useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation #TAUTHOR_TAG .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, et al different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress']","['', '', '', '', 'the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation #TAUTHOR_TAG .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, et al 22, different machine learning methods and linguistics resources are applied in this task, which has madeRL task progress fast.']","['', '', '', '', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation #TAUTHOR_TAG .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0
"['', '', '', '', '', '', '', '', '', 'We use the same data setting with #TAUTHOR_TAG , however a bit different from #AUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', 'We use the same data setting with #TAUTHOR_TAG , however a bit different from #AUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', 'We use the same data setting with #TAUTHOR_TAG , however a bit different from #AUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', 'We use the same data setting with #TAUTHOR_TAG , however a bit different from #AUTHOR_TAG .']",5
"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , #AUTHOR_TAG and #TAUTHOR_TAG .', ' #AUTHOR_TAG preliminary SRL without any large semantically annotpus', '', '', '( 3 built and #AUTHOR_TAG more', '', '', '', '', '', 'crucial', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , #AUTHOR_TAG and #TAUTHOR_TAG .', ' #AUTHOR_TAG preliminary SRL without any large semantically annotated corpus', '', '', 'built, and #AUTHOR_TAG more', '', '', '', '', '', 'crucial.', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , #AUTHOR_TAG and #TAUTHOR_TAG .', ' #AUTHOR_TAG the preliminary work Chinese SRL without any large semantically annotated corpus', '', '', '( built and #AUTHOR_TAG more', '', '', '', '', '', 'crucial', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , #AUTHOR_TAG and #TAUTHOR_TAG .', ' #AUTHOR_TAG did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', '', '', '', '', '', '', '', '', '', '']",0
"['', '', '', '', '', '', '', '', '', '', '.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #TAUTHOR_TAG reassured these findings .']","['', '', '', '', '', '', '', '', '', '', 'classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #TAUTHOR_TAG reassured these findings .']","['', '', '', '', '', '', '', '', '', '', '.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #TAUTHOR_TAG reassured these findings .']","['', '', '', '', '', '', '', '', '', '', '', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #TAUTHOR_TAG reassured these findings .']",0
"['', '', '', '', '', 'Xuemer was built and', ' #TAUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', '', '', '', '', '', '']","['', '', '', '', '', '(Xue Palmer was built, and', ' #TAUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', '', '', '', '', '', '']","['', '', '', '', '', 'Xue was built and', ' #TAUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', '', '', '', '', '', '']","['', '', '', '', '', '', ' #TAUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', '', '', '', '', '', '']",0
"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , #TAUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , #TAUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , #TAUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '', '']","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , #TAUTHOR_TAG and #AUTHOR_TAG .', '', '', '', '', '', '', '', '', '', '', '']",0
"['use', '', '', '', '', '', '', '', 'cbb', 'We use the same data setting with #AUTHOR_TAG , however a bit different from #TAUTHOR_TAG .']","['use', '', '', '', '', '', '', '', '', 'We use the same data setting with #AUTHOR_TAG , however a bit different from #TAUTHOR_TAG .']","['We use', '', '', '', '', '', '', '', 'cbb', 'We use the same data setting with #AUTHOR_TAG , however a bit different from #TAUTHOR_TAG .']","['', '', '', '', '', '', '', '', '', 'We use the same data setting with #AUTHOR_TAG , however a bit different from #TAUTHOR_TAG .']",1
['The candidate feature templates include : Voice from #TAUTHOR_TAG .'],['The candidate feature templates include : Voice from #TAUTHOR_TAG .'],['The candidate feature templates include : Voice from #TAUTHOR_TAG .'],['The candidate feature templates include : Voice from #TAUTHOR_TAG .'],5
"['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank #TAUTHOR_TAG .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure ', '', '', '', '']","['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank #TAUTHOR_TAG .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1', '', '', '', '']","['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank #TAUTHOR_TAG .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1', '', '', '', '']","['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank #TAUTHOR_TAG .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', '', '', '', '', '']",5
"['Semantic Role labeling ( SRL ) was first defined in #TAUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag of the sem relation with the', 'ical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc', 'can useful information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 24), Information ExtractionSurde3 Machine TransBo', '2 and linguistics this, made']","['Semantic Role labeling ( SRL ) was first defined in #TAUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag of the semantic relation with the', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'can useful information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction Machine Translation', 'and linguistics this task, made']","['Semantic Role labeling ( SRL ) was first defined in #TAUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag of the semantic relation with', 'ical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information ExtractionSurde3 Machine TranslationBo', '2 and linguistics resources this task, has made']","['Semantic Role labeling ( SRL ) was first defined in #TAUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002).', '']",0
"['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguitying the structure of.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']",0
"['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguitying the structure of.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']",0
"['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguitying the structure of.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']",0
"['', '', 'indocument coreference resolution has been applied to produce summaries of text surroundingrences', 'text is present in most as the only feature and sometimes in combination with otherssee for instance #AUTHOR_TAG -', 'Other representations use the link structure #TAUTHOR_TAG or generate graph representations of the extracted features #AUTHOR_TAG .', '', '', '', '']","['', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences', 'text is present in most as the only feature and sometimes in combination with otherssee for instance #AUTHOR_TAG -.', 'Other representations use the link structure #TAUTHOR_TAG or generate graph representations of the extracted features #AUTHOR_TAG .', '', '', '', '']","['', '', 'indocument coreference resolution has been applied to produce summaries of text surroundingrences', 'is present in most as the only feature and sometimes in combination with otherssee for instance #AUTHOR_TAG -', 'Other representations use the link structure #TAUTHOR_TAG or generate graph representations of the extracted features #AUTHOR_TAG .', '', '', '', '']","['', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature #AUTHOR_TAG and sometimes in combination with otherssee for instance #AUTHOR_TAG -.', 'Other representations use the link structure #TAUTHOR_TAG or generate graph representations of the extracted features #AUTHOR_TAG .', '', '', '', '']",0
"['', '', 'has been', '', '', 'Some researchers #TAUTHOR_TAG have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipediaions', 'brit and be in way', '']","['', '', 'has been', '', '', 'Some researchers #TAUTHOR_TAG have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia mentions', 'and be in way.', '']","['', '', 'has been applied', '', '', 'Some researchers #TAUTHOR_TAG have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia', 'and in', '']","['', '', '', '', '', 'Some researchers #TAUTHOR_TAG have explored the use of Wikipedia information to improve the disambiguation process .', '', '', '']",0
"['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense DisambiguationW Crossdocument Coreference', 'of early person ambig focuses the W.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own #TAUTHOR_TAG .']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation Cross-document Coreference', 'of early person ambiguity focuses the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own #TAUTHOR_TAG .']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense DisambiguationW Crossdocument Coreference', 'of person name ambiguity focuses the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own #TAUTHOR_TAG .']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) #AUTHOR_TAG and Cross-document Coreference (CDC) #AUTHOR_TAG .', '', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own #TAUTHOR_TAG .']",0
"['2The WePS-1 corpus includes data from the Web03 testbed #TAUTHOR_TAG which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']","['2The WePS-1 corpus includes data from the Web03 testbed #TAUTHOR_TAG which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']","['2The WePS-1 corpus includes data from the Web03 testbed #TAUTHOR_TAG which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']","['2The WePS-1 corpus includes data from the Web03 testbed #TAUTHOR_TAG which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']",5
"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #TAUTHOR_TAG .', 'is present in only feature', '', '', '', '', '']","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #TAUTHOR_TAG .', 'is present in only feature', '', '', '', '', '']","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #TAUTHOR_TAG .', 'is present in the only feature', '', '', '', '', '']","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #TAUTHOR_TAG .', '', '', '', '', '', '']",0
"['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense DisambiguationW Crossdocument Coreference', 'of early person ambig focuses the W.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own #TAUTHOR_TAG .']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation Cross-document Coreference', 'of early person ambiguity focuses the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own #TAUTHOR_TAG .']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense DisambiguationW Crossdocument Coreference', 'of person name ambiguity focuses the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own #TAUTHOR_TAG .']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) #AUTHOR_TAG and Cross-document Coreference (CDC) #AUTHOR_TAG .', '', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own #TAUTHOR_TAG .']",0
"['have to an ambig is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #AUTHOR_TAG and sometimes in combination with others see for instance #TAUTHOR_TAG - .', 'link #AUTHOR_TAG or extracted features #AUTHOR_TAG .', '', '', '', '']","['have to an ambiguous is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #AUTHOR_TAG and sometimes in combination with others see for instance #TAUTHOR_TAG - .', 'link #AUTHOR_TAG or extracted features #AUTHOR_TAG .', '', '', '', '']","['to an ambiguous name is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #AUTHOR_TAG and sometimes in combination with others see for instance #TAUTHOR_TAG - .', ' #AUTHOR_TAG or the extracted features #AUTHOR_TAG .', '', '', '', '']","['', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #AUTHOR_TAG and sometimes in combination with others see for instance #TAUTHOR_TAG - .', '', '', '', '', '']",0
"['ambig is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences the', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature #AUTHOR_TAG and sometimes in combination with otherssee for instance #AUTHOR_TAG -.', 'Other representations use the link structure #AUTHOR_TAG or generate graph representations of the extracted features #TAUTHOR_TAG .', '', '', '', '']","['ambiguous is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences the', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature #AUTHOR_TAG and sometimes in combination with otherssee for instance #AUTHOR_TAG -.', 'Other representations use the link structure #AUTHOR_TAG or generate graph representations of the extracted features #TAUTHOR_TAG .', '', '', '', '']","['is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences the name', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature #AUTHOR_TAG and sometimes in combination with otherssee for instance #AUTHOR_TAG -.', 'Other representations use the link structure #AUTHOR_TAG or generate graph representations of the extracted features #TAUTHOR_TAG .', '', '', '', '']","['', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature #AUTHOR_TAG and sometimes in combination with otherssee for instance #AUTHOR_TAG -.', 'Other representations use the link structure #AUTHOR_TAG or generate graph representations of the extracted features #TAUTHOR_TAG .', '', '', '', '']",0
"['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names #TAUTHOR_TAG .', '', 'WW more are mentioned', '']","['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names #TAUTHOR_TAG .', '', 'WWW more are mentioned', '']","['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names #TAUTHOR_TAG .', '', 'more are mentioned', '']","['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names #TAUTHOR_TAG .', '', '', '']",0
"['', '', '', '', ' #TAUTHOR_TAG the performace of NEs versus BoW features', '', '']","['', '', '', '', ' #TAUTHOR_TAG the performace of NEs versus BoW features', '', '']","['', '', '', '', ' #TAUTHOR_TAG the performace of NEs versus BoW features .', '', '']","['', '', '', '', ' #TAUTHOR_TAG compared the performace of NEs versus BoW features .', '', '']",0
"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #TAUTHOR_TAG .', 'is present in only feature', '', '', '', '', '']","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #TAUTHOR_TAG .', 'is present in only feature', '', '', '', '', '']","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #TAUTHOR_TAG .', 'is present in the only feature', '', '', '', '', '']","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #TAUTHOR_TAG .', '', '', '', '', '', '']",0
"['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types #TAUTHOR_TAG .', 'arseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex']","['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types #TAUTHOR_TAG .', 'sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.']","['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types #TAUTHOR_TAG .', 'the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex']","['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types #TAUTHOR_TAG .', 'Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.']",5
"['A study of the query log of the AllTheWeb and Altista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names #AUTHOR_TAG .', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people #TAUTHOR_TAG .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', ', a query for a common name in the Web will usually produce a list of results where different people are mentioned.']","['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names #AUTHOR_TAG .', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people #TAUTHOR_TAG .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']","['A study of the query log of the AllTheWeb and Altista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names #AUTHOR_TAG .', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people #TAUTHOR_TAG .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', ', a query for a common name in the Web will usually produce a list of results where different people are mentioned.']","['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names #AUTHOR_TAG .', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people #TAUTHOR_TAG .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']",0
"['', '', 'has been', '', '', 'Some researchers #TAUTHOR_TAG have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipediaions', 'brit and be in way', '']","['', '', 'has been', '', '', 'Some researchers #TAUTHOR_TAG have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia mentions', 'and be in way.', '']","['', '', 'has been applied', '', '', 'Some researchers #TAUTHOR_TAG have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia', 'and in', '']","['', '', '', '', '', 'Some researchers #TAUTHOR_TAG have explored the use of Wikipedia information to improve the disambiguation process .', '', '', '']",0
"['This leaves to the user the task of finding the pages relevant to the person he is interested in.', 'The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'dom the sharing', 'The Web People Search task , as defined in the first WePS evaluation campaign #TAUTHOR_TAG , consists of grouping search results for a given name according to the different people that share it .']","['This leaves to the user the task of finding the pages relevant to the person he is interested in.', 'The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'dominate the sharing', 'The Web People Search task , as defined in the first WePS evaluation campaign #TAUTHOR_TAG , consists of grouping search results for a given name according to the different people that share it .']","['This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'dom the task', 'The Web People Search task , as defined in the first WePS evaluation campaign #TAUTHOR_TAG , consists of grouping search results for a given name according to the different people that share it .']","['This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', '', 'The Web People Search task , as defined in the first WePS evaluation campaign #TAUTHOR_TAG , consists of grouping search results for a given name according to the different people that share it .']",0
"['We have used the testbeds from WePS-1 #TAUTHOR_TAG , 2007)2 and WePS-2 #AUTHOR_TAG evaluation campaigns 3.']","['We have used the testbeds from WePS-1 #TAUTHOR_TAG , 2007)2 and WePS-2 #AUTHOR_TAG evaluation campaigns 3.']","['We have used the testbeds from WePS-1 #TAUTHOR_TAG , 2007)2 and WePS-2 #AUTHOR_TAG evaluation campaigns 3.']","['We have used the testbeds from WePS-1 #TAUTHOR_TAG , 2007)2 and WePS-2 #AUTHOR_TAG evaluation campaigns 3.']",5
"['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguitying the structure of.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']","['The most used feature for the Web People Search task, however, are NEs.', ' #AUTHOR_TAG introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance #TAUTHOR_TAG .', '', '', '', '']",0
"['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) #AUTHOR_TAG and Cross-document Coreference ( CDC ) #TAUTHOR_TAG .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses found in the W.', 'It is only recently that the web ambiguity has been approached as problem and defined anLP task']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) #AUTHOR_TAG and Cross-document Coreference ( CDC ) #TAUTHOR_TAG .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses found in the WSD literature.', 'It is only recently that the web ambiguity has been approached as problem and defined an NLP task']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) #AUTHOR_TAG and Cross-document Coreference ( CDC ) #TAUTHOR_TAG .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as and defined']","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) #AUTHOR_TAG and Cross-document Coreference ( CDC ) #TAUTHOR_TAG .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own #AUTHOR_TAG .']",0
"['', '', 'and', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features #TAUTHOR_TAG .', '', '', '']","['', '', 'and', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features #TAUTHOR_TAG .', '', '', '']","['', '', 'and', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features #TAUTHOR_TAG .', '', '', '']","['', '', '', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features #TAUTHOR_TAG .', '', '', '']",0
"['Many different have to documents an ambiguous is', 'The most', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #TAUTHOR_TAG and sometimes in combination with others see for instance #AUTHOR_TAG - .', 'link #AUTHOR_TAG #AUTHOR_TAG .', '', '', '', '']","['Many different have to documents an ambiguous is', 'The most', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #TAUTHOR_TAG and sometimes in combination with others see for instance #AUTHOR_TAG - .', 'link #AUTHOR_TAG #AUTHOR_TAG .', '', '', '', '']","['Many different features to documents an ambiguous name is', 'The most', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #TAUTHOR_TAG and sometimes in combination with others see for instance #AUTHOR_TAG - .', ' #AUTHOR_TAG #AUTHOR_TAG .', '', '', '', '']","['Many different features have been used to represent documents where an ambiguous name is mentioned.', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #TAUTHOR_TAG and sometimes in combination with others see for instance #AUTHOR_TAG - .', '', '', '', '', '']",0
"['have to an ambig is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #AUTHOR_TAG and sometimes in combination with others see for instance #TAUTHOR_TAG - .', 'link #AUTHOR_TAG or extracted features #AUTHOR_TAG .', '', '', '', '']","['have to an ambiguous is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #AUTHOR_TAG and sometimes in combination with others see for instance #TAUTHOR_TAG - .', 'link #AUTHOR_TAG or extracted features #AUTHOR_TAG .', '', '', '', '']","['to an ambiguous name is', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #AUTHOR_TAG and sometimes in combination with others see for instance #TAUTHOR_TAG - .', ' #AUTHOR_TAG or the extracted features #AUTHOR_TAG .', '', '', '', '']","['', '', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name #AUTHOR_TAG .', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature #AUTHOR_TAG and sometimes in combination with others see for instance #TAUTHOR_TAG - .', '', '', '', '', '']",0
"['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', ' #AUTHOR_TAG have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach #TAUTHOR_TAG .', 'The number and type ( of GRs for whichs can be reliably sp']","['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', ' #AUTHOR_TAG have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach #TAUTHOR_TAG .', 'The number and type (and of GRs for which SPs can be reliably']","['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', ' #AUTHOR_TAG have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach #TAUTHOR_TAG .', 'The number and type ( of GRs for whichs can be reliably sp']","['In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', ' #AUTHOR_TAG have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach #TAUTHOR_TAG .', '']",3
"['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', ' #AUTHOR_TAG have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach #TAUTHOR_TAG .', 'The number and type ( of GRs for whichs can be reliably sp']","['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', ' #AUTHOR_TAG have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach #TAUTHOR_TAG .', 'The number and type (and of GRs for which SPs can be reliably']","['our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', ' #AUTHOR_TAG have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach #TAUTHOR_TAG .', 'The number and type ( of GRs for whichs can be reliably sp']","['In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', ' #AUTHOR_TAG have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach #TAUTHOR_TAG .', '']",3
"['ject based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #TAUTHOR_TAG for self training', '']","['based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #TAUTHOR_TAG for self training', '']","['on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #TAUTHOR_TAG for self training .', '']","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #TAUTHOR_TAG for self training .', '']",3
"['ject based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #TAUTHOR_TAG for self training', '']","['based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #TAUTHOR_TAG for self training', '']","['on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #TAUTHOR_TAG for self training .', '']","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #TAUTHOR_TAG for self training .', '']",3
"['the EM training algorithm is able to exploit the information available in both gold and labeled data with more complex.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #AUTHOR_TAG for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations #TAUTHOR_TAG , although training would be much slower compared to using generative models , as in our case .']","['the EM training algorithm is able to exploit the information available in both gold and labeled data with more complex', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #AUTHOR_TAG for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations #TAUTHOR_TAG , although training would be much slower compared to using generative models , as in our case .']","['the EM training algorithm is able to exploit the information available in both gold and labeled data with more.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #AUTHOR_TAG for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations #TAUTHOR_TAG , although training would be much slower compared to using generative models , as in our case .']","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches #AUTHOR_TAG for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations #TAUTHOR_TAG , although training would be much slower compared to using generative models , as in our case .']",3
"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']",3
"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']",3
"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #TAUTHOR_TAG .']",3
"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #AUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #AUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #AUTHOR_TAG .']","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG or shallow semantic trees , #AUTHOR_TAG .']",3
"['', 'build', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique #TAUTHOR_TAG or a memory-efficient trie implementation based on a succinct data structure #AUTHOR_TAG to reduce required memory usage .']","['', 'build', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique #TAUTHOR_TAG or a memory-efficient trie implementation based on a succinct data structure #AUTHOR_TAG to reduce required memory usage .']","['', '', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique #TAUTHOR_TAG or a memory-efficient trie implementation based on a succinct data structure #AUTHOR_TAG to reduce required memory usage .']","['', '', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique #TAUTHOR_TAG or a memory-efficient trie implementation based on a succinct data structure #AUTHOR_TAG to reduce required memory usage .']",3
"['The reordering models we describe follow our previous work using function word models for translation #TAUTHOR_TAG .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'this insight for alignment ', '']","['The reordering models we describe follow our previous work using function word models for translation #TAUTHOR_TAG .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'this insight for alignment,', '']","['The reordering models we describe follow our previous work using function word models for translation #TAUTHOR_TAG .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'this insight for alignment', '']","['The reordering models we describe follow our previous work using function word models for translation #TAUTHOR_TAG .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '', '']",2
"['Our reordering model is closely related7 with respect to conditioning the reordering items', 'model fixed words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system #TAUTHOR_TAG .', 'However, U uses deep syntactic analysis and hand-crafted heuristics in its model.']","['Our reordering model is closely related with respect to conditioning the reordering items.', 'model fixed words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system #TAUTHOR_TAG .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.']","['Our reordering model is closely related with respect to conditioning the reordering predictions', 'model function words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system #TAUTHOR_TAG .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.']","['Our reordering model is closely related to the model proposed by #AUTHOR_TAG ;2007a), with respect to conditioning the reordering predictions on lexical items.', '', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system #TAUTHOR_TAG .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.']",1
"['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TAUTHOR_TAG .', ', this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).', ', the reordering into one of the following four orientation values (borrowed from #AUTHOR_TAG ) with respect to the function wordacentap andap', '', '']","['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TAUTHOR_TAG .', 'this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).', 'particular, the reordering into one of the following four orientation values (borrowed from #AUTHOR_TAG ) with respect to the function word: Gap and Gap', '', '']","['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TAUTHOR_TAG .', ', this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).', ', o the reordering into one of the following four orientation values (borrowed from #AUTHOR_TAG ) with respect to the function word and', '', '']","['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #TAUTHOR_TAG .', 'Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).', '', '', '']",5
"['The reordering models we describe follow our previous work using function word models for translation #TAUTHOR_TAG .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'this insight for alignment ', '']","['The reordering models we describe follow our previous work using function word models for translation #TAUTHOR_TAG .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'this insight for alignment,', '']","['The reordering models we describe follow our previous work using function word models for translation #TAUTHOR_TAG .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'this insight for alignment', '']","['The reordering models we describe follow our previous work using function word models for translation #TAUTHOR_TAG .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', '', '']",2
"['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TAUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the form']","['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TAUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the form']","['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TAUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the form']","['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #TAUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the form']",5
"['In our previous work #TAUTHOR_TAG , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that work on entail.', '', '']","['In our previous work #TAUTHOR_TAG , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that work on entailment.', '', '']","['In our previous work #TAUTHOR_TAG , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that previous work onual entailment.', '', '']","['In our previous work #TAUTHOR_TAG , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that is motivated by previous work on textual entailment.', '', '']",2
"['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work #TAUTHOR_TAG and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']","['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work #TAUTHOR_TAG and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']","['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work #TAUTHOR_TAG and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']","['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work #TAUTHOR_TAG and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']",2
"['Note that in our original work #TAUTHOR_TAG , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']","['Note that in our original work #TAUTHOR_TAG , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']","['Note that in our original work #TAUTHOR_TAG , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']","['Note that in our original work #TAUTHOR_TAG , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']",1
"['', '', '', '', 'Figure 3 shows an Example. in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in #TAUTHOR_TAG .']","['', '', '', '', 'Figure 3 shows an Example in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in #TAUTHOR_TAG .']","['', '', '', '', 'Figure 3 shows an example Example 2. in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in #TAUTHOR_TAG .']","['', '', '', '', 'Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in #TAUTHOR_TAG .']",5
"['To address this limitation , our previous work #TAUTHOR_TAG has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', ', as in Example , the hypothesis can be entailed from the segment the', '', '']","['To address this limitation , our previous work #TAUTHOR_TAG has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', 'instance, as in Example 1, the hypothesis can be entailed from the segment the', '', '']","['To address this limitation , our previous work #TAUTHOR_TAG has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', ', as in Example 1, the first hypothesis can be entailed from the conversation segment', '', '']","['To address this limitation , our previous work #TAUTHOR_TAG has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', 'For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', '', '']",2
"['In our previous work #TAUTHOR_TAG , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This based a is entailed from segment is condition independent']","['In our previous work #TAUTHOR_TAG , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This based a is entailed from segment is conditionally independent']","['In our previous work #TAUTHOR_TAG , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This is based a clause is entailed from is conditionally independent']","['In our previous work #TAUTHOR_TAG , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', '']",2
"['Linear learning machines are one of the most popular machines used for classification problems.', 'The objective of a typical classification problem is to learn a function that separates the data into different classes.', 'The data is usually in the form of features extracted from abstract objects like strings, trees', 'A of learning using complex functions is that complex functions do not general well thus tend to over', '', '', '', '', '', '', '', '', '', '', 'our classification task', 'we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by #TAUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'derived from a the underlying', '', '', '']","['Linear learning machines are one of the most popular machines used for classification problems.', 'The objective of a typical classification problem is to learn a function that separates the data into different classes.', 'The data is usually in the form of features extracted from abstract objects like strings, trees,', 'A of learning using complex functions is that complex functions do not generalize well thus tend to', '', '', '', '', '', '', '', '', '', '', 'our classification task.', 'we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by #TAUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'derived from a the underlying', '', '', '']","['Linear learning machines are one of the most popular machines used for classification problems.', 'The objective of a typical classification problem is to learn a function that separates the data into different classes.', 'The data is usually in the form of features extracted from abstract objects like strings', 'of learning using complex functions is that complex functions do not generalize well thus tend to over', '', '', '', '', '', '', '', '', '', '', 'our classification task.', 'we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by #TAUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'are derived from a variation', '', '', '']","['Linear learning machines are one of the most popular machines used for classification problems.', 'The objective of a typical classification problem is to learn a function that separates the data into different classes.', 'The data is usually in the form of features extracted from abstract objects like strings, trees, etc.', 'A drawback of learning by using complex functions is that complex functions do not generalize well and thus tend to over-fit.', '', '', '', '', '', '', '', '', '', '', '', 'Now we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by #TAUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', '', '', '', '']",5
"['', '', '', '', 'dependency structures perform the.', 'This revalidates the observation of #TAUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', '']","['', '', '', '', 'structures perform the best.', 'This revalidates the observation of #TAUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', '']","['', '', '', '', 'dependencybased structures perform the best.', 'This revalidates the observation of #TAUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', '']","['', '', '', '', '', 'This revalidates the observation of #TAUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', '']",1
"['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'inserted) the', 'Here , the PET and GR kernel perform similar : this is different from the results of #TAUTHOR_TAG where GR performed much worse than PET']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'inserted nodes) the', 'Here , the PET and GR kernel perform similar : this is different from the results of #TAUTHOR_TAG where GR performed much worse than PET']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'are inserted', 'Here , the PET and GR kernel perform similar : this is different from the results of #TAUTHOR_TAG where GR performed much worse than PET']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Here , the PET and GR kernel perform similar : this is different from the results of #TAUTHOR_TAG where GR performed much worse than PET']",1
"['Our results also confirm the insights gained by #TAUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.']","['Our results also confirm the insights gained by #TAUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.']","['Our results also confirm the insights gained by #TAUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.']","['Our results also confirm the insights gained by #TAUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.']",1
"['', '', '', '', '', 'employ the opinion target vocabularies are substantially', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation #TAUTHOR_TAG , perform in comparison to our approach .', '', '', '', '']","['', '', '', '', '', 'employ the opinion target vocabularies are substantially', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation #TAUTHOR_TAG , perform in comparison to our approach .', '', '', '', '']","['', '', '', '', '', 'we employ the opinion target vocabularies are substantially', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation #TAUTHOR_TAG , perform in comparison to our approach .', '', '', '', '']","['', '', '', '', '', '', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation #TAUTHOR_TAG , perform in comparison to our approach .', '', '', '', '']",3
"['', '', '', '', '', 'employ the opinion target vocabularies are substantially', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation #TAUTHOR_TAG , perform in comparison to our approach .', '', '', '', '']","['', '', '', '', '', 'employ the opinion target vocabularies are substantially', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation #TAUTHOR_TAG , perform in comparison to our approach .', '', '', '', '']","['', '', '', '', '', 'we employ the opinion target vocabularies are substantially', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation #TAUTHOR_TAG , perform in comparison to our approach .', '', '', '', '']","['', '', '', '', '', '', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation #TAUTHOR_TAG , perform in comparison to our approach .', '', '', '', '']",3
"['An implementation of the transition-based dependency parsing frame- work #TAUTHOR_TAG using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #AUTHOR_TAG with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', '', '']","['An implementation of the transition-based dependency parsing frame- work #TAUTHOR_TAG using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #AUTHOR_TAG with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', '', '']","['An implementation of the transition-based dependency parsing frame- work #TAUTHOR_TAG using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #AUTHOR_TAG with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', '', '']","['An implementation of the transition-based dependency parsing frame- work #TAUTHOR_TAG using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in #AUTHOR_TAG with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', '', '']",5
"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and Mc #AUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #TAUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and Mc #AUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #TAUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and Mc #AUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #TAUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and Mc #AUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #TAUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']",0
"['', 'We use the non-projective k-best MST algorithm to generate k-best lists #TAUTHOR_TAG , where k = 8 for the experiments in this paper .', '', '']","['', 'We use the non-projective k-best MST algorithm to generate k-best lists #TAUTHOR_TAG , where k = 8 for the experiments in this paper .', '', '']","['', 'We use the non-projective k-best MST algorithm to generate k-best lists #TAUTHOR_TAG , where k = 8 for the experiments in this paper .', '', '']","['', 'We use the non-projective k-best MST algorithm to generate k-best lists #TAUTHOR_TAG , where k = 8 for the experiments in this paper .', '', '']",5
"['Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', ' #TAUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table', '', '', '', '', '']","['Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', ' #TAUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table', '', '', '', '', '']","['Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', ' #TAUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table', '', '', '', '', '']","['Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', ' #TAUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', '', '', '', '', '', '']",1
['1Our rules are similar to those from #TAUTHOR_TAG .'],['1Our rules are similar to those from #TAUTHOR_TAG .'],['1Our rules are similar to those from #TAUTHOR_TAG .'],['1Our rules are similar to those from #TAUTHOR_TAG .'],1
"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #TAUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #TAUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #TAUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #TAUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']",0
"['An implementation of graph- based parsing algorithms with an arc-factored parameterization #TAUTHOR_TAG .', '', '', '']","['An implementation of graph- based parsing algorithms with an arc-factored parameterization #TAUTHOR_TAG .', '', '', '']","['An implementation of graph- based parsing algorithms with an arc-factored parameterization #TAUTHOR_TAG .', '', '', '']","['An implementation of graph- based parsing algorithms with an arc-factored parameterization #TAUTHOR_TAG .', '', '', '']",5
"['A recent study by also investigates the task of training parsers to improve MT reordering.', 'In that work, a is used to a re to produce k-best lists.', 'The parse with the reordering is then fixed and added back to the training set and a new parser is trained on resulting data', 'method is called targeted self-training as it is similar in vein to self-training #TAUTHOR_TAG , with the exception that the new parse data is targeted to produce accurate word reorderings', '', '', '', '', '', '', '']","['A recent study by also investigates the task of training parsers to improve MT reordering.', 'In that work, a is used to a reordered to produce k-best lists.', 'The parse with the reordering is then fixed and added back to the training set and a new parser is trained on resulting data.', 'method is called targeted self-training as it is similar in vein to self-training #TAUTHOR_TAG , with the exception that the new parse data is targeted to produce accurate word reorderings', '', '', '', '', '', '', '']","['A recent study by also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to a set re to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training #TAUTHOR_TAG , with the exception that the new parse data is targeted to produce accurate word reorderings .', '', '', '', '', '', '', '']","['A recent study by also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training #TAUTHOR_TAG , with the exception that the new parse data is targeted to produce accurate word reorderings .', '', '', '', '', '', '', '']",1
"['', '', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality #TAUTHOR_TAG and is simpler to measure .', '']","['', '', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality #TAUTHOR_TAG and is simpler to measure .', '']","['', '', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality #TAUTHOR_TAG and is simpler to measure .', '']","['', '', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality #TAUTHOR_TAG and is simpler to measure .', '']",4
"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #TAUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #TAUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #TAUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #TAUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']",0
"['• Transition-based: An implementation of the transition-based dependency parsing framework #AUTHOR_TAG using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TAUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', '', '']","['• Transition-based: An implementation of the transition-based dependency parsing framework #AUTHOR_TAG using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TAUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', '', '']","['• Transition-based: An implementation of the transition-based dependency parsing framework #AUTHOR_TAG using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TAUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', '', '']","['• Transition-based: An implementation of the transition-based dependency parsing framework #AUTHOR_TAG using an arc-eager transition strategy and are trained using the perceptron algorithm as in #TAUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', '', '']",5
"['The and of- dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The and of dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The accuracy and of-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering sentiment analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', '', '', '', '']",0
"['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) #TAUTHOR_TAG .', 'We also make use of the Brown corpus, and the Question Treebank (QTB) #AUTHOR_TAG']","['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) #TAUTHOR_TAG .', 'We also make use of the Brown corpus, and the Question Treebank (QTB) #AUTHOR_TAG']","['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) #TAUTHOR_TAG .', 'We also make use of the Brown corpus, and the Question Treebank (QTB) #AUTHOR_TAG']","['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) #TAUTHOR_TAG .', 'We also make use of the Brown corpus, and the Question Treebank (QTB) #AUTHOR_TAG']",5
"['The and of- dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The and of dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The accuracy and of-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering sentiment analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', '', '', '', '']",0
"['The work that is most similar to ours is that of #TAUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented can be', '', '', '', '']","['The work that is most similar to ours is that of #TAUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented-loss can be', '', '', '', '']","['The work that is most similar to ours is that of #TAUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented-loss algorithm can be viewed', '', '', '', '']","['The work that is most similar to ours is that of #TAUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', '', '', '', '', '']",1
"['investig', '', '', '', '', '', 'evaluate the method on alternate extrinsic loss functions', ' #TAUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'to the-ranker loss function here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their is considerably they want to incorporate additional features into their model and define an objective function which allows them to do so for objective to adapt the parser model parameters to downstream tasks or alternative intrinsicing']","['investigates', '', '', '', '', '', 'evaluate the method on alternate extrinsic loss functions.', ' #TAUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'to the inline-ranker loss function here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their is considerably they want to incorporate additional features into their model and define an objective function which allows them to do so; for objective to adapt the parser model parameters to downstream tasks or alternative intrinsic']","['investig', '', '', '', '', '', 'evaluate the method on alternate extrinsic loss functions.', ' #TAUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'to the inline-ranker loss function here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their is considerably they want to incorporate additional features into their model and define an objective function which allows them to do so; for to adapt the parser model parameters to downstream tasks or alternative intrinsicing']","['', '', '', '', '', '', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', ' #TAUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']",1
"['A recent study by #TAUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training it is similar in vein to selftrainingMc with exception that', '', '', '', '', '', '', '']","['A recent study by #TAUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training it is similar in vein to self-training with exception that', '', '', '', '', '', '', '']","['A recent study by #TAUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training it is similar in vein to selftrainingMc with the exception that', '', '', '', '', '', '', '']","['A recent study by #TAUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', '', '', '', '', '', '', '', '']",1
"['', '', '', '', '', '', 'Optimizing for dependency length is particularly important as parsers tend to do worse on longer dependencies (Mc #AUTHOR_TAG and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction #TAUTHOR_TAG and textual entailment #AUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency length is particularly important as parsers tend to do worse on longer dependencies (Mc #AUTHOR_TAG and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction #TAUTHOR_TAG and textual entailment #AUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (Mc #AUTHOR_TAG and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction #TAUTHOR_TAG and textual entailment #AUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (Mc #AUTHOR_TAG and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction #TAUTHOR_TAG and textual entailment #AUTHOR_TAG .']",0
"['separ', '', '', '', '', '3).', 'def', 'Proof.', 'Identical to the standard perceptron proof , e.g. , #TAUTHOR_TAG , by inserting in loss-separability for normal separability .']","['', '', '', '', '', 'y).', '', 'Proof.', 'Identical to the standard perceptron proof , e.g. , #TAUTHOR_TAG , by inserting in loss-separability for normal separability .']","['separ', '', '', '', '', '3).', '', 'Proof.', 'Identical to the standard perceptron proof , e.g. , #TAUTHOR_TAG , by inserting in loss-separability for normal separability .']","['', '', '', '', '', '', '', 'Proof.', 'Identical to the standard perceptron proof , e.g. , #TAUTHOR_TAG , by inserting in loss-separability for normal separability .']",0
"['', '', '', '', '', '', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies #TAUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction #AUTHOR_TAG and textual entailment #AUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies #TAUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction #AUTHOR_TAG and textual entailment #AUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies #TAUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction #AUTHOR_TAG and textual entailment #AUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies #TAUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction #AUTHOR_TAG and textual entailment #AUTHOR_TAG .']",4
"['The and of- dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The and of dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The accuracy and of-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering sentiment analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', '', '', '', '']",0
"['One obvious approach to this problem is to employ parser reranking #TAUTHOR_TAG .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-ated jack-knife training framework).', 'The reranker can then be trained to optimize for the down or extrinsic', 'the original base']","['One obvious approach to this problem is to employ parser reranking #TAUTHOR_TAG .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extrinsic', 'the original base']","['One obvious approach to this problem is to employ parser reranking #TAUTHOR_TAG .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extr', 'the original base parser.']","['One obvious approach to this problem is to employ parser reranking #TAUTHOR_TAG .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extrinsic objective.', '']",0
"['', '', '', '', '', '', 'Optimizing for dependency length is particularly important as parsers tend to do worse on longer dependencies (Mc #AUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction #AUTHOR_TAG and textual entailment #TAUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency length is particularly important as parsers tend to do worse on longer dependencies (Mc #AUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction #AUTHOR_TAG and textual entailment #TAUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (Mc #AUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction #AUTHOR_TAG and textual entailment #TAUTHOR_TAG .']","['', '', '', '', '', '', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (Mc #AUTHOR_TAG and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction #AUTHOR_TAG and textual entailment #TAUTHOR_TAG .']",0
"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and Mc #AUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #TAUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and Mc #AUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #TAUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and Mc #AUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #TAUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and Mc #AUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #TAUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']",1
"['An implementation of the transition-based dependency parsing framework #TAUTHOR_TAG using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', '', '']","['An implementation of the transition-based dependency parsing framework #TAUTHOR_TAG using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', '', '']","['An implementation of the transition-based dependency parsing framework #TAUTHOR_TAG using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', '', '']","['An implementation of the transition-based dependency parsing framework #TAUTHOR_TAG using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', '', '']",5
"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation #TAUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #AUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the to framework draw connections to it.', 'In these the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation #TAUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #AUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the to framework draw connections to it', 'In these the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation #TAUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #AUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the to framework we draw connections to it', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation #TAUTHOR_TAG , posterior regularization #AUTHOR_TAG and constraint driven learning #AUTHOR_TAG .', 'The work of #AUTHOR_TAG on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', '', '']",0
"['next present a set of scoring functions can be usedanker loss, resulting in augmented-loss for each one.', '', 'one', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) #TAUTHOR_TAG .']","['next present a set of scoring functions can be used reranker loss framework, resulting in augmented-loss for each one.', '', 'one', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) #TAUTHOR_TAG .']","['we present a set of scoring functions can be used the inline reranker loss framework, resulting in a new augmented-loss for each one.', '', '', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) #TAUTHOR_TAG .']","['In the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one.', '', '', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) #TAUTHOR_TAG .']",5
"['The and of- dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The and of dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The accuracy and of-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This on question answering sentiment analysis MT reordering , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #AUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data #TAUTHOR_TAG .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', '', '', '', '']",0
"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #TAUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #TAUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #TAUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering #TAUTHOR_TAG , sentiment analysis #AUTHOR_TAG , MT reordering #AUTHOR_TAG , and many other tasks .', '', '', '', '', '', '']",0
['criteria and data used in our experiments are based on the work of #TAUTHOR_TAG .'],['criteria and data used in our experiments are based on the work of #TAUTHOR_TAG .'],['criteria and data used in our experiments are based on the work of #TAUTHOR_TAG .'],['criteria and data used in our experiments are based on the work of #TAUTHOR_TAG .'],5
"['In this paper , inspired by KNN-SVM #TAUTHOR_TAG , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', 'with online', '', '', '']","['In this paper , inspired by KNN-SVM #TAUTHOR_TAG , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', 'with online', '', '', '']","['In this paper , inspired by KNN-SVM #TAUTHOR_TAG , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', 'with', '', '', '']","['In this paper , inspired by KNN-SVM #TAUTHOR_TAG , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', '', '', '', '']",4
"['Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', 'Motivated by #TAUTHOR_TAG , 2003; #AUTHOR_TAG , we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST0 N05 N .39 6.3 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65-Hiero 31.24 27.07 26.32 9. Table', '']","['Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', 'Motivated by #TAUTHOR_TAG , 2003; #AUTHOR_TAG , we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table', '']","['Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', 'Motivated by #TAUTHOR_TAG , 2003; #AUTHOR_TAG , we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison', '']","['Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', '', '']",4
"[' #TAUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']","[' #TAUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']","[' #TAUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']","[' #TAUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']",0
"['Our method resorts to some translation examples , which is similar as example-based translation or translation memory #TAUTHOR_TAG .', 'of using translation examples to construct translation rules for enlarging the decoding, we employed them to discriminatively learn local weights']","['Our method resorts to some translation examples , which is similar as example-based translation or translation memory #TAUTHOR_TAG .', 'of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.']","['Our method resorts to some translation examples , which is similar as example-based translation or translation memory #TAUTHOR_TAG .', 'of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.']","['Our method resorts to some translation examples , which is similar as example-based translation or translation memory #TAUTHOR_TAG .', 'Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.']",1
"['', '', '', 'if we use LSH technique #TAUTHOR_TAG in retrieval process , the local method can be easily scaled to a larger training data .']","['', '', '', 'if we use LSH technique #TAUTHOR_TAG in retrieval process , the local method can be easily scaled to a larger training data .']","['', '', '', 'if we use LSH technique #TAUTHOR_TAG in retrieval process , the local method can be easily scaled to a larger training data .']","['', '', '', 'Actually , if we use LSH technique #TAUTHOR_TAG in retrieval process , the local method can be easily scaled to a larger training data .']",3
"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it', '.']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', 'functions.']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it', '']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', '']",1
"['The local training method #TAUTHOR_TAG is widely employed in computer vision #AUTHOR_TAG .', 'the', '']","['The local training method #TAUTHOR_TAG is widely employed in computer vision #AUTHOR_TAG .', 'the', '']","['The local training method #TAUTHOR_TAG is widely employed in computer vision #AUTHOR_TAG .', 'the', '']","['The local training method #TAUTHOR_TAG is widely employed in computer vision #AUTHOR_TAG .', '', '']",0
"['where f and ( are', 'feature vector a . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set', '', '', '']","['where f and (e are', 'feature vector a . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']","['where f and ( are', 'a feature vector a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']","['', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']",0
"[' #AUTHOR_TAGpus to', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits #AUTHOR_TAG with modified Kneser-Ney smoothing #TAUTHOR_TAG .', '', '', '']","[' #AUTHOR_TAG corpus to', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits #AUTHOR_TAG with modified Kneser-Ney smoothing #TAUTHOR_TAG .', '', '', '']","[' #AUTHOR_TAG to', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits #AUTHOR_TAG with modified Kneser-Ney smoothing #TAUTHOR_TAG .', '', '', '']","['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits #AUTHOR_TAG with modified Kneser-Ney smoothing #TAUTHOR_TAG .', '', '', '']",5
"[' #AUTHOR_TAGpus', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits #TAUTHOR_TAG with modified Kneser-Ney smoothing #AUTHOR_TAG .', '', '', '']","[' #AUTHOR_TAG corpus', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits #TAUTHOR_TAG with modified Kneser-Ney smoothing #AUTHOR_TAG .', '', '', '']","[' #AUTHOR_TAG', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits #TAUTHOR_TAG with modified Kneser-Ney smoothing #AUTHOR_TAG .', '', '', '']","['', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits #TAUTHOR_TAG with modified Kneser-Ney smoothing #AUTHOR_TAG .', '', '', '']",5
"['where f and ( are', 'feature vector a . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set', '', '', '']","['where f and (e are', 'feature vector a . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']","['where f and ( are', 'a feature vector a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']","['', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']",0
"['Compared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work #TAUTHOR_TAG , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', '']","['Compared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work #TAUTHOR_TAG , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', '']","['Compared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work #TAUTHOR_TAG , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', '']","['Compared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work #TAUTHOR_TAG , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', '']",0
"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used estimation to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used estimation to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']",1
"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used estimation to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used estimation to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', 'used to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']",1
"['', '', 'm-', '', 'The significance testing is performed by paired bootstrap re-sampling #TAUTHOR_TAG .']","['', '', '', '', 'The significance testing is performed by paired bootstrap re-sampling #TAUTHOR_TAG .']","['', '', 'm-', '', 'The significance testing is performed by paired bootstrap re-sampling #TAUTHOR_TAG .']","['', '', '', '', 'The significance testing is performed by paired bootstrap re-sampling #TAUTHOR_TAG .']",5
"['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking #TAUTHOR_TAG , 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'the classification', '']","['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking #TAUTHOR_TAG , 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'the classification', '']","['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking #TAUTHOR_TAG , 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'the transformed classification problem', '']","['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking #TAUTHOR_TAG , 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', '', '']",0
"['Several works have proposed discriminative techniques to train log-linear model for SMT.', ' #TAUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', 'employed an evaluation metric as a loss function and directly optimized it', '']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', ' #TAUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', 'employed an evaluation metric as a loss function and directly optimized it.', '']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', ' #TAUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', 'employed an evaluation metric as a loss function and directly optimized it', '']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', ' #TAUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', ' #AUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', '']",1
"['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , #TAUTHOR_TAG defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']","['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , #TAUTHOR_TAG defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']","['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , #TAUTHOR_TAG defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']","['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , #TAUTHOR_TAG defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']",5
"['where f and ( are', 'feature vector a . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set', '', '', '']","['where f and (e are', 'feature vector a . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']","['where f and ( are', 'a feature vector a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']","['', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #TAUTHOR_TAG , margin #AUTHOR_TAG and ranking #AUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']",0
"['', '', '', '', 'We employ the idea of ultraconservative update #TAUTHOR_TAG to propose two incremental methods for local training in Algorithm 2 as follows .']","['', '', '', '', 'We employ the idea of ultraconservative update #TAUTHOR_TAG to propose two incremental methods for local training in Algorithm 2 as follows .']","['', '', '', '', 'We employ the idea of ultraconservative update #TAUTHOR_TAG to propose two incremental methods for local training in Algorithm 2 as follows .']","['', '', '', '', 'We employ the idea of ultraconservative update #TAUTHOR_TAG to propose two incremental methods for local training in Algorithm 2 as follows .']",5
"['Several works have proposed discriminative techniques to train log-linear model for SMT.', 'used to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '.']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', 'used to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', 'functions.']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', 'used to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', ' #TAUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it .', '']",1
"['We use an in-house developed hierarchical phrase-based translation #TAUTHOR_TAG as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se- #AUTHOR_TAG', '', '']","['We use an in-house developed hierarchical phrase-based translation #TAUTHOR_TAG as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se- #AUTHOR_TAG', '', '']","['We use an in-house developed hierarchical phrase-based translation #TAUTHOR_TAG as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se- #AUTHOR_TAG .', '', '']","['We use an in-house developed hierarchical phrase-based translation #TAUTHOR_TAG as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se- #AUTHOR_TAG .', '', '']",5
"['We run GIZA + + #TAUTHOR_TAG on the training corpus in both directions #AUTHOR_TAG to obtain the word alignment for each sentence pair .', '', '', '', '']","['We run GIZA + + #TAUTHOR_TAG on the training corpus in both directions #AUTHOR_TAG to obtain the word alignment for each sentence pair .', '', '', '', '']","['We run GIZA + + #TAUTHOR_TAG on the training corpus in both directions #AUTHOR_TAG to obtain the word alignment for each sentence pair .', '', '', '', '']","['We run GIZA + + #TAUTHOR_TAG on the training corpus in both directions #AUTHOR_TAG to obtain the word alignment for each sentence pair .', '', '', '', '']",5
"['Several works have proposed discriminative techniques to train log-linear model for SMT.', 'used estimation weights for MT.', ' #AUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #TAUTHOR_TAG proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', 'used estimation weights for MT.', ' #AUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #TAUTHOR_TAG proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', 'used weights for MT.', ' #AUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #TAUTHOR_TAG proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']","['Several works have proposed discriminative techniques to train log-linear model for SMT.', ' #AUTHOR_TAG used maximum likelihood estimation to learn weights for MT.', ' #AUTHOR_TAG employed an evaluation metric as a loss function and directly optimized it.', ' #TAUTHOR_TAG proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']",1
"['where f and ( are', 'feature vector . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #AUTHOR_TAG , margin #AUTHOR_TAG and ranking #TAUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a set', '', '', '']","['where f and (e are', 'feature vector . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #AUTHOR_TAG , margin #AUTHOR_TAG and ranking #TAUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a set.', '', '', '']","['where f and ( are', 'a feature vector a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #AUTHOR_TAG , margin #AUTHOR_TAG and ranking #TAUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']","['', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood #AUTHOR_TAG , error rate #AUTHOR_TAG , margin #AUTHOR_TAG and ranking #TAUTHOR_TAG , and among which minimum error rate training ( MERT ) #AUTHOR_TAG is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', '', '', '']",0
"[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work #TAUTHOR_TAG .', '']","['Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work #TAUTHOR_TAG .', '']","[', our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work #TAUTHOR_TAG .', '']","['Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work #TAUTHOR_TAG .', '']",1
"['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features #AUTHOR_TAG .', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions #TAUTHOR_TAG .', 'For ax']","['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features #AUTHOR_TAG .', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions #TAUTHOR_TAG .', 'For a']","['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features #AUTHOR_TAG .', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions #TAUTHOR_TAG .', 'For a lexicon']","['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features #AUTHOR_TAG .', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions #TAUTHOR_TAG .', '']",2
"['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work #TAUTHOR_TAG .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to a joint perceptual basis, expression generation (REG) an important in situated', '', '']","['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work #TAUTHOR_TAG .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to a joint perceptual basis, expression generation (REG) an important in situated', '', '']","['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work #TAUTHOR_TAG .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to a joint perceptual basis, referring expression generation (REG) an equally important problem in', '', '']","['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work #TAUTHOR_TAG .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.', '', '']",2
"['approaches mult research have by abstracting away raw perceptual information and using high-level representations instead.', '', '', '', 'this topic', 'In a similar vein , #TAUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', '', '']","['approaches research have by abstracting away raw perceptual information and using high-level representations instead.', '', '', '', 'this topic', 'In a similar vein , #TAUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', '', '']","['Many approaches mult research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', '', '', '', 'this work', 'In a similar vein , #TAUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', '', '']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', '', '', '', '', 'In a similar vein , #TAUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', '', '']",0
"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat egory, the two most common representations have been association norms, subjects are givenue words']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat- egory, the two most common representations have been association norms, subjects are given cue words']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat egory, the two most common representations have been association norms, subjects are given words']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', '']",0
"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', 'They use a Bag of Visual Words ( BoVW ) model #TAUTHOR_TAG to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocforms a purelyual word association and word prediction', '', '', '', '']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', 'They use a Bag of Visual Words ( BoVW ) model #TAUTHOR_TAG to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocabulary a purely textual word association and word prediction.', '', '', '', '']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', 'They use a Bag of Visual Words ( BoVW ) model #TAUTHOR_TAG to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocforms a purelyual word association and', '', '', '', '']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', 'They use a Bag of Visual Words ( BoVW ) model #TAUTHOR_TAG to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '', '', '', '']",0
"['computer techniques have improved over the, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b', '', '', '', '', ' #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition #TAUTHOR_TAG , subst for feature', 'Other work on modeling the meanings of verbs using video recognition has begun promise #AUTHOR_TAG']","['computer techniques have improved over the decade, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b).', '', '', '', '', ' #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition #TAUTHOR_TAG , substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has begun promise #AUTHOR_TAG']","['computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b).', '', '', '', '', ' #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition #TAUTHOR_TAG , for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #AUTHOR_TAG .']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', '', '', '', '', 'More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition #TAUTHOR_TAG , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #AUTHOR_TAG .']",0
"['The language grounding', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #TAUTHOR_TAG .']","['The language grounding', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #TAUTHOR_TAG .']","['The language grounding problem', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #TAUTHOR_TAG .']","['', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #TAUTHOR_TAG .']",0
"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #TAUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'these approaches have differed in model definition enhance']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #TAUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'these approaches have differed in model definition, enhance']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #TAUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'these approaches have differed in model definition enhance']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #TAUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', '']",0
"['That is, we simply take the original mLDA model of #TAUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', 'task become more observed task remains roughly difficulty tuple conditionally independent']","['That is, we simply take the original mLDA model of #TAUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', 'task become more observed task remains roughly difficulty, tuple conditionally independent']","['That is, we simply take the original mLDA model of #TAUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', 'should become more the task remains roughly a tuple are conditionally independent']","['That is, we simply take the original mLDA model of #TAUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', '']",2
"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning #TAUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some tasks']","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning #TAUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some tasks']","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning #TAUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts tasks']","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning #TAUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', '']",0
"[' #TAUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'their, a document consists of a set of (word, feature) pairs and documents stilled mi', '', 'The process is amended to']","[' #TAUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'their model, a document consists of a set of (word, feature) pairs, and documents still modeled mixtures', '', 'The process is amended to']","[' #TAUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'their, a document consists of a set of (word, feature) pairs and documents are stilled mi', '', 'The generative process is amended to']","[' #TAUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', '', '', '']",0
"['', '', '', 's', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering #TAUTHOR_TAG , and images are then quantized over the 5,000 codewords .', '', '']","['', '', '', '', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering #TAUTHOR_TAG , and images are then quantized over the 5,000 codewords .', '', '']","['', '', '', '', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering #TAUTHOR_TAG , and images are then quantized over the 5,000 codewords .', '', '']","['', '', '', '', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering #TAUTHOR_TAG , and images are then quantized over the 5,000 codewords .', '', '']",5
"['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features #TAUTHOR_TAG .']","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features #TAUTHOR_TAG .']","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features #TAUTHOR_TAG .']","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features #TAUTHOR_TAG .']",1
"['computer techniques have improved the, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b', '', '', '', '', 'features', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #TAUTHOR_TAG .']","['computer techniques have improved the decade, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b).', '', '', '', '', 'feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #TAUTHOR_TAG .']","['computer vision techniques have improved the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b).', '', '', '', '', 'feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #TAUTHOR_TAG .']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', '', '', '', '', '', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #TAUTHOR_TAG .']",0
"['Many approaches to multimal research have by abstracting away raw perceptual in- formation and-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations #AUTHOR_TAG , while others choose to employ concepts elicited from psych- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , #AUTHOR_TAG ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #TAUTHOR_TAG .']","['Many approaches to multimodal research have by abstracting away raw perceptual in- formation and high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations #AUTHOR_TAG , while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , #AUTHOR_TAG ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #TAUTHOR_TAG .']","['Many approaches to multimal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations #AUTHOR_TAG , while others choose to employ concepts elicited from psych- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , #AUTHOR_TAG ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #TAUTHOR_TAG .']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations #AUTHOR_TAG , while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , #AUTHOR_TAG ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #TAUTHOR_TAG .']",0
"['The language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning #AUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions #TAUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts tack tasks suchaption']","['The language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning #AUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions #TAUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts tackled tasks such caption']","['The language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning #AUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions #TAUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts tasks such']","['The language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning #AUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions #TAUTHOR_TAG or robot commands #AUTHOR_TAG .', '']",0
"['The language', '', 'automatic m language instructions to execut, such as or', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #TAUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['The language', '', 'automatic mappings language instructions to executable actions, such as or', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #TAUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['', '', 'automatic mappings natural language instructions to execut, such as or', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #TAUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['', '', '', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #TAUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']",0
"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #TAUTHOR_TAG .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in to most criticing words words #AUTHOR_TAG']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #TAUTHOR_TAG .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in to most criticisms words words"" #AUTHOR_TAG']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #TAUTHOR_TAG .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in to most words']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #TAUTHOR_TAG .', '']",0
"['evaluate our algorithms we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #TAUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words grounded features are all given the placeholder feature, word.', '', '']","['evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #TAUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words grounded features are all given the placeholder feature, word-', '', '']","['evaluate our algorithms we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #TAUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words grounded features are all given the same placeholder feature,', '', '']","['In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #TAUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5', '', '']",5
"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']",0
"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TAUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #AUTHOR_TAG .', 'It has also been to be useful', '', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TAUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #AUTHOR_TAG .', 'It has also been to be useful', '', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TAUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #AUTHOR_TAG .', 'It has also been shown to be useful', '', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #TAUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #AUTHOR_TAG .', '', '', '', '']",5
"['', '', 'object words which share associ are related through settings and objects that with them.', ""This seems to provide additional evidence of #TAUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible .""]","['', '', 'words which share associates are related through settings and objects that with them.', ""This seems to provide additional evidence of #TAUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible .""]","['', '', 'object words which share associates are related through common settings and objects that with them.', ""This seems to provide additional evidence of #TAUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible .""]","['', '', '', ""This seems to provide additional evidence of #TAUTHOR_TAG b ) 's suggestion that something like a distributional hypothesis of images is plausible .""]",1
"['compute G', '', '', 'It is frequently used in tasks like scene identification , and #TAUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'G, each textual word is represented as theroid GIST vector of all its images, forming the GIST modality.']","['compute GIST', '', '', 'It is frequently used in tasks like scene identification , and #TAUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']","['compute G', '', '', 'It is frequently used in tasks like scene identification , and #TAUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', ', each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']","['', '', '', 'It is frequently used in tasks like scene identification , and #TAUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']",4
"['As computer vision techniques have improved over the decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', 'They a WW model #AUTHOR_TAG to create a bimodal vocabulary describing documents', 'The topic using the bodal vocabulary outperforms a purely textual model in word association and wordity prediction', ' #TAUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', '', '', '']","['As computer vision techniques have improved over the decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', 'They a Words model #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'The topic using the bimodal vocabulary outperforms a purely textual model in word association and word similarity prediction.', ' #TAUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', '', '', '']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', 'They use a BagW) model #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual model in word association and word similarity prediction.', ' #TAUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', '', '', '']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', 'They use a Bag of Visual Words (BoVW) model #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', ' #TAUTHOR_TAG a ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', '', '', '']",0
"['The languageing', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG', 'Some efforts have tackled tasks such as automatic image caption generation #TAUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['The language grounding', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG', 'Some efforts have tackled tasks such as automatic image caption generation #TAUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['The language grounding problem', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation #TAUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation #TAUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']",0
"['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group #TAUTHOR_TAG containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length than 100.', 'The resulting corpus has 138883 documents consisting758 word types and 466M word tokens.']","['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group #TAUTHOR_TAG containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length than 100.', 'The resulting corpus has 1,038,883 documents consisting word types and 466M word tokens.']","['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group #TAUTHOR_TAG containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length than 100.', 'The resulting corpus has 138883 documents consisting758 word types and 466M word tokens.']","['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group #TAUTHOR_TAG containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100.', 'The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens.']",5
"['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TAUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TAUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TAUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #TAUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']",0
"['approaches mult research have by abstracting away raw perceptual information and using high-level representations instead', '', '', '', '', 'blank', ' #TAUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', '']","['approaches research have by abstracting away raw perceptual information and using high-level representations instead.', '', '', '', '', '', ' #TAUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', '']","['Many approaches mult research have succeeded by abstracting away raw perceptual information and using high-level representations instead', '', '', '', '', '', ' #TAUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', '']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', '', '', '', '', '', ' #TAUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', '']",0
"['as this', 'Following #TAUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible of words in our voc compute negative symmetric', '', '']","['as this', 'Following #TAUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible of words in our compute negative symmetric', '', '']","['as this evaluation', 'Following #TAUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our voc we compute', '', '']","['', 'Following #TAUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', '', '', '']",5
"['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TAUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in to predict association norms over traditional text-only L.', 'UR']","['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TAUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in to predict association norms over traditional text-only LDA.', '']","['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TAUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in to predict association norms over traditional text-only LDA.', 'UR']","['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #TAUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.', '']",5
"['We also compute GIST vectors #TAUTHOR_TAG for every image using LearGIST #AUTHOR_TAG .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image', '', 'It is frequently used #AUTHOR_TAG', 'ual word is represented as the GIST all its, forming the GIST modality.']","['We also compute GIST vectors #TAUTHOR_TAG for every image using LearGIST #AUTHOR_TAG .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', '', 'It is frequently used #AUTHOR_TAG', 'textual word is represented as the GIST all its images, forming the GIST modality.']","['We also compute GIST vectors #TAUTHOR_TAG for every image using LearGIST #AUTHOR_TAG .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', '', 'It is frequently used #AUTHOR_TAG shows', 'each textual word is represented as the centroid GIST vector all its, forming the GIST modality.']","['We also compute GIST vectors #TAUTHOR_TAG for every image using LearGIST #AUTHOR_TAG .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', '', '', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']",5
"['The language grounding', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #TAUTHOR_TAG .']","['The language grounding', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #TAUTHOR_TAG .']","['The language grounding problem', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #TAUTHOR_TAG .']","['', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #AUTHOR_TAG , or automatic location identification of Twitter users #TAUTHOR_TAG .']",0
"['ingual information andlevel representations instead', 'itedition', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., Mc #AUTHOR_TAG ).', ' #TAUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis #AUTHOR_TAG in the prediction of association norms .', '', '', '', '']","['abstracting information and representations instead.', 'cognition', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., Mc #AUTHOR_TAG ).', ' #TAUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis #AUTHOR_TAG in the prediction of association norms .', '', '', '', '']","['and instead', 'ited', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., Mc #AUTHOR_TAG ).', ' #TAUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis #AUTHOR_TAG in the prediction of association norms .', '', '', '', '']","['', '', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., Mc #AUTHOR_TAG ).', ' #TAUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis #AUTHOR_TAG in the prediction of association norms .', '', '', '', '']",0
"['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and #TAUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and #TAUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and #TAUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and #TAUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']",0
"['BilderNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets #TAUTHOR_TAG .', 'Multiple synsets exist for each meaning of a word.', 'example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, the other contains images of the computer peripheral', 'This BilderNetle set mappings from German noun']","['BilderNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets #TAUTHOR_TAG .', 'Multiple synsets exist for each meaning of a word.', 'example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, the other contains images of the computer peripheral.', 'This BilderNetle set mappings from German noun']","['BilderNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets #TAUTHOR_TAG .', 'Multiple synsets exist for each meaning of a word.', 'example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, the other contains images of the computer peripheral.', 'This BilderNetle data set mappings from German noun types']","['BilderNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets #TAUTHOR_TAG .', 'Multiple synsets exist for each meaning of a word.', 'For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.', '']",5
"['', '', '', 'an successfully weighted mi based', ' #TAUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']","['', '', '', 'an successfully weighted mixtures based', ' #TAUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']","['', '', '', 'successfully weighted mi based', ' #TAUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']","['', '', '', '', ' #TAUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']",0
"['computer have improved other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is', '', '', '', '', 'recently , #TAUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition #AUTHOR_TAG , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #AUTHOR_TAG']","['computer have improved other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is', '', '', '', '', 'recently , #TAUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition #AUTHOR_TAG , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #AUTHOR_TAG']","['have improved other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is', '', '', '', '', 'recently , #TAUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition #AUTHOR_TAG , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #AUTHOR_TAG .']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', '', '', '', '', 'More recently , #TAUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition #AUTHOR_TAG , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #AUTHOR_TAG .']",0
"['Dirich at  atch', '.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TAUTHOR_TAG .']","['Dirichlet at', 'time.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TAUTHOR_TAG .']","['at atch sizes', '.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TAUTHOR_TAG .']","['', '', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #TAUTHOR_TAG .']",5
"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning with information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning with information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning withual information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', '']",0
"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']",0
"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #TAUTHOR_TAG .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers #AUTHOR_TAG .', 'al Lafter', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #TAUTHOR_TAG .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers #AUTHOR_TAG .', 'LDA', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #TAUTHOR_TAG .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers #AUTHOR_TAG .', 'after', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #TAUTHOR_TAG .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers #AUTHOR_TAG .', '', '', '']",0
"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #TAUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'these approaches have differed in model definition enhance']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #TAUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'these approaches have differed in model definition, enhance']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #TAUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'these approaches have differed in model definition enhance']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #TAUTHOR_TAG b ; #AUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', '']",0
"['computer techniques have improved the, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b', '', '', '', '', 'features', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #TAUTHOR_TAG .']","['computer techniques have improved the decade, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b).', '', '', '', '', 'feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #TAUTHOR_TAG .']","['computer vision techniques have improved the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to this with topic models is #AUTHOR_TAG b).', '', '', '', '', 'feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #TAUTHOR_TAG .']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAG b).', '', '', '', '', '', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise #TAUTHOR_TAG .']",0
"['Association Norms ( AN ) is a collection of association norms collected by Schulte im #TAUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'only the data set a total 95 cue-response pairs noun 56 response']","['Association Norms ( AN ) is a collection of association norms collected by Schulte im #TAUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'only the data set a total cue-response pairs nouns 5,716 response']","['Association Norms ( AN ) is a collection of association norms collected by Schulte im #TAUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'the data set a total 954 cue-response pairs2 nouns 56 response types.']","['Association Norms ( AN ) is a collection of association norms collected by Schulte im #TAUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', '']",5
"['Latent Dirichlet Allocation #TAUTHOR_TAG , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the cor is generated foling process']","['Latent Dirichlet Allocation #TAUTHOR_TAG , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated fol-lowing process:']","['Latent Dirichlet Allocation #TAUTHOR_TAG , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated using']","['Latent Dirichlet Allocation #TAUTHOR_TAG , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated using the fol-lowing generative process:']",0
"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephen./research/ emnlp13 cue word and name the first (or) words that come mind.,', '', '', '', '', '']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) words that come mind', '', '', '', '', '']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) words that come mind.,', '', '', '', '', '']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., Mc #AUTHOR_TAG ).', '', '', '', '', '']",0
"['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #TAUTHOR_TAG and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #TAUTHOR_TAG and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #TAUTHOR_TAG and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #TAUTHOR_TAG and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']",0
"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephen./research/ emnlp13 cue word and name the first (or) words that come mind.,', '', '', '', '', '']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) words that come mind', '', '', '', '', '']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) words that come mind.,', '', '', '', '', '']","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations #TAUTHOR_TAG , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., #AUTHOR_TAG ), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., Mc #AUTHOR_TAG ).', '', '', '', '', '']",0
"['informationlevel representations', '', 'representations', 'Dir Allper Latent Semantic Analysis #AUTHOR_TAG in the prediction of association norms', ' #TAUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', '', '', '']","['information representations', '', 'representations', 'Allocation Latent Semantic Analysis #AUTHOR_TAG in the prediction of association norms.', ' #TAUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', '', '', '']","['', '', '', 'per Latent Semantic Analysis #AUTHOR_TAG in the prediction of association norms.', ' #TAUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', '', '', '']","['', '', '', '', ' #TAUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', '', '', '']",0
"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning with information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning with information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning withual information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', '']",0
"['Our experiments are based on the multimodal extension of Latent Dirich Allocation developed by #AUTHOR_TAG', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #AUTHOR_TAG .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers #TAUTHOR_TAG .', 'These multimodal LDA models (hereafter, mLDA) have toitatively', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #AUTHOR_TAG .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers #TAUTHOR_TAG .', 'These multimodal LDA models (hereafter, mLDA) have to qualitatively', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #AUTHOR_TAG .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers #TAUTHOR_TAG .', 'These multimodal LDA models (hereafter, mLDA) to be qualitatively', '', '']","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together #AUTHOR_TAG .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers #TAUTHOR_TAG .', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks #AUTHOR_TAG .', '', '']",0
"['The languageing', '', 'Others provide automatic mappings of language instructions to execut actions, such as interpre navigation directions #AUTHOR_TAG or #AUTHOR_TAG', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #TAUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['The language grounding', '', 'Others provide automatic mappings of language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or #AUTHOR_TAG', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #TAUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['The language grounding problem', '', 'Others provide automatic mappings of natural language instructions to execut actions, such as interpreting navigation directions #AUTHOR_TAG or #AUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #TAUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']","['', '', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions #AUTHOR_TAG or robot commands #AUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation #AUTHOR_TAG a ; #AUTHOR_TAG , text illustration #TAUTHOR_TAG , or automatic location identification of Twitter users #AUTHOR_TAG .']",0
"['The language grounding problem has come in many different flavors with as different.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning #AUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions #AUTHOR_TAG or robot commands #TAUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation a;']","['The language grounding problem has come in many different flavors with as different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning #AUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions #AUTHOR_TAG or robot commands #TAUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation a; #AUTHOR_TAG']","['The language grounding problem has come in many different flavors with as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning #AUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions #AUTHOR_TAG or robot commands #TAUTHOR_TAG .', 'Some efforts have tackled tasks such as automatic image caption generation a;']","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning #AUTHOR_TAG .', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions #AUTHOR_TAG or robot commands #TAUTHOR_TAG .', '']",0
"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #AUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #TAUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #AUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #TAUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #AUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #TAUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #AUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #TAUTHOR_TAG .']",0
"['To solve these scaling issues , we implement Online Variational Bayesian Inference #TAUTHOR_TAG for our models .', 'esianferenceBI one approximates the true posterior', '', '', '', '']","['To solve these scaling issues , we implement Online Variational Bayesian Inference #TAUTHOR_TAG for our models .', 'Bayesian Inference one approximates the true posterior', '', '', '', '']","['To solve these scaling issues , we implement Online Variational Bayesian Inference #TAUTHOR_TAG for our models .', 'BI), one approximates the true posterior using', '', '', '', '']","['To solve these scaling issues , we implement Online Variational Bayesian Inference #TAUTHOR_TAG for our models .', '', '', '', '', '']",5
"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning with information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning with information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning withual information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', '']",0
"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning with information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning with information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', 'Although these approaches have differed in model definition, to enhance word meaning withual information in']","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #AUTHOR_TAG b ; #TAUTHOR_TAG a ; #AUTHOR_TAG b ; #AUTHOR_TAG .', '']",0
"['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features #TAUTHOR_TAG .']","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features #TAUTHOR_TAG .']","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features #TAUTHOR_TAG .']","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features #TAUTHOR_TAG .']",1
"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']",0
"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , #AUTHOR_TAG ) , computing power , improved computer vision models #TAUTHOR_TAG and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; #AUTHOR_TAG Aziz- #AUTHOR_TAG .']",0
"['informationlevel representations instead', '', '', ' #AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis #TAUTHOR_TAG in the prediction of association norms .', '', '', '', '']","['information representations instead.', '', '', ' #AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis #TAUTHOR_TAG in the prediction of association norms .', '', '', '', '']","['instead', '', '', ' #AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis #TAUTHOR_TAG in the prediction of association norms .', '', '', '', '']","['', '', '', ' #AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis #TAUTHOR_TAG in the prediction of association norms .', '', '', '', '']",0
"['To solve these scaling issues , we implement Online Variational Bayesian Inference #TAUTHOR_TAG for our models .', 'esianferenceBI one approximates the true posterior', '', '', '', '']","['To solve these scaling issues , we implement Online Variational Bayesian Inference #TAUTHOR_TAG for our models .', 'Bayesian Inference one approximates the true posterior', '', '', '', '']","['To solve these scaling issues , we implement Online Variational Bayesian Inference #TAUTHOR_TAG for our models .', 'BI), one approximates the true posterior using', '', '', '', '']","['To solve these scaling issues , we implement Online Variational Bayesian Inference #TAUTHOR_TAG for our models .', '', '', '', '', '']",5
"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #TAUTHOR_TAG b ) .', 'They use a Bag of Visual Words (BoVW) model #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '', '', '', '']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #TAUTHOR_TAG b ) .', 'They use a Bag of Visual Words (BoVW) model #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '', '', '', '']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #TAUTHOR_TAG b ) .', 'They use a Bag of Visual Words (BoVW) model #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '', '', '', '']","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #TAUTHOR_TAG b ) .', 'They use a Bag of Visual Words (BoVW) model #AUTHOR_TAG to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '', '', '', '']",0
"['', 'model we rely on was originally developed by #AUTHOR_TAG and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity #TAUTHOR_TAG .', '', '', '', '', '']","['', 'model we rely on was originally developed by #AUTHOR_TAG and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity #TAUTHOR_TAG .', '', '', '', '', '']","['', 'The model we rely on was originally developed by #AUTHOR_TAG and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity #TAUTHOR_TAG .', '', '', '', '', '']","['', 'The model we rely on was originally developed by #AUTHOR_TAG and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity #TAUTHOR_TAG .', '', '', '', '', '']",2
"['a labeling', '', 'choice is motivated by an observation we made previously #TAUTHOR_TAG preced']","['a labeling', '', 'choice is motivated by an observation we made previously #TAUTHOR_TAG preceding']","['', '', 'This choice is motivated by an observation we made previously #TAUTHOR_TAG']","['', '', '']",2
"['Frame-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a.', 'Following our previous work on stance classification #TAUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR #AUTHOR_TAG .', '', '']","['Frame-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification #TAUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR #AUTHOR_TAG .', '', '']","['Frame-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification #TAUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR #AUTHOR_TAG .', '', '']","['Frame-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification #TAUTHOR_TAG c ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR #AUTHOR_TAG .', '', '']",2
"['', '', 'Our experimental design with professional bilingual translators follows our previous work #TAUTHOR_TAG a ) comparing scratch translation to post-edit .', '', '']","['', '', 'Our experimental design with professional bilingual translators follows our previous work #TAUTHOR_TAG a ) comparing scratch translation to post-edit .', '', '']","['', '', 'Our experimental design with professional bilingual translators follows our previous work #TAUTHOR_TAG a ) comparing scratch translation to post-edit .', '', '']","['', '', 'Our experimental design with professional bilingual translators follows our previous work #TAUTHOR_TAG a ) comparing scratch translation to post-edit .', '', '']",2
"[""Table1 shows theson's product correlation between each topical feature and candidate's power."", '', 'inencies is significantlyrelated', '', 'This is in line with our previous findings from #TAUTHOR_TAG that candidates with higher power attempt to shift topics less often than others when responding to moderators', '', '']","[""Table 1 shows the Pearson's product correlation between each topical feature and candidate's power."", '', 'in tendencies is significantly correlated', '', 'This is in line with our previous findings from #TAUTHOR_TAG that candidates with higher power attempt to shift topics less often than others when responding to moderators', '', '']","[""Table1 shows the Pearson's product correlation between each topical feature and candidate's power."", '', 'inencies is significantlyrelated', '', 'This is in line with our previous findings from #TAUTHOR_TAG that candidates with higher power attempt to shift topics less often than others when responding to moderators .', '', '']","[""Table 1 shows the Pearson's product correlation between each topical feature and candidate's power."", '', '', '', 'This is in line with our previous findings from #TAUTHOR_TAG that candidates with higher power attempt to shift topics less often than others when responding to moderators .', '', '']",1
"['Bridging orative widely', '', 'We follow our previous work #TAUTHOR_TAG b ) and restrict bridging to non-coreferential cases .', 'We']","['Bridging or associative widely', '', 'We follow our previous work #TAUTHOR_TAG b ) and restrict bridging to non-coreferential cases .', 'We']","['Bridging or', '', 'We follow our previous work #TAUTHOR_TAG b ) and restrict bridging to non-coreferential cases .', 'We']","['', '', 'We follow our previous work #TAUTHOR_TAG b ) and restrict bridging to non-coreferential cases .', '']",2
"['', '', '', 'resources', '', 'We augment mlSystem ruleFeats with more features from our previous work #TAUTHOR_TAG a ; #AUTHOR_TAG b ) on bridging anaphora recognition and antecedent selection .', '']","['', '', '', 'resources', '', 'We augment mlSystem ruleFeats with more features from our previous work #TAUTHOR_TAG a ; #AUTHOR_TAG b ) on bridging anaphora recognition and antecedent selection .', '']","['', '', '', '', '', 'We augment mlSystem ruleFeats with more features from our previous work #TAUTHOR_TAG a ; #AUTHOR_TAG b ) on bridging anaphora recognition and antecedent selection .', '']","['', '', '', '', '', 'mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work #TAUTHOR_TAG a ; #AUTHOR_TAG b ) on bridging anaphora recognition and antecedent selection .', '']",2
"['', '', '', '', '', '', 'in history-based models #TAUTHOR_TAG , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.']","['', '', '', '', '', '', 'in history-based models #TAUTHOR_TAG , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.']","['', '', '', '', '', '', 'in history-based models #TAUTHOR_TAG , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.']","['', '', '', '', '', '', 'in history-based models #TAUTHOR_TAG , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.']",5
"['The most important step in designing a statistical parser with a-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history #TAUTHOR_TAG .']","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history #TAUTHOR_TAG .']","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history #TAUTHOR_TAG .']","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history #TAUTHOR_TAG .']",1
['7A11 our results are computed with the evalb program following the now-standard criteria in #TAUTHOR_TAG .'],['7A11 our results are computed with the evalb program following the now-standard criteria in #TAUTHOR_TAG .'],['7A11 our results are computed with the evalb program following the now-standard criteria in #TAUTHOR_TAG .'],['7A11 our results are computed with the evalb program following the now-standard criteria in #TAUTHOR_TAG .'],5
"['Many statistical parsers #AUTHOR_TAG are based on a history-based probability model #TAUTHOR_TAG , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'a hand-crafted set', '', '', '', '']","['Many statistical parsers #AUTHOR_TAG are based on a history-based probability model #TAUTHOR_TAG , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'a hand-crafted set', '', '', '', '']","['Many statistical parsers #AUTHOR_TAG are based on a history-based probability model #TAUTHOR_TAG , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'a hand-crafted finite set', '', '', '', '']","['Many statistical parsers #AUTHOR_TAG are based on a history-based probability model #TAUTHOR_TAG , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', '', '', '', '', '']",0
"['The most important step in designing a statistical parser with a-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history #TAUTHOR_TAG .']","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history #TAUTHOR_TAG .']","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history #TAUTHOR_TAG .']","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history #TAUTHOR_TAG .']",1
"['about', 'difficulty is the of a on performance, is feas', 'features use', '', '', '', '', '', '', ""re-ranking a parser but the to compute the kernel efficiently and the results are not as good as Collins ' previous work on re-ranking using a finite set of features #TAUTHOR_TAG .""]","['about', 'difficulty is the of a on performance system, is feasible', 'features use', '', '', '', '', '', '', ""re-ranking a parser but the to compute the kernel efficiently and the results are not as good as Collins ' previous work on re-ranking using a finite set of features #TAUTHOR_TAG .""]","['about', 'The difficulty is the choice of on the performance feas', 'features use', '', '', '', '', '', '', ""re-rank a parser but the need to compute the kernel efficiently and the results are not as good as Collins ' previous work on re-ranking using a finite set of features #TAUTHOR_TAG .""]","['', '', '', '', '', '', '', '', '', "" #AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features #TAUTHOR_TAG .""]",0
"['in', ', D) includes nodes which are structally local to top', ""These nodes are the- and 's most recent child (which  any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial #TAUTHOR_TAG , as has conditioning on the left-corner child #AUTHOR_TAG .', 'a step always has access information', '']","['', 'reason, D(top) includes nodes which are structurally local to top,.', ""These nodes are the and 's most recent child (which 1 any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial #TAUTHOR_TAG , as has conditioning on the left-corner child #AUTHOR_TAG .', 'a step always has access information', '']","['', 'D) includes nodes which are structurally local to top', ""These nodes are and 's most recent child (which  any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial #TAUTHOR_TAG , as has conditioning on the left-corner child #AUTHOR_TAG .', 'always has access any information', '']","['', 'For this reason, D(top) includes nodes which are structurally local to top,.', '', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial #TAUTHOR_TAG , as has conditioning on the left-corner child #AUTHOR_TAG .', '', '']",0
"['about', 'a performance', '', '', '', '', '', '', ', but then efficiency becomes a problem.', "" #TAUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features #AUTHOR_TAG .""]","['about', 'a performance', '', '', '', '', '', '', 'sets, but then efficiency becomes a problem.', "" #TAUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features #AUTHOR_TAG .""]","['about', 'the performance', '', '', '', '', '', '', 'but then efficiency becomes a problem.', "" #TAUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features #AUTHOR_TAG .""]","['', '', '', '', '', '', '', '', 'feature sets, but then efficiency becomes a problem.', "" #TAUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features #AUTHOR_TAG .""]",0
"['', 'We used a publicly available tagger #TAUTHOR_TAG to tag the words and then used these in the input to the system .']","['', 'We used a publicly available tagger #TAUTHOR_TAG to tag the words and then used these in the input to the system .']","['', 'We used a publicly available tagger #TAUTHOR_TAG to tag the words and then used these in the input to the system .']","['', 'We used a publicly available tagger #TAUTHOR_TAG to tag the words and then used these in the input to the system .']",5
"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6 precision error and only lex.', 'The SSN parser achieves this using much lessxical,', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% precision error and only lexicalized model.', 'The SSN parser achieves this using much less lexical approaches,', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% less precision error and', 'The SSN parser achieves this result using much lessxical knowledge', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% less precision error and only 11% less recall error than the lexicalized model.', '', '']",1
"['Many statistical parsers #TAUTHOR_TAG are based on a history-based probability model #AUTHOR_TAG , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous have used a hand-crafted finite set of toed', '', '', '', '']","['Many statistical parsers #TAUTHOR_TAG are based on a history-based probability model #AUTHOR_TAG , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous have used a hand-crafted finite set of to', '', '', '', '']","['Many statistical parsers #TAUTHOR_TAG are based on a history-based probability model #AUTHOR_TAG , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of to', '', '', '', '']","['Many statistical parsers #TAUTHOR_TAG are based on a history-based probability model #AUTHOR_TAG , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history #AUTHOR_TAG .', '', '', '', '']",0
"['in', ',) includes nodes which are structally local to top', 'nodes are the most', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial #AUTHOR_TAG , as has conditioning on the left-corner child #TAUTHOR_TAG .', 'featurescoror and the most recent child, a derivation step i always has access to the history features from the previous derivation step i - and ( induction) any information from the history could in stored', '']","['', 'reason, includes nodes which are structurally local to top,.', 'nodes are the most', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial #AUTHOR_TAG , as has conditioning on the left-corner child #TAUTHOR_TAG .', 'features ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i and (by induction) any information from the history could in stored', '']","['', ') includes nodes which are structurally local to top', 'These nodes are', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial #AUTHOR_TAG , as has conditioning on the left-corner child #TAUTHOR_TAG .', 'and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and ( induction) any information from could in be stored', '']","['', 'For this reason, D(top) includes nodes which are structurally local to top,.', '', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial #AUTHOR_TAG , as has conditioning on the left-corner child #TAUTHOR_TAG .', 'Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.', '']",0
"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6 precision error and only lex.', 'The SSN parser achieves this using much lessxical,', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% precision error and only lexicalized model.', 'The SSN parser achieves this using much less lexical approaches,', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% less precision error and', 'The SSN parser achieves this result using much lessxical knowledge', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% less precision error and only 11% less recall error than the lexicalized model.', '', '']",1
"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6 precision error and only lex.', 'The SSN parser achieves this using much lessxical,', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% precision error and only lexicalized model.', 'The SSN parser achieves this using much less lexical approaches,', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% less precision error and', 'The SSN parser achieves this result using much lessxical knowledge', '']","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers #TAUTHOR_TAG .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model #AUTHOR_TAG has only 6% less precision error and only 11% less recall error than the lexicalized model.', '', '']",1
"['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'method is a form of multi-layered artificial neural network called Simple Synchrony Networks #AUTHOR_TAG', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in #AUTHOR_TAG .', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers #TAUTHOR_TAG .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', '', '', '']","['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'method is a form of multi-layered artificial neural network called Simple Synchrony Networks #AUTHOR_TAG', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in #AUTHOR_TAG .', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers #TAUTHOR_TAG .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', '', '', '']","['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks #AUTHOR_TAG .', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in #AUTHOR_TAG .', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers #TAUTHOR_TAG .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', '', '', '']","['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks #AUTHOR_TAG .', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in #AUTHOR_TAG .', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers #TAUTHOR_TAG .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', '', '', '']",1
"['', 'decreases after n4 conver and conver L', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over #TAUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', '', 'in']","['', 'decreases after n=14 converges and converges', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over #TAUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', '', 'in']","['', 'OP decreases after n4 conver and conver L', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over #TAUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', '', 'in']","['', '', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over #TAUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', '', '']",1
"['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG showed how the perceptron algorithm can', ""be used to efficiently compute the best parse with DOP1 's sub"", 'trees , reporting a 5.1 % relative reduction in error rate over the model in #AUTHOR_TAG on the WSJ .', '', '']","['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG showed how the perceptron algorithm can', ""be used to efficiently compute the best parse with DOP1 's sub"", 'trees , reporting a 5.1 % relative reduction in error rate over the model in #AUTHOR_TAG on the WSJ .', '', '']","['', '', '', '', '', '', '', '', '', 'showed how the perceptron algorithm can', ""be used to efficiently compute the best parse with DOP1 's sub"", 'trees , reporting a 5.1 % relative reduction in error rate over the model in #AUTHOR_TAG on the WSJ', '', '']","['', '', '', '', '', '', '', '', '', '', ""be used to efficiently compute the best parse with DOP1 's sub"", 'trees , reporting a 5.1 % relative reduction in error rate over the model in #AUTHOR_TAG on the WSJ . #AUTHOR_TAG furthermore showed', '', '']",0
"[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s () and."", 'We that these PCFG-reductions in 60 times speedup in processing time.', ', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #TAUTHOR_TAG .""]","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s and"", 'We that these PCFG-reductions in 60 times speedup in processing time', ', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #TAUTHOR_TAG .""]","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s () and"", 'We show that these PCFG-reductions result in a 60 times speedup in', ', 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #TAUTHOR_TAG .""]","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and #AUTHOR_TAG estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', ' #AUTHOR_TAG Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #TAUTHOR_TAG .""]",0
"['', 'decre n conver L and conver LOP', 'The highest accuracy is obtained by SLOP at 12 n 4: LP L', 'Table 1', 'Compared to the reranking technique in #TAUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before in']","['', 'decreases converges and converges', 'The highest accuracy is obtained by SL-DOP at 12 n 14: LP LR', 'Table 1.', 'Compared to the reranking technique in #TAUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before in']","['', 'n conver L and conver LOP', 'The highest accuracy is obtained by SLOP at 12 n 14: an LP', 'Table 1.', 'Compared to the reranking technique in #TAUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before in']","['', '', '', '', 'Compared to the reranking technique in #TAUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before in']",1
"[""This paper presents the first published results with Goodmans P-reductions of both Bonnema et al.'s"", 'We that these PCFG-reductions result in a 60 times speedup in processing time w.', 'Bod , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and #TAUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s"", 'We that these PCFG-reductions result in a 60 times speedup in processing time', 'Bod , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and #TAUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]","[""This paper presents the first published results with Goodmans PCFG-reductions of both Bonnema et al.'s"", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', ' #AUTHOR_TAG Bod , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and #TAUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and #AUTHOR_TAG estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', ' #AUTHOR_TAG Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and #TAUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]",0
"['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.', 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but will ""Likelihood-DOP']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.', 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but will ""Likelihood-DOP""']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.', 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but will ""Likelihood-DOP']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.', 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of #AUTHOR_TAG given in Section 2.2.']",0
"['u', 'Our first experimental goal was to the two PCFG-reductions in we', 'and Bon', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #TAUTHOR_TAG .', '', '', '', '', '']","['', 'Our first experimental goal was to the two PCFG-reductions in we', 'and', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #TAUTHOR_TAG .', '', '', '', '', '']","['', 'Our first experimental goal was to the two PCFG-reductions in', 'and Bon', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #TAUTHOR_TAG .', '', '', '', '', '']","['', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', '', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #TAUTHOR_TAG .', '', '', '', '', '']",1
"['For our experiments we used the standard division of the WSJ #TAUTHOR_TAG , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As, all trees were off their semantic tags, co-reference information and quotation marks.', 'ofality to binary branch (', '', '', '']","['For our experiments we used the standard division of the WSJ #TAUTHOR_TAG , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were off their semantic tags, co-reference information and quotation marks.', 'of to binary branching (and', '', '', '']","['For our experiments we used the standard division of the WSJ #TAUTHOR_TAG , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As, all trees were stripped off their semantic tags, co-reference information and quotation marks.', 'ofality to binary branching (', '', '', '']","['For our experiments we used the standard division of the WSJ #TAUTHOR_TAG , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', '', '', '', '']",5
"['nyu', 'Our first experimental goal was to compare the two PCFG-reductions in Section2. we will to', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', ' #TAUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', '', '', '', '', '']","['', 'Our first experimental goal was to compare the two PCFG-reductions in Section we will to', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', ' #TAUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', '', '', '', '', '']","['', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, we will refer to', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', ' #TAUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', '', '', '', '', '']","['', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', ' #TAUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', '', '', '', '', '']",1
"[""This paper presents the first published results withmans P-reductions of bothn et al.'"", 'that these P-reductions result in a 60 times speedup in processing time', '2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TAUTHOR_TAG and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]","[""This paper presents the first published results with Goodman's PCFG-reductions of both et al.'s"", 'that these PCFG-reductions result in a 60 times speedup in processing time', '2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TAUTHOR_TAG and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]","[""This paper presents the first published results withmans PCFG-reductions of both Bonnema et al.'"", 'that these PCFG-reductions result in a 60 times speedup in', '2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TAUTHOR_TAG and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]","['', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', '', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #TAUTHOR_TAG and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]",0
"['', '', '', '', '', 'i ; 1998).', ""And #TAUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in #AUTHOR_TAG .""]","['', '', '', '', '', '1998).', ""And #TAUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in #AUTHOR_TAG .""]","['', '', '', '', '', '', ""And #TAUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in #AUTHOR_TAG .""]","['', '', '', '', '', '', ""And #TAUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in #AUTHOR_TAG .""]",0
"['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG and #TAUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood- DOP models, but we will by ""Likelihood-DOP']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG and #TAUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood- DOP models, but we will by ""Likelihood-DOP""']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG and #TAUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood- DOP models, but we will by ""Likelihood-DOP']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG and #TAUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood- DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of #AUTHOR_TAG given in Section 2.2.']",0
"['Most DOP models , such as in #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.', 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but will by ""Likelihood-DOP']","['Most DOP models , such as in #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.', 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but will by ""Likelihood-DOP""']","['Most DOP models , such as in #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.', 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but will by ""Likelihood-DOP']","['Most DOP models , such as in #AUTHOR_TAG , #TAUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.', 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of #AUTHOR_TAG given in Section 2.2.']",0
"['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but will ""Likelihood-DOP']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but will ""Likelihood-DOP""']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but will ""Likelihood-DOP']","['Most DOP models , such as in #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of #AUTHOR_TAG given in Section 2.2.']",0
"['Thus DOP1 considers counts of subtrees of a wide of sizes in computing the probability of a tree: everything from counts of single-level rules to of entire trees.', 'A disadvantage this is an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #TAUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based on #AUTHOR_TAG .']","['Thus DOP1 considers counts of subtrees of a wide of sizes in computing the probability of a tree: everything from counts of single-level rules to of entire trees.', 'A disadvantage this is an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #TAUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based on #AUTHOR_TAG .']","['Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to of entire trees.', 'A disadvantage this model is an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #TAUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based on #AUTHOR_TAG .']","['Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #TAUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based on #AUTHOR_TAG .']",0
"['Waegner 1992; Pereira and Schabes 1992).', 'The DOP model on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of #TAUTHOR_TAG , 1999 ) , #AUTHOR_TAG , 1997 ) , #AUTHOR_TAG , #AUTHOR_TAG , and many others .']","['Waegner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of #TAUTHOR_TAG , 1999 ) , #AUTHOR_TAG , 1997 ) , #AUTHOR_TAG , #AUTHOR_TAG , and many others .']","['Waegner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of #TAUTHOR_TAG , 1999 ) , #AUTHOR_TAG , 1997 ) , #AUTHOR_TAG , #AUTHOR_TAG , and many others .']","['Waegner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of #TAUTHOR_TAG , 1999 ) , #AUTHOR_TAG , 1997 ) , #AUTHOR_TAG , #AUTHOR_TAG , and many others .']",4
"['', '', '', ').', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #TAUTHOR_TAG ; Goodman 1998 ) .', '']","['', '', '', '', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #TAUTHOR_TAG ; Goodman 1998 ) .', '']","['', '', '', '', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #TAUTHOR_TAG ; Goodman 1998 ) .', '']","['', '', '', '', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #TAUTHOR_TAG ; Goodman 1998 ) .', '']",0
"['', '', '', '', 'becomeeg.', '1999;Chiak 2000;Good 1998).', "" #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TAUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in #AUTHOR_TAG .""]","['', '', '', '', 'become (e.g.', '1999;Charniak 2000;Goodman 1998).', "" #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TAUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in #AUTHOR_TAG .""]","['', '', '', '', 'has becomeeg.', 'Collins 1999;Charniak 2000;Goodman 1998).', "" #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TAUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in #AUTHOR_TAG .""]","['', '', '', '', '', 'Collins 1999;Charniak 2000;Goodman 1998).', "" #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #TAUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in #AUTHOR_TAG .""]",4
"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 #TAUTHOR_TAG .', 'Goodmans main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.', 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the', '', '', '', '']","['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 #TAUTHOR_TAG .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the', '', '', '', '']","['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 #TAUTHOR_TAG .', 'Goodmans main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.', 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in', '', '', '', '']","['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 #TAUTHOR_TAG .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', '', '', '', '']",0
"['One instant of DOP has received considerable is the model known as DOP1  ', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see', '', '', '', ' #AUTHOR_TAG gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some is a reasonable approximation of the most probable parse.', ' #TAUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'Goods method does still not for an efficient most, does the ""maximum', '', '', '', '', '', '']","['One instantiation of DOP has received considerable is the model known as DOP1 2', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see', '', '', '', ' #AUTHOR_TAG gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some is a reasonable approximation of the most probable parse.', ' #TAUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'Goodman\'s method does still not for an efficient most does the ""maximum', '', '', '', '', '', '']","['One instantiation of DOP has received considerable interest is the model known as DOP1 2 ', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see', '', '', '', ' #AUTHOR_TAG gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse.', ' #TAUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'Goods method does still not for an efficient computation most, the ""maximum constituents', '', '', '', '', '', '']","['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', '', '', '', ' #AUTHOR_TAG gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse.', ' #TAUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', '', '', '', '', '', '', '']",0
"['', '', '', '', '', '', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit #TAUTHOR_TAG .', 'We run MERT separately for each system.', 'recaser used is the', 'is the supplied training', '', '']","['', '', '', '', '', '', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit #TAUTHOR_TAG .', 'We run MERT separately for each system.', 'recaser used is the', 'is the supplied training', '', '']","['', '', '', '', '', '', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit #TAUTHOR_TAG .', 'We run MERT separately for each system.', 'The recaser used is the', 'It is supplied', '', '']","['', '', '', '', '', '', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit #TAUTHOR_TAG .', 'We run MERT separately for each system.', '', '', '', '']",5
"['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of #TAUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data', 'It merging']","['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of #TAUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It merging']","['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of #TAUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It']","['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of #TAUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', '']",5
"['For compound splitting , we follow #TAUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., PO #AUTHOR_TAG ) or are (almost) knowledge-free (e.g., #AUTHOR_TAG ).', '.', '', '', '', '']","['For compound splitting , we follow #TAUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., PO #AUTHOR_TAG ) or are (almost) knowledge-free (e.g., #AUTHOR_TAG ).', 'studied.', '', '', '', '']","['For compound splitting , we follow #TAUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., PO #AUTHOR_TAG ) or are (almost) knowledge-free (e.g., #AUTHOR_TAG ).', '.', '', '', '', '']","['For compound splitting , we follow #TAUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., PO #AUTHOR_TAG ) or are (almost) knowledge-free (e.g., #AUTHOR_TAG ).', '', '', '', '', '']",5
"['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', ' #TAUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']","['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', ' #TAUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']","['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', ' #TAUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']","['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', ' #TAUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']",0
"['', '', '', '', 'We follow #TAUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features used found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets.']","['', '', '', '', 'We follow #TAUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features used found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets.']","['', '', '', '', 'We follow #TAUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets.']","['', '', '', '', 'We follow #TAUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets.']",5
"['', '', '', '', '', '', '', '', 'deals de G and Mariño (2008), deals with verbal morphology and attached pronouns', 'solving', '', '', 'to solve the inflection by simply building an SMT system for translating from stems to inflected forms.', ' #TAUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', 'in data sets', '', '']","['', '', '', '', '', '', '', '', 'deals de and Mariño (2008), deals with verbal morphology and attached pronouns.', 'solving', '', '', 'to solve the inflection by simply building an SMT system for translating from stems to inflected forms.', ' #TAUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', 'ineffective data sets.', '', '']","['', '', '', '', '', '', '', '', 'deals de G and Mariño (2008), deals with verbal morphology and attached pronouns', '', '', '', 'to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', ' #TAUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', 'in large data sets.', '', '']","['', '', '', '', '', '', '', '', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.', '', '', '', ' #AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', ' #TAUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', '', '', '']",1
"['For compound follow #AUTHOR_TAG using linguistic en-coded in abasedological analyser and then selecting the analysis the mean word frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , PO #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , #TAUTHOR_TAG .', 'Compound merging is less.', '', '', '', '']","['For compound follow #AUTHOR_TAG using linguistic en-coded in a morphological analyser and then selecting the analysis the mean word frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , PO #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , #TAUTHOR_TAG .', 'Compound merging is less studied.', '', '', '', '']","['For we follow #AUTHOR_TAG using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis the geometric mean word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , PO #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , #TAUTHOR_TAG .', 'Compound merging is less well.', '', '', '', '']","['For compound splitting, we follow #AUTHOR_TAG , using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , PO #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , #TAUTHOR_TAG .', 'Compound merging is less well studied.', '', '', '', '']",1
"['', '', '', '', '', '', 'deals de G and Marin__ (200 deals with verbal morphology and attached pronouns', '', '', '', '', '', 'in on large data sets', ' #TAUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CR to use more- text']","['', '', '', '', '', '', 'deals de and Marin__o (2008), deals with verbal morphology and attached pronouns.', '', '', '', '', '', 'ineffective on large data sets.', ' #TAUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF to use more con- text']","['', '', '', '', '', '', 'deals de G and Marin__ (200 deals with verbal morphology and attached pronouns', '', '', '', '', '', 'in on large data sets.', ' #TAUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CR to use']","['', '', '', '', '', '', '', '', '', '', '', '', 'Both efforts were ineffective on large data sets.', ' #TAUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', '']",1
"['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German #AUTHOR_TAG and the BitPar parser , which is a state-of-the-art parser of German #TAUTHOR_TAG .']","['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German #AUTHOR_TAG and the BitPar parser , which is a state-of-the-art parser of German #TAUTHOR_TAG .']","['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German #AUTHOR_TAG and the BitPar parser , which is a state-of-the-art parser of German #TAUTHOR_TAG .']","['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German #AUTHOR_TAG and the BitPar parser , which is a state-of-the-art parser of German #TAUTHOR_TAG .']",5
"['We prepare the training data by splitting compounds in two steps , following the technique of #TAUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training .', 'modifying words the to right)', '']","['We prepare the training data by splitting compounds in two steps , following the technique of #TAUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training', 'modifying words the to rightmost word)', '']","['We prepare the training data by splitting compounds in two steps , following the technique of #TAUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training data', 'The formerly modifying words the words to', '']","['We prepare the training data by splitting compounds in two steps , following the technique of #TAUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', '', '', '']",5
"['', '', '', '', '', 'As improves, the performance of linguistic-feature-based approaches will increase.', ' #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', ', this not deal with marked.', '', '']","['', '', '', '', '', 'As improves, the performance of linguistic-feature-based approaches will increase.', ' #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', 'However, this not deal with marked', '', '']","['', '', '', '', '', 'As improves, the performance of linguistic-feature-based approaches will increase.', ' #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', ', this does not deal directly with marked.', '', '']","['', '', '', '', '', 'As parsing performance improves, the performance of linguistic-feature-based approaches will increase.', ' #AUTHOR_TAG , #AUTHOR_TAG , #AUTHOR_TAG , #TAUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', '', '', '']",1
"['', '', '', '', '', '', '', '', 'deals de G and Mariño (2008), deals with verbal morphology and attached pronouns', 'other on solving inflection.', '', 'We use more complex context features.', ' #TAUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', 'ing prepos with the case theyone of the most importantups in our system', '', '', '']","['', '', '', '', '', '', '', '', 'deals de and Mariño (2008), deals with verbal morphology and attached pronouns.', 'other on solving inflection.', '', 'We use more complex context features.', ' #TAUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', 'marking prepositions with the case they (one of the most important markups in our system).', '', '', '']","['', '', '', '', '', '', '', '', 'deals de G and Mariño (2008), deals with verbal morphology and attached pronouns', 'other work on solving inflection.', '', 'We use more complex context features.', ' #TAUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', 'ing prepositions with the case they markone of the most important markups in our system', '', '', '']","['', '', '', '', '', '', '', '', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', '', 'We use more complex context features.', ' #TAUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', ' #AUTHOR_TAG improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', '', '', '']",1
"['', '', '', '', '', '', 'that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', ' #TAUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', ' #AUTHOR_TAG to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '', '', '', '']","['', '', '', '', '', '', 'that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', ' #TAUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', ' #AUTHOR_TAG to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '', '', '', '']","['', '', '', '', '', '', 'that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', ' #TAUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', ' #AUTHOR_TAG to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '', '', '', '']","['', '', '', '', '', '', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', ' #TAUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', ' #AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '', '', '', '']",1
"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , #TAUTHOR_TAG and others .', 'Toutanova et.', 'als work showed that is to co', '']","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , #TAUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that is to"", '']","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , #TAUTHOR_TAG and others .', 'Toutanova et.', 'als work showed that to', '']","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , #TAUTHOR_TAG and others .', 'Toutanova et.', '', '']",1
"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TAUTHOR_TAG , #AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is to target co"", 'source information markup']","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TAUTHOR_TAG , #AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is to target"", 'source information markup']","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TAUTHOR_TAG , #AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is most to model target"", 'the markup']","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #TAUTHOR_TAG , #AUTHOR_TAG and others .', 'Toutanova et.', '', '']",1
"['For compound #AUTHOR_TAG using linguistic-oded aological analyser and then selecting the word frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #TAUTHOR_TAG or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', '', '', '', '', '']","['For compound #AUTHOR_TAG using linguistic en-coded a morphological analyser and then selecting the word frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #TAUTHOR_TAG or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', '', '', '', '', '']","['For #AUTHOR_TAG using linguistic knowledge-oded a rule-based morphological analyser and word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #TAUTHOR_TAG or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', '', '', '', '', '']","['', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #TAUTHOR_TAG or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', '', '', '', '', '']",1
"['Our approach to extract and classify social events builds on our previous work #TAUTHOR_TAG , which in turn builds on work from the relation extraction community #AUTHOR_TAG .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', '', '', '', '', '']","['Our approach to extract and classify social events builds on our previous work #TAUTHOR_TAG , which in turn builds on work from the relation extraction community #AUTHOR_TAG .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', '', '', '', '', '']","['Our approach to extract and classify social events builds on our previous work #TAUTHOR_TAG , which in turn builds on work from the relation extraction community #AUTHOR_TAG .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', '', '', '', '', '']","['Our approach to extract and classify social events builds on our previous work #TAUTHOR_TAG , which in turn builds on work from the relation extraction community #AUTHOR_TAG .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', '', '', '', '', '']",2
"['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in #TAUTHOR_TAG .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]","['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in #TAUTHOR_TAG .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]","['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in #TAUTHOR_TAG .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]","['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in #TAUTHOR_TAG .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]",0
['I A more detailed discussion of various aspects of the proposed parser can be found in #TAUTHOR_TAG .'],['I A more detailed discussion of various aspects of the proposed parser can be found in #TAUTHOR_TAG .'],['I A more detailed discussion of various aspects of the proposed parser can be found in #TAUTHOR_TAG .'],['I A more detailed discussion of various aspects of the proposed parser can be found in #TAUTHOR_TAG .'],0
"['Combining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser #TAUTHOR_TAG presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", '', '', '', '', '', 'an important gu and are best interpreted- given', '', '']","['Combining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser #TAUTHOR_TAG presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", '', '', '', '', '', 'an important guiding and are best interpreted given', '', '']","['Combining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser #TAUTHOR_TAG presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", '', '', '', '', '', 'an important guiding role and are best interpreted- given', '', '']","['Combining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser #TAUTHOR_TAG presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", '', '', '', '', '', '', '', '']",0
"['originally', 'among, (Ramakrishnan et al.1).', 'As shown in #TAUTHOR_TAG â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']","['originally', 'among others, (Ramakrishnan et al.', 'As shown in #TAUTHOR_TAG â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']","['originally', 'among, (Ramakrishnan et al.1).', 'As shown in #TAUTHOR_TAG â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']","['', 'See, among others, (Ramakrishnan et al. 1992).', 'As shown in #TAUTHOR_TAG â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']",0
"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TAUTHOR_TAG', '3 #AUTHOR_TAG propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in #AUTHOR_TAG .', 'Typed feature structures as normal form ir~~E terms are merely syntactic objects.']","['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TAUTHOR_TAG', '3 #AUTHOR_TAG propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in #AUTHOR_TAG .', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects.""]","['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TAUTHOR_TAG', '3 #AUTHOR_TAG propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in #AUTHOR_TAG .', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects.""]","['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #TAUTHOR_TAG', '3 #AUTHOR_TAG propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in #AUTHOR_TAG .', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects.""]",0
"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar #TAUTHOR_TAG .', '3 #AUTHOR_TAG a ofxical rules into clauses.', '', '', '']","['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar #TAUTHOR_TAG .', '3 #AUTHOR_TAG a of lexical rules into clauses entries.', '', '', '']","['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar #TAUTHOR_TAG .', '3 #AUTHOR_TAG a compilation ofxical rules into', '', '', '']","['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar #TAUTHOR_TAG .', '', '', '', '']",0
"['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , #TAUTHOR_TAG .', 'is an interesting with respect to natural language processing as it', '', '', '']","['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , #TAUTHOR_TAG .', 'is an interesting with respect to natural language processing as it', '', '', '']","['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , #TAUTHOR_TAG .', 'is an interesting technique with respect to natural language processing as', '', '', '']","['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , #TAUTHOR_TAG .', '', '', '', '']",0
"['', '', ' #AUTHOR_TAG b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', 'have to refrain from an example.', 'The ConTroll grammar development system as described in #TAUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']","['', '', ' #AUTHOR_TAG b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', 'have to refrain from an example.', 'The ConTroll grammar development system as described in #TAUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']","['', '', ' #AUTHOR_TAG b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', 'we have to refrain from an example.', 'The ConTroll grammar development system as described in #TAUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']","['', '', ' #AUTHOR_TAG b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in #TAUTHOR_TAG b ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']",0
"['Coroutining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also #TAUTHOR_TAG .']","['Coroutining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also #TAUTHOR_TAG .']","['Coroutining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also #TAUTHOR_TAG .']","['Coroutining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also #TAUTHOR_TAG .']",0
"['ded', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of #TAUTHOR_TAG .', 'Unlike the ALE parser, though, the selective magic parser not presuppose a phrase structure backbone more flexible sub-computations tabled/filtered', '']","['deduction', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of #TAUTHOR_TAG .', 'Unlike the ALE parser, though, the selective magic parser not presuppose a phrase structure backbone more flexible sub-computations tabled/filtered.', '']","['', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of #TAUTHOR_TAG .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone is more flexible sub-computations tabled/filtered', '']","['', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of #TAUTHOR_TAG .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.', '']",1
"['with', 'this we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based Feature', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG as discussed in #TAUTHOR_TAG a ) and #AUTHOR_TAG .', 'dealt']","['with', 'this we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based Feature', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG as discussed in #TAUTHOR_TAG a ) and #AUTHOR_TAG .', 'dealt']","['with', 'this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG as discussed in #TAUTHOR_TAG a ) and #AUTHOR_TAG .', '']","['', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;G #AUTHOR_TAG .', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG as discussed in #TAUTHOR_TAG a ) and #AUTHOR_TAG .', '']",2
"['` See #TAUTHOR_TAG for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', ' #AUTHOR_TAG b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', 'refr', 'gram']","['` See #TAUTHOR_TAG for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', ' #AUTHOR_TAG b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', 'refrain', 'grammar']","['` See #TAUTHOR_TAG for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', ' #AUTHOR_TAG b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', 'refr', '']","['` See #TAUTHOR_TAG for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', ' #AUTHOR_TAG b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '', '']",0
"['', '', 'headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-ing.', 'This contrasts with one of the traditional approaches ( e.g. , #TAUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']","['', '', 'headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. , #TAUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']","['', '', ', headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. , #TAUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']","['', '', 'For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. , #TAUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']",1
"['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers #TAUTHOR_TAG and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']","['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers #TAUTHOR_TAG and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']","['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers #TAUTHOR_TAG and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']","['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers #TAUTHOR_TAG and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']",5
"['At the same time , we believe our method has advantages over the approach developed initially at IBM #TAUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One is that our method attempts to model the natural decomposition of sentences into phrases.', '', '']","['At the same time , we believe our method has advantages over the approach developed initially at IBM #TAUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One is that our method attempts to model the natural decomposition of sentences into phrases.', '', '']","['At the same time , we believe our method has advantages over the approach developed initially at IBM #TAUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', '', '']","['At the same time , we believe our method has advantages over the approach developed initially at IBM #TAUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', '', '']",1
"['GEN constructs itssson', '', '', ""9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' #TAUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical StructureM"", '']","['IGEN constructs its', '', '', ""9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' #TAUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure"", '']","['GEN constructs its', '', '', ""Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' #TAUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure TheoryM"", '']","['', '', '', ""1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' #TAUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987)."", '']",0
"['generally separated the task into distinct text', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #TAUTHOR_TAG .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']","['generally separated the task into distinct text', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #TAUTHOR_TAG .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']","['has generally separated the task into distinct text planning', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #TAUTHOR_TAG .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']","['', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #TAUTHOR_TAG .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']",0
"['The opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) #TAUTHOR_TAG .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', ""Certainly an approach to generation that handle these interactions be an improvement'""]","['The opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) #TAUTHOR_TAG .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', 'Certainly an approach to generation that handle these interactions be an improvement,']","['The opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) #TAUTHOR_TAG .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', 'Certainly an approach to generation that does handle these interactions would be an improvement,']","['The opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) #TAUTHOR_TAG .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', '']",0
"['generally the task into distinct', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component #TAUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']","['generally the task into distinct', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component #TAUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']","['has generally the task into', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component #TAUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']","['', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component #TAUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']",0
"['produce differentxical real a', 'the only goal we could dispense with the feedback mechanism and simply design some sort of discrimination (or similar device to information being expressed', '', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #TAUTHOR_TAG a ) .']","['produce different lexical realizations a', 'the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination (or similar device) to information being expressed.', '', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #TAUTHOR_TAG a ) .']","['can produce different lexical realizations', 'the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network (or similar device) to the information being expressed', '', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #TAUTHOR_TAG a ) .']","['', 'If that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network (or similar device) to test various features of the information being expressed.', '', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #TAUTHOR_TAG a ) .']",0
"['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner #TAUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c )"", '', '', '']","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner #TAUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c )"", '', '', '']","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner #TAUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c )"", '', '', '']","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner #TAUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", '', '', '']",0
"['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; #TAUTHOR_TAG , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 8a , "", '', '']","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; #TAUTHOR_TAG , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a ,"", '', '']","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; #TAUTHOR_TAG , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , "", '', '']","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; #TAUTHOR_TAG , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", '', '']",0
"['One possible response would be to abandon the separation the could be a single component that handles all of the work.', 'This approach has occasionally been taken , as in #AUTHOR_TAG and #AUTHOR_TAG and , at least implicitly , in #TAUTHOR_TAG and #AUTHOR_TAG ; this approach all of the and simplicity of mod design is lost']","['One possible response would be to abandon the separation; the could be a single component that handles all of the work.', 'This approach has occasionally been taken , as in #AUTHOR_TAG and #AUTHOR_TAG and , at least implicitly , in #TAUTHOR_TAG and #AUTHOR_TAG ; this approach all of the and simplicity of modular design is lost']","['One possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as in #AUTHOR_TAG and #AUTHOR_TAG and , at least implicitly , in #TAUTHOR_TAG and #AUTHOR_TAG ; this approach all of the flexibility and simplicity of modular design is lost .']","['One possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as in #AUTHOR_TAG and #AUTHOR_TAG and , at least implicitly , in #TAUTHOR_TAG and #AUTHOR_TAG ; however , under this approach , all of the flexibility and simplicity of modular design is lost .']",0
"['in to develop modified modular designsators handle components.', ""These include devices such as inter ( 3 ; Appelt 83 , backtracking on failure ( Appelt 1985 ; Nogier 89 ) , allowing the linguistic component to interrogate the planner ( 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning #TAUTHOR_TAG a , 1988c ) ."", 'these approaches', '', '']","['in to develop modified modular designs generators handle components.', ""These include devices such as ( ; Appelt 1983 , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning #TAUTHOR_TAG a , 1988c ) ."", 'these approaches,', '', '']","['in to develop modified modular designsators handle the components.', ""These include devices such as inter ( ; Appelt 1983 , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning #TAUTHOR_TAG a , 1988c ) ."", '', '', '']","['', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning #TAUTHOR_TAG a , 1988c ) ."", '', '', '']",0
"['Hovy has described another text planner that builds similar plans #TAUTHOR_TAG b ) .', 'pattern', '', '', '']","['Hovy has described another text planner that builds similar plans #TAUTHOR_TAG b ) .', 'pattern;', '', '', '']","['Hovy has described another text planner that builds similar plans #TAUTHOR_TAG b ) .', '', '', '', '']","['Hovy has described another text planner that builds similar plans #TAUTHOR_TAG b ) .', '', '', '', '']",0
"['Research in natural language generation has generally separated the task into distinct text planning and linguistic', 'The information linguistic, information', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation #TAUTHOR_TAG .']","['Research in natural language generation has generally separated the task into distinct text planning and linguistic', 'The information linguistic component, information', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation #TAUTHOR_TAG .']","['Research in natural language generation has generally separated the task into distinct text planning and lingu', 'the information the linguistic component, the information', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation #TAUTHOR_TAG .']","['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', '', '', '', '', '', 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation #TAUTHOR_TAG .']",0
"['Research natural generation has generally separated the task into distinct text planning and linguistic', 'The,', '""strategic ""tactical"" ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #TAUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', '', '', '', 'pip a consensus underlying most recent in']","['Research natural generation has generally separated the task into distinct text planning and linguistic', 'The component,', '""strategic"" ""tactical"" ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #TAUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', '', '', '', 'a consensus underlying most recent in']","['Research natural language generation has generally separated the task into distinct text planning and lingu', '', '""strategic ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #TAUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', '', '', '', 'a consensus architecture underlying most recent work in']","['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', '', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #TAUTHOR_TAG a ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', '', '', '', '']",0
"['Surveys and articles on the topic include #AUTHOR_TAG , de #AUTHOR_TAG , and #TAUTHOR_TAG .', 'of proceed a pace, we aim nonetheless to include here enough details to make the present paper self-contained.']","['Surveys and articles on the topic include #AUTHOR_TAG , de #AUTHOR_TAG , and #TAUTHOR_TAG .', 'of proceeding a pace, we aim nonetheless to include here enough details to make the present paper self-contained.']","['Surveys and articles on the topic include #AUTHOR_TAG , de #AUTHOR_TAG , and #TAUTHOR_TAG .', 'of proceed a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained.']","['Surveys and articles on the topic include #AUTHOR_TAG , de #AUTHOR_TAG , and #TAUTHOR_TAG .', 'Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained.']",0
"['', 'atory categ itself all (or only) conc', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #TAUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']","['', 'categorial itself all (or only)', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #TAUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']","['', 'atory categ itself all (or only)', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #TAUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']","['', '', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #TAUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']",0
"['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated #TAUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the calculus if F ~ A is a theorem of the regulated calculus.', '', '']","['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated #TAUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the calculus iff F ~ A is a theorem of the regulated calculus.', '', '']","['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated #TAUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the Lambek calculus iff F ~ A is a theorem of the regulated calculus.', '', '']","['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated #TAUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the Lambek calculus iff F ~ A is a theorem of the regulated calculus.', '', '']",0
"['DA classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases #TAUTHOR_TAG can serve as explicit indicators of discourse structure .', 'Similarly find and', '', '', '', '', '', '']","['DA classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases #TAUTHOR_TAG can serve as explicit indicators of discourse structure .', 'Similarly, find and', '', '', '', '', '', '']","['DA classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases #TAUTHOR_TAG can serve as explicit indicators of discourse structure .', 'Similarly we find and', '', '', '', '', '', '']","['DA classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases #TAUTHOR_TAG can serve as explicit indicators of discourse structure .', '', '', '', '', '', '', '']",4
"['', '', 'many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay- #AUTHOR_TAG is transformation-based learning #TAUTHOR_TAG .', 'tasks']","['', '', 'many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay- #AUTHOR_TAG is transformation-based learning #TAUTHOR_TAG .', 'tasks']","['', '', 'many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay- #AUTHOR_TAG is transformation-based learning #TAUTHOR_TAG .', '']","['', '', 'However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay- #AUTHOR_TAG is transformation-based learning #TAUTHOR_TAG .', '']",1
"['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging #TAUTHOR_TAG .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', '', '']","['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging #TAUTHOR_TAG .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', '', '']","['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging #TAUTHOR_TAG .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', '', '']","['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging #TAUTHOR_TAG .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', '', '']",1
"['The combination of likelihood and prior modeling, Hs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct #TAUTHOR_TAG .', '', '']","['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct #TAUTHOR_TAG .', '', '']","['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct #TAUTHOR_TAG .', '', '']","['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct #TAUTHOR_TAG .', '', '']",0
"[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TAUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.']","[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TAUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.']","[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TAUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.']","[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #TAUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.']",1
"['s here a typed logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #TAUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing )', '', '']","['here, a typed logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #TAUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing )', '', '']","['s here a typed higher-order logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis,, etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #TAUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing )', '', '']","['What is required is that QLFs are, as here, expressed in a typed higher-order logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, focus, etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #TAUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .', '', '']",0
"['a dissaction with certain quasiCLE', 'In the CLE-QLF approach, as rationally reconstructed by #TAUTHOR_TAG and #AUTHOR_TAG , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']","['a dissatisfaction with certain', 'In the CLE-QLF approach, as rationally reconstructed by #TAUTHOR_TAG and #AUTHOR_TAG , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']","['a dissatisfaction with certain aspects quasiCLE', 'In the CLE-QLF approach, as rationally reconstructed by #TAUTHOR_TAG and #AUTHOR_TAG , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']","['', 'In the CLE-QLF approach, as rationally reconstructed by #TAUTHOR_TAG and #AUTHOR_TAG , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']",1
"['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TAUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution: for example, those that can only arise via a violation of scoping or binding constraints', 'resolution (', '', '', '', '', '', '']","['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TAUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', 'resolution (for', '', '', '', '', '', '']","['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TAUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', 'resolution rules (', '', '', '', '', '', '']","['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #TAUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', '', '', '', '', '', '', '']",1
"['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TAUTHOR_TAG and #AUTHOR_TAG ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the ``ue language approach .']","['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TAUTHOR_TAG and #AUTHOR_TAG ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language approach .']","['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TAUTHOR_TAG and #AUTHOR_TAG ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the ``ue language approach #AUTHOR_TAG .']","['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #TAUTHOR_TAG and #AUTHOR_TAG ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language approach of #AUTHOR_TAG .']",1
"[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TAUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", '', '']","[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TAUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", '', '']","[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TAUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", '', '']","[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #TAUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", '', '']",1
"['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any #TAUTHOR_TAG , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']","['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any #TAUTHOR_TAG , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']","['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any #TAUTHOR_TAG , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']","['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any #TAUTHOR_TAG , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']",0
"['several stateg might be pursued.', 'One is to adopt Pinkal\'s ""radical underspecification"" (Pinkal 1995) and use underspecified representations of ambig', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure #TAUTHOR_TAG , with the resolution process as described here ."", '']","['several stategies might be pursued.', 'One is to adopt Pinkal\'s ""radical underspecification"" (Pinkal 1995) and use underspecified representations of', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure #TAUTHOR_TAG , with the resolution process as described here ."", '']","['several stategies might be pursued.', 'One is to adopt Pinkal\'s ""radical underspecification"" approach (Pinkal 1995) and use underspecified representations of ambig', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure #TAUTHOR_TAG , with the resolution process as described here ."", '']","['There are several stategies that might be pursued.', 'One is to adopt Pinkal\'s ""radical underspecification"" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure #TAUTHOR_TAG , with the resolution process as described here ."", '']",3
"['implement a deduct theoryifier scope using the conditional equival mechanism.', 'The version proposed here combines a basic insight from #AUTHOR_TAG with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TAUTHOR_TAG , 1991 ) , with some differences that are commented on below .', '', '', '']","['implement a deductive theory quantifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight from #AUTHOR_TAG with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TAUTHOR_TAG , 1991 ) , with some differences that are commented on below .', '', '', '']","['a deductive theoryifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight from #AUTHOR_TAG with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TAUTHOR_TAG , 1991 ) , with some differences that are commented on below .', '', '', '']","['We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight from #AUTHOR_TAG with higher-order unification to give an analysis that has a strong resemblance to that proposed in #TAUTHOR_TAG , 1991 ) , with some differences that are commented on below .', '', '', '']",1
"['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #AUTHOR_TAG and #TAUTHOR_TAG , 1991 ) .', 'that in their treatment, quantified noun phrases are treated in two', '', '']","['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #AUTHOR_TAG and #TAUTHOR_TAG , 1991 ) .', 'that in their treatment, quantified noun phrases are treated in two', '', '']","['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #AUTHOR_TAG and #TAUTHOR_TAG , 1991 ) .', 'that in their treatment, quantified noun phrases are treated in', '', '']","['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and #AUTHOR_TAG and #TAUTHOR_TAG , 1991 ) .', '', '', '']",1
"['only the available five relative scopings of the quantifiers are produced #TAUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ;¢ the equivalences are reversible , and thus the sentences cart be generatedoped forms']","['only the available five relative scopings of the quantifiers are produced #TAUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; the equivalences are reversible , and thus the sentences cart be generated scoped forms']","['only the available five relative scopings of the quantifiers are produced #TAUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ;¢ the equivalences are reversible , and thus the above sentences cart be generatedoped logical forms']",[''],0
"['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS #TAUTHOR_TAG work to our own framework .', '', '']","['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS #TAUTHOR_TAG work to our own framework .', '', '']","['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS #TAUTHOR_TAG work to our own framework .', '', '']","['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS #TAUTHOR_TAG work to our own framework .', '', '']",5
"[' #TAUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise']","[' #TAUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise']","[' #TAUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']","[' #TAUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']",0
"['heavily exploitedistic', 'encouraged many researchers to move away from extensive domain andistic knowledge and to embark instead upon knowledge-or anhora resolution', 'ev  ; Williams , Harvey , and Preston 1996 ; #TAUTHOR_TAG ; Mitkov 1996 , 1998b ) .']","['heavily exploited linguistic', 'encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution', '; Williams , Harvey , and Preston 1996 ; #TAUTHOR_TAG ; Mitkov 1996 , 1998b ) .']","['heavily exploitedistic knowledge', 'encouraged many researchers to move away from extensive domain andistic knowledge and to embark instead upon knowledge-or anaphora resolution strategies.', '; Williams , Harvey , and Preston 1996 ; #TAUTHOR_TAG ; Mitkov 1996 , 1998b ) .']","['', '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impetus to core', '', '', 'the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #TAUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 998a , ', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impetus to coreference', '', '', 'the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #TAUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a ,', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task a considerable impetus to core', '', '', 'the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #TAUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , ', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', '', '', '']",0
"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach #TAUTHOR_TAG ."", '', '']","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach #TAUTHOR_TAG ."", '', '']","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach #TAUTHOR_TAG ."", '', '']","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach #TAUTHOR_TAG ."", '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impus core', '', '', 'form Abracos and Lopes  ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution #TAUTHOR_TAG a , 2001b ) .', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impetus coreference', '', '', 'form Abracos and Lopes ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution #TAUTHOR_TAG a , 2001b ) .', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task a considerable impetus core', '', '', 'ised form Abracos and L ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution #TAUTHOR_TAG a , 2001b ) .', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', '', '', '']",0
"['of the in anap heavily exploited domain andistic knowledgener 9oyell', 'robust encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution', 'istic and reported promising in-or operational environments ( Dagan and Itai 1990 , 99 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #TAUTHOR_TAG ; Williams , Harvey , and Preston 1996 ;win 1997 ; Mit 6 , b']","['of the in anaphora heavily exploited domain and guistic knowledge', 'robust encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution', 'linguistic and reported promising in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #TAUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov ,']","['of in anap heavily exploited domain and- guistic knowledgeell', 'robust encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'and reported promising results in-or operational environments ( Dagan and Itai 1990 , 99 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #TAUTHOR_TAG ; Williams , Harvey , and Preston 1996 ;win 1997 ; Mit , b']","['', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #TAUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impus core', '', '', 'andstaff the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #TAUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998 , .']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impetus coreference', '', '', 'and the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #TAUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , .']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task a considerable impetus core', '', '', 'and the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #TAUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , .']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impus core', '', '', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #TAUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in , used rev form ( Abrac and', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impetus coreference', '', '', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #TAUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in , used revised form ( Abracos and', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task a considerable impetus core', '', '', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #TAUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in , used either revised form ( Abrac and', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', '', '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task) gave a considerable impetus to the development of coreference resolution algorithms and, such as in', 'decade a of Japanese', 'mult McKee Azzam , Hys , and Gaizausas 199 ;abagiu and Maiorano  ; Mitkov and Barbu 2000 ; #TAUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task gave a considerable impetus to the development of coreference resolution algorithms and systems, such as in', 'decade a of Japanese,', 'McKee Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano ; Mitkov and Barbu 2000 ; #TAUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task) gave a considerable impetus to the development of coreference resolution algorithms and, such as in', 'a number of', 'mult McK Azzam , Hys , and Gaizausas 1998 ;abagiu and Maior ; Mitkov and Barbu 2000 ; #TAUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #TAUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']",0
"['of the in anap heavily exploitedistic knowledge', 'robustLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution', 'istic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #TAUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ;win 1997 ; ,']","['of the in anaphora heavily exploited guistic knowledge', 'robust NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution', 'linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #TAUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; ,']","['of in anap heavily exploited- guistic knowledge', 'robust practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #TAUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ;win 1997 ; ,']","['', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #TAUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']",0
"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #TAUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input', 'encouraged many researchers to move away fromistic knowledge embark', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #TAUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input', 'encouraged many researchers to move away from linguistic knowledge embark', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #TAUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'encouraged many researchers to move away fromistic knowledge embark instead', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #TAUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task the) gave a considerable impetus to the development of coreference resolution algorithms and, such as in', 'decade 2 saw a of anap resolution projects for Japanese', 'mult , multual /ference has gained considerable momentum in recent ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #TAUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task the gave a considerable impetus to the development of coreference resolution algorithms and systems, such as in', 'decade 20th saw a of anaphora resolution projects for Japanese,', ', multilingual / coreference has gained considerable momentum in recent ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #TAUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task) gave a considerable impetus to the development of coreference resolution algorithms and, such as in', 'saw a number of anaphora resolution projects for', 'mult , mult has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #TAUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #TAUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']",0
"['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field #TAUTHOR_TAG a ) .', 'needs further investigation is how far the performance of anaphora algorithms can go and what the ofpoor methods are', '', '', '', '']","['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field #TAUTHOR_TAG a ) .', 'needs further investigation is how far the performance of anaphora algorithms can go and what the of knowledge-poor methods are.', '', '', '', '']","['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field #TAUTHOR_TAG a ) .', 'needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations ofpoor methods are.', '', '', '', '']","['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field #TAUTHOR_TAG a ) .', 'A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.', '', '', '', '']",3
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impetus to core', '', '', 'the continuing inering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #TAUTHOR_TAG ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'inclusion of coreference task a considerable impetus to coreference', '', '', 'the continuing in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #TAUTHOR_TAG ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task a considerable impetus to core', '', '', 'the continuing interest inering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #TAUTHOR_TAG ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', '', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #TAUTHOR_TAG ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', '']",0
"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; 1987 ; Rich and LuperFoy 1988 ; #TAUTHOR_TAG , which was difficult both to represent and to process , and which required considerable human input .', 'encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-or an resolution', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; 1987 ; Rich and LuperFoy 1988 ; #TAUTHOR_TAG , which was difficult both to represent and to process , and which required considerable human input .', 'encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor resolution', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #TAUTHOR_TAG , which was difficult both to represent and to process , and which required considerable human input .', 'encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #TAUTHOR_TAG , which was difficult both to represent and to process , and which required considerable human input .', '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'The last decade of the 2 century saw a of anaphora resolution for Japanese', '', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a of anaphora resolution for Japanese,', '', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for', '', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #TAUTHOR_TAG , #AUTHOR_TAG , and #AUTHOR_TAG .', '', '', '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task) gave a considerable impetus to the development of coreference resolution algorithms and, such as in', 'decade 2 a ofap resolution for Japanese', 'Against the background of a growing in multual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #TAUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task gave a considerable impetus to the development of coreference resolution algorithms and systems, such as in', 'decade 20th a of resolution for Japanese,', 'Against the background of a growing in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #TAUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task) gave a considerable impetus to the development of coreference resolution algorithms and, such as in', 'a number ofap for', 'Against the background of a growing interest in multual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #TAUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #TAUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', '', '']",0
"['of heavily exploitedistic', 'many researchers to move away from domainistic knowledge and to embark instead upon knowledge-', ', Harvey , and Preston 996 ;win 1997 ; #TAUTHOR_TAG , 1998b ) .']","['of heavily exploited linguistic', 'many researchers to move away from domain linguistic knowledge and to embark instead upon knowledge-poor', ', Harvey , and Preston 1996 ; Baldwin 1997 ; #TAUTHOR_TAG , 1998b ) .']","['of heavily exploitedistic knowledge', 'many researchers to move away from extensive domainistic knowledge and to embark instead upon knowledge-', ', Harvey , and Preston 1996 ;win 1997 ; #TAUTHOR_TAG , 1998b ) .']","['', '', '']",0
"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm #TAUTHOR_TAG , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", '', '']","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm #TAUTHOR_TAG , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", '', '']","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm #TAUTHOR_TAG , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", '', '']","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm #TAUTHOR_TAG , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task) gave a considerable impetus to coreference resolution algorithms and', 'decade', 'andbu  ; Mitkov 1999 ; #TAUTHOR_TAG ; Mitkov , Belguith , and Stys 1998', 'abil', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task gave a considerable impetus to coreference resolution algorithms and', 'decade', 'and Barbu ; Mitkov 1999 ; #TAUTHOR_TAG ; Mitkov , Belguith , and Stys 1998', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task) gave a considerable impetus to coreference resolution algorithms and', '', 'and ; Mitkov 1999 ; #TAUTHOR_TAG ; Mitkov , Belguith , and Stys 1998', 'abil', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', '', '', '', '', '']",0
"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , #AUTHOR_TAG , and #TAUTHOR_TAG .', 'The last decade of the 2 century saw a number of anaphora resolution projects for Japanese', '', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , #AUTHOR_TAG , and #TAUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for Japanese,', '', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , #AUTHOR_TAG , and #TAUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for', '', '', '']","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , #AUTHOR_TAG , and #TAUTHOR_TAG .', '', '', '', '']",0
"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge #TAUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input', 'encouraged many researchers to move away from extensive domain andistic knowledge and embark instead upon knowledgeor resolution', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge #TAUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input', 'encouraged many researchers to move away from extensive domain and linguistic knowledge and embark instead upon knowledge-poor resolution', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge #TAUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'encouraged many researchers to move away from extensive domain andistic knowledge and embark instead upon knowledge', '']","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge #TAUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', '', '']",0
"['Proper names are the main concern of the named-entity recognition subtask #TAUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There theambiguation of the in a sentence ( other ambig of the central 2% of', 'surn', '', '', '']","['Proper names are the main concern of the named-entity recognition subtask #TAUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There the disambiguation of the in a sentence (and other ambiguous of the central 20% of', 'surname', '', '', '']","['Proper names are the main concern of the named-entity recognition subtask #TAUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There the disambiguation of in a sentence ( of the central problems: 2% of', '', '', '', '']","['Proper names are the main concern of the named-entity recognition subtask #TAUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', '', '', '', '', '']",0
"['', '', '', '', '', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TAUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']","['', '', '', '', '', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TAUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']","['', '', '', '', '', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TAUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']","['', '', '', '', '', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #TAUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']",1
"['punct', 'text information about or is a common word is crucial for the, document-cent fly ambigu entire.', 'This is implemented as a cascade of simple strategies , which were briefly described in #TAUTHOR_TAG .']","['punctuation', 'text information about or is a common word is crucial for the task, document-centered fly ambiguously entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in #TAUTHOR_TAG .']","['', 'about or is a common word is crucial for a document-centered approach the fly ambigu the entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in #TAUTHOR_TAG .']","['', '', 'This is implemented as a cascade of simple strategies , which were briefly described in #TAUTHOR_TAG .']",5
"['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus #TAUTHOR_TAG and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Alt there are about 500 documents in the Brown, with length .']","['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus #TAUTHOR_TAG and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown with length tokens.']","['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus #TAUTHOR_TAG and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Alt there are about 500 documents in the Brown corpus, with ']","['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus #TAUTHOR_TAG and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens.']",5
"['', '', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition #TAUTHOR_TAG .', 'Gale, Church, and Yarowskys observation is also used in our DCA, especially for the identification of abbreviations.', 'ation']","['', '', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition #TAUTHOR_TAG .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", '']","['', '', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition #TAUTHOR_TAG .', 'Gale, Church, and Yarowskys observation is also used in our DCA, especially for the identification of abbreviations.', '']","['', '', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition #TAUTHOR_TAG .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", '']",0
"['atically trainable software is generally seen way producing sys tems are quickly retrain for a a or for.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers #TAUTHOR_TAG , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat theBD task as a', '', '']","['Automatically trainable software is generally seen way producing sys- tems are quickly retrainable for a a or for language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers #TAUTHOR_TAG , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a', '', '']","['atically trainable software is generally seen a way producing sys- tems are quickly retrain for or for', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers #TAUTHOR_TAG , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as', '', '']","['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers #TAUTHOR_TAG , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', '', '', '']",0
"['atically trainable software is generally seen way producing sys tems are quickly retrain a a for.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks #TAUTHOR_TAG , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a', '', '']","['Automatically trainable software is generally seen way producing sys- tems are quickly retrainable a a for language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks #TAUTHOR_TAG , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a', '', '']","['atically trainable software is generally seen a way producing sys- tems are quickly retrain for', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks #TAUTHOR_TAG , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as', '', '']","['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks #TAUTHOR_TAG , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', '', '', '']",0
"['', '', '', '', '', ""since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #TAUTHOR_TAG a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']","['', '', '', '', '', ""since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #TAUTHOR_TAG a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']","['', '', '', '', '', ""since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #TAUTHOR_TAG a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']","['', '', '', '', '', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #TAUTHOR_TAG a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']",1
"['', '', '', '', '', 'ms ( , Brillsill1995a ] , and MaxEnt [ #TAUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .', '', '', '', '', '']","['', '', '', '', '', ""( , Brill 's Brill 1995a ] , and MaxEnt [ #TAUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", '', '', '', '', '']","['', '', '', '', '', '( , Brills [ Brill 1995a ] , and MaxEnt [ #TAUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .', '', '', '', '', '']","['', '', '', '', '', '', '', '', '', '', '']",1
"['Row C of Table4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system #TAUTHOR_TAG with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', '', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system #TAUTHOR_TAG with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', '', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system #TAUTHOR_TAG with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', '', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system #TAUTHOR_TAG with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', '', '', '']",1
"['There exist two large classes of SBD systems: rule based and machine learning.', 'The rule-', 'To a is and to a-based system with good performance is quite a-consuming enterprise.', 'For instance , the Alembic workbench #TAUTHOR_TAG contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', 'ged short rulebased such systems are usually closely tailored to a corpus sub and not easily.']","['There exist two large classes of SBD systems: rule based and machine learning.', 'The rule-based', 'To a is and to a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench #TAUTHOR_TAG contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', 'shortcoming rule-based such systems are usually closely tailored to a corpus sublanguage and not easily domains.']","['There exist two large classes of SBD systems: rule based and machine learning.', 'The rule-based systems', 'To is and to a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench #TAUTHOR_TAG contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', 'rulebased systems such systems are usually closely tailored to a particular corpus sub and are not easily.']","['There exist two large classes of SBD systems: rule based and machine learning.', '', 'To put together a few rules is fast and easy, but to develop a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench #TAUTHOR_TAG contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', '']",0
"['performance reported', '(iley 19 0 error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TAUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', 'error rates to very, they', '', '', '']","['performance reported', '(Riley 1989: error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TAUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', 'error rates to very small, they', '', '', '']","['the performance reported', '(% error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TAUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', 'these error rates to be very,', '', '', '']","['', '', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #TAUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', '', '', '', '']",1
"['', '', '', ""This done purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #TAUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'OS patterns the right combinations the', '']","['', '', '', ""This done purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #TAUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'POS patterns the right combinations the', '']","['', '', '', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #TAUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'patterns over the right combinations the', '']","['', '', '', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #TAUTHOR_TAG b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '', '']",1
"['Row C of Table4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1% measured on the and thepus.', 'The best performance on the WSJpus Ale', '', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TAUTHOR_TAG .', 'of abbreviations with comprehensive on the or']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the and the corpus.', 'The best performance on the WSJ corpus', '', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TAUTHOR_TAG .', 'of abbreviations with comprehensive on the or']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1% measured on and', 'The best performance on the WSJ corpus', '', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TAUTHOR_TAG .', 'of abbreviations with comprehensive evaluation on or']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', '', '', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #TAUTHOR_TAG .', '']",1
"['ation is usually handled', 'As #TAUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', 'For example, the capital wordActs is in the Brown Corpus, both as a proper noun (in a title).', 'It']","['is usually handled', 'As #TAUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' is in the Brown Corpus, both as a proper noun (in a title)."", 'It']","['ation is usually handled', 'As #TAUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' in the Brown Corpus, both times as a proper noun (in a title)."", '']","['', 'As #TAUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title)."", '']",1
"['', '', '', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #TAUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '', '']","['', '', '', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #TAUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '', '']","['', '', '', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #TAUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '', '']","['', '', '', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #TAUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", '', '']",1
"['to with the normalization issue', 'Before using the DCA method , we applied a Russian morphological processor #TAUTHOR_TAG to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'to ( we', '', '']","['to with the normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor #TAUTHOR_TAG to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'to we', '', '']","['to with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor #TAUTHOR_TAG to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'to ( we retained', '', '']","['', 'Before using the DCA method , we applied a Russian morphological processor #TAUTHOR_TAG to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', '', '', '']",5
"['', '', '', '', '', '', '', '', '', '', '', '', 'applied tasks wordation5)', '', 'we use this assumption with caution and first apply strategies that rely not just on', 'single words but on words together with their local contexts (n-grams). This is similar to ""one sense per collocation"" idea of #TAUTHOR_TAG', '.']","['', '', '', '', '', '', '', '', '', '', '', '', 'applied several tasks word95', '', 'however, we use this assumption with caution and first apply strategies that rely not just on', 'single words but on words together with their local contexts (n-grams). This is similar to ""one sense per collocation"" idea of #TAUTHOR_TAG', '.']","['', '', '', '', '', '', '', '', '', '', '', '', 'applied wordation', '', 'we use this assumption with caution and first apply strategies that rely not just on', 'single words but on words together with their local contexts (n-grams). This is similar to ""one sense per collocation"" idea of #TAUTHOR_TAG', '.']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'single words but on words together with their local contexts (n-grams). This is similar to ""one sense per collocation"" idea of #TAUTHOR_TAG', '.']",1
"['rel a list of words', 'This list includes common words for a, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. , #TAUTHOR_TAG .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method', 'pre', '', '', '', '']","['relies a list of words.', 'This list includes common words for a language, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. , #TAUTHOR_TAG .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).', '', '', '', '', '']","['a list of', 'This list includes common words for a, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. , #TAUTHOR_TAG .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method', 'pre', '', '', '', '']","['', 'This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. , #TAUTHOR_TAG .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).', '', '', '', '', '']",0
"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before #TAUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', '', '', '', '', '']","['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before #TAUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', '', '', '', '', '']","['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before #TAUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', '', '', '', '', '']","['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before #TAUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', '', '', '', '', '']",1
"['In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 199).', 'In some systems such dependencies are learned from labeled examples #TAUTHOR_TAG .', 'The the namedentity approach is only', '', '']","['In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples #TAUTHOR_TAG .', 'The the namedentity approach is only', '', '']","['In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples #TAUTHOR_TAG .', 'The advantage the namedentity approach is only', '', '']","['In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples #TAUTHOR_TAG .', '', '', '']",0
"['abbreviation (as opposed word) These four lists can be acquired completely automatically from raw (unlabeled', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) #TAUTHOR_TAG .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', '']","['abbreviation (as opposed word) These four lists can be acquired completely automatically from raw (unlabeled)', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) #TAUTHOR_TAG .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', '']","['abbreviation (as opposed word) These four lists can be acquired completely automatically from raw (unlabeled', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) #TAUTHOR_TAG .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', '']","['• abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) #TAUTHOR_TAG .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', '']",5
"['Row C of Table4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the cor and theJ corpus.', 'The best performance on the WSJ corpus was achieved by a of the SATZ system (Palmer and Hearst 1997) with the Ale system (Aberdeen et al. 1995): a 0.% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TAUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the corpus and the corpus.', 'The best performance on the WSJ corpus was achieved by a of the SATZ system (Palmer and Hearst 1997) with the system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TAUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TAUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #TAUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', '', '']",1
"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #TAUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to', '', '', '', '', '', '', '', '', '']","['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #TAUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to', '', '', '', '', '', '', '', '', '']","['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #TAUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to', '', '', '', '', '', '', '', '', '']","['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #TAUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', '', '', '', '', '', '', '', '', '', '']",0
"['in speech', '', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation #TAUTHOR_TAG and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', 'Gale, Church, and Yarowskys observation is also used in DCA, especially for the of abvi.', 'capitalized-word disambiguation, however']","['in speech', '', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation #TAUTHOR_TAG and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in DCA, especially for the of abbreviations."", 'capitalized-word disambiguation, however']","['in', '', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation #TAUTHOR_TAG and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', 'Gale, Church, and Yarowskys observation is also used in DCA, especially for the identification of abvi.', 'capitalized-word disambiguation, however']","['', '', '', '', '', '', '', '', '', '', '', '', '', ' #AUTHOR_TAG showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation #TAUTHOR_TAG and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however']",0
"['Another important task of text normalization is sentence boundary disambiguation ( or sentence', '', '', ').', 'A detailed introduction to the SBD problem can be found in #TAUTHOR_TAG .']","['Another important task of text normalization is sentence boundary disambiguation or sentence', '', '', '', 'A detailed introduction to the SBD problem can be found in #TAUTHOR_TAG .']","['Another important task of text normalization is sentence boundary disambiguation ( or', '', '', ').', 'A detailed introduction to the SBD problem can be found in #TAUTHOR_TAG .']","['Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting.', '', '', '', 'A detailed introduction to the SBD problem can be found in #TAUTHOR_TAG .']",0
"['', '', '', ' very short documents of one to three sentences also present a difficulty for our approach', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #TAUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']","['', '', '', '8 very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #TAUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']","['', '', '', 'very short documents of one to three sentences also present a difficulty for our approach', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #TAUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']","['', '', '', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #TAUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']",1
"['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists', 'TheCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'has been applied not only to the identification of proper names, as described in this article, but to their (Mikheev, Gro, and Moens ', '', '', '', '']","['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'has been applied not only to the identification of proper names, as described in this article, but to their (Mikheev, Grover, and Moens', '', '', '', '']","['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but to their (Mikheev, Gro, and Mo', '', '', '', '']","['', '', '', '', '', '', '', '', '', ' #TAUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', '', '', '', '']",5
"['we combined our main (evaluated in row D of Table4) POS.', 'Unlike other POS taggers , this POS tagger #TAUTHOR_TAG was also trained to disambiguate sentence boundaries .']","['we combined our main (evaluated in row D of Table 4) POS', 'Unlike other POS taggers , this POS tagger #TAUTHOR_TAG was also trained to disambiguate sentence boundaries .']","['we combined our main configuration (evaluated in row D of Table 4)', 'Unlike other POS taggers , this POS tagger #TAUTHOR_TAG was also trained to disambiguate sentence boundaries .']","['', 'Unlike other POS taggers , this POS tagger #TAUTHOR_TAG was also trained to disambiguate sentence boundaries .']",5
"['Row C of Table4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system #TAUTHOR_TAG : a 0.5 % error rate .', '', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system #TAUTHOR_TAG : a 0.5 % error rate .', '', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system #TAUTHOR_TAG : a 0.5 % error rate .', '', '', '']","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system #TAUTHOR_TAG : a 0.5 % error rate .', '', '', '']",1
"['', '', '', 'in Section 8 that very short documents of one to three sentences also present a difficulty for our approach', 'This is where robust syntactic systems like SATZ #TAUTHOR_TAG or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']","['', '', '', 'in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ #TAUTHOR_TAG or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']","['', '', '', 'in Section 8 that very short documents of one to three sentences also present a difficulty for our approach', 'This is where robust syntactic systems like SATZ #TAUTHOR_TAG or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']","['', '', '', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ #TAUTHOR_TAG or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']",1
"['speech', '', '', '', 'unlike multipass', "" #TAUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", 'DCA a', '', '', '', '', '', '', '', '', '', '']","['speech', '', '', '', 'unlike multipass', "" #TAUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", 'DCA a', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', 'unlike', "" #TAUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", 'the DCA system', '', '', '', '', '', '', '', '', '', '']","['', '', '', '', '', "" #TAUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", '', '', '', '', '', '', '', '', '', '', '']",2
"['The description of the EAGLE workbench for linguistic engineering #TAUTHOR_TAG mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', 'AG. performance details']","['The description of the EAGLE workbench for linguistic engineering #TAUTHOR_TAG mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', 'al. performance details.']","['The description of the EAGLE workbench for linguistic engineering #TAUTHOR_TAG mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', '']","['The description of the EAGLE workbench for linguistic engineering #TAUTHOR_TAG mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', '']",1
"['is generally seen re', ', the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling #TAUTHOR_TAG .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', '', '']","['is generally seen', ', the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling #TAUTHOR_TAG .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', '', '']","['is generally seen re', ', the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling #TAUTHOR_TAG .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', '', '']","['', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling #TAUTHOR_TAG .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', '', '']",0
"['received it plays an important in many.', '', 'open', '', '', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', "" #TAUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do nt refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences because some capitalized words stand for proper names (such as Contin, the name of an airline) and some']","['received it plays an important in many tasks.', '', 'open', '', '', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', "" #TAUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences because some capitalized words stand for proper names (such as Continental, the name of an airline) and some']","['has received it plays an important role in many tasks.', '', '', '', '', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', "" #TAUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences because some capitalized words stand for proper names (such as Contin, the name of an airline) and some']","['', '', '', '', '', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', "" #TAUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']",0
"['', '', '', '', ' #TAUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'no harm (apart from performance issues) proposing too many abbreviations, because only those that can be linked to their definitions will be retained', 'abviation recognizer two to', 'andid known', '']","['', '', '', '', ' #TAUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'no harm (apart from performance issues) proposing too many abbreviations, because only those that can be linked to their definitions will be retained.', 'abbreviation recognizer two to', 'known', '']","['', '', '', '', ' #TAUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'no harm (apart from the performance issues) proposing too many abbreviations, because only those that can be linked to their definitions will be retained', 'Therefore the abbreviation recognizer treats two to', 'andid', '']","['', '', '', '', ' #TAUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', '', '', '']",0
"['approach', 'system', 'some SBD systems can be trained on relatively of labeled examples, their performance in cases is somewhat than', 'For instance #TAUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', 'This is a be manually marked in a', 'But the error rate (1.5%) of the decision tree classifier trained on was about 50 6 examples (']","['approach', 'system', 'some SBD systems can be trained on relatively of labeled examples, their performance in cases is somewhat than', 'For instance #TAUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', 'This is a be manually marked in a', 'But the error rate (1.5%) of the decision tree classifier trained on was about 50% examples']","['this approach', 'system', 'some SBD systems can be trained on of labeled examples, their performance in such cases is somewhat than', 'For instance #TAUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', 'This is can be manually marked in', 'But the error rate (1.5%) of the decision tree classifier trained on was about 50 6 (']","['', '', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #TAUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', '', 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']",1
"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', '', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #TAUTHOR_TAG . an consider the translation into French of the house collapsed .']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', '', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #TAUTHOR_TAG .7 an consider the translation into French of the house collapsed .']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', '', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #TAUTHOR_TAG . an example consider the translation into French of the house collapsed .']","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', '', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #TAUTHOR_TAG .7 As an example , consider the translation into French of the house collapsed .']",0
