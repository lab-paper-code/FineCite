unique_id,citing_id,citing_title,cited_title,cited_authors,section_title,cited_abstract,citation_context,cite_context_paragraph,citation_class_label,dynamic_contexts_combined
CC1,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,a fast and portable realizer for text generation systems,"['B Lavoie', 'Rambow']",,"{PG,1 A mixer block used in rotary drums to improve the efficiency of mixing, drying, cooling, heating, or calcining of solid materials such as gravel, stone, fluxes and the like to produce a more uniform product with minimal production of fines and dust is described. The mixer block is especially useful when used as part of the refractory lining in a rotary kiln to calcine fluxstone such as limestone, dolomite, dolomitic limestone, magnesite and the like.",The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG ) .,"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).', 'The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG ) .']",0,"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).', 'The framework was originally developed for the realization of deep-syntactic structures in NLG ( #AUTHOR_TAG ) .']"
CC2,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,a fast and portable realizer for text generation systems,"['B Lavoie', 'Rambow']",,"{PG,1 A mixer block used in rotary drums to improve the efficiency of mixing, drying, cooling, heating, or calcining of solid materials such as gravel, stone, fluxes and the like to produce a more uniform product with minimal production of fines and dust is described. The mixer block is especially useful when used as part of the refractory lining in a rotary kiln to calcine fluxstone such as limestone, dolomite, dolomitic limestone, magnesite and the like.","Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG ) .","['The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991 In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG ) .']",2,"['The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991 In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).', 'Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( #AUTHOR_TAG ) .']"
CC3,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,machine translation divergences a formal description and proposed solution,['B J Don'],,"There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system.","More details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) .","['--o II a failli pleuvoir.', 'More details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) .']",0,"['More details on how the structural divergences described in ( #AUTHOR_TAG ) can be accounted for using our formalism can be found in ( Nasr et al. , 1998 ) .']"
CC4,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,applied text generation,['T Korelsky'],introduction,"While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion.","Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .","['In this paper we present a linguistically motivated framework for uniform lexicostructural processing.', 'It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).', 'Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .', 'Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.']",2,"['In this paper we present a linguistically motivated framework for uniform lexicostructural processing.', 'It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).', 'Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and #AUTHOR_TAG ) , and LFS ( Iordanskaja et al. , 1992 ) .']"
CC5,A00-1009,A framework for MT and multilingual NLG systems based on uniform lexico-structural processing,applied text generation,['T Korelsky'],,"While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion.","The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and #AUTHOR_TAG ) .","['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and #AUTHOR_TAG ) .', 'The framework was originally developed for the realization of deep-syntactic structures in NLG .']",2,"['The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and #AUTHOR_TAG ) .']"
CC6,A00-1012,Experiments on sentence boundary detection,a simple rulebased part of speech tagger,['E Brill'],,"Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. 1. I N T R O D U C T I O N There has been a dramat ic increase in the application of probabilistic models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact tha t very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automat ic par t of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12]. Stochastic taggers have *A version of this paper appears in Proceedings of the Third Conference on Applied Computational Linguistics (ACL), Trento, Italy, 1992. Used by permission of the Association for Computational Linguistics; copies of the publication from which this material is derived can can be obtained from Dr. Donald E. Walker (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910, Morristown, NJ 07960-1910, USA. The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. DAAL 03-89-G0031 PRI. 112 obtained a high degree of accuracy without performing any syntactic analysis on the input. These stochastic par t of speech taggers make use of a Markov model which captures lexical and contextual information. The parameters of the model can be est imated from tagged [1, 3, 4, 6, 12] or untagged [2, 9, 11] text. Once the parameters of the model are est imated, a sentence can then be automatical ly tagged by assigning it the tag sequence which is assigned the highest probabil i ty by the model. Performance is often enhanced with the aid of various higher level preand postprocessing procedures or by manually tuning the model. A number of rule-based taggers have been built [10, 7, 8]. [10] and [7] both have error rates substantial ly higher than s tate of the art stochastic taggers. [8] disambiguates words within a deterministic parser. We wanted to determine whether a simple rule-based tagger without any knowledge of syntax can perform as well as a stochastic tagger, or if par t of speech tagging really is a domain to which stochastic techniques are bet ter suited. In this paper we describe a rule-based tagger which performs as well as taggers based upon probabilistic models. The rule-based tagger overcomes the limitations common in rule-based approaches to language processing: it is robust, and the rules are automatical ly acquired. In addition, the tagger has many advantages over stochastic taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules as opposed to the large tables of statistics needed for stochastic taggers, ease of finding and implementing improvements to the tagger, and bet ter portabil i ty from one tag set or corpus genre to another. 2. T H E T A G G E R The tagger works by automatical ly recognizing and remedying its weaknesses, thereby incrementally improving its performance. The tagger initially tags by assigning each word its most likely tag, est imated by examining a large tagged corpus, without regard to context. In both sentences below, run would be tagged as a verb: Report Documentation Page Form Approved OMB No. 0704-0188 Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number.",Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .,"['Figure 1: Example text shown in standard and ASR format ation which is not available in ASR output is sentence boundary information.', 'However, knowledge of sentence boundaries is required by many NLP technologies.', ""Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .""]",0,"[""Part of speech taggers typically require input in the format of a single sentence per line ( for example Brill 's tagger ( #AUTHOR_TAG ) ) and parsers generally aim to produce a tree spanning each sentence .""]"
CC7,A00-1020,Multilingual coreference resolution,knowledgelean coreference resolution and its relation to textual cohesion and coherence,"['Sanda M Harabagiu', 'Steve J Maiorano']",introduction,"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology.","SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .","['For both languages, we resolved coreference by using SWIZZLE,our implementation of a bilingual coreference resolver.', 'SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .', 'When COCKTAIL was applied separately on the English and the Ro- manian texts, coreferring links were identified for each English and Romanian document respectively.', 'When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuris- tics for coreference.', 'Our experiments show that SWIZZLEoutperformed COCKTAILon both English and Romanian test documents.']",2,"['SWIZZLE is a multilingual enhancement of COCKTAIL ( #AUTHOR_TAG ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information  .']"
CC8,A00-1020,Multilingual coreference resolution,robust pronoun resolution with limited knowledge,['Ruslan Mitkov'],,"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications.","Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) .","['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', 'Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge.', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) .', 'For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'For this research, we used a coreference resolution sys- tem ((Harabagiu and Malorano, 1999)) that imple- ments different sets of heuristics corresponding to various forms of coreference.', 'This system, called COCKTAIL,resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g. subjects of communication verbs are more likely to refer to the last person mentioned in the text).', 'These constraints are implemented as a set of heuristics ordered by their priority.', 'Moreover, the COCKTAILframework uniformly addresses the prob- lem of interaction between different forms of coref- erence, thus making the extension to multilingual coreference very natural.']",0,"['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', 'Traditionally, these techniques have combined extensive syntactic, se- mantic, and discourse knowledge.', 'Nevertheless , recent results show that knowledge-poor methods perform with amazing accuracy ( cfXXX ( #AUTHOR_TAG ) , ( Kennedy and Boguraev , 1996 ) ( Kameyama , 1997 ) ) .', 'For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'Moreover, the COCKTAILframework uniformly addresses the prob- lem of interaction between different forms of coref- erence, thus making the extension to multilingual coreference very natural.']"
CC9,A00-1020,Multilingual coreference resolution,knowledgelean coreference resolution and its relation to textual cohesion and coherence,"['Sanda M Harabagiu', 'Steve J Maiorano']",,"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology.",Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG ) .,"['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'On other occasions, coercions are obtained as paths of meronyms (e.g. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG ) .']",0,"['The third class of heuristics resolves coreference by coercing nominals.', 'Sometimes coercions involve only derivational morphology - linking verbs with their nominalizations.', 'On other occasions, coercions are obtained as paths of meronyms (e.g. is-part re- lations) and hypernyms (e.g. is-a relations).', 'Consistency checks implemented for this class of coref- erence are conservative: either the adjuncts must be identical or the adjunct of the referent must be less specific than the antecedent.', 'Table 1 lists the top performing heuristics of COCKTAILfor pronominal and nominal coreference.', 'Examples of the heuristics operation on the MUC data are presented presented in Table 2.', 'Details of the top performing heuristics of COCKTAIL were reported in ( #AUTHOR_TAG ) .']"
CC10,A00-1020,Multilingual coreference resolution,knowledgelean coreference resolution and its relation to textual cohesion and coherence,"['Sanda M Harabagiu', 'Steve J Maiorano']",,"In this paper we present a new empirical method for coreference resolution, implemented in the COCKTAIL system. The resuits of COCKTAIL are used for lightweight abduction of cohesion and coherence structures. We show that referential cohesion can be integrated with lexical cohesion to produce pragmatic knowledge. Upon this knowledge coherence abduction takes place. I M o t i v a t i o n Coreference evaluation was introduced as a new domain-independent task at the 6th Message Understandi~ Conference (MUC-6) in 1995. The task focused on a subset of coreference, namely the ide~tiQ/ coreference, established between nouns, pronouns and noun phrases (including proper names) that refer to the same entity. In d ~ ; , ~ the coreference task (d. (Hirschnum and Chinchor, 1997)) special care was taken to use the coreference output not only for supporting Information Extraction(IE), the central task of the MUCs, but also to create means for re .arch on corefea~mce and discourse p h e n o m ~ independent of IE. Annotated corpora were made available, using SGML tagging with~, the text stream. The annotated texts served as tralz~g examples for a variety. of corderence resolution methods, that had to focus not only on precision and recall, but also on robustness. Two general classes of approaches were distinguished. The first class is characterized by adaptations of previously known reference algon'thms (e.g. (Lappin and Leass, 1994), (Brennan et al., 1987)) the scarce syntactic and semantic knowledge available m an w. system (e.g. (Kameyama, 1997)). The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations (e.g. (Aone and Bennett, 1994) (Kehler, 1997)). * In the past two MUC competitions, the high scoring systems achieved a recall in the high 50's to low 60's and a precision in the low 70's (d. (Hirschman et al., 1998)). A study z of the contribution of each form of coreference to the overall performance shows that generally, proper name anaphora resolution have the highest precision (69%), followed by pronominal reference (62%). T h e worse .precision is obtained by the resolution of d~_ n!te nominals anaphors (46%). However, these results need to be contrasted with the distribution of coreferential links on the tagged corpora. The majority of coreference links (38.42%) connect names Of people, organizations or locations. In addition, 19.68% of the tagged c o ~ c e links are accounted by appositives. Only 16.35% of the tagged coreferences are pronominal. Nominal anaphors account for 25.55% of the coreference links, and their resolution is generally poorly represented in IE systems. Due to the distribution of coreference links in newswire texts, a coreference module that is merely capable of handling recognition of appositives with high precision and incorporates rules of name alias identification can achieve a baseline coreference precision up to 58.1%, without sophisticated syntactic or discourse information. Precision increase is obtained by extending lfigh-performance pronoun resolution methods (e.g. (Lappin and Leass, 1994)) to nominal corderence as well. Such enhancements rely on semantic and discourse knowledge. In this paper we describe COCKTAIL, a highperformance coreference resolution system that operatas on a mixture of heuristics that combine semantic and discourse information. The resulting tThe study, reported in (Kameyama, 1997), was performed on the coreference module of SRI's FASTUS (Appelt et al., I993), an IE system representative of today's IE technology.","For this research , we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .","['Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques.', 'Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge.', 'The acquisition of such knowledge is time-consuming, difficult, and error-prone.', 'Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf.', '(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).', 'For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.', 'For this research , we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', 'This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g.', 'subjects of communication verbs are more likely to refer to the last person mentioned in the text).', 'These constraints are implemented as a set of heuristics ordered by their priority.', 'Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural.']",5,"['For this research , we used a coreference resolution system ( ( #AUTHOR_TAG ) ) that implements different sets of heuristics corresponding to various forms of coreference .', 'This system, called COCKTAIL, resolves coreference by exploiting several textual cohesion constraints (e.g. term repetition) combined with lexical and textual coherence cues (e.g.', 'Moreover, the COCKTAIL framework uniformly addresses the problem of interaction between different forms of coreference, thus making the extension to multilingual coreference very natural.']"
CC11,A00-1024,Categorizing unknown words,language identification with confidence limits,['D Elworthy'],experiments,"A statistical classification algorithm and its application to language identification from noisy input are described. The main innovation is to compute confidence limits on the classification, so that the algorithm terminates when enough evidence to make a clear decision has been made, and so avoiding problems with categories that have similar characteristics. A second application, to genre identification, is briefly ex- amined. The results show that some of the problems of other language identification tech- niques can be avoided, and illustrate a more important point: that a statistical language process can be used to provide feedback about its own success rate","Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG ) .","['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word.']",4,"['To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of unknown word.', 'For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc.', 'Each component will return a confidence measure of the reliability of its prediction , c.f. ( #AUTHOR_TAG ) .', 'The results from each component are evaluated to determine the final category of the word.']"
CC12,A00-1024,Categorizing unknown words,a stochastic parts program and noun phrase parser for unrestricted text,['K Church'],experiments,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .,"['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD).', 'The tag set contains just one tag to identify nouns.']",5,"['The first feature represents the part of speech of the word.', 'We use an in-house statistical tagger ( based on ( #AUTHOR_TAG ) ) to tag the text in which the unknown word occurs .', 'The tag set contains just one tag to identify nouns.']"
CC13,A00-1024,Categorizing unknown words,detecting and correcting morphosyntactic errors in real texts,['T Vosse'],,,Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG ) .,"['Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Capitalization is his sole means of identifying names.', 'However, capitalization information is not available in closed captions.', 'Hence, his system would be ineffective on the closed caption domain with which we are working.', '(Granger, 1983) uses expectations generated by scripts to anMyze unknown words.', 'The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages.']",1,"['Research that is more similar in goal to that outlined in this paper is Vosse ( #AUTHOR_TAG ) .', 'Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names.', 'Hence, his system would be ineffective on the closed caption domain with which we are working.', '(Granger, 1983) uses expectations generated by scripts to anMyze unknown words.']"
CC14,A00-1024,Categorizing unknown words,detecting and correcting morphosyntactic errors in real texts,['T Vosse'],experiments,,Corpus frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .,"['Corpus frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.', 'Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions.']",5,"['Corpus frequency : ( #AUTHOR_TAG ) differentiates between misspellings and neologisms ( new words ) in terms of their frequency .', 'His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms.']"
CC15,A00-2028,An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email,automatic detection of poor speech recognition at the dialogue level,"['D J Litman', 'M A Walker', 'M J Kearns']",conclusion,"The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction. An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand. To do so, a system must be able to identify dialogue properties that suggest adaptation. This paper focuses on identifying situations where the speech recognizer is performing poorly. We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations. Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm.",Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ) .,"['The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.', 'Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ) .', 'However, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans.', 'In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime.']",2,['Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( #AUTHOR_TAG ) .']
CC16,A00-2028,An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email,automatic detection of poor speech recognition at the dialogue level,"['D J Litman', 'M A Walker', 'M J Kearns']",,"The dialogue strategies used by a spoken dialogue system strongly influence performance and user satisfaction. An ideal system would not use a single fixed strategy, but would adapt to the circumstances at hand. To do so, a system must be able to identify dialogue properties that suggest adaptation. This paper focuses on identifying situations where the speech recognizer is performing poorly. We adopt a machine learning approach to learn rules from a dialogue corpus for identifying these situations. Our results show a significant improvement over the baseline and illustrate that both lower-level acoustic features and higher-level dialogue features can affect the performance of the learning algorithm.",The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG ) .,"['The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG ) .']",4,"['The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and number of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percentconfirms, percent-subdials).', 'The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( #AUTHOR_TAG ) .']"
CC17,D08-1004,Modeling annotators,thumbs up sentiment classification using machine learning techniques,"['B Pang', 'L Lee', 'S Vaithyanathan']",method,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.","We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .","['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']",5,"['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( #AUTHOR_TAG ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.']"
CC18,D08-1006,Refining generative language models using discriminative learning,classbased ngram models of natural language,"['F Brown', 'Vincent J Della Pietra', 'Peter V deSouza', 'Jenifer C Lai', 'Robert L Mercer']",conclusion,,"In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #AUTHOR_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .","['The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach.', ""In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #AUTHOR_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .""]",3,"[""In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( #AUTHOR_TAG ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .""]"
CC19,D08-1006,Refining generative language models using discriminative learning,a discriminative language model with pseudonegative samples,"['Daisuke Okanohara', ""Jun'ichi Tsujii""]",,,"Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .","['Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', 'Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data.', 'Đn both cases essentially linear classifiers were used as features.', 'As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples.', 'Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', 'While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data.', 'This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications.', 'For this reason we use a different sampling scheme which we refer to as rejection sampling.', ""This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline.""]",4,"['Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling.', 'Unfortunately , as shown in ( #AUTHOR_TAG ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .', ""This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline.""]"
CC20,D08-1006,Refining generative language models using discriminative learning,a discriminative language model with pseudonegative samples,"['Daisuke Okanohara', ""Jun'ichi Tsujii""]",experiments,,"As shown in ( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .","['For our features we used large-margin classifiers trained using the online algorithm described in (Crammer et al., 2006).', 'The code for the classifier was generously provided by Daisuke Okanohara.', 'This code was extensively optimized to take advantage of the very sparse sentence representation described above.', 'As shown in ( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .', 'Therefore, we used a 3rd order polynomial kernel, which was found to give good results.', 'No special effort was otherwise made in order to optimize the parameters of the classifiers.']",4,"['As shown in ( #AUTHOR_TAG ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .']"
CC21,D08-1006,Refining generative language models using discriminative learning,an empirical study of smoothing techniques for language modeling,['Goodman'],experiments,"We present a tutorial introduction to n-gram models for language modeling and survey the most widely-used smoothing algorithms for such models. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980), Katz (1987), Bell, Cleary, and Witten (1990), Ney, Essen, and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g., Brown versus Wall Street Journal), count cutoffs, and n-gram order (bigram versus trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. Our results show that previous comparisons have not been complete enough to fully characterize smoothing algorithm performance. We introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser-Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.Engineering and Applied Science","Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .","['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .', '2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained.', 'A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled.', ""The value of the n'th coordinate in the vector representation of Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001."", 'This indicates that satisfying the constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated.']",5,"['Baseline language model : For P0 we used a trigram with modified kneser-ney smoothing [ Chen and #AUTHOR_TAG ] , which is still considered one of the best smoothing methods for n-gram language models .']"
CC22,D08-1010,Maximum entropy based rule selection model for syntax-based statistical machine translation,treetostring alignment template for statistical machine translation,"['Yang Liu', 'Qun Liu', 'Shouxun Lin']",experiments,"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.",The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .,"['The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .', 'When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.', 'Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately.', 'We set the iteration number to 100 and Gaussian prior to 1.']",2,"['The features can be easily obtained by modifying the TAT extraction algorithm described in ( #AUTHOR_TAG ) .', 'When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees.', 'Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately.']"
CC23,D08-1016,Dependency parsing by belief propagation,coarsetofine nbest parsing and maxent discriminative reranking,"['E Charniak', 'M Johnson']",conclusion,,"For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']",3,"['For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( #AUTHOR_TAG ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .']"
CC24,D08-1016,Dependency parsing by belief propagation,forest reranking discriminative parsing with nonlocal features,['L Huang'],conclusion,"Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.","For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']",3,"['For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) and history-based parsing ( Nivre and McDonald , 2008 ) .']"
CC25,D08-1016,Dependency parsing by belief propagation,integrating graphbased and transitionbased dependency parsers,"['J Nivre', 'R McDonald']",conclusion,,"For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #AUTHOR_TAG ) .","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #AUTHOR_TAG ) .', 'We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993;Buch-Kromann, 2006).']",3,"['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( #AUTHOR_TAG ) .']"
CC26,D08-1016,Dependency parsing by belief propagation,probabilistic cfg with latent annotations,"['T Matsuzaki', 'Y Miyao', 'J Tsujii']",conclusion,"This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences <= 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.","We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .","['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005;Huang, 2008) and history-based parsing (Nivre and McDonald, 2008).', 'We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .']",3,"['Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.', 'We could also introduce new variables , e.g. , nonterminal refinements ( #AUTHOR_TAG ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .']"
CC27,D08-1022,Forest-based translation rule extraction,forestbased translation,"['Haitao Mi', 'Liang Huang', 'Qun Liu']",introduction,,"Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .","['We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists.', 'Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', 'When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.']",2,"['Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; #AUTHOR_TAG ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .', 'When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.']"
CC28,D08-1022,Forest-based translation rule extraction,forestbased translation,"['Haitao Mi', 'Liang Huang', 'Qun Liu']",related work,,The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .,"['The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).', 'The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .', 'This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.']",2,"['The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).', 'The first direct application of parse forest in translation is our previous work ( #AUTHOR_TAG ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .']"
CC29,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,using predicateargument structures for information extraction,"['Mihai Surdeanu', 'Sanda Harabagiu', 'John Williams', 'Paul Aarseth']",introduction,"In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.","Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .","['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0,"['Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( #AUTHOR_TAG ) , and Machine Translation ( Boas 2002 ) .']"
CC30,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic semantic role labeling for chinese verbs,"['Nianwen Xue', 'Martha Palmer']",experiments,"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling.","To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , Xue ( 2008 ) .","['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , Xue ( 2008 ) .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.', 'From the table 6, we can find that our system is better than both of the related systems.', 'Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%.']",1,"['To prove that our method is effective , we also make a comparison between the performances of our system and #AUTHOR_TAG , Xue ( 2008 ) .', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.']"
CC31,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,question answering based on semantic structures,"['Srini Narayanan', 'Sanda Harabagiu']",introduction,The ability to answer complex questions posed in Natural Language depends on (1) the depth of the available semantic representations and (2) the inferential mechanisms they support. In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model. A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM). In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions. The results indicate enhanced accuracy over current state-of-the-art Q/A systems.,"Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .","['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0,"['Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( #AUTHOR_TAG ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( Boas 2002 ) .']"
CC32,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,the specification of the semantic knowledgebase of contemporary chinese,"['Hui Wang', 'Weidong Zhan', 'Shiwen Yu']",,,The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .,"['SemCat (semantic category) of predicate, Sem-Cat of first word, SemCat of head word, SemCat of last word, SemCat of predicate + SemCat of first word, SemCat of predicate + SemCat of last word, predicate + SemCat of head word, SemCat of predicate + head word.', 'The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .']",5,['The semantic categories of verbs and other words are extracted from the Semantic Knowledge-base of Contemporary Chinese ( #AUTHOR_TAG ) .']
CC33,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,calibrating features for semantic role labeling,"['Nianwen Xue', 'Martha Palmer']",introduction,"This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed. We further show that different features are needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis.",#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .,"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', '#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,['#AUTHOR_TAG did very encouraging work on the feature calibration of semantic role labeling .']
CC34,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,annotating the propositions in the penn chinese treebank,"['Nianwen Xue', 'Martha Palmer']",introduction,"In this paper, we describe an approach to annotate the propositions in the Penn Chinese Treebank. We describe how diathesis alternation patterns can be used to make coarse sense distinctions for Chinese verbs as a necessary step in annotating the predicate-structure of Chinese verbs. We then discuss the representation scheme we use to label the semantic arguments and adjuncts of the predicates. We discuss several complications for this type of annotation and describe our solutions. We then discuss how a lexical database with predicate-argument structure information can be used to ensure consistent annotation. Finally, we discuss possible applications for this resource.","After the PropBank ( #AUTHOR_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank ( #AUTHOR_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['After the PropBank ( #AUTHOR_TAG ) was built , Xue and Palmer ( 2005 ) and Xue ( 2008 ) have produced more complete and systematic research on Chinese SRL .']"
CC35,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic semantic role labeling for chinese verbs,"['Nianwen Xue', 'Martha Palmer']",introduction,"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling.","Experiments on Chinese SRL ( #AUTHOR_TAG , Xue 2008 ) reassured these findings .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( #AUTHOR_TAG , Xue 2008 ) reassured these findings .']",4,"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( #AUTHOR_TAG , Xue 2008 ) reassured these findings .']"
CC36,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,semantic argument classification exploiting argument interdependence,"['Zheng Ping Jiang', 'Jia Li', 'Hwee Tou Ng']",,"This paper describes our research on automatic semantic argument classification, using the PropBank data [Kingsbury et al., 2002]. Previous research employed features that were based either on a full parse or shallow parse of a sentence. These features were mostly based on an individual semantic argument and the relation between the predicate and a semantic argument, but they did not capture the interdependence among all arguments of a predicate. In this paper, we propose the use of the neighboring semantic arguments of a predicate as additional features in determining the class of the current semantic argument. Our experimental results show significant improvement in the accuracy of semantic argument classification after exploiting argument interdependence. Argument classification accuracy on the standard Section 23 test set improves to 90.50%, representing a relative error reduction of 18%.",#AUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .,"['#AUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .', 'It has turned the single point classification problem into the sequence labeling problem with the introduction of semantic context features.', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', 'We can use window size to represent the scope of the context.', 'Window size [-m, n] means that, in the sequence that all the arguments has constructed, the features of previous m and following n argu- ments will be utilized for the classification of cur- rent semantic role.', 'There are two kinds of argu- ment sequences in Jiang et al. (2005), and we only test the linear sequence.', 'Take the sentence in fig- ure 1 as an example.', 'The linear sequence of the arguments in this sentence is: ____(until then), ____ (the insurance company), _ (has), _ ____ (for the Sanxia Project), ____ (in- surance services).', 'For the argument _ (has), if the semantic context window size is [-1,2], the seman- tic context features e.g. headword, phrase type and etc. of ____ (the insurance company), __ ___ (for the Sanxia Project) and ____ (insurance services) will be utilized to serve the classification task of _ (has).']",5,"['#AUTHOR_TAG has built a semantic role classifier exploiting the interdependence of semantic roles .', 'Se- mantic context features indicates the features ex- tracted from the arguments around the current one.', 'For the argument _ (has), if the semantic context window size is [-1,2], the seman- tic context features e.g. headword, phrase type and etc. of ____ (the insurance company), __ ___ (for the Sanxia Project) and ____ (insurance services) will be utilized to serve the classification task of _ (has).']"
CC37,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,can semantic roles generalize across genres,"['Szu-ting Yi', 'Edward Loper', 'Martha Palmer']",conclusion,"PropBank has been widely used as training data for Semantic Role Labeling. However, because this training data is taken from the WSJ, the resulting machine learning models tend to overfit on idiosyncrasies of that text's style, and do not port well to other genres. In addition, since PropBank was designed on a verb-by-verb basis, the argument labels Arg2 - Arg5 get used for very diverse argument roles with inconsistent training instances. For example, the verb ""make"" uses Arg2 for the ""Material"" argument; but the verb ""multiply"" uses Arg2 for the ""Extent"" argument. As a result, it can be difficult for automatic classifiers to learn to distinguish arguments Arg2-Arg5. We have created a mapping between PropBank and VerbNet that provides a VerbNet thematic role label for each verb-specific PropBank label. Since VerbNet uses argument labels that are more consistent across verbs, we are able to demonstrate that these new labels are easier to learn.",#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .,"['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', 'However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank.', 'What if we could extend the idea of hierarchical architecture to the single semantic role level?', 'Would that help the improvement of SRC?']",1,"['Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained.', '#AUTHOR_TAG has made the first attempt working on the single semantic role level to make further improvement .', 'What if we could extend the idea of hierarchical architecture to the single semantic role level?']"
CC38,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,shallow semantic parsing of chinese,"['Honglin Sun', 'Daniel Jurafsky']",introduction,"In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.","Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as #AUTHOR_TAG , Xue and Palmer ( 2005 ) and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'For semantic analysis, developing features that capture the right kind of information is crucial.']"
CC39,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],experiments,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .","['To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .', 'Xue (2008) is the best SRL system until now and it has the same data setting with ours.', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.', 'From the table 6, we can find that our system is better than both of the related systems.', 'Our system has outperformed Xue (2008) with a relative error reduction rate of 9.8%.']",1,"['To prove that our method is effective , we also make a comparison between the performances of our system and Xue and Palmer ( 2005 ) , #AUTHOR_TAG .', 'The results are presented in Table 6  We have to point out that all the three systems are based on Gold standard parsing.']"
CC40,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,hierarchical semantic role labeling,"['Alessandro Moschitti', 'Ana-Maria Giuglea', 'Bonaventura Coppola', 'Roberto Basili']",,"We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification). To achieve this, we have split the classification step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts). The results show that the non-optimized hierarchical approach is computationally more efficient than the traditional systems and it preserves their accuracy.","be found in figure 2 , which is similar with that in #AUTHOR_TAG .","['Previous semantic role classifiers always did the classification problem in one-step.', 'However, in this paper, we did SRC in two steps.', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in #AUTHOR_TAG .']",1,"['Previous semantic role classifiers always did the classification problem in one-step.', 'The architectures of hierarchical semantic role classifiers can 2 Extra features e.g.', 'predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g.', 'phrase type, are limited.', 'be found in figure 2 , which is similar with that in #AUTHOR_TAG .']"
CC41,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG .","['Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG .']",5,"['Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from #AUTHOR_TAG .']"
CC42,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,bilingual framenet dictionaries for machine translation,['Hans C Boas'],introduction,"This paper describes issues surrounding the planning and design of GermanFrameNet (GFN), a counterpart to the English-based FrameNet project. The goals of GFN are (a) to create lexical entries for German nouns, verbs, and adjectives that correspond to existing FrameNet entries, and (b) to link the parallel lexicon fragments by means of common semantic frames and numerical indexing mechanisms. GFN will take a fine-grained approach towards polysemy that seeks to split word senses based on the semantic frames that underlie their analysis. The parallel lexicon fragments represent an important step towards capturing valuable information about the different syntactic realizations of frame semantic concepts across languages, which is relevant for information retrieval, machine translation, and language generation. 1","Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG ) .","['Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002).', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG ) .', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0,"['Since the arguments can provide useful semantic information , the SRL is crucial to many natural language processing tasks , such as Question and Answering ( Narayanan and Harabagiu 2004 ) , Information Extraction ( Surdeanu et al. 2003 ) , and Machine Translation ( #AUTHOR_TAG ) .']"
CC43,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],experiments,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","We use the same data setting with #AUTHOR_TAG , however a bit different from Xue and Palmer ( 2005 ) .","['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with #AUTHOR_TAG , however a bit different from Xue and Palmer ( 2005 ) .']",5,"['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with #AUTHOR_TAG , however a bit different from Xue and Palmer ( 2005 ) .']"
CC44,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],introduction,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #AUTHOR_TAG .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #AUTHOR_TAG .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , Xue and Palmer ( 2005 ) and #AUTHOR_TAG .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.']"
CC45,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,labeling chinese predicates with semantic roles,['Nianwen Xue'],introduction,"Driven by the availability of semantically annotated corpora such as the FrameNet and the Proposition Bank, recent years have seen a revived interst in semantic parsing by applying statistical and machine-learning methods to human annotated corpora. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this article we report work on Chinese Semantic Role Labeling (SRL), taking advantage of two recently completed corpora, the Chinese Proposition Bank, a semantically annotated corpus of Chinese verbs, and the Chinese Nombank, a companion corpus that annotates the predicateargument structure of nominalized predicates. Since the semantic role labels are assigned to the constituents in a parse tree, we first report experiments in which Semantic Role Labels are automatically assigned to hand-crafted parses in the Chinese Treebank. This gives us a measure of the extent to which Semantic Role Labels can be bootstrapped from the syntactic annotation provided in the treebank. We then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. These experiments gauge how successful Semantic Role Labeling for Chinese can be done in realistic situations. Our results show that when hand-crafted parses are used, SRL accuracy for Chinese is comparable to what has been reported for the state-of-the-art English SRL systems trained and tested on the English Proposition Bank, even though the Chinese Proposition Bank is significantly smaller in size. When an automatic parser is used, however, the accuracy of our system is significantly lower than the English state-of-the-art. This indicates that the improvement in Chinese parsing is critical to high-performance Semantic Role Labeling for Chinese. Our results also show that, in general, SRL accuracy is significantly higher for verbs than for nominalized predicates across all experimental conditions. We believe that this is due to the fact that the mapping from the syntactic structure to the predicate-argument structure is less transparent for nominalized predicates than for verbs. 1","Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG ) reassured these findings .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG ) reassured these findings .']",0,"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL ( Xue and Palmer 2005 , #AUTHOR_TAG ) reassured these findings .']"
CC46,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,hierarchical semantic role labeling,"['Alessandro Moschitti', 'Ana-Maria Giuglea', 'Bonaventura Coppola', 'Roberto Basili']",introduction,"We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification). To achieve this, we have split the classification step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts). The results show that the non-optimized hierarchical approach is computationally more efficient than the traditional systems and it preserves their accuracy.",#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.,"['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),  and Xue (2008).', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', '#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['#AUTHOR_TAG has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.']"
CC47,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic semantic role labeling for chinese verbs,"['Nianwen Xue', 'Martha Palmer']",introduction,"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling.","Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #AUTHOR_TAG and Xue ( 2008 ) .","['Compared to the research on English, the research on Chinese SRL is still in its infancy stage.', 'Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #AUTHOR_TAG and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'This paper made the first attempt on Chinese SRL and produced promising results.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.', 'Experiments on Chinese SRL (Xue andPalmer 2005, Xue 2008) reassured these findings.']",0,"['Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English , such as Sun and Jurafsky ( 2004 ) , #AUTHOR_TAG and Xue ( 2008 ) .', 'Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.', 'They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments.', 'After the PropBank (Xue and Palmer 2003) was built,  and Xue (2008) have produced more complete and systematic research on Chinese SRL.', 'Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic role labeling.', 'However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming.', 'Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling.', 'They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification.', 'For semantic analysis, developing features that capture the right kind of information is crucial.']"
CC48,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic semantic role labeling for chinese verbs,"['Nianwen Xue', 'Martha Palmer']",experiments,"Recent years have seen a revived interest in semantic parsing by applying statistical and machine-learning methods to semantically annotated corpora such as the FrameNet and the Proposition Bank. So far much of the research has been focused on English due to the lack of semantically annotated resources in other languages. In this paper, we report first results on semantic role labeling using a pre-release version of the Chinese Proposition Bank. Since the Chinese Proposition Bank is superimposed on top of the Chinese Tree-bank, i.e., the semantic role labels are assigned to constituents in a treebank parse tree, we start by reporting results on experiments using the handcrafted parses in the treebank. This will give us a measure of the extent to which the semantic role labels can be bootstrapped from the syntactic annotation in the treebank. We will then report experiments using a fully automatic Chinese parser that integrates word segmentation, POS-tagging and parsing. This will gauge how successful semantic role labeling can be done for Chinese in realistic situations. We show that our results using hand-crafted parses are slightly higher than the results reported for the state-of-the-art semantic role labeling systems for English using the Penn English Proposition Bank data, even though the Chinese Proposition Bank is smaller in size. When an automatic parser is used, however, the accuracy of our system is much lower than the English state-of-the-art. This reveals an interesting cross-linguistic difference between the two languages, which we attempt to explain. We also describe a method to induce verb classes from the Proposition Bank ""frame files"" that can be used to improve semantic role labeling.","We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .","['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'to chtb_931.fid,', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', '648 files (from chtb_081 to chtb_899.fid)', 'are used as the training set.', 'The development set includes 40 files, from chtb_041.fid to chtb_080.fid.', 'The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']",1,"['We use Chinese PropBank 1.0 (LDC number: LDC2005T23) in our experiments.', 'PropBank 1.0 includes the annotations for files chtb_001.fid', 'or the first 250K words of the Chinese TreeBank 5.1.', 'For the experiments, the data of PropBank is divided into three parts.', 'are used as the training set.', 'We use the same data setting with Xue ( 2008 ) , however a bit different from #AUTHOR_TAG .']"
CC49,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,shallow semantic parsing of chinese,"['Honglin Sun', 'Daniel Jurafsky']",,"In this paper we address the question of assigning semantic roles to sentences in Chinese. We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set. In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix. Finally, we compare English and Chinese semantic-parsing performance. While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese. We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.",The candidate feature templates include : Voice from #AUTHOR_TAG .,['The candidate feature templates include : Voice from #AUTHOR_TAG .'],5,['The candidate feature templates include : Voice from #AUTHOR_TAG .']
CC50,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,the penn chinese treebank phrase structure annotation of a large corpus,"['Nianwen Xue', 'Fei Xia', 'Fu dong Chiou', 'Martha Palmer']",,"With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with different segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are difficult. As a first step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The first two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking efforts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.",The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .,"['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'It is constituted of two parts.', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1 is an example from the PropBank 1 .', 'We put the word-by-word translation and the translation of the whole sentence below the example.', 'It is quite a complex sentence, as there are many semantic roles in it.', 'In this sentence, all the semantic roles of the verb 提供 (provide) are presented in the syntactic tree.', 'We can separate the semantic roles into two groups.']",5,"['The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( #AUTHOR_TAG ) .', 'One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Treebank.', 'The other is a dictionary which lists the frames of all the labeled predicates.', 'Figure 1 is an example from the PropBank 1 .', 'It is quite a complex sentence, as there are many semantic roles in it.']"
CC51,D08-1034,Improving Chinese semantic role classification with hierarchical feature selection strategy,automatic labeling of semantic roles,"['Daniel Gildea', 'Daniel Jurafsky']",introduction,"We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.",Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .,"['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .', 'The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence.', 'The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate.', 'Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc.', 'Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation (Boas 2002).', 'With the efforts of many researchers (Carreras and Màrquez 2004, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007, different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast.']",0,['Semantic Role labeling ( SRL ) was first defined in #AUTHOR_TAG .']
CC52,D09-1056,The role of named entities in web people search,automatic entity disambiguation benefits to ner relation extraction link analysis and inference,['Matthias Blume'],related work,"Entity disambiguation resolves the many-to-many correspondence between mentions of entities in text and unique real-world entities. Entity disambiguation can bring to bear global (corpus-level) statistics to improve the performance of named entity recognition systems. More importantly , intelligence analysts are keenly interested in relationships between real-world entities. Entity disambiguation makes possible additional types of relation assertions and affects relation extraction performance assessment. Finally, link analysis and inference inherently operate at the level of entities, not text strings. Thus, entity disambiguation is a prerequisite to carrying out these higher-level operations on information extracted from plain text. This paper describes Fair Isaac's automatic entity disambiguation capability and its performance.","In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .","['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( #AUTHOR_TAG ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.']"
CC53,D09-1056,The role of named entities in web people search,cucomsem exploring rich features for unsupervised web personal name disambiguation,"['Ying Chen', 'James H Martin']",related work,,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .","['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; #AUTHOR_TAG ; Popescu and Magnini , 2007 ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.']"
CC54,D09-1056,The role of named entities in web people search,disambiguation algorithm for people search on the web in,"['Dmitri V Kalashnikov', 'Stella Chen', 'Rabia Nuray', 'Sharad Mehrotra', 'Naveen Ashish']",related work,,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .","['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; Popescu and Magnini , 2007 ; #AUTHOR_TAG ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.']"
CC55,D09-1056,The role of named entities in web people search,unsupervised name disambiguation via social network similarity,['Bradley Malin'],related work,"Though names reference actual entities it is nontrivial to resolve which entity a particular name observation represents. Even when names are devoid of typographical error, the resolution process is confounded by both ambiguity, where the same name correctly references multiple entities, and by variation, when an entity is correctly referenced by multiple names. Thus, before link analysis for surveillance or intelligence-gathering purposes can proceed, it is necessary to ensure vertices and edges of the network are correct. In this paper, we concentrate on ambiguity and investigate unsupervised methods which simultaneously learn 1) the number of entities represented by a particular name and 2) which observations correspond to the same entity. The disambiguation methods leverage the fact that an entity's name can be listed in multiple sources, each with a number of related entity's names, which permits the construction of name-based relational networks. The methods studied in this paper differ based on the type of network similarity exploited for disambiguation. The first method relies upon exact name similarity and employs hierarchical clustering of sources, where each source is considered a local network. In contrast, the second method employs a less strict similarity requirement by using random walks between ambiguous observations on a global social network constructed from all sources, or a community similarity. While both methods provide better than simple baseline results on a subset of the Internet Movie Database, findings suggest methods which measure similarity based on community, rather than exact, similarity provide more robust disambiguation capability.","Other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Other representations use the link structure ( #AUTHOR_TAG ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']"
CC56,D09-1056,The role of named entities in web people search,named entity disambiguation a hybrid statistical and rulebased incremental approach,"['Hien T Nguyen', 'Tru H Cao']",related work,"The rapidly increasing use of large-scale data on the Web makes named entity disambiguation become one of the main challenges to research in Information Extraction and development of Semantic Web. This paper presents a novel method for detecting proper names in a text and linking them to the right entities in Wikipedia. The method is hybrid, containing two phases of which the first one utilizes some heuristics and patterns to narrow down the candidates, and the second one employs the vector space model to rank the ambiguous cases to choose the right candidate. The novelty is that the disambiguation process is incremental and includes several rounds that filter the candidates, by exploiting previously identified entities and extending the text by those entity attributes every time they are successfully resolved in a round. We test the performance of the proposed method in disambiguation of names of people, locations and organizations in texts of the news domain. The experiment results show that our approach achieves high accuracy and can be used to construct a robust named entity disambiguation system.","Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Some researchers ( Cucerzan , 2007 ; #AUTHOR_TAG ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']"
CC57,D09-1056,The role of named entities in web people search,the semeval2007 weps evaluation establishing a benchmark for the web people search task,"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']",related work,"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.","It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG ) .","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998).', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG ) .']",0,"['It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( Artiles et al. , 2005 ; #AUTHOR_TAG ) .']"
CC58,D09-1056,The role of named entities in web people search,multidocument statistical fact extraction and fusion,['Gideon S Mann'],experiments,"This dissertation presents original techniques for statistical fact extraction and fusion from multiple documents. Fact extraction, or relationship extraction, is a process where natural language text is scanned to find instances of a predetermined class of facts (e.g. birthday(x,y)). A framework for training statistical fact extractors from example is used wherein a set of examples and a target model are used to annotate an automatically collected corpus. This annotation is then used to provide training data for classifiers (Phrase Conditional Likelihood and Native Bayes) or sequence models (Conditional Random Fields).  Fact extractors are used in two information retrieval tasks. In question answering the set of candidate answers is narrowed using fine-grained proper noun ontological facts (is-a(X, Y)) extracted from a corpus by rote classifiers leading to higher performance. Extracted facts are also used for name-referent disambiguation, or cross-document coreference, where one personal name may refer to multiple potential people in the world. The distinguishing biographic facts for each person, such as birthday(x,y) and occupation (x,y), are automatically extracted from plain text and these biographic facts are used along with other statistical methods to distinguish between mentions of each of the referents.  This dissertation presents novel techniques for fusion which integrate facts extracted from multiple sources. For the task of biographic fact extraction, fusion of factual information extracted from multiple documents improves the precision of the resulting information. Further improvements result from cascaded fact extraction, where certain facts are extracted and fused and then these facts are used to extract additional information. The technique of cascaded fact extraction and fusion is also applied to time-bounded facts, where a cascade of fact extractors produce a timeline of corporate management succession.  Collectively, this research demonstrates the utility of multi-document fact extraction and fusion. It shows that facts can serve as a building-block for deeper text processing such as finding coreferent names in a series of documents, finding the answers to questions, and constructing a timeline for time-variable facts. The key aspects to the process are training with minimal supervision, high-performance statistical fact extraction, fusion across multiple sources of information, and cascaded extraction.","2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .","['2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']",5,"['2The WePS-1 corpus includes data from the Web03 testbed ( #AUTHOR_TAG ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .']"
CC59,D09-1056,The role of named entities in web people search,crossdocument coreference on a large scale corpus,"['Chung Heong Gooi', 'James Allan']",related work,,"Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( Bagga and Baldwin , 1998 ; #AUTHOR_TAG ) .']"
CC60,D09-1056,The role of named entities in web people search,a testbed for people searching strategies in the www,"['Javier Artiles', 'Julio Gonzalo', 'Felisa Verdejo']",related work,This paper describes the creation of a testbed to evaluate people searching strategies on the World-Wide-Web. This task involves resolving person names' ambiguity and locating relevant information characterising every individual under the same name.,"It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 ) .","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998).', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 ) .']",0,"['Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task Web People Search on its own ( #AUTHOR_TAG ; Artiles et al. , 2007 ) .']"
CC61,D09-1056,The role of named entities in web people search,cucomsem exploring rich features for unsupervised web personal name disambiguation,"['Ying Chen', 'James H Martin']",related work,,"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( #AUTHOR_TAG ; Popescu and Magnini , 2007 ) - .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']"
CC62,D09-1056,The role of named entities in web people search,disambiguation algorithm for people search on the web in,"['Dmitri V Kalashnikov', 'Stella Chen', 'Rabia Nuray', 'Sharad Mehrotra', 'Naveen Ashish']",related work,,"Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ) .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( #AUTHOR_TAG ) .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'These approaches are yet to be applied to the specific task of grouping search results.']"
CC63,D09-1056,The role of named entities in web people search,searching for people on web search engines,"['Amanda Spink', 'Bernard Jansen', 'Jan Pedersen']",introduction,"The Web is a communication and information technology that is often used for the distribution and retrieval of personal information. Many people and organizations mount Web sites containing large amounts of information on individuals, particularly about celebrities. However, limited studies have examined how people search for information on other people, using personal names, via Web search engines. Explores the nature of personal name searching on Web search engines. The specific research questions addressed in the study are: ""Do personal names form a major part of queries to Web search engines?""; ""What are the characteristics of personal name Web searching?""; and ""How effective is personal name Web searching?"". Random samples of queries from two Web search engines were analyzed. The findings show that: personal name searching is a common but not a major part of Web searching with few people seeking information on celebrities via Web search engines; few personal name queries include double quotations or additional identifying terms; and name searches on Alta Vista included more advanced search features relative to those on AlltheWeb.com. Discusses the implications of the findings for Web searching and search engines, and further research.",A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ) .,"['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ) .', 'According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (Artiles et al., 2005).', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']",0,['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( #AUTHOR_TAG ) .']
CC64,D09-1056,The role of named entities in web people search,experiments on semanticbased clustering for crossdocument coreference,['Horacio Saggion'],related work,,#AUTHOR_TAG compared the performace of NEs versus BoW features .,"['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research, NEs (person, location and organisations) are extracted from the text and used as a source of evidence to calculate the similarity between documents -see for instance (Blume, 2005; Chen and Martin, 2007; Popescu and Magnini, 2007; Kalashnikov et al., 2007)- .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of NEs versus BoW features .', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', '#AUTHOR_TAG compared the performace of NEs versus BoW features .', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']"
CC65,D09-1056,The role of named entities in web people search,entitybased crossdocument coreferencing using the vector space model,"['Amit Bagga', 'Breck Baldwin']",related work,,"Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name ( #AUTHOR_TAG ; Gooi and Allan , 2004 ) .']"
CC66,D09-1056,The role of named entities in web people search,extended named entity ontology with attribute information,['Satoshi Sekine'],experiments,"Named Entities (NE) are regarded as an important type of semantic knowledge in many natural language processing (NLP) applications. Originally, a limited number of NE categories were proposed. In MUC, it was 7 categories - people, organization, location, time, date, money and percentage expressions. However, it was noticed that such a limited number of NE categories is too small for many applications. The author has proposed Extended Named Entity (ENE), which has about 200 categories (Sekine and Nobata 04). During the development of ENE, we noticed that many ENE categories have specific attributes, and those provide very important information for the entities. For example, rivers have attributes like source location, outflow, and length. Some such information is essential to knowing about the river, while the name is only a label which can be used to refer to the river. Also, such attributes are important information for many NLP applications. In this paper, we report on the design of a set of attributes for ENE categories. We used a bottom up approach to creating the knowledge using a Japanese encyclopedia, which contains abundant descriptions of ENE instances.",It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG ) .,"['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG ) .', 'Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.']",5,"['OAK 7 is a rule based English analyser that includes many functionalities (POS tagger, stemmer, chunker, Named Entity (NE) tagger, dependency analyser, parser, etc).', 'It provides a fine grained NE recognition covering 100 different NE types ( #AUTHOR_TAG ) .', 'Given the sparseness of most of these fine-grained NE types, we have merged them in coarser groups: event, facility, location, person, organisation, product, periodx, timex and numex.']"
CC67,D09-1056,The role of named entities in web people search,a testbed for people searching strategies in the www,"['Javier Artiles', 'Julio Gonzalo', 'Felisa Verdejo']",introduction,This paper describes the creation of a testbed to evaluate people searching strategies on the World-Wide-Web. This task involves resolving person names' ambiguity and locating relevant information characterising every individual under the same name.,"According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG ) .","['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al., 2004).', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']",0,"['A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al., 2004).', 'According to the data available from 1990 U.S. Census Bureau , only 90,000 different names are shared by 100 million people ( #AUTHOR_TAG ) .', 'As the amount of information in the WWW grows, more of these people are mentioned in different web pages.', 'Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned.']"
CC68,D09-1056,The role of named entities in web people search,large scale named entity disambiguation based on wikipedia data,['Silviu Cucerzan'],related work,"This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.","Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless, the full document text is present in most systems, sometimes as the only feature (Sugiyama and Okumura, 2007) and sometimes in combination with otherssee for instance (Chen and Martin, 2007;Popescu and Magnini, 2007)-.', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Some researchers ( #AUTHOR_TAG ; Nguyen and Cao , 2008 ) have explored the use of Wikipedia information to improve the disambiguation process .', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.']"
CC69,D09-1056,The role of named entities in web people search,the semeval2007 weps evaluation establishing a benchmark for the web people search task,"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']",introduction,"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.","The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it .","['This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in.', 'The user might refine the original query with additional terms, but this risks excluding relevant documents in the process.', 'In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominate the ranking of search results, complicating the task of finding information about other people sharing her name.', 'The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it .']",0,"['The Web People Search task , as defined in the first WePS evaluation campaign ( #AUTHOR_TAG ) , consists of grouping search results for a given name according to the different people that share it .']"
CC70,D09-1056,The role of named entities in web people search,the semeval2007 weps evaluation establishing a benchmark for the web people search task,"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']",related work,"This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.","We have used the testbeds from WePS-1 ( #AUTHOR_TAG , 2007)2 and WePS-2 (Artiles et al., 2009) evaluation campaigns 3.","['We have used the testbeds from WePS-1 ( #AUTHOR_TAG , 2007)2 and WePS-2 (Artiles et al., 2009) evaluation campaigns 3.']",5,"['We have used the testbeds from WePS-1 ( #AUTHOR_TAG , 2007)2 and WePS-2 (Artiles et al., 2009) evaluation campaigns 3.']"
CC71,D09-1056,The role of named entities in web people search,irstbp web people search using name entities,"['Octavian Popescu', 'Bernardo Magnini']",related work,,"In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .","['The most used feature for the Web People Search task, however, are NEs.', 'Ravin (1999) introduced a rule-based approach that tackles both variation and ambiguity analysing the structure of names.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .', 'For instance, Blume (2005) uses NEs coocurring with the ambiguous mentions of a name as a key feature for the disambiguation process.', 'Saggion ( 2008) compared the performace of NEs versus BoW features.', 'In his experiments a only a representation based on Organisation NEs outperformed the word based approach.', 'Furthermore, this result is highly dependent on the choice of metric weighting (NEs achieve high precision at the cost of a low recall and viceversa for BoW).']",0,"['The most used feature for the Web People Search task, however, are NEs.', 'In most recent research , NEs ( person , location and organisations ) are extracted from the text and used as a source of evidence to calculate the similarity between documents - see for instance ( Blume , 2005 ; Chen and Martin , 2007 ; #AUTHOR_TAG ; Kalashnikov et al. , 2007 ) .']"
CC72,D09-1056,The role of named entities in web people search,entitybased crossdocument coreferencing using the vector space model,"['Amit Bagga', 'Breck Baldwin']",related work,,"The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .","['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .', 'Most of early research work on person name ambiguity focuses on the CDC problem or uses methods found in the WSD literature.', 'It is only recently that the web name ambiguity has been approached as a separate problem and defined as an NLP task -Web People Search -on its own (Artiles et al., 2005;Artiles et al., 2007).']",0,"['The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks : Word Sense Disambiguation ( WSD ) ( Agirre and Edmonds , 2006 ) and Cross-document Coreference ( CDC ) ( #AUTHOR_TAG ) .']"
CC73,D09-1056,The role of named entities in web people search,weps 2 evaluation campaign overview of the web people search clustering task,"['Javier Artiles', 'Julio Gonzalo', 'Satoshi Sekine']",related work,"The second WePS (Web People Search) Evaluation cam-paign took place in 2008-2009 with the participation of 19 re-search groups from Europe, Asia and North America. Given the output of a Web Search Engine for a (usually ambiguous) person name as query, two tasks were addressed: a clustering task, which consists of grouping together web pages referring to the same person, and an extraction task, which consists of extracting salient attributes for each of the persons shar-ing the same name. This paper presents the definition, re-sources, methodology and evaluation metrics, participation and comparative results for the clustering task","In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .","['Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation.', 'This makes NEs the second most common type of feature; only the BoW feature was more popular.', 'Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc.', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .', 'Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.', 'But the WePS campaigns did provide a useful, standardised resource to perform the type of studies that were not possible before.', 'In the next Section we describe this dataset and how it has been adapted for our purposes.']",0,"['Among the 16 teams that submitted results for the first WePS campaign, 10 of them 1 used NEs in their document representation.', 'Other features used by the systems include noun phrases (Chen and Martin, 2007), word n-grams (Popescu and Magnini, 2007), emails and URLs (del Valle-Agudo et al., 2007), etc.', 'In 2009 , the second WePS campaign showed similar trends regarding the use of NE features ( #AUTHOR_TAG ) .', 'Due to the complexity of systems, the results of the WePS evaluation do not provide a direct answer regarding the advantages of using NEs over other computationally lighter features such as BoW or word n-grams.']"
CC74,D09-1056,The role of named entities in web people search,titpi web people search task using semisupervised clustering approach,"['Kazunari Sugiyama', 'Manabu Okumura']",related work,,"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( #AUTHOR_TAG ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; Popescu and Magnini , 2007 ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'These approaches are yet to be applied to the specific task of grouping search results.']"
CC75,D09-1056,The role of named entities in web people search,irstbp web people search using name entities,"['Octavian Popescu', 'Bernardo Magnini']",related work,,"Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .","['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Withindocument coreference resolution has been applied to produce summaries of text surrounding occurrences of the name (Bagga and Baldwin, 1998;Gooi and Allan, 2004).', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', 'Other representations use the link structure (Malin, 2005) or generate graph representations of the extracted features (Kalashnikov et al., 2007).', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'The obvious limitation of this approach is that only celebrities and historical figures can be identified in this way.', 'These approaches are yet to be applied to the specific task of grouping search results.']",0,"['Many different features have been used to represent documents where an ambiguous name is mentioned.', 'The most basic is a Bag of Words (BoW) representation of the document text.', 'Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( Chen and Martin , 2007 ; #AUTHOR_TAG ) - .', 'Some researchers (Cucerzan, 2007;Nguyen and Cao, 2008) have explored the use of Wikipedia information to improve the disambiguation process.', 'Wikipedia provides candidate entities that are linked to specific mentions in a text.', 'These approaches are yet to be applied to the specific task of grouping search results.']"
CC76,D09-1067,Improving verb clustering with automatically acquired selectional preferences,discriminative learning of selectional preference from unlabeled text,"['Shane Bergsma', 'Dekang Lin', 'Randy Goebel']",conclusion,"We present a discriminative method for learning selectional preferences from unlabeled text. Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations. We train a Support Vector Machine classifier to distinguish the positive from the negative instances. We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances. The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments. Compared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and resolves 37 % more pronouns correctly in a pronoun resolution experiment.","Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ) .","['In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ) .', 'The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.']",3,"['Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; #AUTHOR_TAG ) .']"
CC77,D09-1067,Improving verb clustering with automatically acquired selectional preferences,a simple similaritybased model for selectional preferences,['Katrin Erk'],conclusion,,"Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .","['In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.', 'Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.', 'Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .', 'The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.']",3,"['Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( #AUTHOR_TAG ; Bergsma et al. , 2008 ) .']"
CC78,D09-1087,Self-training PCFG grammars with latent annotations across languages,forest reranking discriminative parsing with nonlocal features,['Liang Huang'],conclusion,"Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.","Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .', 'Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case.']",3,"['Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; #AUTHOR_TAG ) for self training .']"
CC79,D09-1087,Self-training PCFG grammars with latent annotations across languages,coarsetofine nbest parsing and maxent discriminative reranking,"['Eugene Charniak', 'Mark Johnson']",conclusion,,"Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training .","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training .', 'Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008), although training would be much slower compared to using generative models, as in our case.']",3,"['Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( #AUTHOR_TAG ; Huang , 2008 ) for self training .']"
CC80,D09-1087,Self-training PCFG grammars with latent annotations across languages,sparse multiscale grammars for discriminative latent variable parsing,"['Slav Petrov', 'Dan Klein']",conclusion,"We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars.","Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case .","['We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting.', 'Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005;Huang, 2008) for self training.', 'Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case .']",3,"['Self-training should also benefit other discriminatively trained parsers with latent annotations ( #AUTHOR_TAG ) , although training would be much slower compared to using generative models , as in our case .']"
CC81,D09-1143,"Convolution kernels on constituent, dependency and sequential structures for relation extraction",a semantic kernel for predicate argument classification,"['Alessandro Moschitti', 'Cosmin Bejan']",conclusion,"Automatically deriving semantic structures from text is a challenging task for machine learning. The flat feature representations, usually used in learning models, can only partially describe structured data. This makes difficult the processing of the semantic information that is embedded into parse-trees. In this paper a new kernel for automatic classification of predicate arguments has been designed and experimented. It is based on subparse-trees annotated with predicate argument information from PropBank corpus. This kernel, exploiting the convolution properties of the parse-tree kernel, enables us to learn which syntactic structures can be associated with the arguments defined in PropBank. Support Vector Machines (SVMs) using such a kernel classify arguments with a better accuracy than SVMs based on linear kernel.","Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']",3,"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; #AUTHOR_TAG ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"
CC82,D09-1143,"Convolution kernels on constituent, dependency and sequential structures for relation extraction",semantic role labeling via framenet verbnet and propbank,"['Ana-Maria Giuglea', 'Alessandro Moschitti']",conclusion,"This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources: FrameNet, VerbNet and PropBank. The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs. We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes. The PropBank corpus, which is tightly connected to the VerbNet lexicon, is used to increase the verb coverage and also to test the effectiveness of our approach. The results indicate that our model is an interesting step towards the design of more robust semantic parsers.","Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']",3,"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; #AUTHOR_TAG ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"
CC83,D09-1143,"Convolution kernels on constituent, dependency and sequential structures for relation extraction",tree kernels for semantic role labeling,"['Alessandro Moschitti', 'Daniele Pighin', 'Roberto Basili']",conclusion,"The availability of large scale data sets of manually annotated predicate-argument structures has recently favored the use of machine learning approaches to the design of automated semantic role labeling (SRL) systems. The main research in this area relates to the design choices for feature representation and for effective decompositions of the task in different learning models. Regarding the former choice, structural properties of full syntactic parses are largely employed as they represent ways to encode different principles suggested by the linking theory between syntax and semantics. The latter choice relates to several learning schemes over global views of the parses. For example, re-ranking stages operating over alternative predicate-argument sequences of the same sentence have shown to be very effective. In this article, we propose several kernel functions to model parse tree properties in kernel-based machines, for example, perceptrons or support vector machines. In particular, we define different kinds of tree kernels as general approaches to feature engineering in SRL. Moreover, we extensively experiment with such kernels to investigate their contribution to individual stages of an SRL architecture both in isolation and in combination with other traditional manually coded features. The results for boundary recognition, classification, and re-ranking stages provide systematic evidence about the significant impact of tree kernels on the overall accuracy, especially when the amount of training data is small. As a conclusive result, tree kernels allow for a general and easily portable feature engineering method which is applicable to a large family of natural language processing tasks.","Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG ) .","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG ) .']",3,"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; #AUTHOR_TAG ) .']"
CC84,D09-1143,"Convolution kernels on constituent, dependency and sequential structures for relation extraction",effective use of wordnet semantics via kernelbased learning,"['Roberto Basili', 'Marco Cammisa', 'Alessandro Moschitti']",conclusion,"Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.    In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available.","Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .","['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']",3,"['Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( #AUTHOR_TAGa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .']"
CC85,D09-1160,Polynomial to linear,small statistical models by random feature mixing,"['Kuzman Ganchev', 'Mark Dredze']",conclusion,The application of statistical NLP systems to resource constrained devices is limited by the need to maintain parameters for a large number of features and an alphabet mapping features to parameters. We introduce random feature mixing to eliminate alphabet storage and reduce the number of parameters without severely impacting model performance.,"When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .","['We plan to apply our method to wider range of classifiers used in various NLP tasks.', 'To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs.', 'When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .']",3,"['When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( #AUTHOR_TAG ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .']"
CC86,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,ordering phrases with function words,"['Hendra Setiawan', 'Min-Yen Kan', 'Haizhou Li']",,"This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios.","The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .","['The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']",2,"['The reordering models we describe follow our previous work using function word models for translation ( #AUTHOR_TAG ; Setiawan et al. , 2009 ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.']"
CC87,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,improved word alignment with statistics and linguistic heuristics,['Ulf Hermjakob'],related work,"We present a method to align words in a bitext that combines elements of a tra-ditional statistical approach with linguis-tic knowledge. We demonstrate this ap-proach for Arabic-English, using an align-ment lexicon produced by a statistical word aligner, as well as linguistic re-sources ranging from an English parser to heuristic alignment rules for function words. These linguistic heuristics have been generalized from a development cor-pus of 100 parallel sentences. Our aligner, UALIGN, outperforms both the commonly used GIZA++ aligner and the state-of-the-art LEAF aligner on F-measure and pro-duces superior scores in end-to-end sta-tistical machine translation, +1.3 BLEU points over GIZA++, and +0.7 over LEAF.","With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG ) .","['Our reordering model is closely related to the model proposed by Zhang and Gildea (2005;2007a), with respect to conditioning the reordering predictions on lexical items.', 'These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words.', 'With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG ) .', 'However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.']",1,"['With respect to the focus on function words , our reordering model is closely related to the UALIGN system ( #AUTHOR_TAG ) .']"
CC88,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,ordering phrases with function words,"['Hendra Setiawan', 'Min-Yen Kan', 'Haizhou Li']",,"This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words' arguments and improving translation quality in both perfect and noisy word alignment scenarios.","To model o ( Li , S â T ) , o ( Ri , S â T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG .","['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG .', 'Formally, this model takes the form of probability distribution P ori (o(L i,S→T ), o(R i,S→T )|Y i,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases).', 'In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG).', 'The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro-This heuristic is commonly used in learning phrase pairs from parallel text.', 'The maximality ensures the uniqueness of L and R.']",5,"['To model o ( Li , S â\x86\x92 T ) , o ( Ri , S â\x86\x92 T ) , i.e. the reordering of the neighboring phrases of a function word , we employ the orientation model introduced by #AUTHOR_TAG .']"
CC89,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,topological ordering of function words in hierarchical phrasebased translation,"['Hendra Setiawan', 'Min Yen Kan', 'Haizhou Li', 'Philip Resnik']",,"Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance.","The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .","['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .', 'The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them.', 'To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases.', 'This section provides a high level overview of our reordering model, which attempts to leverage this information.']",2,"['The reordering models we describe follow our previous work using function word models for translation ( Setiawan et al. , 2007 ; #AUTHOR_TAG ) .']"
CC90,D10-1052,Does Destination Image and Perceived Destination Quality Influence Tourist Satisfaction and Word of Mouth of Culinary Tourism,topological ordering of function words in hierarchical phrasebased translation,"['Hendra Setiawan', 'Min Yen Kan', 'Haizhou Li', 'Philip Resnik']",,"Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterize both local and long-distance reorderings, but they also make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotation-driven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance.","To model d ( FWi â 1 , S â T ) , d ( FWi +1 , S â T ) , i.e. whether Li , S â T and Ri , S â T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .","['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .', 'Taking d(F W i−1,S→T ) as a case in point, this model takes the form']",5,"['To model d ( FWi â\x88\x92 1 , S â\x86\x92 T ) , d ( FWi +1 , S â\x86\x92 T ) , i.e. whether Li , S â\x86\x92 T and Ri , S â\x86\x92 T extend beyond the neighboring function word phrase pairs , we utilize the pairwise dominance model of #AUTHOR_TAG .']"
CC91,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",related work,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","In our previous work ( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .","['In our previous work ( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .', 'We have collected a dataset of 875 instances.', 'Each instance consists of a conversation segment and a hypothesis (as described in Section 1).', 'The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions.', 'We developed an approach that is motivated by previous work on textual entailment.', 'We use clauses in the logic-based approaches as the underlying representation of our system.', 'Based on this representation, we apply a two stage entailment process similar to MacCartney et al. (2006) developed for textual entailment: an alignment stage followed by an entailment stage.']",2,"['In our previous work ( #AUTHOR_TAG ) , we started an initial investigation on conversation entailment .']"
CC92,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",method,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .","['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']",2,"['Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( #AUTHOR_TAG ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .']"
CC93,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",experiments,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","Note that in our original work ( #AUTHOR_TAG ) , only development data were used to show some initial observations.","['Note that in our original work ( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']",1,"['Note that in our original work ( #AUTHOR_TAG ) , only development data were used to show some initial observations.', 'Here we trained our mod- els on the development data and results shown are from the testing data.']"
CC94,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",method,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.",This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG ) .,"['This string representation of paths is used to capture both the subject consistency and the object consistency.', 'Since they are non-numerical features, and the variability of their values can be extremely large, so we applied an instance-based classification model (e.g., k-nearest neighbor) to determine alignments between verb terms.', 'We measure the distance between two path features by their minimal string edit distance, and then simply use the Euclidean distance to measure the closeness between any two verbs.', 'Again this model is trained from our development data described in Zhang and Chai (2009).', 'Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG ) .']",5,"['We measure the distance between two path features by their minimal string edit distance, and then simply use the Euclidean distance to measure the closeness between any two verbs.', 'Figure 3 shows an example of alignment between the conversation terms and hypothesis terms in Example 2. Note that in this figure the alignment between x 5 = suggests from the hypothesis and u 4 = opinion from the conversation segment is a pseudo alignment, which directly maps a verb term in the hypothesis to an utterance term represented by its dialogue act.', 'This alignment is obtained by following the same set of rules learned from the development dataset as in ( #AUTHOR_TAG ) .']"
CC95,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",introduction,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .","['To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .', 'The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H.', 'For instance, as in Example 1, the first hypothesis can be entailed from the conversation segment while the second hypothesis cannot.', 'While our previous work has provided some interesting preliminary observations, it mostly focused on data collection and initial experiments and analysis using a small set of development data.', 'It is not clear whether the previous results are generally applicable, how different components in the entailment framework interact with each other, and how different representations may influence the entailment outcome.']",2,"['To address this limitation , our previous work ( #AUTHOR_TAG ) has initiated an investigation on the problem of conversation entailment .']"
CC96,D10-1074,What do we know about conversation participants,what do we know about conversation participants experiments on conversation entailment,"['Chen Zhang', 'Joyce Chai']",,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important. Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants. This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.","In our previous work ( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â§ ... â§ dm , and a hypothesis H represented by another set of clauses H = h1 â§ ... â§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .","['In our previous work ( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .', 'This is based on a simple as- sumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses.']",2,"['In our previous work ( #AUTHOR_TAG ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 â\x88§ ... â\x88§ dm , and a hypothesis H represented by another set of clauses H = h1 â\x88§ ... â\x88§ hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .']"
CC97,D10-1100,Automatic Detection of Micro-Arousals,convolution kernels on constituent dependency and sequential structures for relation extraction,"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']",,"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art.","We use the structures previously used by #AUTHOR_TAG , and propose one new structure .","['Linear learning machines are one of the most popular machines used for classification problems.', 'The objective of a typical classification problem is to learn a function that separates the data into different classes.', 'The data is usually in the form of features extracted from abstract objects like strings, trees, etc.', 'A drawback of learning by using complex functions is that complex functions do not generalize well and thus tend to over-fit.', 'The research community therefore prefers linear classifiers over other complex classifiers.', 'But more often than not, the data is not linearly separable.', 'It can be made linearly separable by increasing the dimensionality of data but then learning suffers from the curse of dimensionality and classification becomes computationally intractable.', 'This is where kernels come to the rescue.', 'The well-known kernel trick aids us in finding similarity between feature vectors in a high dimensional space without having to write down the expanded feature space.', 'The essence of kernel methods is that they compare two feature vectors in high dimensional space by using a dot product that is a function of the dot product of feature vectors in the lower dimensional space.', 'Moreover, Convolution Kernels (first introduced by Haussler (1999)) can be used to compare abstract objects instead of feature vectors.', 'This is because these kernels involve a recursive calculation over the ""parts"" of a discrete structure.', 'This calculation is usually made computationally efficient using Dynamic Programming techniques.', 'Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data).', 'Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task.', 'Now we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by #AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).', 'For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).', 'We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs.', 'Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities.', 'Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities.', 'In Figure 1, since the target entities are at the leftmost and rightmost branch of the depen- 3 We omitted SK6, which is the worst performing sequence kernel in (Nguyen et al., 2009).', 'Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corresponding parent in DW, we get a GR tree.', 'For example, in Figure 1, replacing Toujan Faisal by nsubj, 54 by appos, she by nsubjpass and so on.', 'Grammatical Relation Word (GRW) tree: We get this tree by adding the grammatical relations as separate nodes between a node and its parent.', 'For example, in Figure 1, adding nsubj as a node between T1-Individual and Toujan Faisal, appos as a node between 54 and Toujan Faisal, and so on.', 'Sequence Kernel of words (SK1): This is the sequence of words between the two entities, including their tags.', 'For our example in Figure 1, it would be T1-Individual Toujan Faisal 54 said she was informed of the refusal by an T2-Group Interior Ministry committee.', 'Sequence in GRW tree (SqGRW): This is the new structure that we introduce which, to the best of our knowledge, has not been used before for similar tasks.', 'It is the sequence of nodes from one target to the other in the GRW tree.', 'For example, in Figure 1, this would be Toujan Faisal nsubj T1-Individual said ccomp informed prep by T2-Group pobj committee.']",5,"['The data is usually in the form of features extracted from abstract objects like strings, trees, etc.', 'The research community therefore prefers linear classifiers over other complex classifiers.', 'This is where kernels come to the rescue.', 'Moreover, Convolution Kernels (first introduced by Haussler (1999)) can be used to compare abstract objects instead of feature vectors.', 'This is because these kernels involve a recursive calculation over the ""parts"" of a discrete structure.', 'Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data).', 'Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task.', 'Now we present the ""discrete"" structures followed by the kernel we used.', 'We use the structures previously used by #AUTHOR_TAG , and propose one new structure .', 'Although we experimented with all of their structures, 3 here we only present the ones that perform best for our classification task.', 'All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).', 'For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).', 'We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs.', 'Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities.', 'Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities.', 'In Figure 1, since the target entities are at the leftmost and rightmost branch of the depen- 3 We omitted SK6, which is the worst performing sequence kernel in (Nguyen et al., 2009).', 'Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corresponding parent in DW, we get a GR tree.', 'Grammatical Relation Word (GRW) tree: We get this tree by adding the grammatical relations as separate nodes between a node and its parent.', 'Sequence Kernel of words (SK1): This is the sequence of words between the two entities, including their tags.']"
CC98,D10-1100,Automatic Detection of Micro-Arousals,convolution kernels on constituent dependency and sequential structures for relation extraction,"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']",conclusion,"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art.",This revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .,"['In this paper, we have introduced the novel tasks of social event detection and classification.', 'We show that data sampling techniques play a crucial role for the task of relation detection.', 'Through oversampling we achieve an increase in F1-measure of 22.2% absolute over a baseline system.', 'Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information.', 'Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best.', 'This revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .', 'We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks.']",1,"['Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information.', 'Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best.', 'This revalidates the observation of #AUTHOR_TAG that phrase structure representations and dependency representations add complimentary value to the learning task .']"
CC99,D10-1100,Automatic Detection of Micro-Arousals,convolution kernels on constituent dependency and sequential structures for relation extraction,"['Truc-Vien T Nguyen', 'Alessandro Moschitti', 'Giuseppe Riccardi']",experiments,"This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas se-mantics concerns to entity types and lex-ical sequences. We investigate the effec-tiveness of such representations in the au-tomated relation extraction from text. We process the above data by means of Sup-port Vector Machines along with the syn-tactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combina-tion of the above kernels achieves high ef-fectiveness and significantly improves the current state-of-the-art.","Here , the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where GR performed much worse than PET for ACE data .","['Social event detection is the task of detecting if any social event exists between a pair of entities in a sentence.', 'We formulate the problem as a binary classification task by labeling an example that does not have a social event as class -1 and by labeling an example that either has an INR or COG social event as class 1.', 'First we present results for our baseline system.', 'Our baseline system uses various structures and their combinations but without any data balancing.', '1 presents results for our baseline system.', 'Grammatical relation tree structure (GR), a structure derived from dependency tree by replacing the words by their grammatical relations achieves the best precision.', 'This is probably because the clas-sifier learns that if both the arguments of a predicate contain target entities then it is a social event.', 'Among kernels for single structures, the path enclosed tree for PSTs (PET) achieves the best recall.', 'Furthermore, a combination of structures derived from PSTs and DTs performs best.', 'The sequence kernels, perform much worse than SqGRW (F1-measure as low as 0.45).', 'Since it is the same case for all subsequent experiments, we omit them from the discussion.', 'We now turn to experiments involving sampling.', 'Table 2 presents results for under-sampling, i.e. randomly removing examples belonging to the negative class until its size matches the positive class.', 'Table 2 shows a large gain in F1-measure of 9.72% absolute over the baseline system (Table 1).', 'We found that worst performing kernel with under-sampling is SK1 with an F1-measure of 39.2% which is better than the best performance without undersampling.', 'These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples).', 'This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy.', 'absolute.', 'As in the baseline system, a combination of structures performs best.', 'As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall.', 'Here , the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where GR performed much worse than PET for ACE data .', 'This exemplifies the difference in the nature of our event annotations from that of ACE relations.', 'Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.', 'This means that implicit feature space is much sparser and thus not the best representation.', '4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are ""close"" to the original examples.', 'This method achieves a gain 16.78% over the baseline system.', 'We expected this system to perform better than the over-sampled system but it does not.', 'This suggests that our over-sampled system is not over-fitting; a concern with using oversampling techniques.']",1,"['First we present results for our baseline system.', '1 presents results for our baseline system.', 'Grammatical relation tree structure (GR), a structure derived from dependency tree by replacing the words by their grammatical relations achieves the best precision.', 'This is probably because the clas-sifier learns that if both the arguments of a predicate contain target entities then it is a social event.', 'Among kernels for single structures, the path enclosed tree for PSTs (PET) achieves the best recall.', 'The sequence kernels, perform much worse than SqGRW (F1-measure as low as 0.45).', 'As in the baseline system, a combination of structures performs best.', 'As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall.', 'Here , the PET and GR kernel perform similar : this is different from the results of ( #AUTHOR_TAG ) where GR performed much worse than PET for ACE data .', 'Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.', 'This means that implicit feature space is much sparser and thus not the best representation.', '4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are ""close"" to the original examples.', 'This method achieves a gain 16.78% over the baseline system.']"
CC100,D10-1101,Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields,biographies bollywood boomboxes and blenders domain adaptation for sentiment classification,"['John Blitzer', 'Mark Dredze', 'Fernando Pereira']",experiments,"Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.","Our results also confirm the insights gained by #AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .","['Our results also confirm the insights gained by #AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .', 'Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.']",1,"['Our results also confirm the insights gained by #AUTHOR_TAG , who observed that in crossdomain polarity analysis adding more training data is not always beneficial .']"
CC101,D10-1101,Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields,instance weighting for domain adaptation in nlp,"['Jing Jiang', 'ChengXiang Zhai']",conclusion,"Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.","For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .","['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']",3,"['For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; #AUTHOR_TAG ) , perform in comparison to our approach .']"
CC102,D10-1101,Extracting Opinion Targets in a Single-and Cross-Domain Setting with Conditional Random Fields,biographies bollywood boomboxes and blenders domain adaptation for sentiment classification,"['John Blitzer', 'Mark Dredze', 'Fernando Pereira']",conclusion,"Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.","For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .","['In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting.', 'We have presented a comparative evaluation of our approach on datasets from four different domains.', 'In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets.', 'Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction.', 'Our CRF-based approach also yields promising results in the crossdomain setting.', 'The features we employ scale well across domains, given that the opinion target vocabularies are substantially different.', 'For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .', 'Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard.', 'We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences.', 'Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality.', 'It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.']",3,"['For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( #AUTHOR_TAG ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .']"
CC103,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,algorithms for deterministic incremental dependency parsing,['J Nivre'],experiments,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.",An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.,"['An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word iden- tity of the syntactic head of the top word on the stack (if available); dependency arc label iden- tities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunc- tions are included.']",5,['An implementation of the transition-based dependency parsing frame- work ( #AUTHOR_TAG ) using an arc-eager transi- tion strategy and are trained using the percep- tron algorithm as in Zhang and Clark (2008) with a beam size of 8. Beams with varying sizes can be used to produce k-best lists.']
CC104,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,structured output learning with indirect supervision,"['M Chang', 'D Goldwasser', 'D Roth', 'V Srikumar']",introduction,"We present a novel approach for structure prediction that addresses the difficulty of obtaining labeled structures for training. We observe that structured output problems often have a companion learning problem of determining whether a given input possesses a good structure. For example, the companion problem for the part-of-speech (POS) tagging task asks whether a given sequence of words has a corresponding sequence of POS tags that is ""legitimate"". While obtaining direct supervision for structures is difficult and expensive, it is often very easy to obtain indirect supervision from the companion binary decision problem.    In this paper, we develop a large margin framework that jointly learns from both direct and indirect forms of supervision. Our experiments exhibit the significant contribution of the easy-to-get indirect binary supervision on three important NLP structure learning problems.","This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ) .","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']",0,"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; #AUTHOR_TAG ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']"
CC105,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,kbest spanning tree parsing,['K Hall'],experiments,"This paper introduces a Maximum Entropy dependency parser based on an efficient kbest Maximum Spanning Tree (MST) algorithm. Although recent work suggests that the edge-factored constraints of the MST algorithm significantly inhibit parsing accuracy, we show that generating the 50-best parses according to an edge-factored model has an oracle performance well above the 1-best performance of the best dependency parsers. This motivates our parsing approach, which is based on reranking the kbest parses generated by an edge-factored model. Oracle parse accuracy results are presented for the edge-factored model and 1-best results for the reranker on eight languages (seven from CoNLL-X and English).","We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .","['• Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (McDonald et al., 2005).', 'We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .', 'The graphbased parser features used in the experiments in this paper are defined over a word, w i at position i; the head of this word w ρ(i) where ρ(i) provides the index of the head word; and partof-speech tags of these words t i .', 'We use the following set of features similar to McDonald et al. ( 2005):']",5,"['* Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (McDonald et al., 2005).', 'We use the non-projective k-best MST algorithm to generate k-best lists ( #AUTHOR_TAG ) , where k = 8 for the experiments in this paper .']"
CC106,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,uptraining for accurate deterministic question parsing,"['S Petrov', 'P C Chang', 'M Ringgaard', 'H Alshawi']",experiments,"It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.",#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .,"['Another application of the augmented-loss framework is to improve parser domain portability in the presence of partially labeled data.', 'Consider, for example, the case of questions.', '#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'Table 2   We consider the situation where it is possible to ask annotators a single question about the target domain that is relatively easy to answer.', 'The question should be posed so that the resulting answer produces a partially labeled dependency tree.', 'Root-F1 scores from Table 2 suggest that one simple question is ""what is the main verb of this sentence?""', 'for sentences that are questions.', 'In most cases this task is straight-forward and will result in a single dependency, that from the root to the main verb of the sentence.', 'We feel this is a realistic partial labeled training setting where it would be possible to quickly collect a significant amount of data.']",1,"['#AUTHOR_TAG observed that dependency parsers tend to do quite poorly when parsing questions due to their limited exposure to them in the news corpora from the PennTreebank .', 'for sentences that are questions.']"
CC107,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,using a dependency parser to improve smt for subjectobjectverb languages in,"['P Xu', 'J Kang', 'M Ringgaard', 'F Och']",experiments,,1Our rules are similar to those from #AUTHOR_TAG .,['1Our rules are similar to those from #AUTHOR_TAG .'],1,['1Our rules are similar to those from #AUTHOR_TAG .']
CC108,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,dependency treebased sentiment classification using crfs with hidden variables,"['T Nakagawa', 'K Inui', 'S Kurohashi']",introduction,"In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features.","This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( #AUTHOR_TAG ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( #AUTHOR_TAG ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( #AUTHOR_TAG ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .']"
CC109,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,online largemargin training of dependency parsers,"['R McDonald', 'K Crammer', 'F Pereira']",experiments,"We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.",An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG ) .,"['An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG ) .', 'We use the non-projective k-best MST algorithm to generate k-best lists (Hall, 2007), where k = 8 for the experiments in this paper.', 'The graph- based parser features used in the experiments in this paper are defined over a word, wi at po- sition i; the head of this word w_(i) where _(i) provides the index of the head word; and part- of-speech tags of these words ti.', 'We use the following set of features similar to McDonald et al. (2005):']",5,"['An implementation of graph- based parsing algorithms with an arc-factored parameterization ( #AUTHOR_TAG ) .', 'The graph- based parser features used in the experiments in this paper are defined over a word, wi at po- sition i; the head of this word w_(i) where _(i) provides the index of the head word; and part- of-speech tags of these words ti.', 'We use the following set of features similar to McDonald et al. (2005):']"
CC110,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,reranking and selftraining for parser adaptation,"['D McClosky', 'E Charniak', 'M Johnson']",related work,,"The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .","['A recent study by  also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system.', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']",1,"['A recent study by  also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training ( #AUTHOR_TAG ) , with the exception that the new parse data is targeted to produce accurate word reorderings .', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']"
CC111,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,meteor an automatic metric for mt evaluation with improved correlation with human judgments,"['S Banerjee', 'A Lavie']",experiments,"We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.","Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure .","['In our experiments we work with a set of English-Japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences.', 'We use a reordering score based on the reordering penalty from the METEOR scoring metric.', 'Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure .', 'Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates.']",4,"['Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( #AUTHOR_TAG ) and is simpler to measure .']"
CC112,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,using a dependency parser to improve smt for subjectobjectverb languages in,"['P Xu', 'J Kang', 'M Ringgaard', 'F Och']",introduction,,"This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( #AUTHOR_TAG ) , and many other tasks .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( #AUTHOR_TAG ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( Wang et al. , 2007 ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( #AUTHOR_TAG ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.']"
CC113,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,a tale of two parsers investigating and combining graphbased and transitionbased dependency parsing,"['Y Zhang', 'S Clark']",experiments,"Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beam-search. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.","• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.","['• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-ofspeech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunctions are included.']",5,"['• Transition-based: An implementation of the transition-based dependency parsing framework ( Nivre , 2008 ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in #AUTHOR_TAG with a beam size of 8 . Beams with varying sizes can be used to produce k-best lists.']"
CC114,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,domain adaptation with structural correspondence learning,"['J Blitzer', 'R McDonald', 'F Pereira']",introduction,"Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.","In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; #AUTHOR_TAG ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.']"
CC115,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,building a large annotated corpus of english the penn treebank computational linguistics,"['M Marcus', 'B Santorini', 'M A Marcinkiewicz']",experiments,,"In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG ) .","['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG ) .', 'We also make use of the Brown corpus, and the Question Treebank (QTB) (Judge et al., 2006']",5,"['In terms of treebank data , the primary training corpus is the Penn Wall Street Journal Treebank ( PTB ) ( #AUTHOR_TAG ) .']"
CC116,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,reranking and selftraining for parser adaptation,"['D McClosky', 'E Charniak', 'M Johnson']",introduction,,"In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; #AUTHOR_TAG ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .']"
CC117,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,guiding semisupervision with constraintdriven learning,"['M W Chang', 'L Ratinov', 'D Roth']",related work,,"The work that is most similar to ours is that of #AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .","['The work that is most similar to ours is that of #AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets).', 'For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints.', 'These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items.', 'The augmented-loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented-loss functions directly (rather than adding a set of examples to the training set).', 'Unlike the CODL approach, we do not perform complete optimization on each iteration over the unlabeled dataset; rather, we incorporate the updates in our online learning algorithm.', 'As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include Mann and Mc-Callum (2010) and Ganchev et al. (2010).', 'Again, these works are typically interested in using the extrinsic metric -or, in general, extrinsic information -to optimize the intrinsic metric in the absence of any labeled intrinsic data.', 'Our goal is to optimize both simultaneously.']",1,"['The work that is most similar to ours is that of #AUTHOR_TAG , who introduced the Constraint Driven Learning algorithm ( CODL ) .', 'As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include Mann and Mc-Callum (2010) and Ganchev et al. (2010).']"
CC118,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,an endtoend discriminative approach to machine translation,"['P Liang', 'A Bouchard-Ct', 'D Klein', 'B Taskar']",related work,,#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .,"['A recent study by  also investigates the task of training parsers to improve MT reordering.', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', '#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']",1,"['#AUTHOR_TAG presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system .', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.']"
CC119,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,training a parser for machine translation reordering,"['J Katz-Brown', 'S Petrov', 'R McDonald', 'D Talbot', 'F Och', 'H Ichikawa', 'M Seno', 'H Kazawa']",related work,"We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.",A recent study by #AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .,"['A recent study by #AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists.', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.', 'The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings.', 'Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning.', 'This allows us to give guarantees of convergence.', 'Furthermore, we also evaluate the method on alternate extrinsic loss functions.', 'Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system.', 'Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score.', 'In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score.', 'Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.']",1,"['A recent study by #AUTHOR_TAG also investigates the task of training parsers to improve MT reordering .', 'The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data.']"
CC120,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,unsupervised methods for determining object and relation synonyms on the web,"['A Yates', 'O Etzioni']",experiments,"The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of RESOLVER's probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic RESOLVER system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus.","Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .","['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .']",0,"['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'The arc-length score is the summed length of all those with correct head assignments (_(_i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for down- stream tasks, e.g., main verb dependencies for tasks like information extraction ( #AUTHOR_TAG ) and textual entailment ( Berant et al. , 2010 ) .']"
CC121,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,discriminative training methods for hidden markov models theory and experiments with perceptron algorithms,['M Collins'],introduction,"We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random elds (CRFs). The al-gorithms rely on Viterbi decoding of training examples, combined with sim-ple additive updates. We describe the-ory justifying the algorithms through a modication of the proof of conver-gence of the perceptron algorithm for classi cation problems. We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.","Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG , by inserting in loss-separability for normal separability .","['A training set D is loss-separable with margin γ > 0 if there exists a vector u with u = 1 such that for all y , y ∈ Y x and (x, y) ∈ D, if L(y , y) < L(y , y), then u•Φ(y )−u•Φ(y ) ≥ γ.', 'Furthermore, let R ≥ ||Φ(y) − Φ(y )||, for all y, y .', 'Assumption 1. Assume training set D is lossseparable with margin γ.', 'Theorem 1.', 'Given Assumption 1.', 'Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y) ∈ D with parameter vector θ when ∃ŷ j ∈ F k-best θ (x) wherê y j =ŷ 1 and L(ŷ j , y) < L(ŷ 1 , y).', 'If training is run indefinitely, then m ≤ R 2 γ 2 .', 'Proof.', 'Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG , by inserting in loss-separability for normal separability .']",0,"['A training set D is loss-separable with margin g > 0 if there exists a vector u with u = 1 such that for all y , y  Y x and (x, y)  D, if L(y , y) < L(y , y), then u*Ph(y )-u*Ph(y ) >= g.', 'Furthermore, let R >= ||Ph(y) - Ph(y )||, for all y, y .', 'Assumption 1. Assume training set D is lossseparable with margin g.', 'Theorem 1.', 'Given Assumption 1.', 'Let m be the number of mistakes made when training the perceptron (Algorithm 2) with inline ranker loss (Algorithm 3) on D, where a mistake occurs for (x, y)  D with parameter vector th when y j  F k-best th (x) where y j =y 1 and L(y j , y) < L(y 1 , y).', 'If training is run indefinitely, then m <= R 2 g 2 .', 'Identical to the standard perceptron proof , e.g. , #AUTHOR_TAG , by inserting in loss-separability for normal separability .']"
CC122,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,characterizing the errors of datadriven dependency parsing models,"['R McDonald', 'J Nivre']",experiments,"We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development.","Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).","['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).']",4,"['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( #AUTHOR_TAG ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010).']"
CC123,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,corpus variation and parser performance,['D Gildea'],introduction,"Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model.","In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( #AUTHOR_TAG ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; Petrov et al. , 2010 ) .']"
CC124,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,discriminative reranking for natural language parsing,['Michael Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation",One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ) .,"['One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.', 'The standard setting involves training the base parser and applying it to a development set (this is often done in a cross-validated jack-knife training framework).', 'The reranker can then be trained to optimize for the downstream or extrinsic objective.', 'While this will bias the reranker towards the target task, it is limited by the oracle performance of the original base parser.']",0,"['One obvious approach to this problem is to employ parser reranking ( #AUTHOR_TAG ) .', 'In such a setting, an auxiliary reranker is added in a pipeline following the parser.']"
CC125,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,global learning of focused entailment graphs,"['J Berant', 'I Dagan', 'J Goldberger']",experiments,"We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms.","Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG ) .","['The augmented-loss framework can be used to incorporate multiple treebank-based loss functions as well.', 'Labeled attachment score is used as our base model loss function.', 'In this set of experiments we consider adding an additional loss function which weights the lengths of correct and incorrect arcs, the average (labeled) arc-length score: __ _(_�,_)(i__) For each word of the sentence we compute the dis- tance between the word�s position i and the posi- tion of the words head _i.', 'The arc-length score is the summed length of all those with correct head assignments (_(_�i,_i) is 1 if the predicted head and the correct head match, 0 otherwise).', 'The score is normalized by the summed arc lengths for the sentence.', 'The labeled version of this score requires that the labels of the arc are also correct.', 'Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG ) .']",0,"['Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks like information extraction ( Yates and Etzioni , 2009 ) and textual entailment ( #AUTHOR_TAG ) .']"
CC126,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,guiding semisupervision with constraintdriven learning,"['M W Chang', 'L Ratinov', 'D Roth']",introduction,,"This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']",1,"['This includes work on generalized expectation ( Mann and McCallum , 2010 ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( #AUTHOR_TAG ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']"
CC127,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,algorithms for deterministic incremental dependency parsing,['J Nivre'],experiments,"Abstract Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.",An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .,"['An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .', 'Beams with varying sizes can be used to produce k-best lists.', 'The features used by all models are: the part-of- speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available).', 'All feature conjunctions are included.']",5,['An implementation of the transition-based dependency parsing framework ( #AUTHOR_TAG ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .']
CC128,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,generalized expectation criteria for semisupervised learning with weakly labeled data,"['G S Mann', 'A McCallum']",introduction,"In this paper, we present an overview of generalized expectation criteria (GE), a simple, robust, scalable method for semi-supervised training using weakly-labeled data. GE fits model parameters by favoring models that match certain expectation constraints, such as marginal label distributions, on the unlabeled data. This paper shows how to apply generalized expectation criteria to two classes of parametric models: maximum entropy models and conditional random fields. Experimental results demonstrate accuracy improvements over supervised training and a number of other state-of-the-art semi-supervised learning methods for these models.","This includes work on generalized expectation ( #AUTHOR_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .","['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( #AUTHOR_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics.', 'For our setting this would mean using weak application specific signals to improve dependency parsing.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']",0,"['There have been a number of efforts to exploit weak or external signals of quality to train better prediction models.', 'This includes work on generalized expectation ( #AUTHOR_TAG ) , posterior regularization ( Ganchev et al. , 2010 ) and constraint driven learning ( Chang et al. , 2007 ; Chang et al. , 2010 ) .', 'The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5.', 'Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wish to optimize, but also desire the benefit from using both data with annotated parse structures and data specific to the task at hand to guide parser training.']"
CC129,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,conllx shared task on multilingual dependency parsing,"['S Buchholz', 'E Marsi']",experiments,"Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?",For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .,"['In the next section, we present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one.', 'Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains.', 'We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score.', 'For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .']",5,['For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( #AUTHOR_TAG ) .']
CC130,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,uptraining for accurate deterministic question parsing,"['S Petrov', 'P C Chang', 'M Ringgaard', 'H Alshawi']",introduction,"It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.","In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks.', 'In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['In most cases , the accuracy of parsers degrades when run on out-of-domain data ( Gildea , 2001 ; McClosky et al. , 2006 ; Blitzer et al. , 2006 ; #AUTHOR_TAG ) .', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.']"
CC131,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,what is the jeopardy model a quasisynchronous grammar for qa,"['M Wang', 'N A Smith', 'T Mitamura']",introduction,"This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimentalresultsusing theTRECdataset are shown to significantly outperform strong state-of-the-art baselines.","This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .","['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']",0,"['The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks.', 'This includes work on question answering ( #AUTHOR_TAG ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .', 'In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001;McClosky et al., 2006;Blitzer et al., 2006;Petrov et al., 2010).', 'But these accuracies are measured with respect to gold-standard out-of-domain parse trees.', 'There are few tasks that actually depend on the complete parse tree.', 'Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model.', 'While this means that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training.', 'The goal being not necessarily to obtain better parse performance, but to exploit the structure induced from human labeled treebank data while targeting specific extrinsic metrics of quality, which can include task specific metrics or external weak constraints on the parse structure.']"
CC132,D11-1138,Training dependency parsers by jointly optimizing multiple objectives,a lightweight evaluation framework for machine translation reordering,"['D Talbot', 'H Kazawa', 'H Ichikawa', 'J Katz-Brown', 'M Seno', 'F Och']",experiments,Reordering is a major challenge for machine translation between distant languages. Recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality. Here we present a simple framework for evaluating word order independently of lexical choice by comparing the system's reordering of a source sentence to reference reordering data generated from manually word-aligned translations. When used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming end-to-end machine translation experiments. A novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion. We show that how the alignments are generated can significantly effect the robustness of the evaluation. We also outline some ways in which this framework has allowed our group to analyze reordering errors for English to Japanese machine translation.,criteria and data used in our experiments are based on the work of #AUTHOR_TAG .,['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .'],5,['criteria and data used in our experiments are based on the work of #AUTHOR_TAG .']
CC133,D12-1037,Discriminative Training for Log-Linear Based SMT,svmknn discriminative nearest neighbor classification for visual category recognition,"['Hao Zhang', 'Alexander C Berg', 'Michael Maire', 'Jitendra Malik']",introduction,"We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves the distance function on the collection of neighbors. Our method can be applied to large, multiclass data sets for which it outperforms nearest neighbor and support vector machines, and remains efficient when the problem becomes intractable for support vector machines. A wide variety of distance functions can be used and our experiments show state-of-the-art performance on a number of benchmark data sets for shape and texture classification (MNIST, USPS, CUReT) and object recognition (Caltech- 101). On Caltech-101 we achieved a correct classification rate of 59.05%(+-0.56%) at 15 training images per class, and 66.23%(+-0.48%) at 30 training images.","In this paper , inspired by KNN-SVM ( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .","['In this paper , inspired by KNN-SVM ( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', 'Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.', 'Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set.']",4,"['In this paper , inspired by KNN-SVM ( #AUTHOR_TAG ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .', 'Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.', 'Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.', 'To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set.']"
CC134,D12-1037,Discriminative Training for Log-Linear Based SMT,minimum error rate training in statistical machine translation,['Franz Josef Och'],introduction,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.","Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\u2212WbII2+ A \ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\u03b1W \u00b7 h(fj, e)] P\u03b1(e|fj; W) = (7) Ee'Ec; exp[\u03b1W \u00b7 h(fj, e')], where \u03b1 > 0 is a real number valued smoother.","['Due to the existence of L2 norm in objective function (5), the optimization algorithm MERT can not be applied for this question since the exact line search routine does not hold here.', ""Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW−WbII2+ A � j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[αW · h(fj, e)] Pα(e|fj; W) = (7) Ee'Ec; exp[αW · h(fj, e')], where α > 0 is a real number valued smoother."", 'One can see that, in the extreme case, for α —* oc, (6) converges to (5).']",4,"[""Motivated by (#AUTHOR_TAG, 2003; Smith and Eisner, 2006), we approximate the Error in (5) by the expected loss, and then derive the following function: x 2IIW\\u2212WbII2+ A \\ufffd j=1 Systems NIST02 NIST05 NIST06 NIST08 Moses 30.39 26.31 25.34 19.07 Moses hier 33.68 26.94 26.28 18.65 In-Hiero 31.24 27.07 26.32 19.03 Table 1: The performance comparison of the baseline InHiero VS Moses and Moses hier. with exp[\\u03b1W \\u00b7 h(fj, e)] P\\u03b1(e|fj; W) = (7) Ee'Ec; exp[\\u03b1W \\u00b7 h(fj, e')], where \\u03b1 > 0 is a real number valued smoother.""]"
CC135,D12-1037,Discriminative Training for Log-Linear Based SMT,discriminative training and maximum entropy models for statistical machine translation,"['Franz Josef Och', 'Hermann Ney']",introduction,"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.","#AUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :","['#AUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']",0,"['#AUTHOR_TAG introduced the log-linear model for statistical machine translation ( SMT ) , in which translation is considered as the following optimization problem :']"
CC136,D12-1037,Discriminative Training for Log-Linear Based SMT,examplebased decoding for statistical machine translation,"['Taro Watanabe', 'Eiichiro Sumita']",related work,,"Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .","['Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', 'Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.']",1,"['Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( #AUTHOR_TAG ; He et al. , 2010 ; Ma et al. , 2011 ) .', 'Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them to discriminatively learn local weights.']"
CC137,D12-1037,Discriminative Training for Log-Linear Based SMT,nearoptimal hashing algorithms for approximate nearest neighbor in high dimensions,"['Alexandr Andoni', 'Piotr Indyk']",experiments,"We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice","Actually , if we use LSH technique ( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .","['Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 .', 'This shows that the local method is efficient.', 'Further, compared to the retrieval, the local training is not the bottleneck.', 'Actually , if we use LSH technique ( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .']",3,"['This shows that the local method is efficient.', 'Further, compared to the retrieval, the local training is not the bottleneck.', 'Actually , if we use LSH technique ( #AUTHOR_TAG ) in retrieval process , the local method can be easily scaled to a larger training data .']"
CC138,D12-1037,Discriminative Training for Log-Linear Based SMT,optimal search for minimum error rate training,"['Michel Galley', 'Chris Quirk']",related work,"Abstract Minimum error rate training is a crucial component to many state-of-the-art NLP applications, such as machine translation and speech recognition. However, common evaluation functions such as BLEU or word error rate are generally highly non-convex and thus prone to search errors. In this paper, we present LP-MERT, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming. Given a set of N -best lists produced from S input sentences, this algorithm finds a linear model that is globally optimal with respect to this set. We find that this algorithm is polynomial in N and in the size of the model, but exponential in S. We present extensions of this work that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm","(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimiza- tion objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003; Moore and Quirk, 2008; Zhao and Chen, 2009; #AUTHOR_TAG ) employed an evaluation metric as a loss function and directly optimized it.']"
CC139,D12-1037,Discriminative Training for Log-Linear Based SMT,local learning algorithms,"['L´eon Bottou', 'Vladimir Vapnik']",introduction,"The goal of digital image processing is to capture, transmit, and display images as efficiently as possible. Such tasks are computationally intensive because an image is digitally represented by large amounts of data. It is possible to render an image by reconstructing it with a subset of the most relevant data. One such procedure used to accomplish this task is commonly referred to as sparse coding. For our purpose, we use images of handwritten digits that are presented to an artificial neural network. The network implements Rozell u27s locally competitive algorithm (LCA) to generate a sparse code. This sparse code is then presented to another neural network, a classifier that attempts to place the image in one of ten categories, each representing one of the digits zero through nine. Furthermore, the LCA approach is unique in that it produces quality sparse codes by utilizing highly parallel architectures. Pattern recognition problems have been of interest by industries that rely heavily on data as a core part of their business. Social networking companies use it to analyze, predict, and even influence user behavior. However, as data becomes more cost-effective to collect, it will be important for companies in other industries to extract useful information from said data. Manufacturing companies use it to analyze the performance of their products and financial service companies use it to flag customers likely to default on their loans. Interestingly, the image processing techniques described above can be generalized for use on data other than image data","The local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .","['The local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', 'Compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.', 'It is superior to the global one when the data sets are not evenly distributed (Bottou and Vapnik, 1992;Zhang et al., 2006).']",0,"['The local training method ( #AUTHOR_TAG ) is widely employed in computer vision ( Zhang et al. , 2006 ; Cheng et al. , 2010 ) .', 'Compared with the global training method which tries to fit a single weight on the training data, the local one learns weights based on the local neighborhood information for each test example.']"
CC140,D12-1037,Discriminative Training for Log-Linear Based SMT,a simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters,"['Bing Zhao', 'Shengyuan Chen']",introduction,"We propose a variation of simplex-downhill algo-rithm specifically customized for optimizing param-eters in statistical machine translation (SMT) de-coder for better end-user automatic evaluation met-ric scores for translations, such as versions of BLEU, TER and mixtures of them. Traditional simplex-downhill has the advantage of derivative-free com-putations of objective functions, yet still gives satis-factory searching directions in most scenarios. This is suitable for optimizing translation metrics as they are not differentiable in nature. On the other hand, Armijo algorithm usually performs line search ef-ficiently given a searching direction. It is a deep hidden fact that an efficient line search method will change the iterations of simplex, and hence the searching trajectories. We propose to embed the Armijo inexact line search within the simplex-downhill algorithm. We show, in our experiments, the proposed algorithm improves over the widely-applied Minimum Error Rate training algorithm for optimizing machine translation parameters.","Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .","['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']",0,"['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; #AUTHOR_TAG ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']"
CC141,D12-1037,Discriminative Training for Log-Linear Based SMT,an empirical study of smoothing techniques for language modeling in,"['Stanley F Chen', 'Joshua Goodman']",experiments,"We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t...","We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( #AUTHOR_TAG ) .","['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( #AUTHOR_TAG ) .', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']",5,"['We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser-Ney smoothing ( #AUTHOR_TAG ) .']"
CC142,D12-1037,Discriminative Training for Log-Linear Based SMT,srilm  an extensible language modeling toolkit,['Andreas Stolcke'],experiments,"SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools. 1","We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .","['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']",5,"['We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits ( #AUTHOR_TAG ) with modified Kneser-Ney smoothing ( Chen and Goodman , 1998 ) .', 'as the evaluation tool.']"
CC143,D12-1037,Discriminative Training for Log-Linear Based SMT,optimal search for minimum error rate training,"['Michel Galley', 'Chris Quirk']",introduction,"Abstract Minimum error rate training is a crucial component to many state-of-the-art NLP applications, such as machine translation and speech recognition. However, common evaluation functions such as BLEU or word error rate are generally highly non-convex and thus prone to search errors. In this paper, we present LP-MERT, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming. Given a set of N -best lists produced from S input sentences, this algorithm finds a linear model that is globally optimal with respect to this set. We find that this algorithm is polynomial in N and in the size of the model, but exponential in S. We present extensions of this work that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm","Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .","['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']",0,"['h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; #AUTHOR_TAG ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.']"
CC144,D12-1037,Discriminative Training for Log-Linear Based SMT,incremental and decremental support vector machine learning,"['G Cauwenberghs', 'T Poggio']",,"An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental ""unlearning"" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.","In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .","['Compared with retraining mode, incremental training can improve the training efficiency.', 'In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .', 'The biggest difficulty lies in that the fea- ture vector of a given training example, i.e. translation example, is unavailable until actually decoding the example, since the derivation is a latent variable.', 'In this section, we will investigate the incremental trainingmethodsinSMTscenario.']",0,"['In the field of machine learning research , incremental training has been employed in the work ( #AUTHOR_TAG ; Shilton et al. , 2005 ) , but there is little work for tuning parameters of statistical machine translation .', 'In this section, we will investigate the incremental trainingmethodsinSMTscenario.']"
CC145,D12-1037,Discriminative Training for Log-Linear Based SMT,a simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters,"['Bing Zhao', 'Shengyuan Chen']",related work,"We propose a variation of simplex-downhill algo-rithm specifically customized for optimizing param-eters in statistical machine translation (SMT) de-coder for better end-user automatic evaluation met-ric scores for translations, such as versions of BLEU, TER and mixtures of them. Traditional simplex-downhill has the advantage of derivative-free com-putations of objective functions, yet still gives satis-factory searching directions in most scenarios. This is suitable for optimizing translation metrics as they are not differentiable in nature. On the other hand, Armijo algorithm usually performs line search ef-ficiently given a searching direction. It is a deep hidden fact that an efficient line search method will change the iterations of simplex, and hence the searching trajectories. We propose to embed the Armijo inexact line search within the simplex-downhill algorithm. We show, in our experiments, the proposed algorithm improves over the widely-applied Minimum Error Rate training algorithm for optimizing machine translation parameters.","( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '( Och , 2003 ; Moore and Quirk , 2008 ; #AUTHOR_TAG ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .']"
CC146,D12-1037,Discriminative Training for Log-Linear Based SMT,random restarts in minimum error rate training for statistical machine translation,"['Robert C Moore', 'Chris Quirk']",related work,"Och's (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its benefits. We compare several ways of performing random restarts with MERT. We find that all of our random restart methods outperform MERT without random restarts, and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time.","( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .","['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '(Och and Ney, 2002; Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['Several works have proposed discriminative tech- niques to train log-linear model for SMT.', '( Och , 2003 ; #AUTHOR_TAG ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .']"
CC147,D12-1037,Discriminative Training for Log-Linear Based SMT,statistical significance tests for machine translation evaluation,['Philipp Koehn'],experiments,"If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.",The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .,"['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .']",5,"['We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair.', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling ( #AUTHOR_TAG ) .']"
CC148,D12-1037,Discriminative Training for Log-Linear Based SMT,tuning as ranking,"['Mark Hopkins', 'Jonathan May']",,"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.","(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG, 2011).","['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG, 2011).', 'Since score of e11 is greater than that of e12, (1, 0) corresponds to a possitive example denoted as ”•”, and (−1, 0) corresponds to a negative example denoted as ”*”.', 'Since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile.', 'However, one can obtain e11 and e21 with weights: (1, 1) and (−1, 1), respectively.']",0,"['(b). The nonlinearly separable classification problem transformed from (a) via tuning as ranking (#AUTHOR_TAG, 2011).']"
CC149,D12-1037,Discriminative Training for Log-Linear Based SMT,discriminative training and maximum entropy models for statistical machine translation,"['Franz Josef Och', 'Hermann Ney']",related work,"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.","( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.","['Several works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['Several works have proposed discriminative techniques to train log-linear model for SMT.', '( #AUTHOR_TAG ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT.']"
CC150,D12-1037,Discriminative Training for Log-Linear Based SMT,examplebased decoding for statistical machine translation,"['Taro Watanabe', 'Eiichiro Sumita']",,,"To retrieve translation examples for a test sentence , ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :","['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']",5,"['The metric we consider here is derived from an example-based machine translation.', 'To retrieve translation examples for a test sentence , ( #AUTHOR_TAG ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch Â¨ utze , 1999 ) as follows :']"
CC151,D12-1037,Discriminative Training for Log-Linear Based SMT,minimum error rate training in statistical machine translation,['Franz Josef Och'],introduction,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.","Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .","['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']",0,"['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( #AUTHOR_TAG ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.']"
CC152,D12-1037,Discriminative Training for Log-Linear Based SMT,ultraconservative online algorithms for multiclass problems,"['Koby Crammer', 'Yoram Singer']",,"In this paper we study a paradigm to generalize online classification algorithms for binary classification problems to multiclass problems. The particular hypotheses we investigate maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultraconservativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems. We discuss the form the algorithms take in the binary case and show that all the algorithms from the first family reduce to the Perceptron algorithm while MIRA provides a new Perceptron-like algorithm with a margin-dependent learning rate. We then return to multiclass problems and describe an analogous multiplicative family of algorithms with corresponding mistake bounds. We end the formal part by deriving and analyzing a multiclass version of Li and Long's ROMMA algorithm. We conclude with a discussion of experimental results that demonstrate the merits of our algorithms.","We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .","['Following the notations in Algorithm 2, W b is the baseline weight, D i = { f i j , c i j , r i j } K j=1 denotes training examples for t i .', 'For the sake of brevity, we will drop the index i, D i = { f j , c j , r j } K j=1 , in the rest of this paper.', 'Our goal is to find an optimal weight, denoted by W i , which is a local weight and used for decoding the sentence t i .', 'Unlike the global method which performs tuning on the whole development set Dev + D i as in Algorithm 1, W i can be incrementally learned by optimizing on D i based on W b .', 'We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .']",5,"['We employ the idea of ultraconservative update ( #AUTHOR_TAG ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .']"
CC153,D12-1037,Discriminative Training for Log-Linear Based SMT,minimum error rate training in statistical machine translation,['Franz Josef Och'],related work,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.","( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .","['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .', '(Watanabe et al., 2007;Chiang et al., 2008;Hopkins and May, 2011) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions.']",1,"['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '( #AUTHOR_TAG ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .']"
CC154,D12-1037,Discriminative Training for Log-Linear Based SMT,a hierarchical phrasebased model for statistical machine translation,['David Chiang'],experiments,,"We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .","['We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .', 'To obtain satisfactory baseline performance, we tune In-Hiero system for 5 times using MERT, and then se-    (Koehn et al., 2007).', 'Both of these systems are with default setting.', 'All three systems are trained by MERT with 100 best candidates.']",5,"['We use an in-house developed hierarchical phrase-based translation ( #AUTHOR_TAG ) as our baseline system , and we denote it as In-Hiero .']"
CC155,D12-1037,Discriminative Training for Log-Linear Based SMT,improved statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",experiments,"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen","We run GIZA + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .","['We run GIZA + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']",5,"['We run GIZA + + ( #AUTHOR_TAG ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .', 'We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).', 'In our experiments the translation performances are measured by case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl', 'as the evaluation tool.', 'The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004).']"
CC156,D12-1037,Discriminative Training for Log-Linear Based SMT,tuning as ranking,"['Mark Hopkins', 'Jonathan May']",related work,"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.","( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .","['Several works have proposed discriminative techniques to train log-linear model for SMT.', '(Och and Ney, 2002;Blunsom et al., 2008) used maximum likelihood estimation to learn weights for MT.', '(Och, 2003;Moore and Quirk, 2008;Zhao and Chen, 2009; Galley and Quirk, 2011) employed an evaluation metric as a loss function and directly optimized it.', '( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']",1,"['( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; #AUTHOR_TAG ) proposed other optimization objectives by introducing a margin-based and ranking-based indirect loss functions .']"
CC157,D12-1037,Discriminative Training for Log-Linear Based SMT,tuning as ranking,"['Mark Hopkins', 'Jonathan May']",introduction,"We offer a simple, effective, and scalable method for statistical machine translation pa-rameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chi-ang et al., 2008b), PRO is easy to imple-ment. It uses off-the-shelf linear binary classi-fier software and can be built on top of an ex-isting MERT framework in a matter of hours. We establish PRO's scalability and effective-ness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.","Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .","['where f and e (e ) are source and target sentences, respectively.', 'h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W .', 'Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .', 'All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.', 'We call them a global training method.', 'One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.', 'However, due to the diversity and uneven distribution of source sentences (Li et al., 2010), there are some shortcomings in this pipeline.']",0,"['Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( #AUTHOR_TAG ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .']"
CC158,D13-1038,Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction,the tunareg challenge 2009 overview and evaluation results,"['Albert Gatt', 'Anja Belz', 'Eric Kow']",experiments,,"Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .","['Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception.', 'Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .', ""However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to  the situation where the agent's representation of the shared world is problematic and full of mistakes.""]",1,"['Although evaluated on different data sets , this result is consistent with results from previous work ( Gatt and Belz , 2008 ; #AUTHOR_TAG ) .']"
CC159,D13-1038,Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction,towards mediating shared perceptual basis in situated dialogue,"['Changsong Liu', 'Rui Fang', 'Joyce Y Chai']",,"To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction.","Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .","['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990).', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .', 'For the referring expression generation task here, we also need a lexicon with grounded semantics.']",2,"['Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990).', 'Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; #AUTHOR_TAG ) .']"
CC160,D13-1038,Embodied Collaborative Referring Expression Generation in Situated Human-Robot Interaction,towards mediating shared perceptual basis in situated dialogue,"['Changsong Liu', 'Rui Fang', 'Joyce Y Chai']",introduction,"To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction.",How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ) .,"['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ) .', 'In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.', 'Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.', 'Robots have much lower perceptual capabilities of the environment than humans.', 'How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?']",2,['How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( #AUTHOR_TAG ) .']
CC161,D13-1115,Integrating Theory and Practice: A Daunting Task,combining feature norms and text data with topic models,['Mark Steyvers'],related work,"Many psychological theories of semantic cognition assume that concepts are represented by features. The empirical procedures used to elicit features from humans rely on explicit human judgments which limit the scope of such representations. An alternative computational framework for semantic cognition that does not rely on explicit human judgment is based on the statistical analysis of large text collections. In the topic modeling approach, documents are represented as a mixture of learned topics where each topic is represented as a probability distribution over words. We propose feature-topic models, where each document is represented by a mixture of learned topics as well as predefined topics that are derived from feature norms. Results indicate that this model leads to systematic improvements in generalization tasks. We show that the learned topics in the model play in an important role in the generalization performance by including words that are not part of current feature norms.2009 Elsevier B.V. All rights reserved.","In a similar vein , #AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein , #AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein , #AUTHOR_TAG showed that a different feature-topic model improved predictions on a fill-in-the-blank task .', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"
CC162,D13-1115,Integrating Theory and Practice: A Daunting Task,training a multilingual sportscaster using perceptual context to learn language,"['David L Chen', 'Joohyun Kim', 'Raymond J Mooney']",related work,We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.,"Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter cat- egory, the two most common representations have been association norms, where subjects are given a cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).']",0,"['Some works abstract perception via the usage of symbolic logic representations ( #AUTHOR_TAG ; Chen and Mooney , 2011 ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .']"
CC163,D13-1115,Integrating Theory and Practice: A Daunting Task,distinctive image features from scaleinvariant keypoints,['David G Lowe'],related work,This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ...,They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'They use a Bag of Visual Words ( BoVW ) model ( #AUTHOR_TAG ) to create a bimodal vocabulary describing documents .', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.']"
CC164,D13-1115,Integrating Theory and Practice: A Daunting Task,describing objects by their attributes,"['Ali Farhadi', 'Ian Endres', 'Derek Hoiem', 'David Forsyth']",related work,"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (""spotty dog"", not just ""dog""); to say something about unfamiliar objects (""hairy and four-legged"", not just ""unknown""); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (""spotty"") or discriminative (""dogs have it but sheep do not""). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. 1","More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG ) , act as excellent substitutes for feature","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( #AUTHOR_TAG ) , act as excellent substitutes for feature']"
CC165,D13-1115,Integrating Theory and Practice: A Daunting Task,simple supervised document geolocation with geodesic grids,"['Benjamin Wing', 'Jason Baldridge']",introduction,"We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document's raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset.","Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 ) .']",0,"['Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; #AUTHOR_TAG ; Roller et al. , 2012 ) .']"
CC166,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",introduction,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( #AUTHOR_TAG ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']"
CC167,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",related work,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","That is, we simply take the original mLDA model of #AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.","['That is, we simply take the original mLDA model of #AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', 'At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi.']",2,"['That is, we simply take the original mLDA model of #AUTHOR_TAG (2009) and generalize it in the same way they generalize LDA.', 'At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi.']"
CC168,D13-1115,Integrating Theory and Practice: A Daunting Task,learning language semantics from ambiguous supervision,"['Rohit J Kate', 'Raymond J Mooney']",introduction,"This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers.","Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ) .', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a;Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012).']",0,"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing , where words and sentences are mapped to logical structure meaning ( #AUTHOR_TAG ) .']"
CC169,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",method,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.",#AUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .,"['#AUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'Topics consist of multinomial distributions over words, β k , but are extended to also include multinomial distributions over features, ψ k .', 'The generative process is amended to include these feature distributions:']",0,"['#AUTHOR_TAG extend LDA to allow for the inference of document and topic distributions in a multimodal corpus .', 'In their model, a document consists of a set of (word, feature) pairs, 4 rather than just words, and documents are still modeled as mixtures of shared topics.', 'Topics consist of multinomial distributions over words, b k , but are extended to also include multinomial distributions over features, ps k .', 'The generative process is amended to include these feature distributions:']"
CC170,D13-1115,Integrating Theory and Practice: A Daunting Task,webscale kmeans clustering,['D Sculley'],experiments,,"The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .","['First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (Bay et al., 2008).', 'SURF is a method for selecting points-of-interest within an image.', 'It is faster and more forgiving than the commonly known SIFT algorithm.', 'We compute SURF keypoints for every image in our data set using Sim-pleCV 3 and randomly sample 1% of the keypoints.', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .', 'All images for a given word are summed together to provide an average representation for the word.', 'We refer to this representation as the SURF modality.']",5,"['It is faster and more forgiving than the commonly known SIFT algorithm.', 'The keypoints are clustered into 5,000 visual codewords ( centroids ) using k-means clustering ( #AUTHOR_TAG ) , and images are then quantized over the 5,000 codewords .']"
CC171,D13-1115,Integrating Theory and Practice: A Daunting Task,grounded models of semantic representation,"['Carina Silberer', 'Mirella Lapata']",experiments,"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.","This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .']",1,"['This result is consistent with other works using this model with these features ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .']"
CC172,D13-1115,Integrating Theory and Practice: A Daunting Task,grounding action descriptions in videos,"['Michaela Regneri', 'Marcus Rohrbach', 'Dominikus Wetzel', 'Stefan Thater', 'Bernt Schiele', 'Manfred Pinkal']",related work,"Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information ex-tracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substan-tially when combined with visual information from videos depicting the described actions.","Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG ) .","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG ) .']",0,"['Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( Mathe et al. , 2008 ; #AUTHOR_TAG ) .']"
CC173,D13-1115,Integrating Theory and Practice: A Daunting Task,semantic feature production norms for a large set of living and nonliving things,"['Ken McRae', 'George S Cree', 'Mark S Seidenberg', 'Chris McNorgan']",related work,"Semantic features have provided insight into numerous behavioral phenomena concerning concepts, categorization, and semantic memory in adults, children, and neuropsychological populations. Numerous theories and models in these areas are based on representations and computations involving semantic features. Consequently, empirically derived semantic feature production norms have played, and continue to play, a highly useful role in these domains. This article describes a set of feature norms collected from approximately 725 participants for 541 living (dog) and nonliving (chair) basic-level concepts, the largest such set of norms developed to date. This article describes the norms and numerous statistics associated with them. Our aim is to make these norms available to facilitate other research, while obviating the need to repeat the labor-intensive methods involved in collecting and analyzing such norms. The full set of norms may be downloaded from www.psychonomic.org/archive.","Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG ) .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual in- formation and using high-level representations in- stead.', 'Some works abstract perception via the us- age of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholin- guistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG ) .']",0,"['Within the latter category, the two most common representations have been association norms, where subjects are given a cue word and name the first ( or several ) associated words that come to mind ( e.g. , Nelson et al. ( 2004 ) ) , and feature norms , where subjects are given a cue word and asked to describe typical properties of the cue concept ( e.g. , #AUTHOR_TAG ) .']"
CC174,D13-1115,Integrating Theory and Practice: A Daunting Task,learning to interpret natural language navigation instructions from observations,"['David L Chen', 'Raymond J Mooney']",introduction,"The ability to understand natural-language instructions is crit-ical to building intelligent agents that interact with humans. We present a system that learns to transform natural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ-ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in-structions for these environments is used for training and test-ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys-tem is able to automatically learn to correctly interpret a rea-sonable fraction of the complex instructions in this corpus.","Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different ap- proaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and La- pata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identifica- tion of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012).']",0,"['Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( #AUTHOR_TAG ) or robot commands ( Tellex et al. , 2011 ; Matuszek et al. , 2012 ) .']"
CC175,D13-1115,Integrating Theory and Practice: A Daunting Task,im2text describing images using 1 million captioned photographs,"['Vicente Ordonez', 'Girish Kulkarni', 'Tamara L Berg']",introduction,"We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset - performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.","Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #AUTHOR_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #AUTHOR_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']",0,"['Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; #AUTHOR_TAG ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"
CC176,D13-1115,Integrating Theory and Practice: A Daunting Task,models of semantic representation with visual attributes,"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']",introduction,We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.,"Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; #AUTHOR_TAG ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; #AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; #AUTHOR_TAG ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC177,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",experiments,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","We use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair .","['In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5', 'That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc.', 'The resulting stochastically generated corpus is used in its corresponding experiments.']",5,"['In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our non- textual modalities.', ""We use the same method as #AUTHOR_TAG for generating our multimodal corpora : for each word token in the text corpus , a feature is selected stochastically from the word 's feature distribution , creating a word-feature pair ."", 'Words without grounded features are all given the same placeholder feature, also resulting in a word- feature pair.5', 'That is, for the feature norm modal- ity, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc.', 'The resulting stochastically generated corpus is used in its corresponding experiments.']"
CC178,D13-1115,Integrating Theory and Practice: A Daunting Task,relative attributes,"['Devi Parikh', 'Kristen Grauman']",related work,"Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn increasingly complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases. We introduce a deep neural network architecture for the task of relative attribute prediction. A convolutional neural network (ConvNet) is adopted to learn the features by including an additional layer (ranking layer) that learns to rank the images based on these features. We adopt an appropriate ranking loss to train the whole network in an end-to-end fashion. Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various coarse and fine-grained datasets. Our qualitative results along with the visualization of the saliency maps show that the network is able to learn effective features for each specific attribute. Source code of the proposed method is available at https://github.com/yassersouri/ghiaseddin.Comment: ACCV 201","The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; #AUTHOR_TAG ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"
CC179,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",method,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.",Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .,"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']",5,"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by #AUTHOR_TAG .', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).']"
CC180,D13-1115,Integrating Theory and Practice: A Daunting Task,distributional semantics in technicolor,"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']",experiments,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.",This seems to provide additional evidence of #AUTHOR_TAGb ) 's suggestion that something like a distributional hypothesis of images is plausible .,"['We see that the image modalities are much more useful than they are in compositionality prediction.', 'The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model.', 'Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them.', ""This seems to provide additional evidence of #AUTHOR_TAGb ) 's suggestion that something like a distributional hypothesis of images is plausible .""]",1,"[""This seems to provide additional evidence of #AUTHOR_TAGb ) 's suggestion that something like a distributional hypothesis of images is plausible .""]"
CC181,D13-1115,Integrating Theory and Practice: A Daunting Task,visual and semantic similarity in imagenet,"['Thomas Deselaers', 'Vittorio Ferrari']",experiments,"Many computer vision approaches take for granted positive answers to questions such as ""Are semantic categories visually separable?"" and ""Is visual similarity correlated to semantic similarity?"". In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system. The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category. This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet. We demonstrate experimentally that it outperforms purely visual distances.","It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .","['We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009).', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']",4,"['We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009).', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'It is frequently used in tasks like scene identification , and #AUTHOR_TAG shows that distance in GIST space correlates well with semantic distance in WordNet .', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']"
CC182,D13-1115,Integrating Theory and Practice: A Daunting Task,distributional semantics in technicolor,"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']",related work,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.",#AUTHOR_TAGa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', '#AUTHOR_TAGa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,['#AUTHOR_TAGa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .']
CC183,D13-1115,Integrating Theory and Practice: A Daunting Task,how many words is a picture worth automatic caption generation for news images,"['Yansong Feng', 'Mirella Lapata']",introduction,"In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.","Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']",0,"['Some efforts have tackled tasks such as automatic image caption generation ( #AUTHOR_TAGa ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"
CC184,D13-1115,Integrating Theory and Practice: A Daunting Task,the wacky wide web a collection of very large linguistically processed webcrawled corpora language resources and evaluation,"['Marco Baroni', 'Silvia Bernardini', 'Adriano Ferraresi', 'Eros Zanchetta']",experiments,,"For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .","['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .', 'We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100.', 'The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens.']",5,"['For our Text modality , we use deWaC , a large German web corpus created by the WaCKy group ( #AUTHOR_TAG ) containing approximately 1.7 B word tokens .', 'The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens.']"
CC185,D13-1115,Integrating Theory and Practice: A Daunting Task,improving video activity recognition using object recognition and text mining,"['Tanvi S Motwani', 'Raymond J Mooney']",related work,"Abstract. Recognizing activities in real-world videos is a chal-lenging AI problem. We present a novel combination of standard activity classification, object recognition, and text mining to learn effective activity recognizers without ever explicitly labeling train-ing videos. We cluster verbs used to describe videos to automatically discover classes of activities and produce a labeled training set. This labeled data is then used to train an activity classifier based on spatio-temporal features. Next, text mining is employed to learn the correla-tions between these verbs and related objects. This knowledge is then used together with the outputs of an off-the-shelf object recognizer and the trained activity classifier to produce an improved activity rec-ognizer. Experiments on a corpus of YouTube videos demonstrate the effectiveness of the overall approach.","To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']",0,"['To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and #AUTHOR_TAG show that verb clusters can be used to improve activity recognition in videos .']"
CC186,D13-1115,Integrating Theory and Practice: A Daunting Task,perceptual inference through global lexical similarity,"['Brendan T Johns', 'Michael N Jones']",related work,"The literature contains a disconnect between accounts of how humans learn lexical semantic representations for words. Theories generally propose that lexical semantics are learned either through perceptual experience or through exposure to regularities in language. We propose here a model to integrate these two information sources. Specifically, the model uses the global structure of memory to exploit the redundancy between language and perception in order to generate inferred perceptual representations for words with which the model has no perceptual experience. We test the model on a variety of different datasets from grounded cognition experiments and demonstrate that this diverse set of results can be explained as perceptual simulation (cf. Barsalou, Simmons, Barbey, & Wilson, 2003) within a global memory model.Copyright (c) 2012 Cognitive Science Society, Inc.",#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .,"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', '#AUTHOR_TAG take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity .', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"
CC187,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",experiments,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","Following #AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .","['Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'Following #AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.', 'We then compute the percentile ranks of similarity for each word pair, e.g., ""cat"" is more similar to ""dog"" than 97.3% of the rest of the vocabulary.', 'We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once.']",5,"['Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.', 'Following #AUTHOR_TAG , we measure association norm prediction as an average of percentile ranks .', 'For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.', 'We then compute the percentile ranks of similarity for each word pair, e.g., ""cat"" is more similar to ""dog"" than 97.3% of the rest of the vocabulary.']"
CC188,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",conclusion,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG .","['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.', 'SURF features also provided significant gains over text-only LDA in predicting the compositionality of noun compounds.']",5,"['In this paper , we evaluated the role of low-level image features , SURF and GIST , for their compatibility with the multimodal Latent Dirichlet Allocation model of #AUTHOR_TAG .', 'We found both fea- ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over traditional text-only LDA.', 'SURF features also provided significant gains over text-only LDA in predicting the compositionality of noun compounds.']"
CC189,D13-1115,Integrating Theory and Practice: A Daunting Task,modeling the shape of the scene a holistic representation of the spatial envelope,"['Aude Oliva', 'Antonio Torralba']",experiments,"In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.","We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .","['We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet.', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']",5,"['We also compute GIST vectors ( #AUTHOR_TAG ) for every image using LearGIST ( Douze et al. , 2009 ) .', 'Unlike SURF descriptors, GIST produces a single vector representation for an image.', 'The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall ""gist"" of the whole image.', 'It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet.', 'After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.']"
CC190,D13-1115,Integrating Theory and Practice: A Daunting Task,supervised textbased geolocation using language models on an adaptive grid,"['Stephen Roller', 'Michael Speriosu', 'Sarat Rallapalli', 'Benjamin Wing', 'Jason Baldridge']",introduction,"The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. One common approach for geolocating texts is rooted in information retrieval. Given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell; then a location for a test document is chosen based on the most similar pseudo-document. Uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. We define an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. We also provide a better way of choosing the locations for pseudo-documents. We evaluate these strategies on existing Wikipedia and Twitter corpora, as well as a new, larger Twitter corpus. The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus. The two grid constructions can also be combined to produce consistently strong results across all training sets.","Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG ) .']",0,"['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; #AUTHOR_TAG ) .']"
CC191,D13-1115,Integrating Theory and Practice: A Daunting Task,topics in semantic representation,"['Thomas L Griffiths', 'Mark Steyvers', 'Joshua B Tenenbaum']",related work,"Accounts of language processing have suggested that it requires retrieving concepts from memory in response to an ongoing stream of information. This can be facilitated by inferring the gist of a sentence, conversation, or document, and using that computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads us to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure. Many aspects of perception and cognition can be understood by considering the computational problem that is addressed by a particular human capacity (Andersion, 1990; Marr, 1982). Perceptual capacities such as identifying shape from shading (Freeman, 1994), motion perceptio","#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', '#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['#AUTHOR_TAG helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( Deerwester et al. , 1990 ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"
CC192,D13-1115,Integrating Theory and Practice: A Daunting Task,zeroshot learning through crossmodal transfer,"['Richard Socher', 'Milind Ganjoo', 'Hamsa Sridhar', 'Osbert Bastani', 'Christopher D Manning', 'Andrew Y Ng']",related work,,"To name a few examples , Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']",0,"['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , Rohrbach et al. ( 2010 ) and #AUTHOR_TAG show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']"
CC193,D13-1115,Integrating Theory and Practice: A Daunting Task,imagenet a largescale hierarchical image database,"['Jia Deng', 'Wei Dong', 'Richard Socher', 'Li-Jia Li', 'Kai Li', 'Li Fei-Fei']",experiments,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ""ImageNet"", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.","ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG ) .","['BilderNetle (""little ImageNet"" in Swabian German) is our new data set of German noun-to-ImageNet synset mappings.', 'ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG ) .', 'Multiple synsets exist for each meaning of a word.', 'For example, Im-ageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral.', 'This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet.']",5,"['ImageNet is a large-scale and widely used image database , built on top of WordNet , which maps words into groups of images , called synsets ( #AUTHOR_TAG ) .']"
CC194,D13-1115,Integrating Theory and Practice: A Daunting Task,grounded models of semantic representation,"['Carina Silberer', 'Mirella Lapata']",related work,"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.","#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .","['Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']",0,"['Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG introduce a new method of multimodal integration based on Canonical Correlation Analysis , and performs a systematic comparison between their CCA-based model and others on association norm prediction , held out feature prediction , and word similarity .']"
CC195,D13-1115,Integrating Theory and Practice: A Daunting Task,models of semantic representation with visual attributes,"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']",related work,We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.,"More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,"['More recently , #AUTHOR_TAG show that visual attribute classifiers , which have been immensely successful in object recognition ( Farhadi et al. , 2009 ) , act as excellent substitutes for feature']"
CC196,D13-1115,Integrating Theory and Practice: A Daunting Task,online learning for latent dirichlet allocation,"['Matthew Hoffman', 'David M Blei', 'Francis Bach']",experiments,"We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.","The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG .","['In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents.', 'We do not optimize these hyperparameters or vary them over time.', 'The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG .']",5,"['The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from #AUTHOR_TAG .']"
CC197,D13-1115,Integrating Theory and Practice: A Daunting Task,grounded models of semantic representation,"['Carina Silberer', 'Mirella Lapata']",introduction,"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; #AUTHOR_TAG ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .']"
CC198,D13-1115,Integrating Theory and Practice: A Daunting Task,distinctive image features from scaleinvariant keypoints,['David G Lowe'],related work,This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ...,"The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; #AUTHOR_TAG ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"
CC199,D13-1115,Integrating Theory and Practice: A Daunting Task,grounded models of semantic representation,"['Carina Silberer', 'Mirella Lapata']",method,"A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.","Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .","['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009).', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013).', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']",0,"['Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together ( Andrews et al. , 2009 ; #AUTHOR_TAG ) .']"
CC200,D13-1115,Integrating Theory and Practice: A Daunting Task,combining feature norms and text data with topic models,['Mark Steyvers'],introduction,"Many psychological theories of semantic cognition assume that concepts are represented by features. The empirical procedures used to elicit features from humans rely on explicit human judgments which limit the scope of such representations. An alternative computational framework for semantic cognition that does not rely on explicit human judgment is based on the statistical analysis of large text collections. In the topic modeling approach, documents are represented as a mixture of learned topics where each topic is represented as a probability distribution over words. We propose feature-topic models, where each document is represented by a mixture of learned topics as well as predefined topics that are derived from feature norms. Results indicate that this model leads to systematic improvements in generalization tasks. We show that the learned topics in the model play in an important role in the generalization performance by including words that are not part of current feature norms.2009 Elsevier B.V. All rights reserved.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; #AUTHOR_TAG ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC201,D13-1115,Integrating Theory and Practice: A Daunting Task,learning the abstract motion semantics of verbs from captioned videos,"['Stefan Mathe', 'Afsaneh Fazly', 'Sven Dickinson', 'Suzanne Stevenson']",related work,"We propose an algorithm for learning the semantics of a (motion) verb from videos depicting the action expressed by the verb, paired with sentences describing the action participants and their roles. Acknowledging that commonalities among example videos may not exist at the level of the input features, our approximation algorithm efficiently searches the space of more abstract features for a common solution. We test our algorithm by using it to learn the semantics of a sample set of verbs; results demonstrate the usefulness of the proposed framework, while identifying directions for further improvement.","Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 ) .","['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is Feng and Lapata (2010b).', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 ) .']",0,"['Other work on modeling the meanings of verbs using video recognition has also begun showing great promise ( #AUTHOR_TAG ; Regneri et al. , 2013 ) .']"
CC202,D13-1115,Integrating Theory and Practice: A Daunting Task,association norms of german noun compounds,"['Sabine Schulte im Walde', 'Susanne Borgwaldt', 'Ronny Jauch']",experiments,"This paper introduces association norms of German noun compounds as a lexical-semantic resource for cognitive and computational linguistics research on compositionality. Based on an existing database of German noun compounds, we collected human associations to the compounds and their constituents within a web experiment. The current study describes the collection process and a part-of-speech analysis of the association resource. In addition, we demonstrate that the associations provide insight into the semantic properties of the compounds, and perform a case study that predicts the degree of compositionality of the experiment compound nouns, as relying on the norms. Applying a comparatively simple measure of association overlap, we reach a Spearman rank correlation coefficient of rs = 0.5228, p &lt;.000001, when comparing our predictions with human judgements",Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG .,"['Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.', 'After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types.']",5,"['Association Norms ( AN ) is a collection of association norms collected by Schulte im #AUTHOR_TAG .', 'In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind.', 'With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words.']"
CC203,D13-1115,Integrating Theory and Practice: A Daunting Task,latent dirichlet allocation,"['David M Blei', 'Andrew Y Ng', 'Michael I Jordan']",method,"Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting imagery. However, these models are confined to crisp segmentation, forcing a visual word (i.e., an image patch) to belong to one and only one topic. Yet, there are many images in which some regions cannot be assigned a crisp categorical label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and an associated parameter estimation algorithm. This model can be useful for imagery where a visual word may be a mixture of multiple topics. Experimental results on visual and sonar imagery show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability previous topic modeling methods do not have.Comment: Version 1, Sent for Review. arXiv admin note: substantial text   overlap with arXiv:1511.0282","Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .","['Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .', 'It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as β), and documents are modeled as mixtures of these shared topics (notated as θ).', 'LDA assumes every document in the corpus is generated using the fol-lowing generative process:']",0,"['Latent Dirichlet Allocation ( #AUTHOR_TAG ) , or LDA , is an unsupervised Bayesian probabilistic model of text documents .']"
CC204,D13-1115,Integrating Theory and Practice: A Daunting Task,learning to parse natural language commands to a robot control system,"['Cynthia Matuszek', 'Evan Herbst', 'Luke Zettlemoyer', 'Dieter Fox']",related work,"Abstract As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor environment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our formal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.","Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; Chen and Mooney , 2011 ; #AUTHOR_TAG ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .']"
CC205,D13-1115,Integrating Theory and Practice: A Daunting Task,what helps where–and why semantic relatedness for knowledge transfer,"['Marcus Rohrbach', 'Michael Stark', 'Gy¨orgy Szarvas', 'Iryna Gurevych', 'Bernt Schiele']",related work,"Remarkable performance has been reported to recognize  single object classes. Scalability to large numbers of classes  however remains an important challenge for today's recognition  methods. Several authors have promoted knowledge  transfer between classes as a key ingredient to address this  challenge. However, in previous work the decision, which  knowledge to transfer has required either manual supervision  or at least a few training examples limiting the scalability  of these approaches. In this work we explicitly address  the question of how to automatically decide which information  to transfer between classes without the need of any human  intervention. For this we tap into linguistic knowledge  bases to provide the semantic link between sources (what)  and targets (where) of knowledge transfer. We provide a rigorous  experimental evaluation of different knowledge bases  and state-of-the-art techniques from Natural Language Processing  which goes far beyond the limited use of language  in related work. We also give insights into the applicability  (why) of different knowledge sources and similarity measures  for knowledge transfer","To name a few examples , #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .","['The Computer Vision community has also benefited greatly from efforts to unify the two modalities.', 'To name a few examples , #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']",0,"['To name a few examples , #AUTHOR_TAG and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and Motwani and Mooney ( 2012 ) show that verb clusters can be used to improve activity recognition in videos .']"
CC206,D13-1115,Integrating Theory and Practice: A Daunting Task,learning to interpret natural language navigation instructions from observations,"['David L Chen', 'Raymond J Mooney']",related work,"The ability to understand natural-language instructions is crit-ical to building intelligent agents that interact with humans. We present a system that learns to transform natural-language navigation instructions into executable formal plans. Given no prior linguistic knowledge, the system learns by simply observing how humans follow navigation instructions. The system is evaluated in three complex virtual indoor environ-ments with numerous objects and landmarks. A previously collected realistic corpus of complex English navigation in-structions for these environments is used for training and test-ing data. By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser, the sys-tem is able to automatically learn to correctly interpret a rea-sonable fraction of the complex instructions in this corpus.","Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Some works abstract perception via the usage of symbolic logic representations ( Chen et al. , 2010 ; #AUTHOR_TAG ; Matuszek et al. , 2012 ; Artzi and Zettlemoyer , 2013 ) , while others choose to employ concepts elicited from psycholinguistic and cognition studies .']"
CC207,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",related work,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms.', '#AUTHOR_TAG furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .']"
CC208,D13-1115,Integrating Theory and Practice: A Daunting Task,distributional semantics from text and images,"['Elia Bruni', 'Giang Binh Tran', 'Marco Baroni']",introduction,"We present a distributional semantic model combining text- and image-based features. We evaluate this multimodal semantic model on simulating similarity judgments, concept clus-tering and the BLESS benchmark. When inte-grated with the same core text-based model, image-based features are at least as good as further text-based features, and they capture different qualitative aspects of the tasks, sug-gesting that the two sources of information are complementary.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; #AUTHOR_TAG ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC209,D13-1115,Integrating Theory and Practice: A Daunting Task,models of semantic representation with visual attributes,"['Carina Silberer', 'Vittorio Ferrari', 'Mirella Lapata']",method,We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.,It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ) .,"['Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009).', 'Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009;Silberer and Lapata, 2012).', 'It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ) .', 'These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009).', 'However, prior work using mLDA is limited to two modalities at a time.', 'In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities.']",0,['It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers ( #AUTHOR_TAG ) .']
CC210,D13-1115,Integrating Theory and Practice: A Daunting Task,the story picturing engine—a system for automatic text illustration,"['Dhiraj Joshi', 'James Z Wang', 'Jia Li']",introduction,"We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented.","Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011;Matuszek et al., 2012).', 'Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']",0,"['Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( #AUTHOR_TAG ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; Wing and Baldridge , 2011 ; Roller et al. , 2012 ) .']"
CC211,D13-1115,Integrating Theory and Practice: A Daunting Task,learning to parse natural language commands to a robot control system,"['Cynthia Matuszek', 'Evan Herbst', 'Luke Zettlemoyer', 'Dieter Fox']",introduction,"Abstract As robots become more ubiquitous and capable of performing complex tasks, the importance of enabling untrained users to interact with them has increased. In response, unconstrained natural-language interaction with robots has emerged as a significant research area. We discuss the problem of parsing natural language commands to actions and control structures that can be readily implemented in a robot execution system. Our approach learns a parser based on example pairs of English commands and corresponding control language expressions. We evaluate this approach in the context of following route instructions through an indoor environment, and demonstrate that our system can learn to translate English commands into sequences of desired actions, while correctly capturing the semantic intent of statements involving complex control structures. The procedural nature of our formal representation allows a robot to interpret route instructions online while moving through a previously unknown environment.","Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ) .","['The language grounding problem has come in many different flavors with just as many different approaches.', 'Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007).', 'Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ) .', 'Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a;Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010;Wing and Baldridge, 2011;Roller et al., 2012).']",0,"['Others provide automatic mappings of natural language instructions to executable actions , such as interpreting navigation directions ( Chen and Mooney , 2011 ) or robot commands ( Tellex et al. , 2011 ; #AUTHOR_TAG ) .']"
CC212,D13-1115,Integrating Theory and Practice: A Daunting Task,congruent embodied representations for visually presented actions and linguistic phrases describing actions,"['Lisa Aziz-Zadeh', 'Stephen M Wilson', 'Giacomo Rizzolatti', 'Marco Iacoboni']",related work,"The thesis of embodied semantics holds that conceptual representations accessed during linguistic processing are, in part, equivalent to the sensory-motor representations required for the enactment of the concepts described . Here, using fMRI, we tested the hypothesis that areas in human premotor cortex that respond both to the execution and observation of actions-mirror neuron areas -are key neural structures in these processes. Participants observed actions and read phrases relating to foot, hand, or mouth actions. In the premotor cortex of the left hemisphere, a clear congruence was found between effector-specific activations of visually presented actions and of actions described by literal phrases. These results suggest a key role of mirror neuron areas in the re-enactment of sensory-motor representations during conceptual processing of actions invoked by linguistic stimuli.","The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; #AUTHOR_TAG ) .']"
CC213,D13-1115,Integrating Theory and Practice: A Daunting Task,stochastic variational inference arxiv eprints,"['Matthew Hoffman', 'David M Blei', 'Chong Wang', 'John Paisley']",method,,"To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .","['To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']",5,"['To solve these scaling issues , we implement Online Variational Bayesian Inference ( Hoffman et al. , 2010 ; #AUTHOR_TAG ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.']"
CC214,D13-1115,Integrating Theory and Practice: A Daunting Task,distributional semantics in technicolor,"['Elia Bruni', 'Gemma Boleda', 'Marco Baroni', 'NamKhanh Tran']",introduction,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; #AUTHOR_TAGa ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC215,D13-1115,Integrating Theory and Practice: A Daunting Task,perceptual inference through global lexical similarity,"['Brendan T Johns', 'Michael N Jones']",introduction,"The literature contains a disconnect between accounts of how humans learn lexical semantic representations for words. Theories generally propose that lexical semantics are learned either through perceptual experience or through exposure to regularities in language. We propose here a model to integrate these two information sources. Specifically, the model uses the global structure of memory to exploit the redundancy between language and perception in order to generate inferred perceptual representations for words with which the model has no perceptual experience. We test the model on a variety of different datasets from grounded cognition experiments and demonstrate that this diverse set of results can be explained as perceptual simulation (cf. Barsalou, Simmons, Barbey, & Wilson, 2003) within a global memory model.Copyright (c) 2012 Cognitive Science Society, Inc.","Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .","['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']",0,"['Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; #AUTHOR_TAG ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .', 'Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the ""meaning of words is entirely given by other words"" (Bruni et al., 2012b).']"
CC216,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",experiments,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .","['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']",1,"['Table 1 shows our results for each of our selected models with our compositionality evaluation.', 'The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test).', 'This result is consistent with other works using this model with these features ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .']"
CC217,D13-1115,Integrating Theory and Practice: A Daunting Task,modeling the shape of the scene a holistic representation of the spatial envelope,"['Aude Oliva', 'Antonio Torralba']",related work,"In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.","The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( #AUTHOR_TAG ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"
CC218,D13-1115,Integrating Theory and Practice: A Daunting Task,describing objects by their attributes,"['Ali Farhadi', 'Ian Endres', 'Derek Hoiem', 'David Forsyth']",related work,"We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (""spotty dog"", not just ""dog""); to say something about unfamiliar objects (""hairy and four-legged"", not just ""unknown""); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (""spotty"") or discriminative (""dogs have it but sheep do not""). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attributebased framework. 1","The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .","['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']",0,"['The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( Oliva and Torralba , 2001 ; Lowe , 2004 ; #AUTHOR_TAG ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm Â¨ uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .']"
CC219,D13-1115,Integrating Theory and Practice: A Daunting Task,indexing by latent semantic analysis,"['Scott Deerwester', 'Susan T Dumais', 'George W Furnas', 'Thomas K Landauer', 'Richard Harshman']",related work,"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (""semantic structure"") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.","Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .","['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Within the latter category, the two most common representations have been association norms, where subjects are given a 1 http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)).', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']",0,"['Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead.', 'Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010;Chen and Mooney, 2011;Matuszek et al., 2012;Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies.', 'Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( #AUTHOR_TAG ) in the prediction of association norms .', 'Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks.', 'In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task.', 'Johns and Jones ( 2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity.', 'Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity.']"
CC220,D13-1115,Integrating Theory and Practice: A Daunting Task,online learning for latent dirichlet allocation,"['Matthew Hoffman', 'David M Blei', 'Francis Bach']",method,"We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.","To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .","['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables.', 'The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors.', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.', 'A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source.']",5,"['To solve these scaling issues , we implement Online Variational Bayesian Inference ( #AUTHOR_TAG ; Hoffman et al. , 2012 ) for our models .', 'Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set.', 'Online VBI easily scales and quickly converges in all of our experiments.']"
CC221,D13-1115,Integrating Theory and Practice: A Daunting Task,how many words is a picture worth automatic caption generation for news images,"['Yansong Feng', 'Mirella Lapata']",related work,"In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that pre-processes the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.",The first work to do this with topic models is #AUTHOR_TAGb ) .,"['As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms.', 'The first work to do this with topic models is #AUTHOR_TAGb ) .', 'They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents.', 'The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction.', 'Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation.', 'Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images.', 'More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms.', 'Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008;Regneri et al., 2013).']",0,['The first work to do this with topic models is #AUTHOR_TAGb ) .']
CC222,D13-1115,Integrating Theory and Practice: A Daunting Task,integrating experiential and distributional data to learn semantic representations,"['Mark Andrews', 'Gabriella Vigliocco', 'David Vinson']",introduction,"The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types.Copyright (c) 2009 APA, all rights reserved.","This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .","['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.', 'Finally, we describe two ways to extend the model by incorporating three or more modalities.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.', 'We release both our code and data to the community for future research. 1']",2,"['In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.', 'This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( #AUTHOR_TAG ; Silberer and Lapata , 2012 ) .', 'While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.', 'We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.']"
CC223,D14-1083,Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates,extralinguistic constraints on stance recognition in ideological debates,"['Kazi Saidul Hasan', 'Vincent Ng']",experiments,,"This choice is motivated by an observation we made previously ( #AUTHOR_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3","['In P2, on the other hand, we recast SC as a se- quence labeling task.', 'In other words, we train a SC model that assumes as input a post sequence and outputs a stance sequence, with one stance la- bel for each post in the input post sequence.', 'This choice is motivated by an observation we made previously ( #AUTHOR_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3']",2,"['This choice is motivated by an observation we made previously ( #AUTHOR_TAGa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3']"
CC224,D14-1083,Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates,extralinguistic constraints on stance recognition in ideological debates,"['Kazi Saidul Hasan', 'Vincent Ng']",experiments,,"Following our previous work on stance classification ( #AUTHOR_TAGc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .","['Frame-semantic features.', 'While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence.', 'Following our previous work on stance classification ( #AUTHOR_TAGc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .', 'Frame-word interaction features encode whether two words appear in different elements of the same frame.', 'Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and']",2,"['Following our previous work on stance classification ( #AUTHOR_TAGc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .']"
CC225,D14-1130,Human Effort and Machine Learnability in Computer Aided Translation,the efficacy of human postediting for language translation,"['S Green', 'J Heer', 'C D Manning']",related work,,Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAGa ) comparing scratch translation to post-edit .,"['The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes.', 'However, he used undergraduate, non-professional subjects, and did not consider re-tuning.', 'Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAGa ) comparing scratch translation to post-edit .', 'Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013).', 'However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature.']",2,['Our experimental design with professional bilingual translators follows our previous work #AUTHOR_TAGa ) comparing scratch translation to post-edit .']
CC226,D14-1157,Staying on Topic: An Indicator of Power in Political Debates,power of confidence how poll scores impact topic dynamics in political debates,"['Vinodkumar Prabhakaran', 'Ashima Arora', 'Owen Rambow']",method,"In this paper, we investigate how topic dynamics during the course of an interaction correlate with the power differences between its participants. We perform this study on the US presidential debates and show that a candidate's power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction.",This is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .,"[""Table 1 shows the Pearson's product correlation between each topical feature and candidate's power."", 'We obtain a highly significant (p = 0.002) negative correlation between topic shift tendency of a candidate (PI) and his/her power.', ""In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings."", 'Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'This is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .', 'It is also in line with the findings by Prabhakaran et al. (2013a) that candidates with higher power tend not to interrupt others.', 'On the other hand, we did not obtain any significant correlation for the features proposed by Nguyen et al. (2013).']",1,"[""In other words, the variation in the topic shifting tendencies is significantly correlated with the candidates' recent poll standings."", 'Candidates who are higher up in the polls tend to stay on topic while the candidates with less power attempt to shift topics more often.', 'This is in line with our previous findings from ( #AUTHOR_TAG ) that candidates with higher power attempt to shift topics less often than others when responding to moderators .']"
CC227,D14-1222,A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents,cascading collective classification for bridging anaphora recognition using a rich linguistic feature set,"['Yufang Hou', 'Katja Markert', 'Michael Strube']",introduction,"Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection fea-tures and integrate these into a cascaded mi-nority preference algorithm that models bridg-ing recognition as a subtask of learning fine-grained information status (IS). We substan-tially improve bridging recognition without impairing performance on other IS classes",We follow our previous work ( #AUTHOR_TAGb ) and restrict bridging to non-coreferential cases .,"['Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993;Löbner, 1998).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference).', 'We follow our previous work ( #AUTHOR_TAGb ) and restrict bridging to non-coreferential cases .', 'We also exclude comparative anaphora (Modjeska et al., 2003).']",2,"['Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975;Prince, 1981;Gundel et al., 1993;Lobner, 1998).', 'Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference).', 'We follow our previous work ( #AUTHOR_TAGb ) and restrict bridging to non-coreferential cases .', 'We also exclude comparative anaphora (Modjeska et al., 2003).']"
CC228,D14-1222,A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents,cascading collective classification for bridging anaphora recognition using a rich linguistic feature set,"['Yufang Hou', 'Katja Markert', 'Michael Strube']",experiments,"Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection fea-tures and integrate these into a cascaded mi-nority preference algorithm that models bridg-ing recognition as a subtask of learning fine-grained information status (IS). We substan-tially improve bridging recognition without impairing performance on other IS classes","mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .","['7 In ISNotes, 71% of NP antecedents occur in the same or up to two sentences prior to the anaphor.', 'Initial experiments show that increasing the window size more than two sentences decreases the performance. 8', 'To deal with data imbalance, the SVM light parameter is set according to the ratio between positive and negative instances in the training set. 9', 'To compare the learning-based approach to the rulebased system described in Section 3 directly, we report the mlSystem ruleFeats We provide mlSystem ruleFeats with the same knowledge resources as the rule-based system.', 'All rules from the rule-based system are incorporated into mlSystem ruleFeats as the features.', 'mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .', 'Some of these features overlap with the atomic features used in the rule-based system.']",2,"['mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; #AUTHOR_TAGa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .']"
CC229,E03-1002,Neural network probability estimation for broad coverage parsing,towards historybased grammars using richer models for probabilistic parsing,"['E Black', 'F Jelinek', 'J Lafferty', 'D Magerman', 'R Mercer', 'S Roukos']",method,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.","in history-based models ( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .","['The probability model we use is generative and history-based.', 'Generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'At each step, the process chooses a characteristic of the tree or predicts a word in the sentence.', 'This sequence of decisions is the derivation of the tree, which we will denote d1,..., dm .', ""Because there is a one-to-one mapping from phrase structure trees to our derivations, the probability of a derivation P(di,..., dm) is equal to the joint probability of the derivation's tree and the input sentence."", 'The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation.', 'in history-based models ( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .', 'This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions.']",5,"['The probability model we use is generative and history-based.', 'Generative models are expressed in terms of a stochastic process which generates both the phrase structure tree and the input sentence.', 'in history-based models ( #AUTHOR_TAG ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .']"
CC230,E03-1002,Neural network probability estimation for broad coverage parsing,a maximumentropyinspired parser,['Eugene Charniak'],,,"The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ) .","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ) .']",1,"['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ) .']"
CC231,E03-1002,Neural network probability estimation for broad coverage parsing,headdriven statistical models for natural language parsing,['Michael Collins'],experiments,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG ) .,['7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG ) .'],5,['7A11 our results are computed with the evalb program following the now-standard criteria in ( #AUTHOR_TAG ) .']
CC232,E03-1002,Neural network probability estimation for broad coverage parsing,towards historybased grammars using richer models for probabilistic parsing,"['E Black', 'F Jelinek', 'J Lafferty', 'D Magerman', 'R Mercer', 'S Roukos']",introduction,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.","Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .","['Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001;Henderson, 2000).', 'Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'The resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state-of-the-art for parsing the Penn Treebank.']",0,"['Many statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2001 ) are based on a history-based probability model ( #AUTHOR_TAG ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .']"
CC233,E03-1002,Neural network probability estimation for broad coverage parsing,headdriven statistical models for natural language parsing,['Michael Collins'],,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 ) .","['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length.', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 ) .']",1,"['The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1).', 'Most probability estimation methods require that there be a finite set of features on which the probability is conditioned.', 'The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2000 ) .']"
CC234,E03-1002,Neural network probability estimation for broad coverage parsing,discriminative reranking for natural language parsing,['Michael Collins'],,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG ) .","['The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms.', 'We replace Chomsky adjunction structures (i.e.', 'structures of the form [X [X ...] [Y ...]]) with a special ""modifier"" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]).', 'These transforms are undone before any evaluation is performed on the output trees.', 'We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG ) .""]",0,"[""Collins and Duffy ( 2002 ) define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( #AUTHOR_TAG ) .""]"
CC235,E03-1002,Neural network probability estimation for broad coverage parsing,pcfg models of linguistic tree representations,['Mark Johnson'],,"The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.","For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) .","['The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.', 'For this reason, D(top) includes nodes which are structurally local to top,.', ""These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) .', 'Because these inputs include the history features of both the left- corner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i � 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.', 'Thus this model is making no a priori hard independence assumptions, just a priori soft biases.']",0,"['The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.', 'For this reason, D(top) includes nodes which are structurally local to top,.', 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( #AUTHOR_TAG ) , as has conditioning on the left-corner child ( Roark and Johnson , 1999 ) .', 'Thus this model is making no a priori hard independence assumptions, just a priori soft biases.']"
CC236,E03-1002,Neural network probability estimation for broad coverage parsing,new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron,"['Michael Collins', 'Nigel Duffy']",,"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.","#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .","['The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features.', 'The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand.', 'One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms.', 'We replace Chomsky adjunction structures (i.e.', 'structures of the form [X [X ...] [Y ...]]) with a special ""modifier"" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link.', 'We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]).', 'These transforms are undone before any evaluation is performed on the output trees.', 'We do not believe these transforms have a major impact on performance, but we have not currently run tests without them.', 'feature sets, but then efficiency becomes a problem.', ""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .""]",0,"[""#AUTHOR_TAG define a kernel over parse trees and apply it to re-ranking the output of a parser , but the resulting feature space is restricted by the need to compute the kernel efficiently , and the results are not as good as Collins ' previous work on re-ranking using a finite set of features ( Collins , 2000 ) .""]"
CC237,E03-1002,Neural network probability estimation for broad coverage parsing,a maximum entropy model for partofspeech tagging,['Adwait Ratnaparkhi'],experiments,,We used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system .,"['In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus.', 'We used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system .']",5,"['In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus.', 'We used a publicly available tagger ( #AUTHOR_TAG ) to tag the words and then used these in the input to the system .']"
CC238,E03-1002,Neural network probability estimation for broad coverage parsing,a maximumentropyinspired parser,['Eugene Charniak'],experiments,,"The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']",1,"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; #AUTHOR_TAG ; Collins , 2000 ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.']"
CC239,E03-1002,Neural network probability estimation for broad coverage parsing,headdriven statistical models for natural language parsing,['Michael Collins'],introduction,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .","['Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .', 'A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.', 'Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999;Collins, 1999;Charniak, 2001).', 'In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.', 'We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001;Henderson, 2000).', 'Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation.', 'The resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999;Costa et al., 2001), and only marginally below the current state-of-the-art for parsing the Penn Treebank.']",0,"['Many statistical parsers ( Ratnaparkhi , 1999 ; #AUTHOR_TAG ; Charniak , 2001 ) are based on a history-based probability model ( Black et al. , 1993 ) , where the probability of each decision in a parse is conditioned on the previous decisions in the parse .']"
CC240,E03-1002,Neural network probability estimation for broad coverage parsing,efficient probabilistic topdown and leftcorner parsing,"['Brian Roark', 'Mark Johnson']",,,"For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #AUTHOR_TAG ) .","['The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality.', 'For this reason, D(top) includes nodes which are structurally local to top,.', ""These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #AUTHOR_TAG ) .', 'Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i -1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features.', 'Thus this model is making no a priori hard independence assumptions, just a priori soft biases.']",0,"['For this reason, D(top) includes nodes which are structurally local to top,.', ""These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any)."", 'For right-branching structures , the leftcorner ancestor is the parent , conditioning on which has been found to be beneficial ( Johnson , 1998 ) , as has conditioning on the left-corner child ( #AUTHOR_TAG ) .']"
CC241,E03-1002,Neural network probability estimation for broad coverage parsing,what is the minimal set of fragments that achieves maximal parse accuracy,['Rens Bod'],experiments,"We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precision of 90.8% and a recall of 90.6%). We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy.","The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']",1,"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; Collins , 2000 ; #AUTHOR_TAG ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.']"
CC242,E03-1002,Neural network probability estimation for broad coverage parsing,discriminative reranking for natural language parsing,['Michael Collins'],experiments,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .","['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance.', 'The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model.', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.', 'It is also achieved without any explicit notion of lexical head.']",1,"['The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; Charniak , 2000 ; #AUTHOR_TAG ; Bod , 2001 ) .', 'The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.']"
CC243,E03-1002,Neural network probability estimation for broad coverage parsing,a maximumentropyinspired parser,['Eugene Charniak'],,,"Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .","['In this work we use a method for automatically inducing a finite set of features for representing the derivation history.', 'The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001;Henderson, 2000).', 'The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999).', 'Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .', 'The difference from previous approaches is in the nature of the input to the log-linear model.', 'We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process.', 'These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities.', 'In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.']",1,"['Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( #AUTHOR_TAG ) .']"
CC244,E03-1005,An efficient implementation of a new DOP model,a maximumentropyinspired parser,['E Charniak'],conclusion,,This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .,"['Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', 'Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.', 'While SL-DOP and LS-DOP have been compared before in']",1,"['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11 % relative reduction in error rate over #AUTHOR_TAG and Bods PCFG-reduction reported in Table 1 .', 'Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.']"
CC245,E03-1005,An efficient implementation of a new DOP model,new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron,"['M Collins', 'N Duffy']",introduction,"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.","#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ .","['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", 'Goodman (1996Goodman ( , 1998 developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]",0,"[""#AUTHOR_TAG showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ .""]"
CC246,E03-1005,An efficient implementation of a new DOP model,a new statistical parser based on bigram lexical dependencies,['M Collins'],introduction,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]",0,"[""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to #AUTHOR_TAG .""]"
CC247,E03-1005,An efficient implementation of a new DOP model,discriminative reranking for natural language parsing,['M Collins'],conclusion,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Compared to the reranking technique in #AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .","['Table 2. Results of SL-DOP and LS-DOP on the WSJ (sentences 100 words) Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.', 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', ""This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod's PCFG-reduction reported in Table 1."", 'Compared to the reranking technique in #AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .', 'While SL-DOP and LS-DOP have been compared before in']",1,"['Compared to the reranking technique in #AUTHOR_TAG , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .']"
CC248,E03-1005,An efficient implementation of a new DOP model,discriminative reranking for natural language parsing,['M Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]",0,"[""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to Charniak ( 2000 ) and #AUTHOR_TAG , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]"
CC249,E03-1005,An efficient implementation of a new DOP model,a dop model for semantic interpretation,"['R Bonnema', 'R Bod', 'R Scha']",,"In data-oriented language processing, an annotated language corpus is used as a stochastic grammar. The most probable analysis of a new sentence is constructed by combining fragments from the corpus in the most probable way. This approach has been successfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Tree-bank. If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence. The present paper explains this semantic interpretation method. A data-oriented semantic interpretation algorithm was tested on two semantically annotated corpora: the English ATIS corpus and the Dutch OVIS corpus. Experiments show an increase in semantic accuracy if larger corpus-fragments are taken into consideration.","Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , #AUTHOR_TAG , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.","[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , #AUTHOR_TAG , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']",0,"[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , #AUTHOR_TAG , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.']"
CC250,E03-1005,An efficient implementation of a new DOP model,a maximumentropyinspired parser,['E Charniak'],experiments,,"Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG ) .","['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG ) .', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'This corresponds to a speedup of over 60 times.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']",1,"['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', 'Collins 1996 , Charniak 1997 , Collins 1999 and #AUTHOR_TAG ) .', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'This corresponds to a speedup of over 60 times.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']"
CC251,E03-1005,An efficient implementation of a new DOP model,building a large annotated corpus of english the penn treebank,"['M Marcus', 'B Santorini', 'M Marcinkiewicz']",experiments,"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.","For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .","['For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', 'Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing).', 'We employed the same unknown (category) word model as in Bod (2001), based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 -87).', 'We used ""evalb"" 4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999).', 'We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.']",5,"['For our experiments we used the standard division of the WSJ ( #AUTHOR_TAG ) , with sections 2 through 21 for training ( approx .', '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', 'We used ""evalb"" 4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999).']"
CC252,E03-1005,An efficient implementation of a new DOP model,a new statistical parser based on bigram lexical dependencies,['M Collins'],experiments,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .","['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.', 'as Bod01 and Bon99.', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'This corresponds to a speedup of over 60 times.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']",1,"['4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/', 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '#AUTHOR_TAG , Charniak 1997 , Collins 1999 and Charniak 2000 ) .', '(1996).', 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001Bod ( , 2003, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', 'In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).']"
CC253,E03-1005,An efficient implementation of a new DOP model,a maximumentropyinspired parser,['E Charniak'],introduction,,"But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .","[""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al.'s (1999) and Bod's (2001) estimators on the WSJ."", 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'Bod (2001Bod ( , 2003.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]",0,"['We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', ""But while Bod 's estimator obtains state-of-the-art results on the WSJ , comparable to #AUTHOR_TAG and Collins ( 2000 ) , Bonnema et al. 's estimator performs worse and is comparable to Collins ( 1996 ) .""]"
CC254,E03-1005,An efficient implementation of a new DOP model,discriminative reranking for natural language parsing,['M Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","And #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .","['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]",0,"[""And #AUTHOR_TAG argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"
CC255,E03-1005,An efficient implementation of a new DOP model,new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron,"['M Collins', 'N Duffy']",,"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.","Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .","[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence ."", 'We will refer to these models as Likelihood- DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']",0,"[""Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and #AUTHOR_TAG , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .""]"
CC256,E03-1005,An efficient implementation of a new DOP model,efficient algorithms for parsing the dop model,['J Goodman'],,"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.","Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e.","[""Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']",0,"[""Most DOP models , such as in Bod ( 1993 ) , #AUTHOR_TAG , Bonnema et al. ( 1997 ) , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e."", 'most probable) tree as a candidate for the best tree of a sentence.']"
CC257,E03-1005,An efficient implementation of a new DOP model,treegram parsing lexical dependencies and structural relations,"[""K Sima'an""]",,"This paper explores the kinds of probabilistic relations that are important in syntactic disambiguation. It proposes that two widely used kinds of relations, lexical dependencies and structural relations, have complementary disambiguation capabilities. It presents a new model based on structural relations, the Tree-gram model, and reports experiments showing that structural relations should benefit from enrichment by lexical dependencies.","Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #AUTHOR_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .","['Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #AUTHOR_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .', 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by ""Likelihood-DOP"" the PCFG-reduction of Bod (2001) given in Section 2.2.']",0,"['Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , Bonnema et al. ( 1997 ) , #AUTHOR_TAG and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .']"
CC258,E03-1005,An efficient implementation of a new DOP model,efficient algorithms for parsing the dop model,['J Goodman'],introduction,"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.","Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG , 2002 ) .","['Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', 'Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG , 2002 ) .', 'Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002).']",0,"['Fortunately , there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities , as shown by #AUTHOR_TAG , 2002 ) .']"
CC259,E03-1005,An efficient implementation of a new DOP model,a new statistical parser based on bigram lexical dependencies,['M Collins'],introduction,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .","['Waegner 1992; Pereira and Schabes 1992).', 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .']",4,"['The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', 'This approach has now gained wide usage , as exemplified by the work of #AUTHOR_TAG , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .']"
CC260,E03-1005,An efficient implementation of a new DOP model,a maximumentropyinspired parser,['E Charniak'],introduction,,The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .,"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include non- lexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .', 'And Collins (2000) argues for ""keeping track of counts of arbitrary fragments within parse trees"", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).']",0,"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include non- lexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'The importance of including nonheadwords has become uncontroversial ( e.g. Collins 1999 ; #AUTHOR_TAG ; Goodman 1998 ) .', 'And Collins (2000) argues for ""keeping track of counts of arbitrary fragments within parse trees"", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).']"
CC261,E03-1005,An efficient implementation of a new DOP model,Motivation,new ranking algorithms for parsing and tagging kernels over discrete structures and the voted perceptron,"['M Collins', 'N Duffy']","This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data.","And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .","['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992;Chiang 2000) that do not include nonlexicalized fragments.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]",4,"['The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997;Johnson 1998a).', 'The importance of including nonheadwords has become uncontroversial (e.g.', 'Collins 1999;Charniak 2000;Goodman 1998).', ""And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in #AUTHOR_TAG who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .""]"
CC262,E03-1005,An efficient implementation of a new DOP model,efficient algorithms for parsing the dop model,['J Goodman'],introduction,"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.","And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .","['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar.""]",0,"['Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', 'And subderivations headed by A1 with external nonterminals only at the leaves , internal nonterminals elsewhere , have probability 1/a1 ( #AUTHOR_TAG ) .', ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar.""]"
CC263,E03-1005,An efficient implementation of a new DOP model,efficient algorithms for parsing the dop model,['J Goodman'],introduction,"Excellent results have been reported for Data-Oriented Parsing (DOP) of natural language texts (Bod, 1993). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.","#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .","['One instantiation of DOP which has received considerable interest is the model known as DOP1 2 (Bod 1992).', 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.', ""Johnson (1998b showed that DOP1's subtree estimation method is statistically biased and inconsistent."", 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', 'Cross-validation is needed to avoid this problem.', 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", ""Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ."", ""Goodman (2002) furthermore showed how Bonnema et al.'s (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions.""]",0,"['DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998;Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999;Chappelier et al. 2002)."", ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '#AUTHOR_TAG , 1998 ) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set , thus converting the exponential number of subtrees to a compact grammar .', 'While Goodman\'s method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the ""maximum constituents parse"", i.e. the parse tree which is most likely to have the largest number of correct constituents.']"
CC264,E12-1068,Modeling Inflection and Word-Formation in SMT,srilm  an extensible language modeling toolkit,['Andreas Stolcke'],experiments,"SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools. 1",The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG ) .,"['To evaluate our end-to-end system, we perform the well-studied task of news translation, using the Moses SMT package.', 'We use the English/German data released for the 2009 ACL Workshop on Machine Translation shared task on translation. 7', 'There are 82,740 parallel sentences from news-commentary09.de-en and 1,418,115 parallel sentences from europarl-v4.de-en.', 'The monolingual data contains 9.8 M sentences. 8', 'o build the baseline, the data was tokenized using the Moses tokenizer and lowercased.', 'We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the ""grow-diag-final-and"" heuristic.', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG ) .', 'We run MERT separately for each system.', 'The recaser used is the same for all systems.', 'It is the standard recaser supplied with Moses, trained on all German training data.', 'The dev set is wmt-2009-a and the test set is wmt-2009-b, and we report end-to-end case sensitive BLEU scores against the unmodified reference SGML file.', 'The blind test set used is wmt-2009-blind (all lines).']",5,"['We use GIZA++ to generate alignments, by running 5 iterations of Model 1, 5 iterations of the HMM Model, and 4 iterations of Model 4. We symmetrize using the ""grow-diag-final-and"" heuristic.', 'Our Moses systems use default settings.', 'The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( #AUTHOR_TAG ) .', 'We run MERT separately for each system.']"
CC265,E12-1068,Modeling Inflection and Word-Formation in SMT,productive generation of compound words in statistical machine translation,"['Sara Stymne', 'Nicola Cancedda']",experiments,"In this article we investigate statistical machine translation (SMT) into Germanic languages, with a focus on compound processing. Our main goal is to enable the generation of novel compounds that have not been seen in the training data. We adopt a split-merge strategy, where compounds are split before training the SMT system, and merged after the translation step. This approach reduces sparsity in the training data, but runs the risk of placing translations of compound parts in non-consecutive positions. It also requires a postprocessing step of compound merging, where compounds are reconstructed in the translation output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order and show that it can lead to improvements both by direct inspection and in terms of standard translation evaluation metrics. We also propose several new methods for compound merging, based on heuristics and machine learning, which outperform previously suggested algorithms. These methods can produce novel compounds and a translation with at least the same overall quality as the baseline. For all subtasks we show that it is useful to include part-of-speech based information in the translation process, in order to handle compounds. 1","Following the work of #AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .","['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Two decisions have to be taken: i) where to merge and ii) how to merge.', 'Following the work of #AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006).']",5,"['After translation, compound parts have to be resynthesized into compounds before inflection.', 'Following the work of #AUTHOR_TAG , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .', 'The CRF is trained on the split monolingual data.', 'It only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006).']"
CC266,E12-1068,Modeling Inflection and Word-Formation in SMT,how to avoid burning ducks combining linguistic analysis and corpus statistics for german compound processing,"['Fabienne Fritzinger', 'Alexander Fraser']",related work,"Compound splitting is an important problem in many NLP applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance.","For compound splitting , we follow #AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .","['For compound splitting , we follow #AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne ( 2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)).', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']",5,"['For compound splitting , we follow #AUTHOR_TAG , using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies .']"
CC267,E12-1068,Modeling Inflection and Word-Formation in SMT,factored translation models,"['Philipp Koehn', 'Hieu Hoang']",introduction,"We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level -- may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.","#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .","['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']",0,"['We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model.', '#AUTHOR_TAG showed that the use of a POS factor only results in negligible BLEU improvements , but we need access to the POS in our inflection prediction models .']"
CC268,E12-1068,Modeling Inflection and Word-Formation in SMT,productive generation of compound words in statistical machine translation,"['Sara Stymne', 'Nicola Cancedda']",related work,"In many languages the use of compound words is very productive. A common practice to reduce sparsity consists in splitting compounds in the training data. When this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order. Furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order. We also propose new heuristic methods for merging components that outperform all known methods, and a learning-based method that has similar accuracy as the heuristic method, is better at producing novel compounds, and can operate with no background linguistic resources.","We follow #AUTHOR_TAG , for compound merging .","['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the ge- ometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging ap- proach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow #AUTHOR_TAG , for compound merging .', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merg- ing) on one of our two test sets.']",5,"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the ge- ometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied.', 'We follow #AUTHOR_TAG , for compound merging .']"
CC269,E12-1068,Modeling Inflection and Word-Formation in SMT,failures in englishczech phrasebased mt,"['Ondˇrej Bojar', 'Kamil Kos']",related work,,#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .,"['Given a stem such as brother, Toutanova et.', 'al\'s system might generate the ""stem and inflection"" corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a ""split in preprocessing and resynthesize in postprocessing"" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.', 'al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']",1,"['We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a ""split in preprocessing and resynthesize in postprocessing"" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marino (2008), which deals with verbal morphology and attached pronouns.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', '#AUTHOR_TAG improved on this by marking prepositions with the case they mark ( one of the most important markups in our system ) .']"
CC270,E12-1068,Modeling Inflection and Word-Formation in SMT,empirical methods for compound splitting,"['Philipp Koehn', 'Kevin Knight']",related work,"Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.Comment: 8 pages, 2 figures. Published at EACL 200","Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .","['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']",1,"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags Stymne ( 2008 ) ) or are ( almost ) knowledge-free ( e.g. , #AUTHOR_TAG ) .', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']"
CC271,E12-1068,Modeling Inflection and Word-Formation in SMT,agreement constraints for statistical machine translation into german,"['Philip Williams', 'Philipp Koehn']",related work,"Languages with rich inflectional morphology pose a difficult challenge for statistical machine translation. To address the problem of morphologically inconsistent output, we add unification-based constraints to the target-side of a string-to-tree model. By integrating constraint evaluation into the decoding process, implausible hypotheses can be penalised or filtered out during search. We use a simple heuristic process to extract agreement constraints for German and test our approach on an English-German system trained on WMT data, achieving a small improvement in translation accuracy as measured by BLEU.",#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.,"['Given a stem such as brother, Toutanova et. al�s system might generate the �stem and inflection� corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a map- ping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., ad- jectives) separating his and brother.', 'This required mapping is a significant problem for generaliza- tion.', 'We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection).', 'We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex con- text features.']",1,"['The situation is worse if there are English words (e.g., ad- jectives) separating his and brother.', 'We view this issue as a different sort of prob- lem entirely, one of word-formation (rather than inflection).', 'We apply a split in preprocessing and resynthesize in postprocessing approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to in- flected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', '#AUTHOR_TAG used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex con- text features.']"
CC272,E12-1068,Modeling Inflection and Word-Formation in SMT,efficient parsing of highly ambiguous contextfree grammars with bit vectors,['Helmut Schmid'],introduction,An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one.,"The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .","['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .']",5,"['The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR , a morphological analyzer/generator of German ( Schmid et al. , 2004 ) and the BitPar parser , which is a state-of-the-art parser of German ( #AUTHOR_TAG ) .']"
CC273,E12-1068,Modeling Inflection and Word-Formation in SMT,how to avoid burning ducks combining linguistic analysis and corpus statistics for german compound processing,"['Fabienne Fritzinger', 'Alexander Fraser']",experiments,"Compound splitting is an important problem in many NLP applications which must be solved in order to address issues of data sparsity. Previous work has shown that linguistic approaches for German compound splitting produce a correct splitting more often, but corpus-driven approaches work best for phrase-based statistical machine translation from German to English, a worrisome contradiction. We address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance.","We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .","['We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .', 'First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies.', 'Training data is then stemmed as described in Section 2.3.', 'The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb.', 'In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization.']",5,"['We prepare the training data by splitting compounds in two steps , following the technique of #AUTHOR_TAG .', 'Training data is then stemmed as described in Section 2.3.', 'The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb.']"
CC274,E12-1068,Modeling Inflection and Word-Formation in SMT,combining morphemebased machine translation with postprocessing morpheme prediction,"['Ann Clifton', 'Anoop Sarkar']",related work,,"Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .","['We have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step.', 'The direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation.', 'However, it is reasonable to expect that the use of features (and morphological generation) could also be problematic as this requires the use of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation.', 'Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT.', 'This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing.', 'As parsing performance improves, the performance of linguistic-feature-based approaches will increase.', 'Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .', 'However, this does not deal directly with linguistic features marked by inflection.', 'In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features.', 'So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.']",1,"['Virpioja et al. ( 2007 ) , Badr et al. ( 2008 ) , Luong et al. ( 2010 ) , #AUTHOR_TAG , and others are primarily concerned with using morpheme segmentation in SMT , which is a useful approach for dealing with issues of word-formation .']"
CC275,E12-1068,Modeling Inflection and Word-Formation in SMT,experiments in morphosyntactic processing for translating to and from german,['Alexander Fraser'],related work,We describe two shared task systems and associated experiments. The German to English system used reordering rules ap-plied to parses and morphological split-ting and stemming. The English to Ger-man system used an additional translation step which recreated compound words and generated morphological inflection,#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .,"['Given a stem such as brother, Toutanova et.', 'al\'s system might generate the ""stem and inflection"" corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a ""split in preprocessing and resynthesize in postprocessing"" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et.', 'al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', 'Koehn and Hoang (2007) introduced factored SMT.', 'We use more complex context features.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']",1,"['The only work that we are aware of which deals with both issues is the work of de Gispert and Marino (2008), which deals with verbal morphology and attached pronouns.', '#AUTHOR_TAG tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms .']"
CC276,E12-1068,Modeling Inflection and Word-Formation in SMT,factored translation models,"['Philipp Koehn', 'Hieu Hoang']",related work,"We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level -- may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.",#AUTHOR_TAG introduced factored SMT .,"['Given a stem such as brother, Toutanova et. al�s system might generate the �stem and inflection� corresponding to and his brother.', 'Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required.', 'The situation is worse if there are English words (e.g., adjectives) separating his and brother.', 'This required mapping is a significant problem for generalization.', 'We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'We apply a �split in preprocessing and resynthesize in postprocessing� approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al.', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', 'There has been other work on solving inflection.', '#AUTHOR_TAG introduced factored SMT .', 'We use more complex context features.', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', 'Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system).', 'Both efforts were ineffective on large data sets.', 'Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model.', 'Our CRF framework allows us to use more complex context features.']",1,"['We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection).', 'The only work that we are aware of which deals with both issues is the work of de Gispert and Marin__o (2008), which deals with verbal morphology and attached pronouns.', '#AUTHOR_TAG introduced factored SMT .', 'Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms.', 'Both efforts were ineffective on large data sets.']"
CC277,E12-1068,Modeling Inflection and Word-Formation in SMT,syntaxtomorphology mapping in factored phrasebased statistical machine translation from english to turkish,"['Reyyan Yeniterzi', 'Kemal Oflazer']",related work,"We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.","Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #AUTHOR_TAG and others .","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #AUTHOR_TAG and others .', 'Toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'Using additional source side information beyond the markup did not produce a gain in performance.']",1,"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of Avramidis and Koehn ( 2008 ) , #AUTHOR_TAG and others .']"
CC278,E12-1068,Modeling Inflection and Word-Formation in SMT,enriching morphologically poor languages for statistical machine translation,"['Eleftherios Avramidis', 'Philipp Koehn']",related work,"We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For English-Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%.","Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .","['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .', 'Toutanova et.', ""al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information."", 'Using additional source side information beyond the markup did not produce a gain in performance.']",1,"['Much previous work looks at the impact of using source side information ( i.e. , feature functions on the aligned English ) , such as those of #AUTHOR_TAG , Yeniterzi and Oflazer ( 2010 ) and others .']"
CC279,E12-1068,Modeling Inflection and Word-Formation in SMT,german compounds in factored statistical machine translation,['Sara Stymne'],related work,"Abstract. An empirical method for splitting German compounds is explored by varying it in a number of ways to investigate the consequences for factored statistical machine translation between English and German in both directions. Compound splitting is incorporated into translation in a preprocessing step, performed on training data and on German translation input. For translation into German, compounds are merged based on part-of-speech in a postprocessing step. Compound parts are marked, to separate them from ordinary words. Translation quality is improved in both translation directions and the number of untranslated words in the English output is reduced. Different versions of the splitting algorithm performs best in the two different translation directions.","Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .","['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .', 'Compound merging is less well studied.', 'Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list.', 'This approach resulted in too many compounds.', 'We follow Stymne and Cancedda (2011), for compound merging.', 'We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.']",1,"['For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies.', 'Other approaches use less deep linguistic resources ( e.g. , POS-tags #AUTHOR_TAG ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .', 'Compound merging is less well studied.', 'We follow Stymne and Cancedda (2011), for compound merging.']"
CC280,E14-1023,Frame Semantic Tree Kernels for Social Network Extraction from Text,automatic detection and classification of social events,"['Apoorv Agarwal', 'Owen Rambow']",related work,"In this paper we introduce the new task of social event extraction from text. We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact. We annotate part of Automatic Content Extraction (ACE) data, and perform experiments using Support Vector Machines with Kernel methods. We use a combination of structures derived from phrase structure trees and dependency trees. A characteristic of our events (which distinguishes them from ACE events) is that the participating entities can be spread far across the parse trees. We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classification tasks. We also use three data sampling approaches to solve the problem of data skewness. Sampling methods improve the F1-measure for the task of relation detection by over 20% absolute over the baseline.","Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .","['Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .', 'Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper.', 'Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003;Culotta and Sorensen, 2004).', 'To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005).', 'Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees.', 'They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees.', 'We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by Harabagiu et al. (2005).']",2,"['Our approach to extract and classify social events builds on our previous work ( #AUTHOR_TAG ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .']"
CC281,E99-1022,Selective magic HPSG parsing,the logic of typed feature structures  with applications to unification grammars logic programs and constraint resolution,['Bob Carpenter'],,,2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ) .,"['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]",0,"['2This view of typed feature structures differs from the perspective on typed feature structures as modeling partial information as in ( #AUTHOR_TAG ) .', ""Typed feature structures as normal form ir~'~Eterms are merely syntactic objects.""]"
CC282,E99-1022,Selective magic HPSG parsing,offline compilation for efficient processing with constraintlogic grammars,['Guido Minnen'],introduction,,I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG ) .,['I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG ) .'],0,['I A more detailed discussion of various aspects of the proposed parser can be found in ( #AUTHOR_TAG ) .']
CC283,E99-1022,Selective magic HPSG parsing,ale — the attribute logic engine users guide version 202,"['Bob Carpenter', 'Gerald Penn']",,"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ...","Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown .","['Combining control strategies depends on a way to differentiate between types of constraints.', ""Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", 'In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', 'A literal (goal) is considered a parse lype literal (goal) if it has as its single argument a typed feature structure of a type specified as a parse type.', '1° All types in the type hierarchy can be used as parse types.', 'This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.', 'However, in the remainder we will concentrate on a specific class of parse types: We assume the specification of type sign and its subtypes as parse types.', '11 This choice is based on the observation that the constraints on type sign and its sub-types play an important guiding role in the parsing process and are best interpreted bottom-up given the lexical orientation of I-IPSG.', ""The parsing process corresponding to such a parse type specification is represented schematically in figure 8. Starting from the lexical entries, i. e., word word word Figure 8: Schematic representation of the selective magic parsing process the :r~'L definite clauses that specify the word objects in the grammar, phrases are built bottomup by matching the parse type literals of the definite clauses in the grammar against the edges in the table."", 'The non-parse type literals are processed according to the top-down control strategy 1°The notion of a parse type literal is closely related to that of a memo literal as in (Johnson and DSrre, 1995).']",0,"[""Proceedings of EACL '99 example , the ALE parser ( #AUTHOR_TAG ) presupposes a phrase structure backbone which can be used to determine whether a constraint is to be interpreted bottom-up or topdown ."", 'In the case of selective magic parsing we use so-called parse types which allow the user to specify how constraints in the grammar are to be interpreted.', '1deg All types in the type hierarchy can be used as parse types.', 'This way parse type specification supports a flexible filtering component which allows us to experiment with the role of filtering.']"
CC284,E99-1022,Selective magic HPSG parsing,magic for filter optimization in dynamic bottomup processing,['Guido Minnen'],introduction,,"As shown in ( #AUTHOR_TAG ) â¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .","['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See, among others, (Ramakrishnan et al. 1992).', 'As shown in ( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']",0,"['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See, among others, (Ramakrishnan et al. 1992).', 'As shown in ( #AUTHOR_TAG ) â\x80¢ The presented research was carried out at the University of Tubingen , Germany , as part of the Sonderforschungsbereich 340 .']"
CC285,E99-1022,Selective magic HPSG parsing,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG ),"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG )', '3 (Meurers and Minnen, 1997) propose a compilation of lexical rules into T~r/: definite clauses 2This view of typed feature structures differs from the perspective on typed feature structures as modehng partial information as in (Carpenter, 1992).', ""Typed feature structures as normal form ir~'~E terms are merely syntactic objects.""]",0,['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; #AUTHOR_TAG )']
CC286,E99-1022,Selective magic HPSG parsing,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",,,Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG ) .,"['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG ) .', '3 ( Meurers and Minnen , 1997 ) propose a compilation of lexical rules into TIT definite clauses which are used to restrict lexical entries.', '(GStz and Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.4', 'Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above men- tioned techniques for compiling an HPSG theory into typed feature grammars.']",0,['Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( #AUTHOR_TAG ) .']
CC287,E99-1022,Selective magic HPSG parsing,efficient bottomup evaluation of logic programs,"['Raghu Ramakrishnan', 'Divesh Srivastava', 'S Sudarshan']",introduction,"In recent years, much work has been directed towards evaluating logic programs and queries on deductive databases by using an iterative bottom-up fixpoint computation. The resulting techniques offer an attractive alternative to Prolog-style top-down evaluation in several situations. They are sound and complete for positive Horn clause programs, are well-suited to applications with large volumes of data (facts), and can support a variety of extensions to the standard logic programming paradigm.","See , among others , ( #AUTHOR_TAG ) .","['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , ( #AUTHOR_TAG ) .', 'As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv�:; GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (GStz and Meurers, 1997a) and (Meurers and Minnen, 1997).', 'Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.']",0,"['Magic is a compilation technique originally developed for goal-directed bottom-up processing of logic programs.', 'See , among others , ( #AUTHOR_TAG ) .', 'As shown in (Minnen, 1996) magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv:; GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) as discussed in (GStz and Meurers, 1997a) and (Meurers and Minnen, 1997).', 'Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.']"
CC288,E99-1022,Selective magic HPSG parsing,interleaving universal principles and relational constraints over typed feature logic,"['Thilo Gotz', 'Detmar Meurers']",,"We introduce a typed feature logic system providing both universal implicational principles as well as definite clauses over feature terms. We show that such an architecture supports a modular encoding of linguistic theories and allows for a compact representation using underspecification. The system is fully implemented and has been used as a workbench to develop and test large HPSG grammars. The techniques described in this paper are not restricted to a specific implementation, but could be added to many current feature-based grammar development systems.",The ConTroll grammar development system as described in ( #AUTHOR_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .,"['aSee (King, 1994) for a discussion of the appropriateness of T~-£: for HPSG and a comparison with other feature logic approaches designed for HPSG.', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in ( #AUTHOR_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']",0,"['aSee (King, 1994) for a discussion of the appropriateness of T~-PS: for HPSG and a comparison with other feature logic approaches designed for HPSG.', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', 'The ConTroll grammar development system as described in ( #AUTHOR_TAGb ) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars .']"
CC289,E99-1022,Selective magic HPSG parsing,prologii manuel de reference et modele theorique,['Alain Colmerauer'],,,"See also ( #AUTHOR_TAG ; Naish , 1986 ) .","['Coroutining appears under many different guises, like for example, suspension, residuation, (goal) freezing, and blocking.', 'See also ( #AUTHOR_TAG ; Naish , 1986 ) .']",0,"['See also ( #AUTHOR_TAG ; Naish , 1986 ) .']"
CC290,E99-1022,Selective magic HPSG parsing,ale — the attribute logic engine users guide version 202,"['Bob Carpenter', 'Gerald Penn']",introduction,"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ...",As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG ) .,"['The proposed parser is related to the so-called Lemma Table deduction system (Johnson and DSrre, 1995) which allows the user to specify whether top-down sub-computations are to be tabled.', ""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.', 'feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goM-directed interpretation of magic-compiled typed feature grammars.']",1,"[""In contrast to Johnson and DSrre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies."", 'As such it resembles the parser of the grammar development system Attribute Language Engine ( ALE ) of ( #AUTHOR_TAG ) .', 'Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered.']"
CC291,E99-1022,Selective magic HPSG parsing,interleaving universal principles and relational constraints over typed feature logic,"['Thilo Gotz', 'Detmar Meurers']",introduction,"We introduce a typed feature logic system providing both universal implicational principles as well as definite clauses over feature terms. We show that such an architecture supports a modular encoding of linguistic theories and allows for a compact representation using underspecification. The system is fully implemented and has been used as a workbench to develop and test large HPSG grammars. The techniques described in this paper are not restricted to a specific implementation, but could be added to many current feature-based grammar development systems.","Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAGa ) and ( Meurers and Minnen , 1997 ) .","['magic is an interesting technique with respect to natural language processing as it incorporates filtering into the logic underlying the grammar and enables elegant control independent filtering improvements.', 'In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (Tgv£:;GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAGa ) and ( Meurers and Minnen , 1997 ) .', 'Typed feature grammar constraints that are inexpensive to resolve are dealt with using the top-down interpreter of the ConTroll grammar development system (GStz and Meurers, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation.']",2,"['In this paper we investigate the selective application of magic to typed feature grammars a type of constraint-logic grammar based on Typed Feature Logic (TgvPS:;GStz, 1995).', 'Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( #AUTHOR_TAGa ) and ( Meurers and Minnen , 1997 ) .']"
CC292,E99-1022,Selective magic HPSG parsing,typed feature structures as descriptions,['Paul King'],,"A description is an entity that can be interpreted as true or false of an object, and using feature structures as descriptions accrues several computational benefits. In this paper, I create an explicit interpretation of a typed feature structure used as a description, define the notion of a satisfiable feature structure, and create a simple and effective algorithm to decide if a feature structure is satisfiable.Comment: COLING 94 reserve paper, 5 pages, LaTeX (no .sty exotica",` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .,"['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'append ([~,[~,[~).', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.', '4 Because of space limitations we have to refrain from an example.', 'The ConTroll grammar development system as described in (GStz and Meurers, 1997b) implements the above mentioned techniques for compiling an HPSG theory into typed feature grammars.']",0,"['` See ( #AUTHOR_TAG ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .', 'Meurers, 1997b) describe a method for compiling implicational constraints into typed feature grammars and interleaving them with relational constraints.']"
CC293,J00-1004,Learning Dependency Translation Models as Collections of Finite-State Head Transducers,machine translation divergences a formal description and proposed solution,['B J Dorr'],method,"There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system.","This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .","['It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages.', 'Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus.', 'For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching.', 'This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']",1,"['This contrasts with one of the traditional approaches ( e.g. , #AUTHOR_TAG ; Watanabe 1995 ) to posing the translation problem , i.e. , the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language .']"
CC294,J00-1004,Learning Dependency Translation Models as Collections of Finite-State Head Transducers,englishtomandarin speech translation with head transducers,"['Hiyan Alshawi', 'Fei Xia']",,"We describe the head transducer model used in an experimental English-toMandarin speech translation system. Head transduction is a translation method in which weighted finite state transducers are associated with sourcetarget word pairs. The method is suitable for speech translation because it allows efficient bottom up processing. The head transducers in the experimental system have a wider range of output positions than input positions. This asymmetry is motivated by a tradeoff between model complexity and search efficiency. 1 I n t r o d u c t i o n In this paper we describe the head transducer model used for translation in an experimental English-to-Mandarin speech translation system. Head transducer models consist of collections of weighted finite state transducers associated with pairs of lexical items in a bilingual lexicon. Head transducers operate ""outwards"" from the heads of phrases; they convert the left and right dependents of a source word into the left and right dependents of a corresponding target word. The transducer model can be characterized as a statistical translation model, but unlike the models proposed by Brown et al. (1990, 1993), these models have non-uniform linguistically motivated structure, at present coded by hand. The underlying linguistic structure of these models is similar to dependency grammar (Hudson 1984), although dependency representations are not explicitly constructed in our approach to translation. The original motivation for the head transducer models was Fei Xia D e p a r t m e n t of C o m p u t e r and I n f o r m a t i o n Science Univers i ty of P e n n s y l v a n i a Ph i l ade lph ia , PA 19104, USA fx i aQc i s .upenn . edu that they are simpler and more amenable to automatic model structure acquisition as compared with earlier transfer models. We first describe the head transduction approach in general in Section 2. In Section 3 we explain properties of the particular head transducers used in the experimental English-to-Mandarin speech translator. In Section 4, we explain how head transducers help satisfy the requirements of the speech translation application, and we conclude in Section 5. 2 B i l i n g u a l H e a d T r a n s d u c t i o n 2.1 Bilingual Head Transducers A head transducer M is a finite state machine associated with a pair of words, a source word w and a target word v. In fact, w is taken from the set V1 consisting of the source language vocabulary augmented by the ""empty word"" e, and v is taken from V~, the target language vocabulary augmented with e. A head transducer reads from a pair of source sequences, a left source sequence L1 and a right source sequence Rt; it writes to a pair of target sequences, a left target sequence L2 and a right target sequence R2 (Figure 1). Head transducers were introduced in Alshawi 1996b, where the symbols in the source and target sequences are source and target words respectively. In the model described in this paper, the symbols written are dependency relation symbols, or the empty symbol e. The use of relation symbols here is a result of the historical development of the system from an earlier transfer model. A conceptually simpler translator can be built using head transducer models with only lexical items, in which case the distinction between different dependents is implicit in the state of a transducer. In head transducer models, the use of relations corresponds to a type of class-based model (cf Je-","In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .","['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']",5,"['In the transducers produced by the training method described in this paper , the source and target positions are in the set -LCB- -1 , 0,1 -RCB- , though we have also used handcoded transducers ( #AUTHOR_TAG ) and automatically trained transducers ( Alshawi and Douglas 2000 ) with a larger range of positions .']"
CC295,J00-1004,Learning Dependency Translation Models as Collections of Finite-State Head Transducers,a statistical approach to machine translation,"['P J Brown', 'J Cocke', 'S A Della Pietra', 'V J Della Pietra', 'J Lafferty', 'R L Mercer', 'P Rossin']",conclusion,"Statistical Machine Translation has successfully been used for translation between many language pairs contributing to its popularity in recent years. It has however not been used for the  English/Persian language pair. This paper presents the first such attempt and describes the problems faced in creating a corpus and building a base line system. Our experience with  the construction of a parallel corpus during this ongoing study and the problems encountered especially with the process of alignment are discussed in this paper. The prototype  constructed and its evaluation using the BiLingual Evaluation Understudy (BLEU) is briefly described and results are analyzed. In the final part of the paper, conclusions are drawn  and work planned for the future is discussed","At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .","['At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .', 'One advantage is that our method attempts to model the natural decomposition of sentences into phrases.', 'Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model.', 'In particular, our search algorithm finds optimal transductions of test sentences in less than ""real time"" on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application.']",1,"['At the same time , we believe our method has advantages over the approach developed initially at IBM ( #AUTHOR_TAG ; Brown et al. 1993 ) for training translation systems automatically .']"
CC296,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,planning text for advisory dialogues,"['Johanna D Moore', 'Cecile Paris']",,"Explanation is an interactive process requiring a dialogue between advice-giver and advice-seeker. In this paper, we argue that in order to participate in a dialogue with its users, a generation system must be capable of reasoning about its own utterances and therefore must maintain a rich representation of the responses it produces. We present a text planner that constructs a detailed text plan, containing the intentional, attentional, and rhetorical structures of the text it generates.","1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG , 203 ) .","['IGEN constructs its plans using a hierarchical planning algorithm (Nilsson 1980).', 'The planner first checks all of its top-level plans to see which have effects that match the goal.', ""Each matching plan's preconditions are checked; if they are currently (believed to be) true, the planner then attempts to find all instantiations of the plan's body."", ""1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987)."", 'In IGEN, the plans can involve any goals or actions that could be achieved via communication.']",0,"['The planner first checks all of its top-level plans to see which have effects that match the goal.', ""1 Â° The body of a plan can be an action or sequence of actions , a goal or sequence 9 Moore and Paris also note that `` a generation system must maintain the kinds of information outlined by Grosz and Sidner '' ( #AUTHOR_TAG , 203 ) ."", ""Their planner uses plan structures similar to IGEN's, except that the plan operators they use are generally instantiations of rhetorical relations drawn from Rhetorical Structure Theory (Mann and Thompson 1987)."", 'In IGEN, the plans can involve any goals or actions that could be achieved via communication.']"
CC297,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,building another bridge over the generation gap,['Leo Wanner'],,"In this paper, we address one of the central problems in text generation: the missing link (&quot;the generation gap&quot; in Meteer&apos;s terms) between the global discourse organization as often provided by text planning modules and the linguistic realization of this organiza- tion. We argue that the link should be established by the lexical choice process using resources derived from Mel&apos;iuk&apos;s ezicel Functions (LFs). In particular, we demonstrate that sequences of LFs may well serve as lexical discourse structure relations which link up to global discourse relations in the output of a Rhelorical Structure Theory style text planner","McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG ) .","['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']",0,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( Meteer 1994 ; Panaget 1994 ; #AUTHOR_TAG ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"
CC298,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,has a consensus nl generation architecture appeared and is it psycholinguistically plausible,['Ehud Reiter'],,"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems","In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ) .","['The opposite approach is to simply ignore the limitations of a modular design and proceed as if there need be no interactions between the components.', 'Whatever problems result will be handled as best they can, on a case-by-case basis.', 'This approach is the one taken (implicitly or explicitly) in the majority of generators.', 'In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ) .', 'While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear.', ""Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity.""]",0,"['In fact , Reiter has even argued in favor of this approach , claiming that the interactions are sufficiently minor to be ignored ( or at least handled on an ad hoc basis ) ( #AUTHOR_TAG ) .', ""Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity.""]"
CC299,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating event descriptions with sage a simulation and generation environment,['Marie W Meteer'],,"The SAGE system (Simulation and Generation Environment) was developed to address issues at the interface between conceptual modelling and natural language generation. In this paper, I describe SAGE and its components in the context of event descriptions. I show how kinds of information, such as the Reichenbachian temporal points and event structure, which are usually treated as unified systems, are often best represented at multiple levels in the overall system. SAGE is composed of a knowledge representation language and simulator, which form the underlying model and constitute the ""speaker""; a graphics component, which displays the actions of the simulator and provides an anchor for locative and deictic relations; and the generator SPOKESMAN, which produces a textual narration of events.","McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .","['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']",0,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', 'McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( #AUTHOR_TAG ; Panaget 1994 ; Wanner 1994 ) .', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"
CC300,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating natural language linder pragmatic constraints lawrence erlbaum,['Eduard H Hovy'],,,"Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAGa ) .","['The point here is not just that IGEN can produce different lexical realizations for a particular concept.', 'If that were the only goal, we could dispense with the feedback mechanism and simply design some sort of discrimination network (or similar device) to test various features of the information being expressed.', 'The planner could supply whatever information is needed to drive the network.', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAGa ) .']",0,"['The point here is not just that IGEN can produce different lexical realizations for a particular concept.', 'Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; #AUTHOR_TAGa ) .']"
CC301,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,an overview of the nigel text generation grammar,['William C Mann'],,"Research on the text generation task has led to creation of a large systemic grammar of English, Nigel, which is embedded in a computer program. The grammar and the systemic framework have been extended by addition of a semantic stratum. The grammar generates sentences and other units under several kinds of experimental control.This paper describes augmentations of various precedents in the systemic framework. The emphasis is on developments which control the text to fulfill a purpose, and on characteristics which make Nigel relatively easy to embed in a larger experimental program.","These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) .","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']",0,"[""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( #AUTHOR_TAG ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']"
CC302,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,telegram a grammar formalism for language planning,['Douglas E Appelt'],,,"These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) .","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.5']",0,"[""These include devices such as interleaving the components ( McDonald 1983 ; #AUTHOR_TAG ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( Hovy 1988a , 1988c ) ."", 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.5']"
CC303,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,intentions structure and expression in multilingual instructions,"['C6cile L Paris', 'Donia R Scott']",,"Instructional tex-ts have been the object of many studies recently, motivated by the increased need to produce manuals (especially multilingual manuals) coupled with the cost of translators and technical writers. Because these studies concentrate on aspects other than the linguistic realismion of instructions for example, the integration of text and graphi c s they all generate a sequence of steps required to achieve a task, using imperatives. Our research so flushows, however, that manuals can iu fact have different styles, i.e., not all instructions are stated using a sequence of imperatives, and that, furthermore, different parts of manuals often use different styles. In this paper, we present our preliminary results from an analysis of over 30 user guides/manuals for consumer appliances and discuss some of the implications.","This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .","['One possible response would be to abandon the separation; the generator could be a single component that handles all of the work.', 'This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .']",0,"['This approach has occasionally been taken , as in Kantrowitz and Bates ( 1992 ) and Danlos ( 1987 ) and , at least implicitly , in #AUTHOR_TAG and Delin et al. ( 1994 ) ; however , under this approach , all of the flexibility and simplicity of modular design is lost .']"
CC304,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating natural language linder pragmatic constraints lawrence erlbaum,['Eduard H Hovy'],,,"These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) .","['There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components.', ""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) ."", 'All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance.', 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']",0,"[""These include devices such as interleaving the components ( McDonald 1983 ; Appelt 1983 ) , backtracking on failure ( Appelt 1985 ; Nogier 1989 ) , allowing the linguistic component to interrogate the planner ( Mann 1983 ; Sondheimer and Nebel 1986 ) , and Hovy 's notion of restrictive ( i.e. , bottom-up ) planning ( #AUTHOR_TAGa , 1988c ) ."", 'The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.', '5']"
CC305,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating natural language linder pragmatic constraints lawrence erlbaum,['Eduard H Hovy'],,,Hovy has described another text planner that builds similar plans ( #AUTHOR_TAGb ) .,"['Hovy has described another text planner that builds similar plans ( #AUTHOR_TAGb ) .', 'This system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern; it is thus not a planner in the sense used here (as Hovy makes clear).', '10 Since text planning was not the primary focus of this work, IGEN is designed to simply assume that any false preconditions are unattainable.', ""IGEN's planner divides the requirements of a plan into two parts: the preconditions, which are not planned for, and those in the plan body, which are."", 'This has no .']",0,['Hovy has described another text planner that builds similar plans ( #AUTHOR_TAGb ) .']
CC306,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,has a consensus nl generation architecture appeared and is it psycholinguistically plausible,['Ehud Reiter'],,"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems",Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG ) .,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary; they have been called ""strategic"" and ""tactical"" components (e.g., McKeown 1985;Thompson 1977;Danlos 1987) 1, ""planning"" and""realization"" (e.g., McDonald 1983;Hovy 1988a), or simply ""what to say"" versus ""how to say it"" (e.g., Danlos 1987;Reithinger 1990).', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994;Panaget 1994;Wanner 1994).', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG ) .']",0,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', 'Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation ( #AUTHOR_TAG ) .']"
CC307,J00-2001,Integrating Text Planning and Linguistic Choice Without Abandoning Modularity: The IGEN Generator,generating natural language linder pragmatic constraints lawrence erlbaum,['Eduard H Hovy'],,,"The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .","['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']",0,"['Research in natural language generation has generally separated the task into distinct text planning and linguistic components.', 'The text planning component selects and organizes the information to be expressed in some internal representation and then sends it to the linguistic component, which converts the information from the internal form into grammatical sentences of some natural language.', 'The names given to the components vary ; they have been called ""strategic"" and ""tactical"" components ( e.g. , McKeown 1985 ; Thompson 1977 ; Danlos 1987 ) 1 , ""planning"" and ""realization"" ( e.g. , McDonald 1983 ; #AUTHOR_TAGa ) , or simply ""what to say"" versus ""how to say it"" ( e.g. , Danlos 1987 ; Reithinger 1990 ) .', 'The precise division of work between the components can also vary, as can the extent to which the text planner is responsible for selecting (as opposed to merely organizing) the information to be expressed.', 'Much (if not most) work in generation, though, continues to rely on this modular approach for its basic design.', ""For example, DIOGENES (Nirenburg et al. 1988), EPICURE (Dale 1989), SPOKESMAN (Meteer 1989), Sibun's work on local organization of text (Sibun 1991), and COMET (Fisher and McKeown 1990) all are organized this way."", 'McDonald has even argued for extending the model to a large number of components (McDonald 1988), and several systems have indeed added an additional component between the planner and the linguistic component (Meteer 1994; Panaget 1994; Wanner 1994). Reiter describes a pipelined modular approach as a consensus architecture underlying most recent work in generation (Reiter 1994).']"
CC308,J00-3002,Incremental Processing and Acceptability,geometry of lexicosyntactic interaction,['Glyn Morrill'],,,"Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and #AUTHOR_TAG .","['Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and #AUTHOR_TAG .', 'Still, at the risk of proceeding at a slightly slower pace, we aim nonetheless to include here enough details to make the present paper self-contained.']",0,"['Surveys and articles on the topic include Lamarche and Retord ( 1996 ) , de Groote and Retord ( 1996 ) , and #AUTHOR_TAG .']"
CC309,J00-3002,Incremental Processing and Acceptability,parsing and derivational equivalence,"['Mark Hepple', 'Glyn Morrill']",,"It is a tacit assumption of much linguistic inquiry that all distinct derivations of a string should assign distinct meanings. But despite the tidiness of such derlvational uniqueness, there seems to be no a priori reason to assume that a gramma r must have this property. If a grammar exhibits derivational equivalence, whereby distinct derivations of a string assign the same meanings, naive exhaustive search for all derivations will be redundant, and quite possibly intractable. In this paper we show how notions of derivation-reduction and normal form can be used to avoid unnecessary work while parsing with grammars exhibiting derivational equivalence. With grammar regarded as analogous to logic, derivations are proofs; what we are advocating is proof-reduction, and normal form proof; the invocation of these logical techniques adds a further paragraph to the story of parsing-as-deduction","An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .","['(17) By a result of Zielonka (1981), the Lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of Lambek calculus.', 'Combinatory categorial grammar does not concern itself with the capture of all (or only) the concatenatively valid combinatory schemata, but rather with incrementality, for example, on a shiftreduce design.', 'An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']",0,"['An approach ( also based on regulation of the succession of rule application ) to the associated problem of spurious ambiguity is given in #AUTHOR_TAG but again , to our knowledge , there is no predictive relation between incremental combinatory processing and the kind of processing phenomena cited in the introduction .']"
CC310,J00-3002,Incremental Processing and Acceptability,parsing as natural deduction,['Esther Konig'],,"The logic behind parsers for categorial grammars can be formalized in several different ways. Lambek Calculus (LC) constitutes an example for a natural deduction1 style parsing method.In natural language processing, the task of a parser usually consists in finding derivations for all different readings of a sentence. The original Lambek Calculus, when it is used as a parser/theorem prover, has the undesirable property of allowing for the derivation of more than one proof for a reading of a sentence, in the general case.In order to overcome this inconvenience and to turn Lambek Calculus into a reasonable parsing method, we show the existence of ""relative"" normal form proof trees and make use of their properties to constrain the proof procedure in the desired way.","One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .","['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .', 'Each sequent has a distinguished category formula (underlined) on which rule applications are keyed: In the regulated calculus there is no spurious ambiguity, and provided there is no explicit or implicit antecedent product, i.e., provided .L is not needed, F ~ A is a theorem of the Lambek calculus iff F ~ A is a theorem of the regulated calculus.', 'However, apart from the issue regarding .L, there is a general cause for dissatisfaction with this approach: it assumes the initial presence of the entire sequent to be proved, i.e., it is in principle nonincremental; on the other hand, allowing incrementality on the basis of Cut would reinstate with a vengeance the problem of spurious ambiguity, for then what are to be the Cut formulas?', 'Consequently, the sequent approach is ill-equipped to address the basic asymmetry of language--the asymmetry of its processing in time---and has never been forwarded in a model of the kind of processing phenomena cited in the introduction.']",0,"['One approach to this problem consists in defining , within the Cut-free atomic-id space , normal form derivations in which the succession of rule application is regulated ( #AUTHOR_TAG , Hepple 1990 , Hendriks 1993 ) .']"
CC311,J00-3003,Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,empirical studies on the disambiguation of cue phrases,"['Julia B Hirschberg', 'Diane J Litman']",,"Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse. For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also has one or more alternate uses. While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed. This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech",It is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .,"['DA classification using words is based on the observation that different DAs use distinctive word strings.', 'It is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .', 'Similarly, we find distinctive correlations between certain phrases and DA types.', 'For example, 92.4% of the uh-huh\'s occur in BACKCHANNELS, and 88.4% of the trigrams ""<start> do you"" occur in YES-NO-QUESTIONS.', 'To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type.', '5.1.1', 'Classification from True Words.', 'Assuming that the true (hand-transcribed) words of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) in a straightforward way, by building a statistical language model for each of the 42 DAs.', 'All DAs of a particular type found in the training corpus were pooled, and a DA-specific trigram model was estimated using standard techniques (Katz backoff [Katz 1987] with Witten-Bell discounting [Witten and Bell 1991]).']",4,['It is known that certain cue words and phrases ( #AUTHOR_TAG ) can serve as explicit indicators of discourse structure .']
CC312,J00-3003,Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,automatic grammar induction and parsing free text a transformationbased approach,['Eric Brill'],,"In this paper we describe a new technique for parsing free text: a transformational grammar1 is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.","A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #AUTHOR_TAG ) .","['All the work mentioned so far uses statistical models of various kinds.', 'As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way.', 'However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification.', 'A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #AUTHOR_TAG ) .', 'Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed.']",1,"['A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( #AUTHOR_TAG ) .']"
CC313,J00-3003,Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,a stochastic parts program and noun phrase parser for unrestricted text,['Kenneth Ward Church'],,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,"The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG ) .","['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG ) .', 'It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']",1,"['The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( #AUTHOR_TAG ) .']"
CC314,J00-3003,Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech,automatic stochastic tagging of natural language texts,"['Evangelos Dermatas', 'George Kokkinakis']",,"Five language and tagset independent stochastic taggers, handling morphological and contextual information, are presented and tested in corpora of seven European languages (Dutch, English, French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small set containing the eleven main grammatical classes and a large set of grammatical categories common to all languages. The unknown words are tagged using an experimentally proven stochastic hypothesis that links the stochastic behavior of the unknown words with that of the less probable known words. A fully automatic training and tagging program has been implemented on an IBM PC-compatible 80386-based computer. Measurements of error rate, time response, and memory requirements have shown that the taggers' performance is satisfactory, even though a small training text is available. The error rate is improved when new texts are used to update the stochastic model parameters.","It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .","['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .', 'To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n.', 'We can compute the per-utterance posterior DA probabilities by summing:']",0,"['The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).', 'It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( #AUTHOR_TAG ) .']"
CC315,J00-4002,Bidirectional Contextual Resolution,categorial semantics and scoping,['Fernando C N Pereira'],,"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings.",This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG .,"[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG ."", 'It will identify a pronoun with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations.']",1,"[""This equivalence is doing essentially the same job as Pereira 's pronoun abstraction schema in #AUTHOR_TAG .""]"
CC316,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved. 1. INTRODUCTION  The monotonicity property of unification based grammar formalisms is perhaps the most important factor in their widespread use for grammatical description and parsing. Monotonicity guarantees that the grammatical analysis of a sentence can proceed incrementally by combining information from rules and lexical entries in a nondestructive way. By contrast, aspects of semantic interpretation, such as reference and quantifier scope resolution, are often realised by non-mo..","These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .","['What is required is that QLFs are, as here, expressed in a typed higher-order logic, augmented with constructs representing the interpretation of context-dependent elements (pronouns, ellipsis, focus, etc.).', 'These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .', 'Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context.', '~']",0,"['These constructs correspond as directly as possible to properties of the linguistic structure that express them and are , to as small an extent as possible , dependent on the requirements of contextual resolution ( unlike , say , the metavariables of standard QLFs [ #AUTHOR_TAG ] , or the labels of UDRS [ Reyle 1996 ] , which are motivated entirely by the mechanisms that operate on them after grammatical processing ) .', 'Syntactic properties relevant for binding constraints, parallelism, scope constraints, and so on, are not directly represented at QLF (again unlike standard QLFs) but are assumed to be available as components of the linguistic context.']"
CC317,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.","In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.","[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE)."", 'In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']",1,"[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and implemented in SRI's Core Language Engine (CLE)."", 'In the CLE-QLF approach, as rationally reconstructed by #AUTHOR_TAG and Crouch and Putman ( 1994 ) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs repre- sent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']"
CC318,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.",A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG .,"['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints.', 'The role of resolution rules (for perfectly good presentational reasons) is completely ignored by their treatment.', 'However, it is really the case that in giving the semantics of a QLF, one is interested only in the set of RQLFs that are obtainable from it under closure of the resolution rules.', 'Ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'This is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right.', 'Anyone who has built a wide-coverage system knows that the range of context-dependent phenomena encountered in real life is a lot wider than the preoccupations of many linguists might suggest.', 'In the CLE, for example, contextual resolution forms a larger part of the system than do syntactic and semantic processing.', 'Unfortunately, in the CLE there is no formal theory of resolution rules, and thus no prospect of capturing their role in assigning a semantics to QLFs.']",1,"['A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by #AUTHOR_TAG .', 'This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume.', 'The role of resolution rules (for perfectly good presentational reasons) is completely ignored by their treatment.', 'Ideally, therefore, we would like a formal reconstruction of resolution rules as well.', 'Anyone who has built a wide-coverage system knows that the range of context-dependent phenomena encountered in real life is a lot wider than the preoccupations of many linguists might suggest.', 'In the CLE, for example, contextual resolution forms a larger part of the system than do syntactic and semantic processing.']"
CC319,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",introduction,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.","We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language  approach of Dalrymple et al. ( 1996 ) .","['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language  approach of Dalrymple et al. ( 1996 ) .']",1,"['We then go on to compare the current approach with that of some other theories with similar aims : the `` standard  version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by #AUTHOR_TAG and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language  approach of Dalrymple et al. ( 1996 ) .']"
CC320,J00-4002,Bidirectional Contextual Resolution,resolving quasi logical forms,['Hiyan Alshawi'],,"The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descripitions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of ""reference resolution rules"" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied.","The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) .","[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) ."", 'In the CLE-QLF approach, as ra-tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994), the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules.', 'Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.']",1,"[""The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in #AUTHOR_TAG , 1992 ) , and implemented in SRI 's Core Language Engine ( CLE ) .""]"
CC321,J00-4002,Bidirectional Contextual Resolution,resolving quasi logical forms,['Hiyan Alshawi'],,"The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descripitions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of ""reference resolution rules"" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied.","We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.","['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']",0,"['We assume that every determiner has its own equivalence , which resolves it as a quantifier : sometimes this can be quite a complicated matter , as with any ( #AUTHOR_TAG ) , which will resolve in different ways depending on its linguistic context , but here we avoid this complexity.']"
CC322,J00-4002,Bidirectional Contextual Resolution,training and scaling preference functions for disambiguation,"['Hiyan Alshawi', 'David M Carter']",,"We present an automatic method for weighting the contributions of preference functions used in disambiguation. Initial scaling factors are derived as the solution to a least squares minimization problem, and improvements are then made by hill climbing. The method is applied to disambiguating sentences in the Air Travel Information System corpus, and the performance of the resulting scaling factors is compared with hand-tuned factors. We then focus on one class of preference function, those based on semantic lexical collocations. Experimental results are presented showing that such functions vary considerably in selecting correct analyses. In particular, we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations.","The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG ) , with the resolution process as described here .","['There are several stategies that might be pursued.', 'One is to adopt Pinkal\'s ""radical underspecification"" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG ) , with the resolution process as described here ."", 'Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here.']",3,"['One is to adopt Pinkal\'s ""radical underspecification"" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity.', ""The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( #AUTHOR_TAG ) , with the resolution process as described here ."", 'Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here.']"
CC323,J00-4002,Bidirectional Contextual Resolution,categorial semantics and scoping,['Fernando C N Pereira'],,"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings.","The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .","['We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism.', 'The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .', ""Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin."", 'We analyze quantified NPs at the QLF level as illustrated in the QLF for: We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (Alshawi 1990), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity.', '6']",1,"['The version proposed here combines a basic insight from Lewin ( 1990 ) with higher-order unification to give an analysis that has a strong resemblance to that proposed in #AUTHOR_TAG , 1991 ) , with some differences that are commented on below .']"
CC324,J00-4002,Bidirectional Contextual Resolution,categorial semantics and scoping,['Fernando C N Pereira'],,"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings.","It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #AUTHOR_TAG , 1991 ) .","['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #AUTHOR_TAG , 1991 ) .', 'Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a ""free variable"" of type e is introduced in the NP position, with an associated ""quantifier assumption,"" which is added as a kind of premise.', 'At a later stage the quantifier assumption is ""discharged,"" capturing all occurrences of the free variable.', 'Thus their analysis of something like every manager disappeared would proceed as follows:']",1,"['It is interesting to compare this analysis with that described in Dalrymple , Shieber , and Pereira ( 1991 ) and #AUTHOR_TAG , 1991 ) .', 'Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a ""free variable"" of type e is introduced in the NP position, with an associated ""quantifier assumption,"" which is added as a kind of premise.']"
CC325,J00-4002,Bidirectional Contextual Resolution,an algorithm for generating quantifier scopings,"['Jerry R Hobbs', 'Stuart M Shieber']",,"The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination. But scope dependencies are not so transparent. As a result, many systems for representing the semantics of sentences have ignored scoping or generating scoping mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow. In this paper, we present an algorithm, along with proofs of some of its important properties, that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure. The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy.Engineering and Applied Science","only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â¢ partial scopings are permitted ( see Reyle [ 19961 ) â¢ scoping can be freely interleaved with other types of reference resolution ; â¢ unscoped or partially scoped forms are available for inference or for generation at every stage .","['only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â\x80¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â\x80¢ partial scopings are permitted ( see Reyle [ 19961 ) â\x80¢ scoping can be freely interleaved with other types of reference resolution ; â\x80¢ unscoped or partially scoped forms are available for inference or for generation at every stage .']",0,"['only the available five relative scopings of the quantifiers are produced ( #AUTHOR_TAG , 47 ) , but without the need for a free variable constraint -- the HOU algorithm will not produce any solutions in which a previously bound variable becomes free ; â\x80¢ the equivalences are reversible , and thus the above sentences cart be generated from scoped logical forms ; â\x80¢ partial scopings are permitted ( see Reyle [ 19961 ) â\x80¢ scoping can be freely interleaved with other types of reference resolution ; â\x80¢ unscoped or partially scoped forms are available for inference or for generation at every stage .']"
CC326,J00-4002,Bidirectional Contextual Resolution,on reasoning with ambiguities,['Uwe Reyle'],,"The paper adresses the problem of reasoning with ambiguities. Semantic representations are presented that leave scope relations between quantifiers and/or other operators unspecified. Truth conditions are provided for these representations and different consequence relations are judged on the basis of intuitive correctness. Finally inference patterns are presented that operate directly on these underspecified structures, i.e. do not rely on any translation into the set of their disambiguations.Comment: EACL 199","But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .","['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .', 'Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'His example is:']",5,"['Developing a calculus for reasoning with QLFs is too large a task to be undertaken here.', 'But the general outlines are reasonably clear , and we can adapt some of the UDRS ( #AUTHOR_TAG ) work to our own framework .', 'Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is.', 'His example is:']"
CC327,J00-4002,Bidirectional Contextual Resolution,monotonic semantic interpretation,"['Hiyan Alshawi', 'Richard Crouch']",,"Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved.","#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .","['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']",0,"['#AUTHOR_TAG present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .']"
CC328,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,cogniac high precision coreference with limited knowledge and linguistic resources,['Breck Baldwin'],,"This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach.","A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; #AUTHOR_TAG ; Mitkov 1996 , 1998b ) .']"
CC329,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,centeringinthelarge computing referential discourse segments,"['Udo Hahn', 'Michael Strube']",,"We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied.","Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; #AUTHOR_TAG ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']"
CC330,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,never look back an alternative to centering,['Michael Strube'],,"I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word.","Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG ) .","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']",0,"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( Hobbs 1978 ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( #AUTHOR_TAG ) .""]"
CC331,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,evaluating anaphora resolution approaches,['Ruslan Mitkov'],,,"Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAGa , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAGa , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( #AUTHOR_TAGa , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"
CC332,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,anaphora for everyone pronominal anaphora resolution without a parser,"['Christopher Kennedy', 'Branimir Boguraev']",,"We present an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass, 1994). In contrast to that work, our algorithm does not require in-depth, full, syntactic parsing of text. Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from the output of a part of speech tagger, enriched only with annotations of grammatical function of lexical items in the input text stream. Evaluation of the results of our implementation demonstrates that accurate anaphora resolution can be realized within natural language processing frameworks which do not---or cannot--- employ robust and reliable parsing components.","A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .","['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; #AUTHOR_TAG ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']"
CC333,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,functional centering,"['Michael Strube', 'Udo Hahn']",,"Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering.","Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']",0,"['Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; #AUTHOR_TAG ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']"
CC334,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,probabilistic coreference in information extraction,['Andrew Kehler'],,"Certain applications require that the out-put of an information extraction system be probabilistic, so that a downstream sys-tem can reliably fuse the output with pos-sibly contradictory information from other sources. In this paper we consider the problem of assigning a probability distri-bution to alternative sets of coreference re-lationships among entity descriptions. We present the results of initial experiments with several approaches to estimating such distributions in an application using SRI's FASTUS information extraction system.","Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; #AUTHOR_TAG ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .']"
CC335,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,multilingual anaphora resolution,['Ruslan Mitkov'],,"This paper presents amultilingual robust, knowledge-poor approach to resolvingpronouns in technical manuals. This approach is a modification of the practicalapproach (Mitkov 1998a) and operates on texts pre-processed by apart-of-speech tagger. Input is checked against agreementand a number of antecedent indicators. Candidates are assigned scores by eachindicator and the candidate with the highest aggregate score isreturned as the antecedent. We propose this approach as aplatform for multilingual pronoun resolution. The robust approach was initiallydeveloped and tested for English, but we have also adaptedand tested it for Polish and Arabic. For bothlanguages, we found that adaptation required minimummodification and that further, even if used unmodified, the approachdelivers acceptable success rates. Preliminary evaluation reports high successrates in the range of over 90%.","Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; #AUTHOR_TAG ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .']"
CC336,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,robust method of pronoun resolution using fulltext information,['Tetsuya Nasukawa'],,"A consistent text contains rich information for resolving ambiguities within its sentences. Even simple syntactic information such as word occurrence and collocation patterns, which can be extracted from the text without deep discourse analysis, improves the accuracy of sentence analysis. Pronoun resolution is a typical proceeding that utilizes this information. Through the use of this information, along with information on the syntactic position of each candidate, 93.8% of pronoun references were resolved correctly in an experiment on computer manuals.","A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .","['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and lin- guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; #AUTHOR_TAG ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; Mitkov 1996 , 1998b ) .']"
CC337,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,an architecture for anaphora resolution,"['Elaine Rich', 'Susann LuperFoy']",,"In this paper, we describe the pronominal anaphora resolution module of Lucy, a porta.ble English understanding system. The design.of thi.s module was motivated by the observation that, al- though there exist many theories of anaphora resolution, no one of these theories is complete. Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose .can- didate antecedents and to evaluate each other&apos;s proposals","Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; #AUTHOR_TAG ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .']"
CC338,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,improving pronoun resolution in two languages by means of bilingual corpora,"['Ruslan Mitkov', 'Catalina Barbu']",,,"Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; #AUTHOR_TAG ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .']"
CC339,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,outstanding issues in anaphora resolution,['Ruslan Mitkov'],,,"The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .","['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .', 'A fundamental question that needs further investigation is how far the performance of anaphora resolution algorithms can go and what the limitations of knowledge-poor methods are.', 'In particular, more research should be carried out on the factors influencing the performance of these algorithms.', 'One of the impediments to the evaluation or fuller utilization of machine learning techniques is the lack of widely available corpora annotated for anaphoric or coreferential links.', 'More work toward the proposal of consistent and comprehensive evaluation is necessary; so too is work in multilingual contexts.', 'Some of these challenges have been addressed in the papers published in this issue, but ongoing research will continue to address them in the near future.']",3,"['The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( #AUTHOR_TAGa ) .']"
CC340,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,analysis of syntaxbased pronoun resolution methods,['Joel Tetreault'],,"This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory (Grosz et al., 1995) and is an alternative to Brennan et al.'s 1987 algorithm. The advantages of this new model, the Left-Right Centering Algorithm (LRC), lie in its incremental processing of utterances and in its low computational overhead. The algorithm is compared with three other pronoun resolution methods: Hobbs' syntax-based algorithm, Strube's S-list approach, and the BFP Centering algorithm. All four methods were implemented in a system and tested on an annotated subset of the Treebank corpus consisting of 2026 pronouns. The noteworthy results were that Hobbs and LRC performed the best.","Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; Hahn and Strube 1997 ; #AUTHOR_TAG ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"
CC341,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,anaphora resolution a multistrategy approach,"['Jaime Carbonell', 'Ralf Brown']",,,"Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; #AUTHOR_TAG ) , which was difficult both to represent and to process , and which required considerable human input .']"
CC342,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,description of the university of pennsylvania system used for muc6,"['Breck Baldwin', 'Jeff Reynar', 'Mike Collins', 'Jason Eisner', 'Adwait Ratnaparki', 'Joseph Rosenzweig', 'Anoop Sarkar', 'Srivinas Bangalore']",,,"The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in #AUTHOR_TAG , Gaizauskas and Humphreys ( 1996 ) , and Kameyama ( 1997 ) .']"
CC343,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,multilingual coreference resolution,"['Sanda Harabagiu', 'Steven Maiorano']",,"The current work investigates the problems that occur when coreference resolution is considered as a multilingual task. We assess the issues that arise when a framework using the mention-pair coreference resolution model and memory-based learning for the resolution process are used. Along the way, we revise three essential subtasks of coreference resolution: mention detection, mention head detection and feature selection. For each of these aspects we propose various multilingual solutions including both heuristic, rule-based and machine learning methods. We carry out a detailed analysis that includes eight different languages (Arabic, Catalan, Chinese, Dutch, English, German, Italian and Spanish) for which datasets were provided by the only two multilingual shared tasks on coreference resolution held so far: SemEval-2 and CoNLL-2012. Our investigation shows that, although complex, the coreference resolution task can be targeted in a multilingual and even language independent way. We proposed machine learning methods for each of the subtasks that are affected by the transition, evaluated and compared them to the performance of rule-based and heuristic approaches. Our results confirmed that machine learning provides the needed flexibility for the multilingual task and that the minimal requirement for a language independent system is a part-of-speech annotation layer provided for each of the approached languages. We also showed that the performance of the system can be improved by introducing other layers of linguistic annotations, such as syntactic parses (in the form of either constituency or dependency parses), named entity information, predicate argument structure, etc. Additionally, we discuss the problems occurring in the proposed approaches and suggest possibilities for their improvement","Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; #AUTHOR_TAG ; Mitkov and Barbu 2000 ; Mitkov 1999 ; Mitkov and Stys 1997 ; Mitkov , Belguith , and Stys 1998 ) .']"
CC344,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,pronoun resolution the practical alternative presented at the discourse anaphora and anaphor resolution colloquium daarc1,['Ruslan Mitkov'],,,"A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG , 1998b ) .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG , 1998b ) .']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988), which was difficult both to represent and to process, and which required considerable human input.', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments ( Dagan and Itai 1990 , 1991 ; Lappin and Leass 1994 ; Nasukawa 1994 ; Kennedy and Boguraev 1996 ; Williams , Harvey , and Preston 1996 ; Baldwin 1997 ; #AUTHOR_TAG , 1998b ) .']"
CC345,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,resolving pronoun references,['Jerry Hobbs'],,"Two approaches to the problem of resolving pronoun references are presented. The first is a naive algorithm that works by traversing the surface parse trees of the sentences of the text in a particular order looking for noun phrases of the correct gender and number. The algorithm clearly does not work in all cases, but the results of an examination of several hundred examples from published texts show that it performs remarkably well. In the second approach, it is shown how pronoun solution can be handled in a comprehensive system for semantic analysis of English texts. The system is described, and it is shown in a detailed treatment of several examples how semantic analysis locates the antecedents of most pronouns as a by-product. Included are the classic examples of Winograd and Charniak.","Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .","[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) ."", 'The LRC is an alternative to the original BFP algorithm in that it processes utterances incrementally.', 'It works by first searching for an antecedent in the current sentence; if none can be found, it continues the search on the Cf-list of the previous and the other preceding utterances in a left-to-right fashion.']",0,"[""Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( #AUTHOR_TAG ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .""]"
CC346,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,robust reference resolution with limited knowledge high precision genrespecific approach for english and polish,"['Ruslan Mitkov', 'Malgorzata Stys']",,"Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labourand time-consuming task. This paper presents a robust, knowledgepoor approach to resolving pronouns in technical manuals in both English and Polish. This approach is a modification of the practical approach reported in [Mitkov 97] and operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and tested for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Preliminary evaluation reports precision of over 90%.","Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) gave a considerable impetus to the development of coreference resolution algorithms and systems, such as those described in Baldwin et al. (1995), Gaizauskas and Humphreys (1996), and Kameyama (1997).', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; #AUTHOR_TAG ; Mitkov , Belguith , and Stys 1998 ) .', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.']"
CC347,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,recognizing referential links an information extraction perspective,['Megumi Kameyama'],,"We present an efficient and robust reference resolution algorithm in an end-to-end state-of-the-art information extraction system, which must work with a considerably impoverished syntactic analysis of the input sentences. Considering this disadvantage, the basic setup to collect, filter, then order by salience does remarkably well with third-person pronouns, but needs more semantic and discourse information to improve the treatments of other expression types.","The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .","['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']",0,"['While the shift toward knowledge-poor strategies and the use of corpora represented the main trends of anaphora resolution in the 1990s, there are other significant highlights in recent anaphora resolution research.', 'The inclusion of the coreference task in the Sixth and Seventh Message Understanding Conferences ( MUC-6 and MUC-7 ) gave a considerable impetus to the development of coreference resolution algorithms and systems , such as those described in Baldwin et al. ( 1995 ) , Gaizauskas and Humphreys ( 1996 ) , and #AUTHOR_TAG .', 'The last decade of the 20th century saw a number of anaphora resolution projects for languages other than English such as French, German, Japanese, Spanish, Portuguese, and Turkish.', 'Against the background of a growing interest in multilingual NLP, multilingual anaphora/coreference resolution has gained considerable momentum in recent years (Aone and McKee 1993;Azzam, Humphreys, and Gaizauskas 1998;Harabagiu and Maiorano 2000;Mitkov and Barbu 2000;Mitkov 1999;Mitkov and Stys 1997;Mitkov, Belguith, and Stys 1998).', 'Other milestones of recent research include the deployment of probabilistic and machine learning techniques (Aone and Bennett 1995;Kehler 1997;Ge, Hale, and Charniak 1998;Cardie and Wagstaff 1999; the continuing interest in centering, used either in original or in revised form (Abra~os and Lopes 1994;Strube and Hahn 1996;Hahn and Strube 1997;Tetreault 1999); and proposals related to the evaluation methodology in anaphora resolution (Mitkov 1998a(Mitkov , 2001b.', 'For a more detailed survey of the state of the art in anaphora resolution, see Mitkov (forthcoming).']"
CC348,J01-4001,Introduction to the Special Issue on Computational Anaphora Resolution,toward a computational theory of definite anaphora comprehension in english,['Candace Sidner'],,"Abstract : This report investigates the process of focussing as a description and explanation of the comprehension of certain anaphoric expressions in English discourse. The investigation centers on the interpretation of definite anaphora, that is, on the personal pronouns, and noun phrases used with a definite article the, this, or that. Focussing is formalized as a process in which a speaker centers attention on a particular aspect of the discourse. An algorithmic description specifies what the speaker can focus on and how the speaker may change the focus of the discourse as the discourse unfolds. The algorithm allows for a simple focussing mechanism to be constructed: an element in focus, an ordered collection of alternate foci, and a stack of old foci. The data structure for the element in focus is a representation which encodes a limited set of associations between it and other elements from the discourse as well as from general knowledge. This report also establishes other constraints which are needed for the successful comprehension of anaphoric expressions. The focussing mechanism is designed to take advantage of syntactic and semantic information encoded as constraints on the choice of anaphora interpretation. These constraints are due to the work of language researchers; and the focussing mechanism provides a principled means for choosing when to apply the constraints in the comprehension process.","Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .","['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .', 'However, the pressing need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies.', 'A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (']",0,"['Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( #AUTHOR_TAG ; Carter 1987 ; Rich and LuperFoy 1988 ; Carbonell and Brown 1988 ) , which was difficult both to represent and to process , and which required considerable human input .']"
CC349,J02-3002,"Periods, Capitalized Words, etc.",overview of muc7”,['Nancy Chinchor'],,,Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG 1998) of information extraction.,"['Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'There the disambiguation of the first word in a sentence (and in other ambiguous positions) is one of the central problems: about 20% of named entities occur in ambiguous positions.', 'For instance, the word Black in the sentenceinitial position can stand for a person�s surname but can also refer to the color.', 'Even in multiword capitalized phrases, the first word can belong to the rest of the phrase or can be just an external modifier.', 'In the sentence Daily, Mason and Partners lost their court case, it is clear that Daily, Mason and Partners is the name of a company.', 'In the sentence Unfortunately, Mason and Partners lost their court case, the name of the company does not include the word Unfortunately, but the word Daily is just as common a word as Unfortunately.']",0,"['Proper names are the main concern of the named-entity recognition subtask ( #AUTHOR_TAG 1998) of information extraction.', 'The main objective of this subtask is the identification of proper names and also their classification into semantic categories (person, organization, location, etc.).1', 'Even in multiword capitalized phrases, the first word can belong to the rest of the phrase or can be just an external modifier.']"
CC350,J02-3002,"Periods, Capitalized Words, etc.",robust partofspeech tagging using a hidden markov model computer speech and language,['Julian Kupiec'],,,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']",1,"['Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ #AUTHOR_TAG ] , Brill 's [ Brill 1995a ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']"
CC351,J02-3002,"Periods, Capitalized Words, etc.",a knowledgefree method for capitalized word disambiguation,['Andrei Mikheev'],,,"This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG .","['The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.', 'Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG .']",5,"['The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign.', 'Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the fly the distribution of ambiguously capitalized words in the entire document.', 'This is implemented as a cascade of simple strategies , which were briefly described in #AUTHOR_TAG .']"
CC352,J02-3002,"Periods, Capitalized Words, etc.",frequency analysis of english usage lexicon and grammar,"['W Nelson Francis', 'Henry Kucera']",,,"There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .","['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scientific papers to fiction and transcribed speech.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.', 'Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens.']",5,"['There are two corpora normally used for evaluation in a number of text-processing tasks : the Brown corpus ( #AUTHOR_TAG ) and the Wall Street Journal ( WSJ ) corpus , both part of the Penn Treebank ( Marcus , Marcinkiewicz , and Santorini 1993 ) .', 'The Brown corpus represents general English.', 'The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure.']"
CC353,J02-3002,"Periods, Capitalized Words, etc.",language independent named entity recognition combining morphological and contextual evidence”,"['Silviu Cucerzan', 'David Yarowsky']",,"Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchicaily smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools","Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG ) .","['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']",0,"['Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( Yarowsky 1995 ) and named-entity recognition ( #AUTHOR_TAG ) .']"
CC354,J02-3002,"Periods, Capitalized Words, etc.",some applications of treebased modeling to speech and language indexing”,['Michael D Riley'],,,"Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .","['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']",0,"['Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( #AUTHOR_TAG ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.']"
CC355,J02-3002,"Periods, Capitalized Words, etc.",adaptive sentence boundary disambiguation”,"['David D Palmer', 'Marti A Hearst']",,"Labeling of sentence boundaries is a necessary prerequisite for many natural language processing tasks, including part-of-speech tagging and sentence alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules. As an alternative, we have developed an efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network. After training for less than one minute, the method correctly labels over 98.5 % of sentence boundaries in a corpus of over 27,000 sentence-boundary marks. We show the method to be efficient and easily adaptable to different text genres, including single-case texts.Comment: This is a Latex version of the previously submitted ps file   (formatted as a uuencoded gz-compressed .tar file created by csh script). The   software from the work described in this paper is available by contacting   dpalmer@cs.berkeley.ed","Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .","['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']",0,"['Automatically trainable software is generally seen as a way of producing sys- tems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( #AUTHOR_TAG ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .', 'Ma- chine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local con- text of a potential sentence-terminating punctuation sign.']"
CC356,J02-3002,"Periods, Capitalized Words, etc.",transformationbased errordriven learning and natural language parsing a case study in partofspeech tagging,['Eric Brill'],,,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']",1,"['Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ #AUTHOR_TAGa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']"
CC357,J02-3002,"Periods, Capitalized Words, etc.",a maximum entropy model for partofspeech tagging”,['Adwait Ratnaparkhi'],,,"We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .","['In our markup convention (Section 2), periods are tokenized as separate tokens regardless of whether they stand for fullstops or belong to abbreviations.', 'Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'There is, however, one difference in the implementation of such a tagger.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'At the same time since this token is unambiguous, it is not affected by the history.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']",1,"['Consequently a POS tagger can naturally treat them similarly to any other ambiguous words.', 'Normally, a POS tagger operates on text spans that form a sentence.', 'This requires resolving sentence boundaries before tagging.', ""We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ Brill 1995a ] , and MaxEnt [ #AUTHOR_TAG ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ."", 'The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history.', 'This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions.', 'For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token.', 'A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.']"
CC358,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.",The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']",1,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( #AUTHOR_TAG ) with the Alembic system ( Aberdeen et al. 1995 ) : a 0.5 % error rate .', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).']"
CC359,J02-3002,"Periods, Capitalized Words, etc.",mitre description of the alembic system used for muc6”,"['John S Aberdeen', 'John D Burger', 'David S Day', 'Lynette Hirschman', 'Patricia Robinson', 'Marc Vilain']",,,"For instance , the Alembic workbench ( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .","['There exist two large classes of SBD systems: rule based and machine learning.', 'The rule-based systems use manually built rules that are usually encoded in terms of regular-expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'To put together a few rules is fast and easy, but to develop a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench ( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', 'Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.']",0,"['The rule-based systems use manually built rules that are usually encoded in terms of regular-expression grammars supplemented with lists of abbreviations, common words, proper names, etc.', 'To put together a few rules is fast and easy, but to develop a rule-based system with good performance is quite a labor-consuming enterprise.', 'For instance , the Alembic workbench ( #AUTHOR_TAG ) contains a sentence-splitting module that employs over 100 regular-expression rules written in Flex .', 'Another well-acknowledged shortcoming of rule-based systems is that such systems are usually closely tailored to a particular corpus or sublanguage and are not easily portable across domains.']"
CC360,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",conclusion,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.",On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .,"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .', 'Although these error rates seem to be very small, they are quite significant.', 'Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).', 'This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.', 'On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.']",1,"['The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate).', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in #AUTHOR_TAG ( 0.44 % vs. 0.5 % error rate ) .']"
CC361,J02-3002,"Periods, Capitalized Words, etc.",transformationbased errordriven learning and natural language parsing a case study in partofspeech tagging,['Eric Brill'],,,"This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .","['We decided to train the tagger with the minimum of preannotated resources.', 'First, we used 20,000 tagged words to ""bootstrap"" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.', 'We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%.', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'Periods as many other closed-class words cannot be successfully covered by such technique.']",1,"[""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ Baum and Petrie 1966 ] or Brill 's [ #AUTHOR_TAGb ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .""]"
CC362,J02-3002,"Periods, Capitalized Words, etc.",tagging sentence boundaries”,['Andrei Mikheev'],,In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.,"In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG .","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG .', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']",1,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'In the disambiguation of capitalized words , the most widespread method is POS tagging , which achieves about a 3 % error rate on the Brown corpus and a 5 % error rate on the WSJ corpus , as reported in #AUTHOR_TAG .']"
CC363,J02-3002,"Periods, Capitalized Words, etc.",a stochastic parts program and nounphrase parser for unrestricted text”,['Kenneth Church'],,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,"As #AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .","['Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'As #AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.', ""For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title)."", 'It would be misleading to infer from this evidence that the word \'Acts\' is always a proper noun.""']",1,"['Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.', 'As #AUTHOR_TAG rightly pointed out , however , `` Proper nouns and capitalized words are particularly problematic : some capitalized words are proper nouns and some are not .', 'Estimates from the Brown Corpus can be misleading.']"
CC364,J02-3002,"Periods, Capitalized Words, etc.",statistical inference for probabilistic functions of finite markov chains,"['Leonard E Baum', 'Ted Petrie']",,,"This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns .","['We decided to train the tagger with the minimum of preannotated resources.', 'First, we used 20,000 tagged words to ""bootstrap"" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.', 'We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%.', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.', 'Periods as many other closed-class words cannot be successfully covered by such technique.']",1,"['First, we used 20,000 tagged words to ""bootstrap"" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision.', ""This was done because purely unsupervised techniques ( e.g. , Baum-Welch [ #AUTHOR_TAG ] or Brill 's [ Brill 1995b ] ) enable regularities to be induced for word classes which contain many entries , exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns ."", 'Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent.']"
CC365,J02-3002,"Periods, Capitalized Words, etc.",russian morphology an engineering approach,"['Andrei Mikheev', 'Liubov Liubushkina']",experiments,"Morphological analysis, which is at the heart of the processing of natural language requires computationally effective morphological processors. In this paper an approach to the organization of an inflectional morphological model and its application for the Russian language are described. The main objective of our morphological processor is not the classification of word constituents, but rather an efficient computational recognition of morpho-syntactic features of words and the generation of words according to requested morpho-syntactic features. Another major concern that the processor aims to address is the ease of extending the lexicon. The templated word-paradigm model used in the system has an engineering flavour: paradigm formation rules are of a bottom-up (word specific) nature rather than general observations about the language, and word formation units are segments of words rather than proper morphemes. This approach allows us to handle uniformly both general cases and exceptions, and requires extremely simple data structures and control mechanisms which can be easily implemented as a finite-state automata. The morphological processor described in this paper is fully implemented for a substantial subset of Russian (more then 1,500,000 word-tokens - 95,000 word paradigms) and provides an extensive list of morpho-syntactic features together with stress positions for words utilized in its lexicon. Special dictionary management tools were built for browsing, debugging and extension of the lexicon. The actual implementation was done in C and C++, and the system is available for the MS-DOS, MS-Windows and UNIX platforms.","Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .","['Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms.', 'Since the documents in the BBC news corpus were rather short, we applied the cache module, as described in Section 11.1.', 'This allowed us to reuse information across the documents.']",5,"['Since, unlike English, Russian is a highly inflected language, we had to deal with the case normalization issue.', 'Before using the DCA method , we applied a Russian morphological processor ( #AUTHOR_TAG ) to convert each word in the text to its main form : nominative case singular for nouns and adjectives , infinitive for verbs , etc. .', 'For words that could be normalized to several main forms (polysemy), when secondary forms of different words coincided, we retained all the main forms.', 'This allowed us to reuse information across the documents.']"
CC366,J02-3002,"Periods, Capitalized Words, etc.",one sense per collocation”,['David Yarowsky'],,"Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse. In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation. We test his empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99 % accuracy for binary ambiguities. We utilize this property in a disambiguation algorithm that achieves precision of 92 % using combined models of very local context. 1","This is similar to ""one sense per collocation"" idea of #AUTHOR_TAG .","['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of #AUTHOR_TAG .']",1,"['This is similar to ""one sense per collocation"" idea of #AUTHOR_TAG .']"
CC367,J02-3002,"Periods, Capitalized Words, etc.",celex a guide for users centre for lexical information,['Gavin Burnage'],,,"A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG ) .","['The first list on which our method relies is a list of common words.', 'This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).', 'We do not have to rely on pre-existing resources, however.', 'A list of common words can be easily obtained automatically from a raw (unannotated in any way) text collection by simply collecting and counting lowercased words in it.', 'We generated such list from the NYT collection.', 'To account for potential spelling and capitalization errors, we included in the list of common words only words that occurred lower-cased at least three times in the NYT texts.', 'The list of common words that we developed from the NYT collection contained about 15,000 English words.']",0,"['This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list.', 'A variety of such lists for many languages are already available ( e.g. , #AUTHOR_TAG ) .', 'Words in such lists are usually supplemented with morphological and POS information (which is not required by our method).']"
CC368,J02-3002,"Periods, Capitalized Words, etc.",some applications of treebased modeling to speech and language indexing”,['Michael D Riley'],conclusion,,The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .,"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate).', 'Although these error rates seem to be very small, they are quite significant.', 'Unlike general POS tagging, in which it is unfair to expect an error rate of less than 2% because even human annotators have a disagreement rate of about 3%, sentence boundaries are much less ambiguous (with a disagreement of about 1 in 5,000).', 'This shows that an error rate of 1 in 200 (0.5%) is still far from reaching the disagreement level.', 'On the other hand, one error in 200 periods means that there is one error in every two documents in the Brown corpus and one error in every four documents in the WSJ corpus.']",1,"['Despite its simplicity, the performance of our approach was on the level with the previously highest reported results on the same test collections.', 'The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( #AUTHOR_TAG : 0.28 % vs. 0.20 % error rate ) .', 'On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate).']"
CC369,J02-3002,"Periods, Capitalized Words, etc.",nymble a high performance learning namefinder”,"['Daniel Bikel', 'Scott Miller', 'Richard Schwartz', 'Ralph Weischedel']",,"This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.",In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ) .,"['In the information extraction field, the disambiguation of ambiguous capitalized words has always been tightly linked to the classification of proper names into semantic classes such as person name, location, and company name.', 'Named-entity recognition systems usually use sets of complex hand-crafted rules that employ a gazetteer and a local context (Krupa and Hausman 1998).', 'In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ) .', 'The advantage of the namedentity approach is that the system not only identifies proper names but also determines their semantic class.', 'The disadvantage is in the cost of building a wide-coverage set of contextual clues manually or producing annotated training data.', 'Also, the contextual clues are usually highly specific to the domain and text genre, making such systems very difficult to port.']",0,['In some systems such dependencies are learned from labeled examples ( #AUTHOR_TAG ) .']
CC370,J02-3002,"Periods, Capitalized Words, etc.",overview of muc7”,['Nancy Chinchor'],,,"For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .","['• abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', 'Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.']",5,"['* abbreviation (as opposed to regular word) These four lists can be acquired completely automatically from raw (unlabeled) texts.', 'For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times ( NYT ) corpus that was supplied as training data for the 7th Message Understanding Conference ( MUC-7 ) ( #AUTHOR_TAG ) .', 'We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7.', 'Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.']"
CC371,J02-3002,"Periods, Capitalized Words, etc.",some applications of treebased modeling to speech and language indexing”,['Michael D Riley'],,,"The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .","['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']",1,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate.', 'The best performance on the Brown corpus , a 0.2 % error rate , was reported by #AUTHOR_TAG , who trained a decision tree classifier on a 25-million-word corpus .', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']"
CC372,J02-3002,"Periods, Capitalized Words, etc.",what is a word what is a sentence problems of tokenization”,"['Gregory Grefenstette', 'Pasi Tapanainen']",,"Any linguistic treatment of freely occurring text must provide an answer to what is considered as a token. In arti cial languages, the de nition of what is considered as a token can be precisely and unambiguously de ned. Natural languages, on the other hand, display such a rich variety that there are many ways to decide upon what will be considered as a unit for a computational approach to text. Here we will discuss tokenization as a problem for computational lexicography. Our discussion will cover the aspects of what is usually considered preprocessing of text in order to prepare it for some automated treatment. We present the roles of tokenization, methods of tokenizing, grammars for recognizing acronyms, abbreviations, and regular expressions such as numbers and dates. We present the problems encountered and discuss the e ects of seemingly innocent choices.","One of the better-known approaches is described in #AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .","['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', 'Park and Byrd (2001) recently described a hybrid method for finding abbreviations and their definitions.', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']",0,"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in #AUTHOR_TAG , which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing .', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'Park and Byrd (2001) recently described a hybrid method for finding abbreviations and their definitions.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']"
CC373,J02-3002,"Periods, Capitalized Words, etc.",unsupervised word sense disambiguation rivaling supervised methods”,['David Yarowsky'],,"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%","Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .","['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']",0,"['Since then this idea has been applied to several tasks , including word sense disambiguation ( #AUTHOR_TAG ) and named-entity recognition ( Cucerzan and Yarowsky 1999 ) .']"
CC374,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",introduction,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.",A detailed introduction to the SBD problem can be found in #AUTHOR_TAG .,"['Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting.', 'Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop).', 'A detailed introduction to the SBD problem can be found in #AUTHOR_TAG .']",0,"['Another important task of text normalization is sentence boundary disambiguation (SBD) or sentence splitting.', 'Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sentence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary.', 'In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary.', 'Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop).', 'A detailed introduction to the SBD problem can be found in #AUTHOR_TAG .']"
CC375,J02-3002,"Periods, Capitalized Words, etc.",tagging sentence boundaries”,['Andrei Mikheev'],conclusion,In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.,"This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .","['With all its strong points, there are a number of restrictions to the proposed approach.', 'First, in its present form it is suitable only for processing of reasonably ""wellbehaved"" texts that consistently use capitalization (mixed case) and do not contain much noisy data.', 'Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']",1,"['We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( Palmer and Hearst 1997 ) or the POS tagger reported in #AUTHOR_TAG , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']"
CC376,J02-3002,"Periods, Capitalized Words, etc.",identifying unknown proper names in newswire text”,"['Inderjeet Mani', 'T Richard MacMillan']",,"The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text. A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction. This paper describes a system which uses text skimming techniques for deriving proper names and their semantic attributes automatically from newswire text, without relying on any listing of name elements. In order to identify new names, the system treats proper names as (potentially) context-dependent linguistic expressions. In addition to using information in the local context, the system exploits a computational model of discourse which identifies individuals based on the way they are described in the text, instead of relying on their description in a pre-existing knowledge base.",#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .,"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence."", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', '#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']",5,['#AUTHOR_TAG pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names .']
CC377,J02-3002,"Periods, Capitalized Words, etc.",tagging sentence boundaries”,['Andrei Mikheev'],,In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.,"Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries .","['To test our hypothesis that DCA can be used as a complement to a local-context approach, we combined our main configuration (evaluated in row D of Table 4) with a POS tagger.', 'Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries .']",5,"['Unlike other POS taggers , this POS tagger ( #AUTHOR_TAG ) was also trained to disambiguate sentence boundaries .']"
CC378,J02-3002,"Periods, Capitalized Words, etc.",mitre description of the alembic system used for muc6”,"['John S Aberdeen', 'John D Burger', 'David S Day', 'Lynette Hirschman', 'Patricia Robinson', 'Marc Vilain']",,,The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.', 'In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000).', 'We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.']",1,"['Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus.', 'The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( #AUTHOR_TAG ) : a 0.5 % error rate .', 'The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus.']"
CC379,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",conclusion,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.","This is where robust syntactic systems like SATZ ( #AUTHOR_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .","['With all its strong points, there are a number of restrictions to the proposed approach.', 'First, in its present form it is suitable only for processing of reasonably ""wellbehaved"" texts that consistently use capitalization (mixed case) and do not contain much noisy data.', 'Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts.', 'We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( #AUTHOR_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']",1,"['We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach.', 'This is where robust syntactic systems like SATZ ( #AUTHOR_TAG ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .']"
CC380,J02-3002,"Periods, Capitalized Words, etc.",language model adaptation using mixtures and an exponentially decaying cache”,"['Philip Clarkson', 'Anthony J Robinson']",,"Presents two techniques for language model adaptation. The first is based on the use of mixtures of language models: the training text is partitioned according to topic, a language model is constructed for each component and, at recognition time, appropriate weightings are assigned to each component to model the observed style of language. The second technique is based on augmenting the standard trigram model with a cache component in which the words' recurrence probabilities decay exponentially over time. Both techniques yield a significant reduction in perplexity over the baseline trigram language model when faced with a multi-domain test text, the mixture-based model giving a 24% reduction and the cache-based model giving a 14% reduction. The two techniques attack the problem of adaptation at different scales, and as a result can be used in parallel to give a total perplexity reduction of 30%.","#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last","['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', 'But unlike the cache model, it uses a multipass strategy.', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last"", 'In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger.', 'Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another.', 'For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed.', 'Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names.', 'They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists.', 'The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists.', 'It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998).', 'Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (""one sense per discourse"").', 'Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999).', ""Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations."", 'In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams).', 'This is similar to ""one sense per collocation"" idea of Yarowsky (1993).']",2,"['The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition.', 'Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage.', 'The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists.', ""#AUTHOR_TAG developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last""]"
CC381,J02-3002,"Periods, Capitalized Words, etc.",eagle an extensible architecture for general linguistic engineering”,"['Breck Baldwin', 'Christine Doran', 'Jeffrey Reynar', 'Michael Niv', 'Bangalore Srinivas', 'Mark Wasson']",,"Over the course of two summer projects, we developed a general purpose natural language system which advances the state-of-the-art in several areas. The system contains demonstrated advancements in part-of-speech tagging, end-of-sentence detection, and coreference resolution. In addition, we believe that we have strong maximal noun phrase detection, and subject-verb-object recognition and a pat tern matching language well suited to a range of tasks. Other features of the system include modularity and interchangeability of components, rapid component integration and a debugging environment.",The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .,"['The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .', 'This heuristic also employs a database of bigrams and unigrams of lower-cased and capitalized words found in unambiguous positions.', 'It is quite similar to our method for capitalized-word disambiguation.', 'The description of the EAGLE case normalization module provided by Baldwin et al. is, however, very brief and provides no performance evaluation or other details.']",1,['The description of the EAGLE workbench for linguistic engineering ( #AUTHOR_TAG ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document .']
CC382,J02-3002,"Periods, Capitalized Words, etc.",a maximum entropy approach to identifying sentence boundaries”,"['Jeffrey C Reynar', 'Adwait Ratnaparkhi']",,"We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of ., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.Comment: 4 pages, uses aclap.sty and covingtn.st","Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .","['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.', 'This implies an investment in the data annotation phase.']",0,"['Automatically trainable software is generally seen as a way of producing systems that are quickly retrainable for a new corpus, for a new domain, or even for another language.', 'Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( #AUTHOR_TAG ) .', 'Machine learning systems treat the SBD task as a classification problem, using features such as word spelling, capitalization, suffix, and word class found in the local context of a potential sentence-terminating punctuation sign.', 'Although training of such systems is completely automatic, the majority of machine learning approaches to the SBD task require labeled examples for training.']"
CC383,J02-3002,"Periods, Capitalized Words, etc.",one term or two” in,['Kenneth Church'],introduction,"Objective SYNTAX score II (SSII) is a long-term mortality prediction model to guide the decision making of the heart-team between coronary artery bypass grafting or percutaneous coronary intervention (PCI) in patients with left main or three-vessel coronary artery disease. This study aims to investigate the long-term predictive value of SSII for all-cause mortality in patients with one- or two-vessel disease undergoing PCI. Methods A total of 628 patients (76% men, mean age: 61+-10 years) undergoing PCI due to stable angina pectoris (43%) or acute coronary syndrome (57%), included between January 2008 and June 2013, were eligible for the current study. SSII was calculated using the original SYNTAX score website (www.syntaxscore.com). Cox regression analysis was used to assess the association between continuous SSII and long-term all-cause mortality. The area under the receiver-operating characteristic curve was used to assess the performance of SSII. Results SSII ranged from 6.6 to 58.2 (median: 20.4, interquartile range: 16.1-26.8). In multivariable analysis, SSII proved to be an independent significant predictor for 4.5-year mortality (hazard ratio per point increase: 1.10; 95% confidence interval: 1.07-1.13; p<0.001). In terms of discrimination, SSII had a concordance index of 0.77. Conclusion In addition to its established value in patients with left main and three-vessel disease, SSII may also predict long-term mortality in PCI-treated patients with one- or two-vessel disease","#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''","['Disambiguation of capitalized words in mixed-case texts has received little attention in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks.', 'In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected.', 'Such mandatory positions include the first word in a sentence, words in titles with all significant words capitalized or table entries, a capitalized word after a colon or open quote, and the first word in a list entry, among others.', 'Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said . . .', ', or they can be just capitalized common words, as in White elephants are . . . .', 'The disambiguation of capitalized words in ambiguous positions leads to the identification of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''"", 'Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.']",0,"[', or they can be just capitalized common words, as in White elephants are . . . .', ""#AUTHOR_TAG , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''""]"
CC384,J02-3002,"Periods, Capitalized Words, etc.",hybrid text mining for finding abbreviations and their definitions”,"['Youngja Park', 'Roy J Byrd']",,"We present a hybrid text mining method for finding abbreviations and their definitions in free format texts. To deal with the problem, this method employs pattern-based abbreviation rules in addition to text markers and cue words. The pattern-based rules describe how abbreviations are formed from definitions. Rules can be generated automatically and/or manually and can be augmented when the system processes new documents. The proposed method has the advantages of high accuracy, high flexibility, wide coverage, and fast recognition.",#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .,"['Not much information has been published on abbreviation identification.', 'One of the better-known approaches is described in Grefenstette and Tapanainen (1994), which suggested that abbreviations first be extracted from a corpus using abbreviation-guessing heuristics akin to those described in Section 6 and then reused in further processing.', 'This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', 'The main reason for restricting abbreviation discovery to a single document is that this does not presuppose the existence of a corpus in which the current document is similar to other documents.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .', 'This method first applies an ""abbreviation recognizer"" that generates a set of ""candidate abbreviations"" for a document.', 'Then for this set of candidates the system tries to find in the text their definitions (e.g., United Kingdom for UK).', 'The abbreviation recognizer for these purposes is allowed to overgenerate significantly.', 'There is no harm (apart from the performance issues) in proposing too many candidate abbreviations, because only those that can be linked to their definitions will be retained.', 'Therefore the abbreviation recognizer treats as a candidate any token of two to ten characters that contains at least one capital letter.', 'Candidates then are filtered through a set of known common words and proper names.', 'At the same time many good abbreviations and acronyms are filtered out because not for all of them will definitions exist in the current document.']",0,"['This is similar to our treatment of abbreviation handling, but our strategy is applied on the document rather than corpus level.', '#AUTHOR_TAG recently described a hybrid method for finding abbreviations and their definitions .']"
CC385,J02-3002,"Periods, Capitalized Words, etc.",adaptive multilingual sentence boundary disambiguation,"['David D Palmer', 'Marti A Hearst']",conclusion,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German.","For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .","['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']",1,"['A significant advantage of this approach is that it can be targeted to new domains completely automatically, without human intervention.', 'The four word lists that our system uses for its operation can be generated automatically from a raw corpus and require no human annotation.', 'Although some SBD systems can be trained on relatively small sets of labeled examples, their performance in such cases is somewhat lower than their optimal performance.', 'For instance , #AUTHOR_TAG report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .', ""This is a relatively small training set that can be manually marked in a few hours' time."", 'But the error rate (1.5%) of the decision tree classifier trained on this small sample was about 50% higher than that when trained on 6,000 labeled examples (1.0%).']"
CC386,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,inducing translation templates for examplebased machine translation,['Michael Carl'],introduction,"This paper describes an example-based machine translation (EBMT) system which relays on various knowledge resources. Morphologic analyses abstract the surface forms of the languages to be translated. A shallow syntactic rule formalism is used to percolate features in derivation trees. Translation examples serve the decomposition of the text to be translated and determine the transfer of lexical values into the target language. Translation templates determine the word order of the target language and the type of phrases (e.g. noun phrase, prepositional phase, ...) to be generated in the target language. An induction mechanism generalizes translation templates from translation examples. The paper outlines the basic idea underlying the EBMT system and investigates the possibilities and limits of the translation template induction process.","Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed .","['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed .']",0,"['When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string.', 'In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction.', 'Note that this ensures that greater importance is attributed to longer chunks , as is usual in most EBMT systems ( cfXXX Sato and Nagao 1990 ; Veale and Way 1997 ; #AUTHOR_TAG ) .7 As an example , consider the translation into French of the house collapsed .']"
CC387,J03-3004,wEBMT: Developing and Validating an Example-Based Machine Translation System Using the World Wide Web,three heads are better than one,"['Robert Frederking', 'Sergei Nirenburg']",conclusion,"With the stress of ongoing budget cuts librarians are tempted to hunker down and focus exclusively on their clients, their college, department or assigned area. But collaboration across campus, within new areas, with different faculty, and different students, can be beneficial to both student and faculty learning. Students often have research needs which cannot be answered by one faculty member or librarian. Cross disciplinary collaboration between multiple librarians and faculty is key to providing the best service to these students. In this case study a team of agribusiness students need help in preparing for a competition on food distribution. During the contest the students play the role of consultants, listen to a client's problem, research the industry and possible solutions, and then present a solution to the client. This competition requires research on commodities, government policies for food safety, food distribution, economics, management, marketing, and merchandising. A team was formed of an agriculture librarian, business librarian, and an agribusiness faculty advisor in order to cover all the elements required for student success. Each person played a specific role in preparing the students for the competition. The business librarian taught a selection of databases and online resources, the agriculture librarian taught agriculture resources and created a LibGuide specific to the contest, and the faculty advisor gave real world examples about the competition and best practices for their presentations. Outcomes of this collaboration included the sharing of knowledge about the research process, building bonds between faculty and librarians, knowledge transfer between the librarians, and successfully preparing the team of students for their competition","These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .","['These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .']",1,"['These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by #AUTHOR_TAG , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .']"
CC774,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a multipurpose interface to an online dictionary,"['Branimir Boguraev', 'David Carter', 'Ted Briscoe']",,,"In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .","['From the master LDOCE file, we have computed alternative indexing information, which allows access into the dictionary via different routes.', 'In addition to headwords, dictionary search through the pronunciation field is available; Carter (1987) has merged information from the pronunciation and hyphenation fields, creating an enhanced phonological representation which allows access to entries by broad phonetic class and syllable structure (Huttenlocher and Zue, 1983).', 'In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .', 'Independently, random selection of dictionary entries is also provided to allow the testing of software on an unbiased sample.']",0,"['In addition , a fully flexible access system allows the retrieval of dictionary entries on the basis of constraints specifying any combination of phonetic , lexical , syntactic , and semantic information ( #AUTHOR_TAG ) .']"
CC775,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,the grammar of english predicate complement constructions,['P S Rosenbaum'],,"A set of phrase structure rules and a set of transformational rules are proposed for which the claim is made that these rules enumerate the underlying and derived sentential structures which exemplify two productive classes of sentential embedding in English. These are sentential embedding in noun phrases and sentential embedding in verb phrases. First, following a statement of the grammatical rules, the phrase structure rules are analyzed and defended. Second, the transformational rules which map the underlying structures generated by the phrase structure rules onto appropriate derived structures are justified with respect to noun phrase and verb phrase complementation. Finally, a brief treatment is offered for the extension of the proposed descriptive apparatus to noun phrase and verb phrase complementation in predicate adjectival constructions. Thesis Supervisor: Noam Chomsky Title: Professor of Modern Languages",We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .,"['We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .', 'Figure 16 gives the number of verbs classified under each category by these authors and the number successfully classified into the same categories by the system.']",5,['We tested the classification of verbs into semantic types using a verb list of 139 pre-classified items drawn from the lists published in #AUTHOR_TAG and Stockwell et al. ( 1973 ) .']
CC776,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,processing dictionary definitions with phrasal pattern hierarchies in this issue,['Hiyan Alshawi'],,"This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns. An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English. A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions. The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones. This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism. Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions.","As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.","['A series of systems in Cambridge are implemented in Lisp running under UnixTM.', 'They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams.', 'A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.', 'To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.']",0,"['A series of systems in Cambridge are implemented in Lisp running under UnixTM.', 'They all make use of an efficient dictionary access system which services requests for s-expression entries made by client pro- grams.', 'A dictionary access process is fired off, which dynamically constructs a search tree and navigates through it from a given homograph directly to the offset in the lispified file from where all the associated information can be retrieved.', 'As #AUTHOR_TAG points out , given that no situations were envisaged where the information from the tape would be altered once installed in secondary storage, this simple and conventional access strategy is perfectly adequate.', 'The use of such standard database indexing techniques makes it possible for an active dictionary process to be very undemanding with respect to main memory utilisation.', 'For reasons of efficiency and flexibility of customisation, namely the use of LDOCE by different client programs and from different Lisp and/or Prolog systems, the dictionary access system is implemented in the programming language C and makes use of the inter-process communication facilities provided by the Unix operating system.', 'To the Lisp programmer, the creation of a dictionary process and subsequent requests for information from the dictionary appear simply as Lisp function calls.']"
CC777,J87-3002,Annotation-based finite state processing in a large-scale NLP arhitecture,a natural language toolkit reconciling theory with practice,['Branimir Boguraev'],introduction,"Generalized Phrase Structure Grammar (GPSG) is a highly restrictive theory of natural language syntax, characterised by complex interaction between its various rule types and constraints. Motivated by desire for declarative semantics, the theory defines these as applying simultaneously in the process of licensing local trees. As a result, as far as practical implementations of GPSG are concerned, the theory loses its apparent efficient parsability and becomes computationally intractable. This paper describes one aspect of an UK collaborative effort to produce a general purpose morphological and syntactic analyser for English within the theoretical framework of Generalized Phrase Structure Grammar, namely the development of a tractable grammatical formalism with clear semantics, capable of supporting the task of writing a substantial grammar. The paper outlines the intellectual and pragmatic background of the development effort and traces the incremental evolution of this formalism, following discussions concerning the fundamental issues of rules interpretation, feature system, grammar organisation, parser strategy, environment for grammar writing and support, and the construction of a lexicon linked to the grammar. Particular emphasis is placed on the quesiton of how theoretical standpoints have been reconciled with practical constraints, and how the commitment to deliver a functional morphological and syntactic analyser of wide scope and coverage of English has influenced the current state of the grammatical formalism.","The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .","['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'Real-time parsing imposes stringent requirements on a dictionary support environment; at the very least it must allow frequent and rapid access to the information in the dictionary via the dictionary head words.', 'The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']",0,"['Recent developments in linguistics, and especially on grammatical theory --for example, Generalised Phrase Structure Grammar (GPSG) (Gazdar et al., 1985), Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) --and on natural language parsing frameworks for example, Functional Unification Grammar (FUG) (Kay, 1984a), PATR-II (Shieber, 1984) --make it feasible to consider the implementation of efficient systems for the syntactic analysis of substantial fragments of natural language.', 'These developments also emphasise that if natural language processing systems are to be able to handle the grammatical and semantic idiosyncracies of individual lexical items elegantly and efficiently, then the lexicon must be a central component of the parsing system.', 'The research described below is taking place in the context of three collaborative projects ( #AUTHOR_TAG ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .', 'One motivation for our interest in machine readable dictionaries is to attempt to provide a substantial lexicon with lexical entries containing grammatical information compatible with the grammatical framework employed by the analyser.']"
CC778,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,synthesis of speech from unrestricted text,['J Allen'],experiments,"E diatribe ekhei san stokho ten melete ton prosodikon kanonon tes ellenikes glossas. Ta apotelesmata mporoun na ensomatothoun se opoiodepote sustema suntheses omilias anexarteta apo ten epilegmene strategike suntheses. E prosodia parousiazetai san polumetrike sunartese tes themeliodous sukhnotetas tes entases kai tes diarkeias ton phonematon kai parousiazontai montela se epipedo lexes toso gia argo oso kai gia gregoro ruthmo ekphoras. Epises parousiazontai montela kai kanones se epipleon protaseis me tropo pou exantlei ola ta suntaktika phainomena tes ellenikes. Proteinetai oti e prosodia se epipedo protases mporei na suntethei apo montela epipedou lexes upertithemena pano se mia pherousa se epipedo protases e klise tes opoias exartatai apo ten uparxe phainomenon emphases.This thesis aims at the study of the prosodic rules of the Greek language for use in a text to speech synthesis from unrestricted text. Regardless of the underlying synthesis stratregie (diphones, phonemes, etc). Prosody is treated as a polymetric function of fundamental frequency, intensity and duration of the phonemes. Prosodic models are presented first for isolated intonation words for various tempos including slow and fast. Models for large sentences are also presented in a way that all syntactic phenomena of the language are included and respected. It is suggested that sentence level prosodic models can be derived and synthesized from word-level models that are superimposed on a carrier spanning the whole sentence. The trend of the carrier is dependent upon various emphatic phenomena such as local stress or sentence emphasis","Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. #AUTHOR_TAG ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.']"
CC779,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,texttospeechan overview,"['S P Olive', 'M Y Liberman']",introduction,,"In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .","['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']",0,"['In previous work ( Bachenko et al. 1986 ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']"
CC780,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,from sad to glad emotional computer voices,['J Cahn'],experiments,"Synthesized English speech is readily distinguished from human speech on the basis of inappropriate intonation and insu cient expressiveness. This is a drawback for conversational computer systems. Intonation is the carrier of emphasis or de-emphasis, serving to clarify meaning for the spoken word much as variations in typeface and punctuation do for the written word. Expressiveness is not tied to word or phrase meaning but is global in scope. It provides the context in which the intonation occurs, and reveals the speaker's intentions and general mental state. In synthesized speech, intonation makes the message easier to understand; enhanced expressiveness contributes to dramatic e ect, making the message easier to listen to.","Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; #AUTHOR_TAG ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']"
CC781,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,texttospeechan overview,"['S P Olive', 'M Y Liberman']",experiments,,We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .,"['We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'Two concerns motivated our implementation.', 'First, we hoped the system would provide us with a research tool for testing our ideas about syntax and phrasing against a large unrestricted collection of sentences.', 'Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer.']",5,"['We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( #AUTHOR_TAG ) .', 'Second, we wished to investigate how well our approach would work for determining prosodic phrasing in a text-to-speech synthesizer.']"
CC782,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,now lets talk about now identifying cue phrases intonationally,"['J Hirschberg', 'D Litman']",introduction,"Cue phrases are words and phrases such as now and by the way which may be used to convey explicit information about the structure of a discourse. However, while cue phrases may convey discourse structure, each may also be used to different effect. The question of how speakers and hearers distinguish between such uses of cue phrases has not been addressed in discourse studies to date. Based on a study of now in natural recorded discourse, we propose that cue and non-cue usage can be distinguished intonationally, on the basis of phrasing and accent.",#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', '#AUTHOR_TAG and Litman and Hirschberg ( 1990 ) also examine the relation between discourse and prosodic phrasing .', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']"
CC783,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,aspects of the theory of syntax,['N Chomsky'],introduction,Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon,"Sentences like 12 , from #AUTHOR_TAG , are frequently cited .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from #AUTHOR_TAG , are frequently cited .', '(Square brackets mark off the NP constituents that contain embed- ded sentences.)']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12 , from #AUTHOR_TAG , are frequently cited .']"
CC784,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the contribution of parsing to prosodic phrasing in an experimental texttospeech system,"['J Bachenko', 'E Fitzpatrick', 'C E Wright']",introduction,"While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody, the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody. We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules. The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries. These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence.We discuss the results of an experiment to determine the performance of our system. We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate.","In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .","['In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'The system generated phrase boundaries using information derived from the syntactic structure of a sentence.', 'While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system.', 'Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing.', 'This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version.', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']",2,"['In previous work ( #AUTHOR_TAG ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .', 'Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.']"
CC785,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,finitestate parsing of phrasestructure languages and the status of readjustment rules in grammar,['D T Langendoen'],introduction,,"#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', '#AUTHOR_TAG proposes readjustment rules similar to those of Chomsky and Halle , but he claims that the readjustment of structure is part of the grammar , not part of the performance model .', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"
CC786,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,performance structures a psycholinguistic and linguistic appraisal,"['J P Gee', 'F Grosjean']",introduction,,"The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .","['The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating I[ the remaining green vegetables, where the subject-predicate boundary finds no prosodic correspondent. 4']",0,"['The psycholinguistic studies of Martin ( 1970 ) , Allen ( 1975 ) , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and #AUTHOR_TAG , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .']"
CC787,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,on stress and linguistic rhythm,"['M Y Liberman', 'A Prince']",introduction,"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry.","3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .","['Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase.', '3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .', 'This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing.']",1,"['3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in #AUTHOR_TAG , which ""are often overwhelmed by the chiaroscuro of highlight and background in discourse , but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option"" (p. 251) .']"
CC788,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the sound pattern of english,"['N Chomsky', 'M Halle']",introduction,"Since this classic work in phonology was published in 1968, there has been no other book that gives as broad a view of the subject, combining generally applicable theoretical contributions with analysis of the details of a single language. The theoretical issues raised in The Sound Pattern of English continue to be critical to current phonology, and in many instances the solutions proposed by Chomsky and Halle have yet to be improved upon.Noam Chomsky and Morris Halle are Institute Professors of Linguistics and Philosophy at MIT.","In #AUTHOR_TAG , this flattening process is not part of the grammar .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In #AUTHOR_TAG , this flattening process is not part of the grammar .', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers, e.g., Cooper and Paccia-Cooper (1980), to claim a direct mapping between the syntactic phrase and the prosodic phra,;e.', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In #AUTHOR_TAG , this flattening process is not part of the grammar .', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"
CC789,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,on stress and linguistic rhythm,"['M Y Liberman', 'A Prince']",experiments,"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. The MIT Press is collaborating with JSTOR to digitize, preserve and extend access to Linguistic Inquiry.","An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .","['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', 'Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing.']",1,"['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on #AUTHOR_TAG is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .', 'Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing.']"
CC790,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,prosodic systems and intonation in english,['D Crystal'],introduction,Preface 1. Some preliminary considerations 2. Past work on prosodic features 3. Voice-quality and sound attributes in prosodic study 4. The prosodic features of English 5. The intonation system of English 6. The grammar of intonation 7. The semantics of intonation Bibliography Index of persons Index of subjects.,"#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', '#AUTHOR_TAG claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject , predicate , modifier , and adjunct .', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.']"
CC791,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,speech rhythm its relation to performance universals and articulatory timing,['G Allen'],introduction,,"The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .","['The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance, and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths; this is the case in Chickens were eating I[ the remaining green vegetables, where the subject-predicate boundary finds no prosodic correspondent. 4']",0,"['The psycholinguistic studies of Martin ( 1970 ) , #AUTHOR_TAG , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .']"
CC792,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,capacity demands in shortterm memory for synthetic and natural speech,"['P A Luce', 'T C Feustel', 'D B Pisoni']",experiments,"Three experiments were performed that compared recall for synthetic and natural lists of monosyllabic words. In the first experiment, presentation intervals of 1, 2, and 5 s per word were used. Although free recall was consistently poorer overall for the synthetic lists at all presentation rates, the decrement for synthetic stimuli did not increase differentially with faster rates. In a second experiment, zero, three, and six digits were presented visually for retention prior to free recall of each spoken word list in a preload paradigm. Fewer subjects were able to correctly recall all of the digits for the six-digit list than the three-digit list when the following word lists were synthetic. The third experiment required ordered recall of lists of natural and synthetic words. Differences in ordered recall between the synthetic and natural word lists were substantially larger for the primacy portion of the serial position curve than the recency portion. These results indicate that difficulties observed in the perception and comprehension of synthetic speech are due, in part, to increased processing demands in short-term memory.","Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .","['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators (e.g.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .', ""And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem."", 'As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech.', 'We believe that one reason for the improvement has to do with the increased pitch range that our system uses.', 'Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations.', 'Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech.', 'In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules?', 'and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?']",4,"['Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand.', 'Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; #AUTHOR_TAG ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .']"
CC793,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,toward treating english nominals correctly,"['R W Sproat', 'M Y Liberman']",experiments,We describe a program for assigning correct stress contours to nominals in English. It makes use of idiosyncratic knowledge about the stress behavior of various nominal types and general knowledge about English stress rules. We have also investigated the related issue of parsing complex nominals in English. The importance of this work and related research to the problem of text-to-speech is &apos;discussed,"Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .","['Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience.', 'An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984), which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree.', 'Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .']",1,"['Although a grid may be more descriptively suitable for some aspects of prosody ( for example , #AUTHOR_TAG use the grid representation for their implementation of stress assignment in compound nominals ) , we are not aware of any evidence for or against a grid representation of discourseneutral phrasing .']"
CC794,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,the contribution of parsing to prosodic phrasing in an experimental texttospeech system,"['J Bachenko', 'E Fitzpatrick', 'C E Wright']",introduction,"While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody, the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody. We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules. The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries. These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence.We discuss the results of an experiment to determine the performance of our system. We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate.","Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",2,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Previous versions of our work , as described in #AUTHOR_TAG also assume that phrasing is dependent on predicate-argument structure .', 'The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985), who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.']"
CC795,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,syntax and speech,"['W Cooper', 'J Paccia-Cooper']",introduction,"Interactions between phonology and syntax are inspected in continuous speech samples from 30 speech-delayed children. Two types of interactions are examined: The co-occurrence of speech and language delay and the effects of phonological reduction on the realization of phonetically complex morphophonemes. Four possible patterns of association between the phonological and syntactic systems are outlined, and subjects are assigned to these patterns based on their phonological and syntactic performance. Results indicate that two-thirds of the subjects display evidence of overall syntactic delay, whereas half show some limitation in the use of phonetically complex morphophonemes, their performance in that area being below the level of their syntactic production. Implications of these findings for a theory of speech delay and for management programming are discussed","This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .","['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'Sentences like 12, from Chomsky (1965) To account for such mismatches, ""readjustment rules"" that change constituent structure by adjoining each embedded sente, nce to the node dominating it have been posited.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'In Chomsky and Halle (1968), this flattening process is not part of the grammar.', 'Rather, it is viewed as ""... a performance factor, related to the difficulty of producing right branching structures such as [ 12]"" (p.', '372).', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'Langendoen (1975) proposes readjustment rules similar to those of Chomsky and Halle, but he claims that the readjustment of structure is part of the grammar, not part of the performance model.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']",0,"['When it comes to sentence-level prosody, especially phrasing, it is often true, as we will see below, that a sequence of words dominated by the same syntactic node cohere more closely than a sequence of words dominated by two different nodes.', 'This observation has led some researchers , e.g. , #AUTHOR_TAG , to claim a direct mapping between the syntactic phrase and the prosodic phrase .', ""However, this claim is controversial because of the misa'dgnments that occur between the two levels of phrasing."", 'For example, in considering the connection between syntax and phrasing, the linguistic literature most often refers to examples of embedded sentences.', 'The result is a flattened structure that more accurately reflects the prosodic phrasing.', 'Thus phrasing, in their approach, is only indirectly related to syntax, since readjustment is done by special rules outside the grammar proper.', 'He thus makes explicit what is often a tacit assumption in both the linguistic and psycholinguistic literature2--that there is a direct connection between syntactic constituency and prosodic phrasing, with apparent misalignments readjusted before syntax interface,; with prosodic phonology.']"
CC796,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,performance structures a psycholinguistic and linguistic appraisal,"['J P Gee', 'F Grosjean']",introduction,,"Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .","['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']",1,"['The psycholinguistic studies of Martin (1970), Allen (1975), Hitlinger et al. (1976, Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency.', 'For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary comes between the verb and object, i.e., the data reveal both X(VY) and (XV)Y groupings.', 'Grosjean et al. (1979) claims that such mismatches are due for the most part to constituent length, which interacts with grammatical structure and, in some cases, overrides it.', 'Thus syntactic and prosodic structure match when the major constituents of a sentence are roughly equal in length; for example, the main prosodic phrase break corresponds to the subject-predicate boundary in Waiters who remember well ][ serve orders correctly.', 'Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in #AUTHOR_TAG ( henceforth G&G ) .']"
CC797,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,aspects of prosody,['J Bing'],introduction,,"The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .","['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing.', 'Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct.', 'Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework.', 'Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure.', 'The problem here is that the phrasing in observed data often ignores the argument status of constituents.', 'In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts.', 'All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others.', '(The complement in 17a and the adjuncts in 17b-f are italicized.)', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer.', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).', 'To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse.', 'Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing.', 'Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.']",0,"['The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations.', 'The relation between discourse and prosodic phrasing has been examined in some detail by #AUTHOR_TAG , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse .', 'This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.).']"
CC798,J90-3003,Prosodic phrasing for speech synthesis of written telecommunications by the deaf,prosodic structure and spoken word recognition,"['F Grosjean', 'J P Gee']",introduction,,"Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .","['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Thus in our analysis, rules of phonological word formation apply to the non-null terminal nodes in a syntax tree.', 'If the terminal is a content word, i.e. noun, verb, adjective, or adverb, then this terminal may have the status of a phonological word on its own.', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.', 'This is accomplished by adjoining a word to its left or right neighbor depending on its lexical category and its position in the tree.', 'Function words, e.g.', 'auxiliary verbs, articles, prepositions, pronouns, and conjunctions, are all eligible for adjunction in certain syntactic contexts.', 'Content words, copular verbs, demonstratives, quantifiers and elements in the complementizer node can serve as hosts for the adjoined material or stand alone.']",5,"['Our rules for phonological word formation are adopted , for the most part , from G & G , #AUTHOR_TAG , and the account of monosyllabic destressing in Selkirk ( 1984 ) .', 'Otherwise the word combines with one or more orthographically distinct words to form a single phonological word that has no internal word or phrase boundaries.']"
CC799,J91-2003,On compositional semantics,logic and conversationquot,['H P Grice'],introduction,,"Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .","['However, there are at least three arguments against iterating PT.', 'First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.', 'Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .', 'Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.', 'Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.']",4,"['Secondly , the cooperative principle of #AUTHOR_TAG , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .', 'Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph.']"
CC800,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .","['At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others.', 'We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .', 'The quoted works seem to be good representatives for each of the directions; they also point to related literature.']",1,"['We are going to make such a comparison with the theories proposed by J. #AUTHOR_TAG , 1982 ) that represent a more computationally oriented approach to coherence , and those of T.A. van Dijk and W. Kintch ( 1983 ) , who are more interested in addressing psychological and cognitive aspects of discourse coherence .']"
CC801,J91-2003,On compositional semantics,domain circumscription a reevaluationquot,"['D W Etherington', 'R E Mercer']",introduction,,"Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.', 'Similarly, the notion of R+ M-abduction is spiritually related to the ""abduc- tive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of Berwick (1986).', 'But, ob- viously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure as- sumption"" (ibid.), ""domain circumscription"" (cf. #AUTHOR_TAG), and their kin.']"
CC802,J91-2003,On compositional semantics,the boundaries of words and their meaningsquot,['W Labov'],introduction,,W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .,['W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .'],0,['W. #AUTHOR_TAG discussed sentences of the form * This is a chair but you can sit on it .']
CC803,J91-2003,On compositional semantics,a grammar of contemporary english,"['R Quirk', 'S Greenbaum', 'G Leech', 'J Svartvik']",introduction,"The publication of this important volume fills the need for an up-to-date survey of the entire scope of English syntax. Though it falls short of a perfectly balanced treatment of the whole system, it touches upon all the essential topics and treats in depth a number of crucial problems of current interest such as case, ellipsis, and information focus. Even the publishers' claims are vindicated to a surprising degree. The statement that it ""constitutes a standard reference grammar"" is reasonably well justified. Recent investigations, including the authors' own research, are integrated into the ""accumulated grammatical tradition"" quite effectively. But whether it is ""the fullest and most comprehensive synchronic description of English grammar ever written"" is arguable. No one acquainted with Poutsma's work would agree with that. Very advanced foreign students o r native speakers of English who want to learn about basic grammar will find some of thel sections suitable for their needs, such as the lesson about restrictive and nonrestrictive relative clauses, though even here some of the explanations require very intensive study. Most of the chapters are rather like an advanced textbook for teachers or linguists. The organization and viewpoint give the impression of a carefully planned university lecture supplemented by diagrams, charts, and lists. A good example is the lesson on auxiliaries and verb phrases, which starts with a set of sample sentences demonstrating that ""should see"" and ""happen to see"" behave differently under various transformations and expansions. After the essential concepts are explained and exemplified-lexical verb, semi-auxiliary, operator, and the like-lists and paradigms are given as in the usual reference work. A particularly useful feature of this chapter is the outline of modal auxiliaries with examples of their divergent meanings.","Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .","['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']",0,"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition (called ""adversative"" or ""contrary-to-expectation"" by Halliday and Hasan 1976;cf. also #AUTHOR_TAG , p. 672 ) .']"
CC804,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , #AUTHOR_TAG , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']"
CC805,J91-2003,On compositional semantics,a dictionary of modern english usage,['H W Fowler'],introduction,,"This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .","['Still, our definition of coherence may not be restrictive enough: two collections of sentences, one referring to ""black"" (about black pencils, black pullovers, and black poodles), the other one about ""death"" (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic ""black + death.""', ""This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .""]",0,"[""This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words ( e.g. , `` colorless green ideas ... '' ) , while before the advent of Chomskyan formalisms , a sentence was defined as the smallest meaningful collection of words ; #AUTHOR_TAG , p. 546 ) gives 10 definitions of a sentence .""]"
CC806,J91-2003,On compositional semantics,mental models,['P N Johnson-Laird'],introduction,"The complexity of conceptualizing mental models has made Virtual Reality an interesting way to enhance communication and understanding between individuals working together on a project or idea. Here, the authors discuss practical applications of using VR for this purpose","This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by #AUTHOR_TAG , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']"
CC807,J91-2003,On compositional semantics,a theory of truth and semantic representationquot,['H Kamp'],introduction,,"But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora .","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But , obviously , there are other possibilities -- for instance , the discourse representation structures ( DRS 's ) of #AUTHOR_TAG , which have been used to translate a subset of English into logical formulas , to model text ( identified with a list of sentences ) , to analyze a fragment of English , and to deal with anaphora ."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC808,J91-2003,On compositional semantics,so what can we talk about nowquot,['B Webber'],introduction,Impure water is made suitable for drinking in an apparatus comprising a pressurizable holding tank attached to a purification cartridge containing an impurities adsorbent and a fine filter. A gas-containing cartridge is pierced to provide a bactericidal gas for killing pathogenic microorganisms and for pressurizing the holding tank to force the water through the purification cartridge.,"Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; Sidner 1983 ) or quantifier scoping ( #AUTHOR_TAG ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"
CC809,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .","['According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', 'However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.)', 'suddenly (for Hobbs) becomes coherent.', ""It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added."", 'On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of the background knowledge, the paragraph is incoherent.', 'And the paragraph obtained by adding the third sentence is coherent.', 'Moreover, coherence here is clearly the result of the existence of the topic ""John likes spinach.""']",1,"['According to #AUTHOR_TAG , p. 67 ) , these two sentences are incoherent .', ""It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added."", 'And the paragraph obtained by adding the third sentence is coherent.', 'Moreover, coherence here is clearly the result of the existence of the topic ""John likes spinach.""']"
CC810,J91-2003,On compositional semantics,artificial intelligence the very idea,['J Haugeland'],introduction,"The idea that human thinking and machine computing are ""radically the same"" provides the central theme for this marvelously lucid and witty book on what artificial intelligence is all about. Although presented entirely in nontechnical","For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap","['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning.', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap']",0,"['This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning.', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance , relating ""they"" to ""apples"" in the sentence ( cfXXX #AUTHOR_TAG p. 195 ; Zadrozny 1987a ) : We bought the boys apples because they were so cheap']"
CC811,J91-2003,On compositional semantics,the episode schema in story processingquot,"['K Haberlandt', 'C Berian', 'J Sandson']",introduction,,Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .,"['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .']",0,"['These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower ( 1979 ) and #AUTHOR_TAG .']"
CC812,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .","['However, there are at least three arguments against iterating PT.', 'First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time.', 'Secondly, the cooperative principle of Grice (1975Grice ( , 1978, under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader.', 'Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .', 'Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.']",4,"['Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by #AUTHOR_TAG strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .']"
CC813,J91-2003,On compositional semantics,krypton a functional approach to knowledge representationquot,"['R J Brachman', 'R E Fikes', 'H J Levesque']",introduction,,"The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .","['The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .', 'Brachman et al. 1985).', ""KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL."", ""However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior."", 'From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard.', 'In our system, we also distinguish between the ""definitional"" and factual information, but the ""definitional"" part contains collections of mutually excluding theories, not just of formulas describing a semantic network.', 'Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, ""coherence"" and ""dominance,"" which are not variants of the standard first order entailment, but abduction.']",1,"['The last point may be seen better if we look at some differences between our system and KRYPTON , which also distinguishes between an object theory and background knowledge ( cfXXX #AUTHOR_TAG ) .']"
CC814,J91-2003,On compositional semantics,abductive inferencequot,['J A Reggia'],introduction,,"Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"[""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of #AUTHOR_TAG , the `` diagnosis from first principles '' of Reiter ( 1987 ) , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .""]"
CC815,J91-2003,On compositional semantics,the interpretation of tense in discoursequot,['B Webber'],introduction,,The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]",0,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event compo- nents.', 'The reader may consult recent papers on this subject ( e.g. Moens and Steedman 1987 ; #AUTHOR_TAG ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.']"
CC816,J91-2003,On compositional semantics,organizational patterns in discoursequot,['J Hinds'],introduction,,"According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .","['The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists, classifies, and discusses various types of inference, by which he means, generally, ""the linguistic-logical notions of consequent and presupposition"" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences).', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), ex- pository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']",0,"['The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists, classifies, and discusses various types of inference, by which he means, generally, ""the linguistic-logical notions of consequent and presupposition"" Crothers (1979:112) have collected convincing evidence of the existence of language chunks--real struc- tures, not just orthographic conventions--that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences).', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to #AUTHOR_TAG , paragraphs are made up of segments , which in turn are made up of sentences or clauses , which in turn are made up of phrases .', 'Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']"
CC817,J91-2003,On compositional semantics,a theory of diagnosis from first principlesquot,['R Reiter'],introduction,,"Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) ."", 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', 'Etherington and Mercer 1987), and their kin.', ""Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of #AUTHOR_TAG , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .""]"
CC818,J91-2003,On compositional semantics,inference without chainingquot,['A Frisch'],introduction,,"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .","['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]",1,"['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; #AUTHOR_TAG ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]"
CC819,J91-2003,On compositional semantics,the game of language,['J Hintikka'],introduction,,"This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG .","['Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references.', ""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG ."", 'Hintikka (1985).']",1,"[""This means that natural language expressions such as `` A is B , '' `` A is the same as B , '' etc. are not directly represented by logical equality ; similarly , `` not '' is often not treated as logical negation ; cfXXX #AUTHOR_TAG .""]"
CC820,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).","['Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).', 'We will have, however, no need for ""strong"" notions in this paper.', 'Also, in a practical system, ""satisfies"" should be probably replaced by ""violates fewest.""']",1,"['Note: The notions of strong provability and strong R + M-abduction can be in- troduced by replacing ""there exists"" by ""all"" in the above definitions (cf. #AUTHOR_TAGb ).', 'We will have, however, no need for ""strong"" notions in this paper.']"
CC821,J91-2003,On compositional semantics,38 examples of elusive antecedents from published texts,['J R Hobbs'],introduction,,"Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .","['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .', 'Later, Hobbs (1979Hobbs ( , 1982 proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using ""salience"" in choosing facts from this knowledge base.']",0,"['Adding selectional restrictions ( semantic feature information , #AUTHOR_TAG ) does not solve the problem , because isolated features offer only part of the background knowledge necessary for reference disambiguation .']"
CC822,J91-2003,On compositional semantics,temporal ontology in natural languagequot,"['M Moens', 'M Steedman']",introduction,,The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.', ""Extending and revising Jackendoff's (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the gram- matical constraint (that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon---ibid.).""]",0,"['The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject ( e.g. #AUTHOR_TAG ; Webber 1987 ) to see what a formal interpretation of events in time might look like .', 'Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator.']"
CC823,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .","['Since it is the ""highest"" path, fint is the most plausible (relative to R) interpretation of the words that appear in the sentence.', 'Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .', 'Zadrozny 1987aZadrozny , 1987b.', 'Another theory, consisting of f~ = {el, sh2, pl, b2~ dl} and S, saying that A space vehicle came into the harbor and caused a disease~illness is less plausible according to that ordering.', 'As it turns out, f~ is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fint would lead to a contradiction, for instance within the higher level context of a science fiction story.']",0,"['Because it is also consistent , it will be chosen as a best interpretation of S , ( cfXXX #AUTHOR_TAGa , 1987b ) .']"
CC824,J91-2003,On compositional semantics,the paragraph as a grammatical unit,['R E Longacre'],introduction,,"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , #AUTHOR_TAG , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.']"
CC825,J91-2003,On compositional semantics,analysis without actual infinityquot,['J Mycielski'],introduction,,"As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983, Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics.', 'As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .', 'Mycielski 1981).']",0,"['For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'As a logical postulate it is not very radical ; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science , such as mathematical analysis ( cfXXX #AUTHOR_TAG ) .', 'Mycielski 1981).']"
CC826,J91-2003,On compositional semantics,disambiguating prepositional phrase attachments by using online dictionary definitionsquot computational linguistics 1334251260 special issue on the lexicon,"['K Jensen', 'J-L Binot']",introduction,,"We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .","['We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']",2,"['We have shown elsewhere ( #AUTHOR_TAG ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .']"
CC827,J91-2003,On compositional semantics,introduction to artificial intelligence,"['E Charniak', 'D McDermott']",introduction,"This book is an introduction on artificial intelligence. Topics include reasoning under uncertainty, robot plans, language understanding, and learning. The history of the field as well as intellectual ties to related disciplines are presented.","The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .","['The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']",0,"['The necessity of this kind of merging of arguments has been recognized before : #AUTHOR_TAG call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']"
CC828,J91-2003,On compositional semantics,universal grammarquot,['R Montague'],introduction,,"The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of #AUTHOR_TAG is more sophisticated , and may be considered another possibility .', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", 'Jackendoff (1983, p. 14) writes ""it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.""', 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC829,J91-2003,On compositional semantics,semantics and cognition,['R Jackendoff'],introduction,"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication","This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , #AUTHOR_TAG , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .']"
CC830,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .","['The textualist approach to paragraph analysis is exemplified by E. J. Crothers.', 'His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'Paragraphs therefore give hierarchical structure to sentences.', 'Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'The three types are procedural (how-to), expository (essay), and narrative (in this case, spontaneous conversation).', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).', 'Segments themselves are composed of clauses and regulated by ""switching"" patterns, such as the question-answer pattern and the remark-reply pattern.']",0,"['The textualist approach to paragraph analysis is exemplified by E. J. Crothers.', 'His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs.', 'He lists , classifies , and discusses various types of inference , by which he means , generally , `` the linguistic-logical notions of consequent and presupposition  #AUTHOR_TAG:112 ) have collected convincing evidence of the existence of language chunks -- real structures , not just orthographic conventions -- that are smaller than a discourse , larger than a sentence , generally composed of sentences , and recursive in nature ( like sentences ) .', 'These chunks are sometimes called ""episodes,"" and sometimes ""paragraphs.""', 'According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases.', 'Paragraphs therefore give hierarchical structure to sentences.', 'Hinds discusses three major types of paragraphs, and their corresponding segment types.', 'For each type, its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined).']"
CC831,J91-2003,On compositional semantics,dont blame the toolquot,['W Woods'],introduction,,"However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .","['Once the word ""up"" is given its meaning relative to our experience with gravity, it is not free to ""slip"" into its opposite.', '""Up"" means up and not down ....', 'We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model.', 'Mothers have a different role than fathers in this model, and thus there is a reason why ""Death is the father of beauty"" fails poetically while ""Death is the mother of beauty"" succeeds ....', 'It is precisely this ""grounding"" of logical predicates in other conceptual structures that we would like to capture.', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .', 'Woods 1987).']",0,"['Once the word ""up"" is given its meaning relative to our experience with gravity, it is not free to ""slip"" into its opposite.', '""Up"" means up and not down ....', 'We investigate here only the ""grounding"" in logical theories.', 'However , it is possible to think about constraining linguistic or logical predicates by simulating physical experiences ( cfXXX #AUTHOR_TAG ) .', 'Woods 1987).']"
CC832,J91-2003,On compositional semantics,semantics and cognition,['R Jackendoff'],introduction,"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication","#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''","['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']",1,"['We assume here that a translation of the surface forms of sentences into a logical formalism is possible.', 'So we will use a very simple formalism, like the one above, resembling the standard first order language.', ""But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora."", 'The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility.', ""Jackendoff's (1983) formalism is richer and resembles more closely an English grammar."", ""#AUTHOR_TAG , p. 14 ) writes `` it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys . ''"", 'Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics.', 'It will also be a model for our simplified logical notation (cf.', 'Section 5).', 'We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.']"
CC833,J91-2003,On compositional semantics,cues people use to paragraph text,"['S J Bond', 'J R Hayes']",introduction,"This paper reports the results of three studies on the paragraph. In Study 1, subjects were asked to paragraph a text from which paragraph indentations had been removed. Results indicate that readers can consistently paragraph unparagraphed text, thus supporting Young and Becker's (1966) assertion that the paragraph is a psychologically real unit of discourse. Results also reveal that readers rely heavily on breaks in text cohesion (e.g., topic shift) and on paragraph length as paragraphing cues. In Study 2, four new subjects were asked to paragraph the same text used in Study 1, and to ""think aloud,"" giving their reasons for paragraphing, as they did so. Analysis of these thinking aloud protocols reveal that, in the absence of a strong paragraphing cue, readers will read ahead in a text, sometimes flagging weaker paragraphing cues as they go. If they feel the unparagraphed text is too long, they will go back and paragraph at these weaker cues until all paragraphs in the text are an acceptable length. Based on the results of Studies 1 and 2, a model of how readers paragraph was devised. The model was tested in Study 3 on new subjects who were asked to think aloud as they paragraphed the same text used in Study 1 , and another, longer text. The model predicted the new data quite accurately. Deciding whether paragraph boundaries are psychologically real or arbitrary is very much like deciding whether geographical boundaries are psychologically real or arbitrary. Typically, state boundaries are not psychologically real because travelers cannot find them without the help of signs. Coast lines, on the other hand, are very real. People who miss them fall into the ocean. In the same way, we would consider paragraph boundaries artificial if people could find them only with the help of paragraphing marks, and real if people could consistently find them in texts from which paragraphing marks had been removed. Some linguists such as Hodges (1941) have viewed the paragraph as an arbitrary device used by the writer to ""give the reader a breathing spell"" (p. 311). Similarly, Rodgers (1967) has suggested that a section of text ""becomes a paragraph not by virtue of its structure, but because the writer elects to indent"" (p. 182). However, Young and Becker (1966) and Koen, Becker, and Young (1969) have provided strong evidence that paragraphs are indeed psychologically real. They asked readers to paragraph text from which all paragraphing markers had been removed and found that their readers Research in the Teaching of English, Vol. 18, No. 2, May 1984",An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .,"['An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980).']",0,"['An example of psycholinguistically oriented research work can be found in #AUTHOR_TAG .', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholinguistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980).']"
CC834,J91-2003,On compositional semantics,paragraph structure inference,['E J Crothers'],introduction,,"#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' .","['This demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri.', '(Other reference works could be treated as additional sources of world knowledge.)', 'This type of consultation uses existing natural language texts as a referential level for processing purposes.', 'It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit.', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", 'With respect to that independent source of knowledge, our main contributions are two.', 'First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation.', 'In other words, we recognize it as a separate logical level--the referential level.', 'Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level.']",0,"['(Other reference works could be treated as additional sources of world knowledge.)', ""#AUTHOR_TAG , p. 112 ) , for example , bemoans the fact that his `` theory lacks a world knowledge component , a mental ` encyclopedia , ' which could be invoked to generate inferences ... '' ."", 'Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill the role of the referential level.']"
CC835,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?","['6.1.1 Was the Use of a Gricean Maxim Necessary?', 'Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?', 'It seems to us that the answer is no.']",0,"['Can one deal effectivelywith the problem of reference without axiomatized Gricean maxims, for instance by using only ""petty conversational implicature"" ( #AUTHOR_TAG ) , or the metarules of Section 5.2?']"
CC836,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .","['We adopt the three-level semantics as a formal tool for the analysis of paragraphs.', 'This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .', 'It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates.', 'For instance, relating ""they"" to ""apples"" in the sentence (cf.', 'Haugeland 1985 p. 195;Zadrozny 1987a):']",5,"['This semantics was constructed ( #AUTHOR_TAGa , 1987b ) as a formal framework for default and commonsense reasoning .']"
CC837,J91-2003,On compositional semantics,learning from positiveonly examples the subset principle and three case studiesquot,['R C Berwick'],introduction,,"Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .","['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', '""domain circumscription"" (cf.', 'Etherington and Mercer 1987), and their kin.', 'Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .', 'But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models.', 'These connections are being examined elsewhere (Zadrozny forthcoming).']",1,"['Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of ""unique-name assumption"" (Genesereth and Nilsson 1987), ""domain closure assumption"" (ibid.),', 'Etherington and Mercer 1987), and their kin.', 'Similarly, the notion of R + M-abduction is spiritually related to the ""abductive inference"" of Reggia (1985), the ""diagnosis from first principles"" of Reiter (1987), ""explainability"" of Poole (1988), and the subset principle of #AUTHOR_TAG .', 'These connections are being examined elsewhere (Zadrozny forthcoming).']"
CC838,J91-2003,On compositional semantics,the episode schema in story processingquot,"['K Haberlandt', 'C Berian', 'J Sandson']",introduction,,"Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , #AUTHOR_TAG ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']"
CC839,J91-2003,On compositional semantics,focusing in the comprehension of definite anaphoraquot,['C Sidner'],introduction,,"Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'Other factors , such as the role of focus ( Grosz 1977 , 1978 ; #AUTHOR_TAG ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']"
CC840,J91-2003,On compositional semantics,death is the mother of beauty,['M Turner'],introduction,"Let's read! We will often find out this sentence everywhere. When still being a kid, mom used to order us to always read, so did the teacher. Some books are fully read in a week and we need the obligation to support reading. What about now? Do you still love reading? Is reading only for you who have obligation? Absolutely not! We here offer you a new book enPDFd death is the mother of beauty to read.","Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .","['The referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of up and down that are based by the way our bodies actually function.']",0,"['The referential structures we are going to use are collections of logical theories, but the concept of reference is more general.', 'Some of the intuitions we associate with this notion have been very well expressed by #AUTHOR_TAG , pp. 7-8 ) : ... Semantics is constrained by our models of ourselves and our worlds .', 'We have models of up and down that are based by the way our bodies actually function.']"
CC841,J91-2003,On compositional semantics,passing markers a theory of contextual influence in language comprehensionquot,['E Charniak'],introduction,,"`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .","['The idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]",1,"[""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( Hirst 1987 ; #AUTHOR_TAG ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]"
CC842,J91-2003,On compositional semantics,a logic of implicit and explicit beliefsquot,['H J Levesque'],introduction,,"Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .","['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]",1,"['Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX #AUTHOR_TAG ; Frisch 1987 ; Patel-Schneider 1985 ) .', 'Levesque 1984;Frisch 1987;Patel-Schneider 1985).', ""But we won't pursue this topic further here.""]"
CC843,J91-2003,On compositional semantics,the representation and use of focus in a system for understanding dialogsquot,['B J Grosz'],introduction,"As a dialog progresses the objects and actions that are most relevant to the conversation, and hence in the focus of attention of the dialog participants, change. This paper describes a representation of focus for language understanding systems, emphasizing its use in understanding task-oriented dialogs. The representation highlights that part of the knowledge base relevant at a given point in a dialog. A model of the task is used both to structure the focus representation and to provide an index into potentially relevant concepts in the knowledge base The use of the focus representation to make retrieval of items from the knowledge base more efficient is described.","Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .","['We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution.', 'They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it).', 'Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .', 'Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself.']",0,"['Other factors , such as the role of focus ( #AUTHOR_TAG , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .']"
CC844,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .","['Note: In our translation from English to logic we are assuming that ""it"" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around).', 'This means that the ""it"" that brought the disease in P1 will not be considered to refer to the infection ""i"" or the death ""d"" in P3.', 'This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .']",4,"['This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( #AUTHOR_TAG , p. 329 ) .']"
CC845,J91-2003,On compositional semantics,parsing strategies in a broadcoverage grammar of english research report rc 12147 ibm tj watson research center,['K Jensen'],introduction,," #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .","['We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.', 'This approach is taken in computational syntactic grammars (e.g.', ' #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .']",0,"['This approach is taken in computational syntactic grammars (e.g.', ' #AUTHOR_TAG ) ; the number of unlikely parses is severely reduced whenever possible , but no attempt is made to define only the so-called grammatical strings of a language .']"
CC846,J91-2003,On compositional semantics,semantic interpretation and the resolution of ambiguity,['G Hirst'],introduction,"Preface 1. Introduction 2. Semantic interpretation 3. The Absity semantic interpreter 4. Lexical disambiguation 5. Polaroid words 6. Structural disambiguation 7. The semantic enquiry desk 8. Conclusion 9. Speculations, partially baked ideas, and exercises for the reader References Index of names Index of subjects.","`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .","['The idea of using preferences among theories is new, hence it was described in more detail.', ""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]",1,"[""`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( #AUTHOR_TAG ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .""]"
CC847,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .","['We do not claim that Gla is the best or unique way of expressing the rule ""assume that the writer did not say too much.""', 'Rather, we stress the possibility that one can axiomatize and productively use such a rule.', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .']",2,"['We do not claim that Gla is the best or unique way of expressing the rule ""assume that the writer did not say too much.""', 'We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by #AUTHOR_TAG .']"
CC848,J91-2003,On compositional semantics,cohesion in english,"['M A K Halliday', 'R Hasan']",introduction,"Despite all the dire predictions, Germany continues to exhibit a high level of social cohesion. Even the country's growing cultural and religious diversity is not at odds with its degree of togetherness. Yet, there are clear indications of potential threats: for example, the generally perceived lack of social justice and the gaping cleavage in togetherness between east and west as well as between structurally weak regions and such that are flourishing. To that end, cohesion is much weaker in Germany's eastern federal states than its western ones. The federal states with the highest levels of cohesion are Saarland, Baden-Wurttemberg, and Bavaria. These are the core findings from the Social Cohesion Radar (SCR), for which Bertelsmann Stiftung collected fresh data in 2017 by surveying more than 5,000 people throughout the country. The goal was to examine cohesion in terms of its strengths, weaknesses, causes, and effects. For this empirical study, social cohesion is defined as the quality of communal life and is viewed as a multidimensional phenomenon. High levels of cohesion result from strong social relations, a positive feeling of connectedness to the community, and a strong focus on the common good","Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .","['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']",0,"['Connectives are function words--like conjunctions and some adverbs--that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units.', '""And,"" ""or,"" and ""but"" are the three main coordinating connectives in English.', 'However, ""but"" does not behave quite like the other two--semantically, ""but"" signals a contradiction, and in this role it seems to have three subfunctions: . .', 'Opposition ( called ""adversative"" or ""contrary-to-expectation"" by #AUTHOR_TAG ; cfXXX also Quirk et al. 1972 , p. 672 ) .']"
CC849,J91-2003,On compositional semantics,the flow of thought and the flow of languagequot,['W L Chafe'],introduction,,"#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .","['Although there are other discussions of the paragraph as a central element of discourse (e.g.', '#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them.', 'Our interest, however, lies precisely in that area.']",1,"['#AUTHOR_TAG , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .', 'Our interest, however, lies precisely in that area.']"
CC850,J91-2003,On compositional semantics,a theory of truth and semantic representationquot,['H Kamp'],introduction,,"This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .","['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf.', 'Mycielski 1981).']",1,"['All logical notions that we are going to consider, such as theory or model, will be finitary.', 'For example, a model would typically contain fewer than a hundred elements of different logical sorts.', 'Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them.', 'The issues of control are not so important for us at this point; we restrict ourselves to describing the logic.', 'This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , #AUTHOR_TAG , and implicitly or explicitly by almost all researchers in computational linguistics .', 'Mycielski 1981).']"
CC851,J91-2003,On compositional semantics,episodes as chunks in narrative memoryquot,"['J B Black', 'G H Bower']",introduction,,Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .,"['An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983).', 'These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']",0,"['These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit.', 'Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information.', 'Other psycholing-uistic studies that confirm the validity of paragraph units can be found in #AUTHOR_TAG and Haberlandt et al. ( 1980 ) .']"
CC852,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.","We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .","['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']",2,"['We have shown elsewhere ( Jensen and Binot 1988 ; #AUTHOR_TAGa , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .', 'This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.']"
CC853,J91-2003,On compositional semantics,intended models circumscription and commonsense reasoningquot,['W Zadrozny'],introduction,"We describe a new method of formalizing commonsense reasoning : Instead of minimizing extensions of predicates we formalize common sense as ""truth in intended models"", and we give a mathematical formulation of the proposed method by defining a class of intended models based on preferences in interpretations.    We propose to use problem independent natural language constraints to exclude interpretations that contradict common sense. For this reason we augment the usual, two-part formal structures consisting of a metalevel and an object level, with a third level - a referential level. We show that such a model is an adequate tool for generating intended interpretations, and also for problems that cannot be satisfactorily solved by circumscription.    We argue that the criticism of Hanks and McDermott (1986) does not apply to the formalization of commonsense reasoning by intended models. Namely, it is not necessary to know the consequences of the intended theory to find the right interpretation, neither needed are complex and problem specific policy axioms, which exclude some possible interpretations.",It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .,"['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']",1,"['Building an interpretation of a paragraph does not mean finding all of its possible meanings; the implausible ones should not be computed at all.', 'This viewpoint has been reflected in the definition of a partial theory as a most plausible interpretation of a sequence of predicates.', 'Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence.', 'We can then later (Section 5.2) define p-models--a category of models corresponding to paragraphs--as models of coherent theories that satisfy all metalevel conditions.', 'The partial theories pick up from the referential level the most obvious or the most important information about a formula.', 'This immediate information may be insufficient to decide the truth of certain predicates.', 'It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX #AUTHOR_TAGb ) .', 'Zadrozny 1987b).']"
CC854,J91-2003,On compositional semantics,coherence and coreferencequot,['J R Hobbs'],introduction,,"Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .","['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]",0,"['Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation.', ""Later , #AUTHOR_TAG , 1982 ) proposed a knowledge base in which information about language and the world would be encoded , and he emphasized the need for using `` salience '' in choosing facts from this knowledge base .""]"
CC855,J91-2003,On compositional semantics,resolving pronoun referencesquot,['J R Hobbs'],introduction,,"The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .","['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']",0,"['The necessity of this kind of merging of arguments has been recognized before : Charniak and McDermott ( 1985 ) call it abductive unification/matching , #AUTHOR_TAG , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .']"
CC856,J91-2003,On compositional semantics,languages with self reference i foundationsquot,['D Perlis'],introduction,,"Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .","['5.1.1', 'Translation to Logic.', 'The text concerns events happening in time.', 'Naturally, we will use a logical notation in which formulas may have temporal and event components.', 'We assume that any formal interpretation of time will agree with the intuitive one.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Moens and Steedman 1987;Webber 1987) to see what a formal interpretation of events in time might look like.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .', 'Extending and revising Jackendoff\'s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (""that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon""---ibid.).', 'However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives.', 'After these remarks we can begin constructing the model of the example paragraph.', 'We assume that constants are introduced by NPs.', 'We have then (i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d), infection(i), death(b), year(1347).']",0,"['5.1.1', 'Translation to Logic.', 'So it is not necessary now to present a formal semantics here.', 'The reader may consult recent papers on this subject (e.g.', 'Since sentences can refer to events described by other sentences , we may need also a quotation operator ; #AUTHOR_TAG describes how first order logic can be augmented with such an operator .']"
CC857,J91-2003,On compositional semantics,lectures on contemporary syntactic theories csli lecture notes,['P Sells'],introduction,,"For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''","['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', ""That is, we can ask in the first sentence of a paragraph about Kissinger's favorite fruit, elaborate the question and the circumstances in the next few sentences, and give the above answer at the end."", 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']",4,"['A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings.', ""For instance , #AUTHOR_TAG , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''"", 'The pairing of these two sentences may be said to create a small paragraph.', 'Our point is that an acceptable structure can be assigned to the utterance ""Reagan thinks bananas"" only within the paragraph in which this utterance occurs.', 'We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one) neighboring sentences, will be sufficient for this task.', 'We do not claim that a paragraph is necessarily described by a set of grammar rules in some grammar formalism (although it may be); rather, it has the grammatical role of providing functional structures that can be assigned to strings.']"
CC858,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the interaction of word recognition and linguistic processing in speech understandingquot,['H Niemann'],introduction,"This contribution describes an approach to integrate a speech understanding and dialog system into a homogeneous architecture based on semantic networks. The definition of the network as well as its use in speech understanding is described briefly. A scoring function for word hypotheses meeting the requirements of a graph search algorithm is presented. The main steps of the linguistic analysis, i.e. syntax, semantics, and pragmatics, are described and their realization in the semantic network is shown. The processing steps alternating between data- and model-driven phases are outlined using an example sentence which demonstrates a tight interaction between word recognition and linguistic processing.","Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , #AUTHOR_TAG , and Young ( 1989 ) .']"
CC859,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,automatic speech recognition the development of the sphinx system appendix i,['K F Lee'],,,A formula for the test set perplexity ( #AUTHOR_TAG ) is :13,['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13'],0,['A formula for the test set perplexity ( #AUTHOR_TAG ) is :13']
CC860,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the voyager speech understanding system preliminary development and evaluationquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"Early experience with the development of the MIT VOYAGER spoken language system is described, and its current performance is documented. The three components of VOYAGER, the speech recognition component, the natural language component, and the application back-end, are described.>",The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,"['The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The third version ( VOYAGER ) serves as an interface both with a recognizer and with a functioning database back-end ( #AUTHOR_TAG ) .']"
CC861,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop,"['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']",,,Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .,"['To obtain training and test data for this task, we had a number of naive subjects use the system as if they were trying to obtain actual information.', 'Their speech was recorded in a simulation mode in which the speech recognition component was excluded.', 'Instead, an experimenter in a separate room typed in the utterances as spoken by the subject.', 'Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .']",5,['Subsequent processing by the natural language and response generation components was done automatically by the computer ( #AUTHOR_TAG ) .']
CC862,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,on whmovementquot in formal syntax edited by,['Noam Chomsky'],,,( #AUTHOR_TAG ) .,['( #AUTHOR_TAG ) .'],0,['( #AUTHOR_TAG ) .']
CC863,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,grammaticallybased automatic word class formationquot,"['L Hirschman', 'R Grishman', 'N Sager']",,,This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .,"['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']",1,"['In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'This approach resembles the work by Grishman et al. ( 1986 ) and #AUTHOR_TAG on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.']"
CC864,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,error bounds for convolutional codes and an asymptotically optimal decoding algorithmquot,['A Viterbi'],,"The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.","The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .","['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system (Zue et al. 1989), which uses a segmental-based framework and includes an auditory model in the front-end processing.', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .']",5,"['This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search ( #AUTHOR_TAG ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .']"
CC865,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the case for casequot in universals in linguistic theory,['C J Fillmore'],,,Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .,"['Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .', 'For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as ""leave.""', 'Thus a flight can ""leave for Chicago from Boston at nine,"" or, equivalently, ""leave at nine for Chicago from Boston.""', 'If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place]s, [to-place]s and [at-time]s is possible.', 'This is of course unacceptable, but it is straightforward to have each node, as it occurs, or in a semantic bit specifying its case frame, and, in turn, fail if that bit has already been set.', 'We have found that this strategy, in conjunction with the capability of erasing all semantic bits whenever a new clause is entered (through the meta level ""detach"" operation mentioned previously) serves the desired goal of eliminating the unwanted redundancies.']",5,['Semantic filters can also be used to prevent multiple versions of the same case frame ( #AUTHOR_TAG ) showing up as complements .']
CC866,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the bbn spoken language systemquot,"['S Boisen', 'Y-L Chow', 'A Haas', 'R Ingria', 'S Roukos', 'D Stallard']",introduction,,"Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .']",0,"['Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in #AUTHOR_TAG , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and Young ( 1989 ) .']"
CC867,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,full integration of speech and language understanding in the mit spoken language systemquot,"['D Goodine', 'S Seneff', 'L Hirschman', 'M Phillips']",,,"We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) .","['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions (Zue et al. 1991), the tightly coupled system allows the parser to discard partial theories that have no way of continuing.', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) ."", ""Ultimately we want to incorporate TINA'S probabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]",5,"[""We have not yet made use of TINA 'S probabilities in adjusting the recognizer scores on the fly , but we have been able to incorporate linguistic scores to resort N-best outputs , giving a significant improvement in performance ( #AUTHOR_TAG ) .""]"
CC868,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,speech database development design and analysis of the acousticphonetic corpusquot,"['L Lamel', 'R H Kassel', 'S Seneff']",,,The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .', 'The second version (RM) concerns the Resource Management task (Pallett 1989) that has been popular within the DARPA community in recent years.', 'The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,['The first version ( TIMIT ) was developed for the 450 phonetically rich sentences of the TIMIT database ( #AUTHOR_TAG ) .']
CC869,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,development and preliminary evaluation of the mit atis systemquot,"['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']",,,"However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .","['Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.', 'However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.', 'It then passes along to the right sibling the same frame that was handed to it from above, with the completed topic slot filled with the information delivered by the children.']",3,"['However , the method we are currently using in the ATIS domain ( #AUTHOR_TAG ) represents our most promising approach to this problem .', 'We have decided to limit semantic frame types to a small set of choices such as CLAUSE (for a sentence-level concept, such as request), PREDICATE (for a functional operation), REFERENCE (essentially proper noun), and QSET (for a set of objects).', 'The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.', 'Each node receives a frame in both a top-down and a bottom-up cycle, and modifies the frame according to specifications based on its broad-class identity (as one of noun, noun-phrase, predicate, quantifier, etc.).', 'For example, a [subject] is a noun-phrase node with the label ""topic.""', 'During the top-down cycle, it creates a blank frame and inserts it into a ""topic"" slot in the frame that was handed to it.', 'It passes the blank frame to its children, who will then fill it appropriately, labeling it as a QSET or as a REFERENCE.']"
CC870,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,development and preliminary evaluation of the mit atis systemquot,"['S Seneff', 'J Glass', 'D Goddeau', 'D Goodine', 'L Hirschman', 'H Leung', 'M Phillips', 'J Polifroni', 'V Zue']",,,"The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.","['We currently have two application domains that can carry on a spoken dialog with a user.', 'One, the VOYAGER domain (Zue et al. 1990), answers questions about places of interest in an urban area, in our case, the vicinity of MIT and Harvard University.', 'The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.']",5,"['We currently have two application domains that can carry on a spoken dialog with a user.', 'The second one, ATIS ( #AUTHOR_TAG et al. 1991), is a system for accessing data in the Official 80 Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains. Our current research is directed at a number of different remaining issues.']"
CC871,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,discovery procedures for sublanguage selectional patterns initial experimentsquot,"['R Grishman', 'L Hirschman', 'N T Nhan']",,"Selectional constraints specify, for a particular domain, the combinations of semantic classes acceptable in subject-verb-object relationships and other syntactic structures. These constraints are important in blocking incorrect analyses in natural language processing systems. However, these constraints are domain-specific and hence must be developed anew when a system is ported to a new domain. A discovery procedure for selectional constraints is therefore essential in enhancing the portability of such systems.This paper describes a semi-automated procedure for collecting the co-occurrence patterns from a sample of texts in a domain, and then using these patterns as the basis for selectional constraints in analyzing further texts. We discuss some of the difficulties in automating the collection process, and describe two experiments that measure the completeness of these patterns and their effectiveness compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how these procedures could be combined with a system that queries a domain expert, in order to produce a more efficient discovery procedure.",This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .,"['Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies.', 'This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains.', 'In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest.', 'Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals.', 'This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .', 'The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences.', 'There is obviously a great deal more work to be done in this important area.']",1,['This approach resembles the work by #AUTHOR_TAG and Hirschman et al. ( 1975 ) on selectional restrictions .']
CC872,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,estimation of probabilities from sparse data for the language model component of a speech recognizerquot assp35,['S M Katz'],,"The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises.","Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .","['This appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in ""three hundred and sixteen.""', 'Included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'Since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'This is a problem to be aware of in building grammars from example sentences.', 'In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .']",0,"['This appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in ""three hundred and sixteen.""', 'Included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.', 'Since there are only five training sentences, a number of the arcs of the original grammar are lost after training.', 'This is a problem to be aware of in building grammars from example sentences.', 'In the absence of a sufficient amount of training data, some arcs will inevitably be zeroed out.', 'Unless it is desired to intentionally filter these out as being outside of the new domain , one can insert some arbitrarily small probability for these arcs , using , for example , an N-gram back-off model ( #AUTHOR_TAG ) .']"
CC873,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,on whmovementquot in formal syntax edited by,['Noam Chomsky'],,,"To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .","['2.5.1 Gaps.', 'The mechanism to deal with gaps resembles in certain respects the Hold register idea of ATNs, but with an important difference, reflecting the design philoso-phy that no node can have access to information outside of its immediate domain.', 'The mechanism involves two slots that are available in the feature vector of each parse node.', 'These are called the CURRENT-FOCUS and the FLOAT-OBJECT, respectively.', 'The CURRENT-FOCUS slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'If the FLOAT-OBJECT slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the FLOAT-OBJECT.', 'The process of getting into the FLOAT-OBJECT slot (which is analogous to the Hold register) requires two steps, executed independently by two different nodes.', 'The first node, the generator, fills the CURRENT-FOCUS slot with the subparse returned to it by its children.', 'The second node, the activator, moves the CURRENT-FOCUS into the FLOAT-OBJECT position, for its children, during the top-down cycle.', 'It also requires that the FLOAT-OBJECT be absorbed somewhere among its descendants by a designated absorber node, a condition that is checked during the bottom-up cycle.', 'The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an [end] node is encountered.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .', 'Finally, certain blocker nodes block the transfer of the FLOAT-OBJECT to their children.']",0,"['2.5.1 Gaps.', 'The mechanism involves two slots that are available in the feature vector of each parse node.', 'The CURRENT-FOCUS slot contains, at any given time, a pointer to the most recently mentioned content phrase in the sentence.', 'If the FLOAT-OBJECT slot is occupied, it means that there is a gap somewhere in the future that will ultimately be filled by the partial parse contained in the FLOAT-OBJECT.', 'The first node, the generator, fills the CURRENT-FOCUS slot with the subparse returned to it by its children.', 'The CURRENT-FOCUS only gets passed along to siblings and their descendants, and hence is unavailable to activators at higher levels of the parse tree.', 'That is to say, the CURRENT-FOCUS is a feature, like verb-mode, that is blocked when an [end] node is encountered.', 'To a first approximation , a CURRENT-FOCUS reaches only nodes that are c-commanded ( #AUTHOR_TAG ) by its generator .']"
CC874,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the voyager speech understanding system preliminary development and evaluationquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",conclusion,"Early experience with the development of the MIT VOYAGER spoken language system is described, and its current performance is documented. The three components of VOYAGER, the speech recognition component, the natural language component, and the application back-end, are described.>","One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .","['We_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', 'The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains.']",5,"['We_currently have two application domains that can carry on a spoken dialog with a user.', 'One , the VOYAGER domain ( #AUTHOR_TAG ) , answers questions about places of interest in an urban area , in our case , the vicinity of MIT and Harvard University .', 'The second one, ATIS (Seneff et al. 1991), is a system for accessing data in the Official Airline Guide and booking flights.', 'Work continues on improving all aspects of these domains.']"
CC875,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,semantics and quantification in natural language question answeringquot,['W A Woods'],,,"The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator .","[""The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."", 'Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator.', 'The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?', 'The CURRENT-FOCUS slot is not restricted to nodes that represent nouns.', 'Some of the generators are adverbial or adjectival parts of speech (pos).', 'An absorber checks for agreement in POS before it can accept the FLOAT-OBJECT as its subparse.', 'As an example, the question, ""(How oily)/do you like your salad dressing (ti)?"" contains a [q-subject] ""how oily"" that is an adjective.', 'The absorber [pred-adjective] accepts the available float-object as its subparse, but only after confirming that POS is ADJECTIVE.']",1,"[""The example used to illustrate the power of ATNs ( #AUTHOR_TAG ) , `` John was believed to have been shot , '' also parses correctly , because the [ object ] node following the verb `` believed '' acts as both an absorber and a ( re ) generator ."", 'The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)?', 'The CURRENT-FOCUS slot is not restricted to nodes that represent nouns.', 'Some of the generators are adverbial or adjectival parts of speech (pos).', 'As an example, the question, ""(How oily)/do you like your salad dressing (ti)?"" contains a [q-subject] ""how oily"" that is an adjective.']"
CC876,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the use of a semantic network in speech dialoguequot,['G Th Niedermair'],introduction,,"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , #AUTHOR_TAG , Niemann ( 1990 ) , and Young ( 1989 ) .']"
CC877,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,benchmark tests for darpa resource management database performance evaluationsquot,['D Pallett'],,"A nominally 1000-word resource management database for continuous speech recognition was developed for use in the DARPA Speech Research Program. This database has now been used at several sites for benchmark tests, and the database is expected to be made available to a wider community in the near future. The author documents the structure of the benchmark tests, including the selection of test material and details of studies of scoring algorithms.>",The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .,"['To date, four distinct domain-specific versions of TINA have been implemented.', 'The first version (TIMIT) was developed for the 450 phonetically rich sentences of the TIMIT database (Lamel et al. 1986).', 'The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .', 'The third version (VOYAGER) serves as an interface both with a recognizer and with a functioning database back-end (Zue et al. 1990).', 'The VOYAGER system can answer a number of different types of questions concerning navigation within a city, as well as provide certain information about hotels, restaurants, libraries, etc., within the region.', 'A fourth domain-specific version is under development for the ATIS (Air Travel Information System) task, which has recently been designated as the new common task for the DARPA community.']",5,['The second version ( RM ) concerns the Resource Management task ( #AUTHOR_TAG ) that has been popular within the DARPA community in recent years .']
CC878,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,a formal basis for the heuristic determination of minimum cost pathsquot,"['P Hart', 'N J Nilsson', 'B Raphael']",,"Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.","For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .","['Some modification of this scheme is necessary when the input stream is not deterministic.', 'For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', 'Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found.']",5,"['For the A * algorithm ( #AUTHOR_TAG ) as applied to speech recognition , the actual path score is typically augmented with an estimated score for the unseen portion .', 'Unless some kind of normalization is done, the short theories have an unfair advantage, simply because fewer probability scores have been multiplied.', 'With a deterministic word sequence it seems reasonable to assume probability 1.0 for what has been found.']"
CC879,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the collection and preliminary analysis of a spontaneous speech databasequot darpa speech and natural language workshop,"['V Zue', 'N Daly', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff', 'M Soclof']",,,"The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .","['At present, we have available at MIT two systems, VOYAGER and ATIS, involving specific application domains in which a person can carry on a dialog with the computer, either through spoken speech or through text input.', 'In both of these systems, TINA provides the interface between the recognizer and the application back-end.', 'In this section, I will describe our current interfaces between TINA and the recognizer and our future plans in this area.', 'In addition, I will describe briefly how we currently translate the parse tree into a semantic frame that serves as the input to database access and text response generation.', 'This aspect of the system is beyond the scope of this paper, and therefore it will not be covered in detail.', 'The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .', 'The lexicon is entered as phonetic pronunciations that are then augmented to account for a number of phonological rules.', 'The search algorithm is the standard Viterbi search (Viterbi 1967), except that the match involves a network-to-network alignment problem rather than sequence-to-sequence.']",5,"['The recognizer for these systems is the SUMMIT system ( #AUTHOR_TAG ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .']"
CC880,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,transition network grammars for natural language analysisquot,['W A Woods'],,"The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described.","The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.","['This section describes how TINA handles several issues that are often considered to be part of the task of a parser.', 'These include agreement constraints, semantic restrictions, subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in ""(which article)/do you think I should read (ti)?"") (Chomsky 1977).', 'The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.']",1,"['This section describes how TINA handles several issues that are often considered to be part of the task of a parser.', 'The gap mechanism resembles the Hold register idea of ATNs ( #AUTHOR_TAG ) and the treatment of bounded domination metavariables in lexical functional grammars ( LFGs ) ( Bresnan 1982 , p. 235 ff . ), but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.']"
CC881,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,the minds system using context and dialog to enhance speech recognitionquot,['S R Young'],introduction,,"Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .","['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']",0,"['Over the past few years, there has been a gradual paradigm shift in speech recognition research both in the U.S. and in Europe.', 'In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding. 1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an ""understanding"" of the intended message.', 'In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.', 'Current advances in research and development of spoken language systems 2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project.', 'Representative systems are described in Boisen et al. ( 1989 ) , De Mattia and Giachin ( 1989 ) , Niedermair ( 1989 ) , Niemann ( 1990 ) , and #AUTHOR_TAG .']"
CC882,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,integration of speech recognition and natural language processing in the mit voyager systemquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>","We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .","['When we first integrated this recognizer with TINA, we used a ""wire"" connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.', 'A simple word-pair grammar constrained the search space.', 'If the parse failed, then the sentence was rejected.', ""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) ."", 'Both the A* and the Viterbi search are left-to-right search algorithms.', 'However, the A* search is contrasted with the Viterbi search in that the set of active hypotheses take up unequal segments of time.', 'That is, when a hypothesis is scoring well it is allowed to procede forward, whereas poorer scoring hypotheses are kept on hold.']",5,"[""We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( #AUTHOR_TAG ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .""]"
CC883,J92-1004,TINA: a probabilistic syntactic parser for speech understanding systems,integration of speech recognition and natural language processing in the mit voyager systemquot,"['V Zue', 'J Glass', 'D Goodine', 'H Leung', 'M Phillips', 'J Polifroni', 'S Seneff']",,"The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.>","Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .","['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]",5,"['Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( #AUTHOR_TAG ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .', 'Following the Viterbi search, each partial theory is first extended by the parser to specify possible next words, which are then scored by the recognizer.', ""We have not yet made use of TINA'Sprobabilities in adjusting the recognizer scores on the fly, but we have been able to incorporate linguistic scores to resort N-best outputs, giving a significant improvement in performance (Goodine et al. 1991)."", ""Ultimately we want to incorporate TINA'Sprobabilities directly into the A* search, but it is as yet unclear how to provide an appropriate upper bound for the probability estimate of the unseen portion of the linguistic model.""]"
CC884,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .', 'This description can then be given the standard set-theoretical interpretation of King (1989, 1994).']",0,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by #AUTHOR_TAG defines a formal lexical rule specification language and provides a semantics for that language in two steps : A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1 .']"
CC885,J97-4003,On Expressing Lexical Generalizations in HPSG,offline constraint propagation for efficient hpsg processing,"['Detmar Meurers', 'Guido Minnen']",introduction,We investigate the use of a technique developed in the constraint programming community called constraint propagation to automatically make a HPSG theory more specific at those places where linguistically motivated underspecification would lead to inefficient processing. We discuss two concrete HPSG examples showing how off-line constraint propagation helps improve processing efficiency.,The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.,"['The most specific generalization does not necessarily provide additional constrain- ing information.', 'However, usually it is the case that lexical entries resulting from lexical rule application differ in very few specifications compared to the number of specifica- tions in a base lexical entry.', 'Most of the specifications of a lexical entry are assumed to be passed unchanged via the automatically generated frame specification.', 'Therefore, after lifting the common information into the extended lexical entry, the out-argument in many cases contains enough information to permit a postponed execution of the interaction predicate.', 'When C is the common information, and D1, ..., Dk are the definitions of the interaction predicate called, we use distributivity to factor out C in (C A D1) V -.. V (C A Dk): We compute C A (D1 V ... V Dk), where the r) are assumed to contain no further common factors.', 'Once we have computed c, we use it to make the extended lexical entry more specific.', 'This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.']",0,"['This technique closely resembles the off-line constraint propagation technique described by Marriott, Naish, and Lassez (1988).', 'The reader is referred to #AUTHOR_TAG for a more detailed discussion of our use of constraint propagation.']"
CC886,J97-4003,On Expressing Lexical Generalizations in HPSG,the compleat lkb,['Ann Copestake'],related work,,"This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .","['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( Copestake 1992 ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( #AUTHOR_TAG , 31 ) .']"
CC887,J97-4003,On Expressing Lexical Generalizations in HPSG,controlling the application of lexical rules,"['Ted Briscoe', 'Ann Copestake']",introduction,"In this paper, we describe an item-familiarity account of the semi-productivity of morphological and lexical rules, and illustrate how it can be applied to practical issues which arise when building large scale lexical knowledge bases which utilize lexical rules. Our approach assumes that attested uses of derived words and senses are explicitly recorded, but that productive use of lexical rules is also possible, though controlled by probabilities associated with rule application. We discuss how the necessary probabilities and estimates of lexical rule productivity may be acquired from corpora.","27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .","['The way these predicates interconnect is represented in Figure 19.', '27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', '28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates.', 'Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given.']",0,"['27 #AUTHOR_TAG argue that semi-productivity of lexical rules , which can be understood as a generalization of exceptions to lexical rules , can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry .', 'Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given.']"
CC888,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC889,J97-4003,On Expressing Lexical Generalizations in HPSG,the representation of lexical semantic information cognitive science research paper csrp 280,['Ann Copestake'],introduction,,"This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .","['This disjunction thus constitutes the base lexicon.', 'The disjuncts in the constraint on derived-word, on the other hand, encode the lexical rules.', 'The in-specification of a lexical rule specifies the IN feature, the out-specification, the derived word itself.', 'Note that the value of the IN feature is of type word and thus also has to satisfy either a base lexical entry or an out-specification of a lexical rule.', 'While this introduces the recursion necessary to permit successive lexical rule application, it also grounds the recursion in a word described by a base lexical entry.', 'Contrary to the MLR setup, the DLR formalization therefore requires all words feeding lexical rules to be grammatical with respect to the theory.', 'Since lexical rules are expressed in the theory just like any other part of the theory, they are represented in the same way, as unary immediate dominance schemata.', 'This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .', 'Both the input and output of a lexical rule, i.e., the mother and the daughter of a phrase structure rule, are available during a generation or parsing process.', 'As a result, in addition to the information present in the lexical entry, syntactic information can be accessed to execute the constraints on the input of a lexical rule.', 'The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.', '9']",0,"['This conception of lexical rules thus can be understood as underlying the computational approach that treats lexical rules as unary phrase structure rules as , for example , adopted in the LKB system ( #AUTHOR_TAG ) .', 'The computational treatment of lexical rules that we propose in this paper is essentially a domain-specific refinement of such an approach to lexical rules.']"
CC890,J97-4003,On Expressing Lexical Generalizations in HPSG,transformations of logic programs foundations and techniques,"['Alberto Pettorossi', 'Maurizio Proietti']",introduction,,"As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .","['The automata resulting from word class specialization group the lexical entries into natural classes.', 'In case the automata corresponding to two lexical entries are identical, the entries belong to the same natural class.', 'However, each lexical rule application, i.e., each transition in an automaton, calls a frame predicate that can have a large number of defining clauses.', 'Intuitively understood, each defining clause of a frame predicate corresponds to a subclass of the class of lexical entries to which a lexical rule can be applied.', 'During word class specialization, though, when the finite-state automaton representing global lexical rule application is pruned with respect to a particular base lexical entry, we know which subclass we are dealing with.', 'For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones.', 'We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984).', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .', 'Given a lexical entry as in Figure 15, we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section.', 'To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.', '3° The successive unfolding steps are schematically represented in Figure 20.']",1,"['We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates.', 'The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984).', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body ( #AUTHOR_TAG ) .', 'To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates.']"
CC891,J97-4003,On Expressing Lexical Generalizations in HPSG,modularizing contexted constraints,['John Griffith'],introduction,"This paper describes a method for compiling a constraint-based grammar into a potentially more efficient form for processing. This method takes dependent disjunctions within a constraint formula and factors them into non-interacting groups whenever possible by determining their independence. When a group of dependent disjunctions is split into smaller groups, an exponential amount of redundant information is reduced. At runtime, this means that an exponential amount of processing can be saved as well. Since the performance of an algorithm for processing constraints with dependent disjunctions is highly determined by its input, the transformation presented in this paper should prove beneficial for all such algorithms. 1 Introduction  There are two facts that conspire to make the treatment of disjunction an important consideration when building a natural language processing (NLP) system. The first fact is that natural languages are full of ambiguities, and in a grammar many of these ambi..",32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .,"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing.']",0,['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( Maxwell and Kaplan 1989 ; Eisele and Dorre 1990 ; #AUTHOR_TAG ) can be used to circumvent constraint propagation .']
CC892,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .","['The translation of the lexical rule into a predicate is trivial.', 'The result is displayed description language.', '12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 Hinrichs and Nakazawa (1996) show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention.', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler.']",0,"['12 In order to focus on the computational aspects of the covariation approach , in this paper we will not go into a discussion of the full lexical rule specification language introduced in #AUTHOR_TAG .', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.']"
CC893,J97-4003,On Expressing Lexical Generalizations in HPSG,partialvp and splitnp topicalization in german an hpsg analysis,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,,"6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .","['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world (Gerdemann and King 1994;Gerdemann 1995).', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['6 The Partial-VP Topicalization Lexical Rule proposed by #AUTHOR_TAG , 10 ) is a linguistic example .']"
CC894,J97-4003,On Expressing Lexical Generalizations in HPSG,open and closed world types in nlp systems,['Dale Gerdemann'],introduction,,4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['The terminology used in the literature varies.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and King 1994 ; #AUTHOR_TAG ) .']"
CC895,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.","Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .","['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AI (McCarthy and Hayes 1969), and we will therefore refer to the specifications left implicit by the linguist as the frame specification, or simply frame, of a lexical rule.', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .']",0,"['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things , but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up ( #AUTHOR_TAG ) .']"
CC896,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.",As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .,"['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', 'Most current HPSG analyses of Dutch, German, Italian, and French fall into that category.', '1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.', 'Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']",4,"['A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon.', '1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time.', 'Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry.', 'This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned.', 'As shown in #AUTHOR_TAG this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .']"
CC897,J97-4003,On Expressing Lexical Generalizations in HPSG,unfoldfold transformation of logic programs,"['Hisao Tamaki', 'Taisuke Sato']",introduction,,The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .,"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .', '29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987).', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of the clause.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']",5,"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( #AUTHOR_TAG ) .', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']"
CC898,J97-4003,On Expressing Lexical Generalizations in HPSG,statische programmtransformationen zur effizienten verarbeitung constraintbasierter grammatiken diplomarbeit,['Annette Opalka'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; #AUTHOR_TAG ; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC899,J97-4003,On Expressing Lexical Generalizations in HPSG,the formalism and implementation of patr ii,"['Stuart Shieber', 'Hans Uszkoreit', 'Fernando Pereira', 'Jane Robinson', 'Mabry Tyson']",related work,,A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( #AUTHOR_TAG ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC900,J97-4003,On Expressing Lexical Generalizations in HPSG,the typed feature structure representation formalism,['Martin Emele'],related work,,A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( Emele and Zajac 1990 ; #AUTHOR_TAG ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC901,J97-4003,On Expressing Lexical Generalizations in HPSG,lexical rules in hpsg what are they,"['Mike Calcagno', 'Carl Pollard']",introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']",0,"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; Calcagno 1995 ; #AUTHOR_TAG ) and the .', 'lexical rules (DLRs; Meurers 1995).']"
CC902,J97-4003,On Expressing Lexical Generalizations in HPSG,an expanded logical formalism for headdriven phrase structure grammar,['Paul King'],,"Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic.",The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .,"['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'Our compiler distinguished seven word classes.', 'Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations.']",5,"['The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and #AUTHOR_TAG ; Gotz and Meurers 1997a ) .', 'We tested the covariation approach with a complex grammar implementing an HPSG analysis covering the so-called aux-flip phenomenon, and partial-VP topicalization in the three clause types of German (Hinrichs, Meurers, and Nakazawa 1994).', 'This test grammar includes eight lexical rules; some serve syntactic purposes, like the Partial-VP Topicalization Lexical Rule, others are of morphological nature as, for example, an inflectional lexical rule that relates nonfinite verbs to their finite form.', 'Some nouns and most verbal lexical entries fed lexical rules, and a single base lexical entry resulted in up to 12 derivations.']"
CC903,J97-4003,On Expressing Lexical Generalizations in HPSG,interpreting lexical rules,['Mike Calcagno'],introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).', '5']",0,"['Two formalizations of lexical rules as used by HPSG linguists have been proposed , the meta-level lexical rules ( MLRs ; #AUTHOR_TAG ; Calcagno and Pollard 1995 ) and the .', 'lexical rules (DLRs; Meurers 1995).']"
CC904,J97-4003,On Expressing Lexical Generalizations in HPSG,flipped out aux in german,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,,"/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( #AUTHOR_TAG ) that also use lexical rules such as the Complement Extraction Lexical Rule (Pollard and Sag 1994) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC905,J97-4003,On Expressing Lexical Generalizations in HPSG,ale—the attribute logic engine users guide version 201,"['Bob Carpenter', 'Gerald Penn']",related work,"ale 3.0 is completely compatible with ale 2.0 grammars, and adds the following new features: * A semantic-head-driven generator, based on the algorithm presented in Shieber et al. (1990). The generator was adapted to the logic of typed feature structures by Octav Popescu in his Carnegie Mellon Master's Thesis, Popescu (1996). Octav also wrote most of the generation code for this release. Grammars can be compiled for parsing only, generation only, or both. Some glue-code is also available from the ale homepage, to parse and generate with different grammars through a unix pipe. * A source-level debugger with a graphical XEmacs interface. This debugger works only with SICStus Prolog 3.0.6 and higher. A debugger with reduced functionality will be made available to SWI Prolog users in a later release. This debugger builds on, and incorporates the functionality of the code for the SICStus source-level debugger, written by Per Mildner at Uppsala University. * a /1 atoms. There are now an infinite number of atoms (types with no appropriate features), implicitly declared in every signature. These atoms can be arbitrary Prolog terms, including unbound variables, and can be used wherever normal ale types can, e.g., f:(a p(3.7)). a /1 atoms are extensional as Prolog terms, i.e., are taken to be identical according to the Prolog predicate , ==/2. In particular, this means that ground atoms behave exactly as ale extensional types. * Optional edge subsumption checking. For completeness of parsing, one only needs to ensure that, for every pair of nodes in the chart, the most general feature structure spanning those nodes is stored in the chart. This can reduce the number of edges in many domains. * An autonomous intro/2 operator. Features can now be declared on their own in a separate part of the grammar. * Default specifications for types. These are NOT default types. If a type appears on the right-hand side of a sub/2 or intro/2 specification, but not on iii iv CONTENTS the left-hand side of one, ale will assume this type is maximal, i.e., assume the specification, T ype sub []. Similarly, if it occurs on a left-hand side, but not on a right-hand side, ale will assume the type is immediately subsumed by bot, the most general type. In both cases, ale will announce these assumptions during compilation. * Several bug corrections and more compile-time warning and error messages. * An SWI Prolog 2.9.7 ...","A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .","['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available.', 'A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'In the ALE system, for example, a depth bound can be specified for this purpose.', 'Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.']",1,"['A common computational treatment of lexical rules adopted , for example , in the ALE system ( #AUTHOR_TAG ) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time .', 'While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation.', 'A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited.', 'In the ALE system, for example, a depth bound can be specified for this purpose.', 'Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.']"
CC906,J97-4003,On Expressing Lexical Generalizations in HPSG,a logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,,"A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .","['A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .', 'The formal language of King allows the expression of grammatical principles using type assignments to refer to the type of an object and path equalities to require the (token) identity of objects.', 'These atomic expressions can be combined using conjunction, disjunction, and negation.', 'The expressions are interpreted by a set-theoretical semantics.']",0,"['A logic that provides the formal architecture required by Pollard and Sag ( 1994 ) was defined by #AUTHOR_TAG , 1994 ) .', 'The expressions are interpreted by a set-theoretical semantics.']"
CC907,J97-4003,On Expressing Lexical Generalizations in HPSG,prolog and natural language analysis csli lecture notes center for the study of language and information,"['Fernando Pereira', 'Stuart Shieber']",introduction,,"The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .","['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29', 'The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'As a result, the literal can be removed from the body of the clause.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']",0,"['The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29', 'The unfolding transformation is also referred to as partial execution , for example , by #AUTHOR_TAG .', 'Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time.', 'Whereas unfolding can be viewed as a symbolic way of going forward in computation, folding constitutes a symbolic step backwards in computation.']"
CC908,J97-4003,On Expressing Lexical Generalizations in HPSG,typed unification grammars,"['Martin Emele', 'Remi Zajac']",related work,"This paper defines unification based ID/LP grammars based on typed feature structures as nonterminals and proposes a variant of Earley's algorithm to decide whether a given input sentence is a member of the language generated by a particular typed unification ID/LP grammar. A solution to the problem of the nonlocal flow of information in unification ID/LP grammars as discussed in Seiffert (1991) is incorporated into the algorithm. At the same time, it tries to connect this technical work with linguistics by presenting an example of the problem resulting from HPSG approaches to linguistics (Hinrichs and Nakasawa 1994, Richter and Sailer 1995) and with computational linguistics by drawing connections from this approach to systems implementing HPSG, especially the TROLL system, Gerdemann et al. (forthcoming).Comment: paper (81 pages), appendix (17 pages, Prolog code), format: .ps   compressed and uuencode",A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31).', 'A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( #AUTHOR_TAG ; Emele 1994 ) .', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC909,J97-4003,On Expressing Lexical Generalizations in HPSG,a logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,,"This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '","['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '"", '11 10 Note that the passivization lexical rule in Figure 2 is only intended to illustrate the mechanism.', 'We do not make the linguistic claim that passives should be analyzed using such a lexical rule.', 'For space reasons, the SYNSEM feature is abbreviated by its first letter.', 'The traditional (First I Rest) list notation is used, and the operator • stands for the append relation in the usual way.', '1l Manandhar (1995) proposes to unify these two steps by including an update operator in the The computational treatment we discuss in the rest of the paper follows this setup in that it automatically computes, for each lexical rule specification, the frames necessary to preserve the properties not changed by it.', '12 We will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to HPSG lexical rules.']",0,"['One thus needs to distinguish the lexical rule specification provided by the linguist from the fully explicit lexical rule relations integrated into the theory.', 'The formalization of DLRs provided by Meurers (1995) defines a formal lexical rule specification language and provides a semantics for that language in two steps: A rewrite system enriches the lexical rule specification into a fully explicit description of the kind shown in Figure 1.', ""This description can then be given the standard set-theoretical interpretation of #AUTHOR_TAG , 1994 ) . '"", 'For space reasons, the SYNSEM feature is abbreviated by its first letter.', '12 We will show that the detection and specification of frames and the use of program transformation to advance their integration into the lexicon encoding is one of the key ingredients of the covariation approach to HPSG lexical rules.']"
CC910,J97-4003,On Expressing Lexical Generalizations in HPSG,hpsg lexicon without lexical rules,['Karel Oliva'],related work,"this paper, I shall try  (i) to show that ueglecting standtu:d insights of tile orgauization of lexicon is detrimental both to the linguistic adequacy and to the practical useful- hess of the lexicon,  (ii) to make a proposal of an alternative reconcil ing the needs of HPSG with the usual lexicographic practic","In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; #AUTHOR_TAG ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC911,J97-4003,On Expressing Lexical Generalizations in HPSG,the update operation in feature logic,['Suresh Manandhar'],introduction,,11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.,['11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.'],0,['11 #AUTHOR_TAG proposes to unify these two steps by including an update operator in the description language.']
CC912,J97-4003,On Expressing Lexical Generalizations in HPSG,an overview of disjunctive constraint satisfaction,"['John Maxwell', 'Ronald Kaplan']",introduction,This paper presents a new algorithm for solving disjunctive systems of constraints. The algorithm determines whether a system is satisfiable and produces the models if the system is satisfiable. There are three main steps for determining whether or not the system is satisfiable: 1 ) turn the disjunctive system into an equi-satisfiable conjunctive system in polynomial time 2) convert the conjunctive system into canonical form using extensions of standard techniques #3) extract and solve a propositional 'disjunctive residue',32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .,"['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .', 'Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup.', 'For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficient processing.']",0,['32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints ( #AUTHOR_TAG ; Eisele and Dorre 1990 ; Griffith 1996 ) can be used to circumvent constraint propagation .']
CC913,J97-4003,On Expressing Lexical Generalizations in HPSG,the generative power of categorial grammars and headdriven phrase structure grammars with lexical rules,['Bob Carpenter'],related work,"In this paper, it is shown that the addition of simple and linguistically motivated forms of lexical rules to grammatical theories based on subcategorization lists, such as categorial grammars (CG) or head-driven phrase structure grammars (HPSG), results in a system that can generate all and only the recursively enumerable languages. The proof of this result is carried out by means of a reduction of generalized rewriting systems. Two restrictions are considered, each of which constrains the generative power of the resulting system to context-free languages.",The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .,"['The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .', 'In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper.']",0,['The powerful mechanism of lexical rules ( #AUTHOR_TAG ) has been used in many natural language processing systems .']
CC914,J97-4003,On Expressing Lexical Generalizations in HPSG,some philosophical problems from the standpoint of artificial intelligence,"['John McCarthy', 'Patrick Hayes']",introduction,,"This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .","['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (Meurers 1994).']",1,"['Only the verb form and some indices are specified to be changed, and thus other input properties, like the phonology, the semantics, or the nonlocal specifications, are preserved in the output.', 'This is so since the lexical rule in Figure 2 ""(like all lexical rules in HPSG) preserves all properties of the input not mentioned in the rule.""', '(Pollard and Sag [1994, 314], following Flickinger [1987]).', 'This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( #AUTHOR_TAG ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .', 'Not having to represent the frame explicitly not only enables the linguist to express only the relevant things, but also allows a more compact representation of lexical rules where explicit framing would require the rules to be split up (Meurers 1994).']"
CC915,J97-4003,On Expressing Lexical Generalizations in HPSG,the craft of prolog,"[""Richard O'Keefe""]",introduction,"Hacking your program is no substitute for understanding your problem. Prolog is different, but not that different. Elegance is not optional. These are the themes that unify Richard O'Keefe's very personal statement on how Prolog programs should be written. The emphasis in ""The Craft of Prolog"" is on using Prolog effectively. It presents a loose collection of topics that build on and elaborate concepts learning in a first course. These may be read in any order following the first chapter, ""Basic Topics in Prolog, "" which provides a basis for the rest of the material in the book.","Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .","['Encoding a finite-state automaton as definite relations is rather straightforward.', 'In fact, one can view the representations as notational variants of one another.', 'Each transition in the automaton is translated into a definite relation in which the corresponding lexical rule predicate is called, and each final state is encoded by a unit clause.', 'Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .', 'Because of the word class specialization step discussed in Section 3.3, the execution avoids trying out many lexical rule applications that are guaranteed to fail.']",5,"['Encoding a finite-state automaton as definite relations is rather straightforward.', 'In fact, one can view the representations as notational variants of one another.', 'Using an accumulator passing technique ( #AUTHOR_TAG ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .']"
CC916,J97-4003,On Expressing Lexical Generalizations in HPSG,applying lexical rules under subsumption,"['Erhard Hinrichs', 'Tsuneko Nakazawa']",introduction,"Lexical rules are used in constraint based grammar formalisms such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994) to express generalizations among lexical entries. This paper discusses a number of lexical rules from recent HPSG analyses of German (Hinrichs and Nakazawa 1994) and shows that the grammar in some cases vastly overgenerates and in other cases introduces massive spurious structural ambiguity, if lexical rules apply under unification. Such problems of overgeneration or spurious ambiguity do not arise, if a lexical rule applies to a given lexical entry iff the lexical entry is subsumed by the left-hand side of the lexical rule. Finally, the paper discusses computational consequences of applying lexical rules under subsumption.",15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .,"['The translation of the lexical rule into a predicate is trivial.', 'The result is displayed description language.', '12 In order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in Meurers (1995).', 'The reader interested in that language and its precise interpretation can find the relevant details in that paper.', '13 A more detailed presentation can be found in Minnen (in preparation).', '14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .', 'We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994).', 'Computationally, a subsumption test could equally well be used in our compiler.']",0,"['14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects.', '15 #AUTHOR_TAG show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention .']"
CC917,J97-4003,On Expressing Lexical Generalizations in HPSG,word formation in lexical type hierarchies a case study of baradjectives in german masters thesis,['Susanne Riehemann'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; #AUTHOR_TAG ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC918,J97-4003,On Expressing Lexical Generalizations in HPSG,verb second by underspecification,['Annette Frank'],related work,,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993 ; Oliva 1994 ; #AUTHOR_TAG ; Opalka 1995 ; Sanfilippo 1995 ) .']"
CC919,J97-4003,On Expressing Lexical Generalizations in HPSG,an expanded logical formalism for headdriven phrase structure grammar,['Paul King'],introduction,"Though Pollard and Sag 1994] assumes that an unspeciied variant of the formal logic of Carpenter 1992] will provide a formalism for HPSG, a precise formulation of the envisaged formalism is not immediately obvious, primarily because a principal tenet of Carpenter 1992], that feature structures represent partial information, seems to connict with a principal tenet of Pollard and Sag 1994], that feature structures represent abstract linguistic entities. This has caused many HPSGians to be mistakenly concerned with partial-information speciic notions, such as subsumption, that are appropriate for the Carpenter 1992] logic but inappropriate for the formalism Pollard and Sag 1994] envisages. This paper hopes to allay this concern and the confusion it engenders by substituting King 1989] for Carpenter 1992] as the basis of the envisaged formalism. It demonstrates that the formal logic of King 1989] provides a formalism for HPSG that meets all Pollard and Sag 1994] asks of the envisaged formalism. It further shows that the most credible variant of the Carpenter 1992] logic consistent with the aims of Pollard and Sag 1994] is not only incompatible with the tenet that feature structures represent partial information, but also an instance of the King 1989] logic.",4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']",0,"['The terminology used in the literature varies.', 'Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes.', 'To avoid confusion, we will only use the terminology introduced in the text.', '4 This interpretation of the signature is sometimes referred to as closed world ( Gerdemann and #AUTHOR_TAG ; Gerdemann 1995 ) .', '5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation).', '6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example.', 'The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input.', 'In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement.']"
CC920,J97-4003,On Expressing Lexical Generalizations in HPSG,french clitic climbing without clitics or climbing,"['Philip Miller', 'Ivan Sag']",introduction,,"de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', 'de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', 'de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( Pollard and Sag 1994 ) or the Complement Cliticization Lexical Rule ( #AUTHOR_TAG ) to operate on those raised elements .', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC921,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).","['16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).', 'In such a Predicative Lexical Rule (which we only note as an example and not as a linguistic proposal) the subtype of the head object undergoing the rule as well as the value of the features only appropriate for the subtypes of substantive either is lost or must be specified by a separate rule for each of the subtypes.']",0,"['16 A linguistic example based on the signature given by #AUTHOR_TAG would be a lexical rule deriving predicative signs from nonpredicative ones, i.e., changing the PRD value of substantive signs from - to +, much like the lexical rule for NPs given by Pollard and Sag (1994, p. 360, fn. 20).']"
CC922,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .","['Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency.', 'The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup.', 'The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .']",2,"['The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and #AUTHOR_TAG , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .']"
CC923,J97-4003,On Expressing Lexical Generalizations in HPSG,passive without lexical rules in,['Andreas Kathol'],related work,,"In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( #AUTHOR_TAG ; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; Sanfilippo 1995 ) .']"
CC924,J97-4003,On Expressing Lexical Generalizations in HPSG,lexical polymorphism and word disambiguation,['Antonio Sanfilippo'],related work,We present an approach to lexical ambiguity where regularities about sense/u~ge extensibillty are represented by underepecifying word entries through lexic~d polymorphism. Word diumbiguation is carried out using contextual information gathered during language processing to ground polymorphic lexical entries.,"In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .","['Lexical rules have not gone unchallenged as a mechanism for expressing generaliza- tions over lexical information.', 'In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992 ; Riehemann 1993 ; Oliva 1994 ; Frank 1994; Opalka 1995 ; #AUTHOR_TAG ) .']"
CC925,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.","['• The authors are listed alphabetically.', 'SFB 340, Kleine Wilhelmstr.', '113, D-72074 Tiibingen, Germany.', 'email: {dm,minnen}@sfs.nphil.uni-tuebingen.de', 'URL: http://www.sfs.nphil.uni-tuebingen.de/sfb', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']",0,"['* The authors are listed alphabetically.', '/b4home.html 1 This is, for example, the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement (Hinrichs and Nakazawa 1989) that also use lexical rules such as the Complement Extraction Lexical Rule ( #AUTHOR_TAG ) or the Complement Cliticization Lexical Rule (Miller and Sag 1993) to operate on those raised elements.', 'Also an analysis treating adjunct extraction via lexical rules (van Noord and Bouma 1994) results in an infinite lexicon.']"
CC926,J97-4003,On Expressing Lexical Generalizations in HPSG,a computational treatment of hpsg lexical rules as covariation in lexical entries,"['Detmar Meurers', 'Guido Minnen']",introduction,We describe a compiler which translates a set of HPSG lexical rules and their interaction into definite relations used to constrain lexical entries. The compiler ensures automatic transfer of properties unchanged by a lexical rule. Thus an operational semantics for the full lexical rule mechanism as used in HPSG linguistics is provided. Program transformation techniques are used to advance the resulting encoding. The final output constitutes a computational counterpart of the linguistic generalizations captured by lexical rules and allows ``on the fly'' application.,"Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .","['Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .', 'We developed a compiler that takes as its input a set of lexical rules, deduces the nec- essary transfer of properties not changed by the individual lexical rules, and encodes the set of lexical rules and their interaction into definite relations constraining lexical entries.', 'Each lexical entry is automatically extended with a definite clause encoding of the lexical rule applications which the entry can undergo.', 'The definite clauses thereby introduce what we refer to as systematic covariationinlexicalentries.']",4,"['Based on the research results reported in #AUTHOR_TAG , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these short- comings and results in a more efficient processing of lexical rules as used in HPSG .']"
CC927,J97-4003,On Expressing Lexical Generalizations in HPSG,the representation of lexical semantic information cognitive science research paper csrp 280,['Ann Copestake'],related work,,"This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .","['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']",1,"['Another common approach to lexical rules is to encode them as unary phrase structure rules.', 'This approach is taken , for example , in LKB ( #AUTHOR_TAG ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .', 'A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990;Emele 1994).', 'The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.']"
CC928,J97-4003,On Expressing Lexical Generalizations in HPSG,on implementing an hpsg theory aspects of the logical architecture the formalization and the implementation of headdriven phrase structure grammars,['Detmar Meurers'],introduction,"The paper presents some aspects involved in the formalization and implementation of HPSG theories. As basis, the logical setups of Carpenter (1992) and King (1989, 1994) are briefly compared regarding their usefulness as basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG theories in the HPSGII architecture and in various computational systems (ALE, Troll, CUF, and TFS) are discussed. Beside a formal characterization of the possibilities, the paper investigates the specific choices for constraints with certain linguistic motivations, i.e. the lexicon, structure licencing, and grammatical principles. An ALE implementation of a theory for German proposed by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is included in the appendix.","However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .","['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'In the latter case, we can also take care of transferring the value of z.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']",4,"['To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case.', 'In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value.', 'However , as discussed by #AUTHOR_TAG , creating several instances of lexical rules can be avoided .', 'Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule.', 'This is accomplished by having each lexical rule predicate call a so-called framepredicate,which can have multiple defining clauses.', 'So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8.17']"
CC929,J97-4003,On Expressing Lexical Generalizations in HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan Sag']",introduction,,"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .","['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']",0,"['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of #AUTHOR_TAG , ch .']"
CC930,J97-4003,On Expressing Lexical Generalizations in HPSG,featurebased inheritance networks for computational lexicons,"['Hans-Ulrich Krieger', 'John Nerbonne']",related work,"The virtues of viewing the lexicon as an inheritance network are its succinctness and its tendency to highlight significant clusters of linguistic properties. From its succinctness follow two practical advantages, namely its ease of maintenance and modification. In this paper we present a feature-based foundation for lexical inheritance. We argue that the feature-based foundation is both more economical and expressively more powerful than non-feature-based systems. It is more economical because it employs only mechanisms already assumed to be present elsewhere in the grammar (viz., in the feature system), and it is more expressive because feature systems are more expressive than other mechanisms used in expressing lexical inheritance (cf. DATR). The lexicon furthermore allows the use of default unification, based on the ideas of default unification, defined by Bouma. These claims are buttressed in sections sketching the opportunities for lexical description in feature-based lexicons in two central lexical topics, inflection and derivation. Briefly, we argue that the central notion of paradigm may be defined in feature structures, and that it may be more satisfactorily (in fact, immediately) linked to the syntactic information in this fashion. Our discussion of derivation is more programmatic; but here, too, we argue that feature structures of a suitably rich sort provide a foundation for the definition of lexical rules. We illustrate theoretical claims in application to German lexis. This work is currently under implementation in a natural language understanding effort (DISCO) at the German Artiffical Intelligence Center (Deutsches Forschungszentrum fur Kunstliche Intelligenz).","In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).","['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']",1,"['Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information.', 'In a number of proposals , lexical generalizations are captured using lexical underspecification ( Kathol 1994 ; #AUTHOR_TAG ; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995).', 'The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.']"
CC931,J97-4003,On Expressing Lexical Generalizations in HPSG,of csli lecture notes center for the study of language and information,"['Carl Pollard', 'Ivan Sag']",introduction,,"Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .","['written as fully specified relations between words, rather, only what is supposed to be changed is specified.', 'Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .', 'This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output.', 'The index that the subject bore in the input is assigned to an optional prepositional complement in the output.']",1,"['Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by #AUTHOR_TAG , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .']"
CC932,J97-4003,On Expressing Lexical Generalizations in HPSG,towards a semantics for lexical rules as used in hpsg,['Detmar Meurers'],introduction,,"Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )","['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']",0,"['While the setup of King provides a clear formal basis for basic HPSG grammars, nothing is said about how special linguistic mechanisms like lexical rules fit into this formal setup.', 'Two formalizations of lexical rules as used by HPSG linguists have been proposed, the meta-level lexical rules (MLRs; Calcagno 1995; Calcagno and Pollard 1995) and the description-level lexical rules ( DLRs ; #AUTHOR_TAG )']"
CC933,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,design challenges and misconceptions in named entity recognition,"['L Ratinov', 'D Roth']",related work,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .","['The task of mention detection is closely related to Named Entity Recognition (NER).', 'Punyakanok and Roth (2001) thoroughly study phrase identification in sentences and propose three different general approaches.', 'They aim to learn several different local classifiers and combine them to optimally satisfy some global constraints.', 'Cardie and Pierce (1998) propose to select certain rules based on a given corpus, to identify base noun phrases.', 'However, the phrases detected are not necessarily mentions that we need to discover.', '#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .', 'NER can be regarded as a sequential labeling problem, which can be modeled by several proposed models, e.g.', 'Hidden Markov Model (Rabiner, 1989) or Conditional Random Fields (Sarawagi and Cohen, 2004).', 'The typical BIO representation was introduced in Ramshaw and Marcus (1995); OC representations were introduced in Church (1988), while Finkel and Manning (2009) further study nested named entity recognition, which employs a tree structure as a representation of identifying named entities within other named entities.']",0,"['#AUTHOR_TAG present detailed studies on the task of named entity recognition , which discusses and compares different methods on multiple aspects including chunk representation , inference method , utility of non-local features , and integration of external knowledge .']"
CC934,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","For this mention-pair coreference model Ï ( u , v ) , we use the same set of features used in #AUTHOR_TAG .","['Here, y u,v = 1 iff mentions u, v are directly linked.', 'Thus, we can construct a forest and the mentions in the same connected component (i.e., in the same tree) are co-referred.', 'For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in #AUTHOR_TAG .']",5,"['For this mention-pair coreference model Ï\x86 ( u , v ) , we use the same set of features used in #AUTHOR_TAG .']"
CC935,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",introduction,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .","['Here, phrases in the brackets are mentions and the underlined simple phrases are mention heads.', 'Moreover, mention boundaries can be nested (the boundary of a mention is inside the boundary of another mention), but mention heads never overlap.', 'This property also simplifies the problem of mention head candidate generation.', 'In the example above, the first ""they"" refers to ""Multinational companies investing in China"" and the second ""They"" refers to ""Domestic manufacturers, who are also suffering"".', 'In both cases, the mention heads are sufficient to support the decisions: ""they"" refers to ""companies"", and ""They"" refers to ""manufacturers"".', 'In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .']",0,"['In fact , most of the features3 implemented in existing coreference resolution systems rely solely on mention heads ( #AUTHOR_TAG ) .']"
CC936,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",experiments,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.","For Berkeley system , we use the reported results from #AUTHOR_TAG .","['The latest scorer is version v8.01, but MUC, B , CEAF e and CoNLL average scores are not changed.', 'For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12', 'We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input.', 'Results for HOTCoref are slightly different from the results reported in Björkelund and Kuhn (2014).', 'For Berkeley system , we use the reported results from #AUTHOR_TAG .']",1,"['The latest scorer is version v8.01, but MUC, B , CEAF e and CoNLL average scores are not changed.', 'For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12', 'We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input.', 'Results for HOTCoref are slightly different from the results reported in Bjorkelund and Kuhn (2014).', 'For Berkeley system , we use the reported results from #AUTHOR_TAG .']"
CC937,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",experiments,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.","Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",1,"['Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'The nonoverlapping mention head assumption in Sec.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines : Stanford system ( Lee et al. , 2011 ) , Berkeley system ( #AUTHOR_TAG ) and HOTCoref system ( Bj Â¨ orkelund and Kuhn , 2014 ) .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"
CC938,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,understanding the value of features for coreference resolution,"['E Bengtson', 'D Roth']",experiments,"In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model.    This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.","We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( Culotta et al. , 2007 ; #AUTHOR_TAG ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Parameters of our proposed system are tuned as a = 0.9, b = 0.8, l 1 = 0.2 and l 2 = 0.3.']"
CC939,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,headdriven statistical models for natural language parsing,['M Collins'],,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .","['Most of the head mentions proposed by the algorithms described in Sec. 3 are positive examples.', 'We ensure a balanced training of the mention head detection model by adding sub-sampled invalid mention head candidates as negative examples.', 'Specifically, after mention head candidate generation (described in Sec.', '3), we train on a set of candidates with precision larger than 50%.', 'We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .', 'When these extracted heads do not overlap with gold mention heads, we treat them as negative examples.']",5,"['Specifically, after mention head candidate generation (described in Sec.', 'We then use Illinois Chunker ( Punyakanok and Roth , 2001 ) 6 to extract more noun phrases from the text and employ Collins head rules ( #AUTHOR_TAG ) to identify their heads .']"
CC940,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",introduction,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .","['Mention detection is rarely studied as a stand-alone research problem (Recasens et al. (2013) is one key exception).', 'Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .', 'However, the lack of emphasis is not due to this being a minor issue, but rather, we think, its difficulty.', 'Indeed, many papers report results in terms of gold mentions versus system generated mentions, as shown in Table 1.', 'Current state-of-the-art systems show a very significant drop in performance when running on system generated mentions.', 'These performance gaps are worrisome, since the real goal of NLP systems is to process raw data.', '1: Performance gaps between using gold mentions and predicted mentions for three state-of-the-art coreference resolution systems.', 'Performance gaps are always larger than 10%.', ""Illinois's system (Chang et al., 2013) is evaluated on CoNLL (2012CoNLL ( , 2011) Shared Task and ACE-2004 datasets."", 'It reports an average F1 score of MUC, B and CEAF e metrics using CoNLL v7.0 scorer.', ""Berkeley's system (Durrett and Klein, 2013) reports the same average score on the CoNLL-2011 Shared Task dataset."", ""Results of Stanford's system (Lee et al., 2011) are for B 3 metric on ACE-2004 dataset."", 'This paper focuses on improving end-to-end coreference performance.', 'We do this by: 1) Developing a new ILP-based joint learning and inference formulation for coreference and mention head detection.', '2) Developing a better mention head candidate generation algorithm.', 'Importantly, we focus on heads rather than mention boundaries since those can be identified more robustly and used effectively in an end-to-end system.', 'As we show, this results in a dramatic improvement in the quality of the MD component and, consequently, a significant reduction in the performance gap between coreference on gold mentions and coreference on raw data.']",0,"['Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( #AUTHOR_TAG ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj Â¨ orkelund and Kuhn , 2014 ) .']"
CC941,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,design challenges and misconceptions in named entity recognition,"['L Ratinov', 'D Roth']",,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .","['Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.', 'The problem is then transformed into a simple, but constrained, 5-class classification problem.']",4,"['Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in #AUTHOR_TAG .', 'The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks.']"
CC942,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,conll2012 shared task modeling multilingual unrestricted coreference in ontonotes,"['S Pradhan', 'A Moschitti', 'N Xue', 'O Uryupina', 'Y Zhang']",experiments,"The CoNLL-2012 shared task involved predicting coreference in three languages -- English, Chinese and Arabic -- using OntoNotes data. It was a follow-on to the English-only task organized in 2011. Until the creation of the OntoNotes corpus, resources in this subfield of language processing have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ACE entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types and covering multiple languages. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, evaluation criteria, and presents and discusses the results achieved by the participating systems. Being a task that has a complex evaluation history, and multiple evalation conditions, it has, in the past, been difficult to judge the improvement in new algorithms over previously reported results. Having a standard test set and evaluation parameters, all based on a resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.","The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents, 68 development documents, and 106 testing documents (Culotta et al., 2007;Bengtson and Roth, 2008).', 'The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( #AUTHOR_TAG ) , contains 3,145 annotated documents .', 'For gold mentions and mention heads, they yield the same performance for coreference.']"
CC943,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",experiments,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore, we preprocess Ontonote-5.0 to derive mention heads using Collins head rules (Collins, 1999) with gold constituency parsing information and gold named entity information.', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Björkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",5,"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'The nonoverlapping mention head assumption in Sec.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bjorkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by #AUTHOR_TAG , using Constrained Latent Left-Linking Model ( CL3M ) as our mention-pair coreference model in the joint framework10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"
CC944,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",related work,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.","In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .","['Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013;Björkelund and Kuhn, 2014;Song et al., 2012).', 'Many of the early rule-based systems like Hobbs (1978) and Lappin and Leass (1994) gained considerable popularity.', 'The early designs were easy to understand and the rules were designed manually.', 'Machine learning approaches were introduced in many works (Connolly et al., 1997;Ng and Cardie, 2002;Bengtson and Roth, 2008;Soon et al., 2001).', 'The introduction of ILP methods has influenced the coreference area too Denis and Baldridge, 2007).', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .']",5,"['Coreference resolution has been extensively studied, with several state-of-the-art approaches addressing this task (Lee et al., 2011;Durrett and Klein, 2013;Bjorkelund and Kuhn, 2014;Song et al., 2012).', 'The introduction of ILP methods has influenced the coreference area too Denis and Baldridge, 2007).', 'In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in #AUTHOR_TAG in our experiments .']"
CC945,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,firstorder probabilistic models for coreference resolution,"['A Culotta', 'M Wick', 'A McCallum']",experiments,"Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method that enables features over sets of noun phrases, resulting in a first-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently.","We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .","['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', 'We report results on the test documents for both datasets.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.8, λ 1 = 0.2 and λ 2 = 0.3.']",5,"['The ACE-2004 dataset contains 443 documents.', 'We use a standard split of 268 training documents , 68 development documents , and 106 testing documents ( #AUTHOR_TAG ; Bengtson and Roth , 2008 ) .', 'The OntoNotes-5.0 dataset, which is released for the CoNLL-2012Shared Task (Pradhan et al., 2012, contains 3,145 annotated documents.', 'These documents come from a wide range of sources which include newswire, bible, transcripts, magazines, and web blogs.', ') indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.']"
CC946,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.",More details can be found in #AUTHOR_TAG et al. (2013).,"['More details can be found in #AUTHOR_TAG et al. (2013).', 'The difference here is that we also consider the validity of mention heads using �(u),�(m)']",0,['More details can be found in #AUTHOR_TAG et al. (2013).']
CC947,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a joint model for entity analysis coreference typing and linking,"['G Durrett', 'D Klein']",related work,"We present a joint model of three core tasks in the entity analysis stack: coreference res-olution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia en-tities). Our model is formally a structured con-ditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.",Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .,"['Several recent works suggest studying coreference jointly with other tasks.', 'Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .', 'The work closest to ours is that of Lassalle and Denis (2015), which studies a joint anaphoricity detection and coreference resolution framework.', 'While their inference objective is similar, their work assumes gold mentions are given and thus their modeling is very different.']",0,['Lee et al. ( 2012 ) model entity coreference and event coreference jointly ; #AUTHOR_TAG consider joint coreference and entity-linking .']
CC948,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,headdriven statistical models for natural language parsing,['M Collins'],experiments,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.","Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .","['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'dataset only has mention annotations.', 'Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'We set these extracted heads as gold, which enables us to train the two layer BILOU-classifier described in Sec.', '3.1.1.', 'The nonoverlapping mention head assumption in Sec.', '3.1.1', 'can be verified empirically on both ACE-2004 and OntoNotes-5.0', 'datasets.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Björkelund and Kuhn, 2014).', 'Developed Systems Our developed system is built on the work by Chang et al. (2013), using Constrained Latent Left-Linking Model (CL 3 M) as our mention-pair coreference model in the joint framework 10 .', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'The mention head candidate generation module along with mention boundary detection module can be grouped together to form a complete mention detection system, and we call it H-M-MD.', 'We can feed the predicted mentions from H-M-MD directly into the mention-pair coref- 9 No parsing information is needed at evaluation time. 10', 'We use Gurobi v5.0.1 as our ILP solver.', '3: Performance of coreference resolution for all systems on the CoNLL-2012 dataset.', 'Subscripts ( M , H ) indicate evaluations on (mentions, mention heads) respectively.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Our proposed H-Joint-M system achieves the highest performance.', 'Parameters of our proposed system are tuned as α = 0.9, β = 0.9, λ 1 = 0.25 and λ 2 = 0.2.', 'erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref.', 'We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'We use the average F1 scores (AVG) of these three metrics as the main metric for comparison.', 'We use the v7.0 scorer provided by CoNLL-2012 Shared Task 11 .', 'We also evaluate the mention detection performance based on precision, recall and F1 score.', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']",5,"['The ACE-2004 dataset is annotated with both mention and mention heads, while the OntoNotes-5.0', 'Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( #AUTHOR_TAG ) with gold constituency parsing information and gold named entity information .', 'The parsing information 9 is only needed to generate training data for the mention head candidate generator and named entities are directly set as heads.', 'The nonoverlapping mention head assumption in Sec.', 'Baseline Systems We choose three publicly available state-of-the-art end-to-end coreference systems as our baselines: Stanford system (Lee et al., 2011), Berkeley system (Durrett and Klein, 2014) and HOTCoref system (Bjorkelund and Kuhn, 2014).', 'When the CL 3 M coreference system uses gold mentions or heads, we call the system Gold; when it uses predicted mentions or heads, we call the system Predicted.', 'For gold mentions and mention heads, they yield the same performance for coreference.', 'Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAF e ) (Luo, 2005).', 'As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads.']"
CC949,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,a constrained latent variable model for coreference resolution,"['K-W Chang', 'R Samdani', 'D Roth']",,"Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L 3 M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L 3 M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L 3 M and its constrained version, CL 3 M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.",Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .,"['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'The mention head detection model here is mainly trained to differentiate valid mention heads from invalid ones.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']",5,"['This section describes our joint coreference resolution and mention head detection framework.', 'Our work is inspired by the latent left-linking model in #AUTHOR_TAG and the ILP formulation from Chang et al. ( 2011 ) .', 'The joint learning and inference model takes as input mention head candidates (Sec.', '3) and jointly (1) determines if they are indeed mention heads and (2) learns a similarity metric between mentions.', 'This is done by simultaneously learning a binary mention head detection classifier and a mention-pair coreference classifier.', 'By learning and making decisions jointly, it also serves as a singleton mention head classifier, building on insights from Recasens et al. (2013).', 'This joint framework aims to improve performance on both mention head detection and on coreference.']"
CC950,K15-1002,A Joint Framework for Coreference Resolution and Mention Head Detection,ontonotes the 90 solution,"['E Hovy', 'M Marcus', 'M Palmer', 'L Ramshaw', 'R Weischedel']",experiments,"We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.","We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .","['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).', 'Our approach results in a substantial reduction in the coreference performance gap between gold and predicted mentions, and significantly outperforms existing stat-of-the-art results on coreference resolution; in addition, it achieves significant performance improvement on MD for both datasets.']",5,"['We present experiments on the two standard coreference resolution datasets , ACE-2004 ( NIST , 2004 ) and OntoNotes-5 .0 ( #AUTHOR_TAG ) .', '(Hovy et al., 2006).']"
CC951,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",method,,We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .,"['For the root, binary, and unary parameters, we want to choose prior means that encode our bias toward cross-linguistically-plausible categories.', 'To formalize the notion of what it means for a category to be more ""plausible"", we extend the category generator of our previous work, which we will call P CAT .', 'We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in §5) with probability p del , or a standard CCG category C:']",0,"['We can define PCAT using a probabilistic grammar ( #AUTHOR_TAG ) .', 'The grammar may first generate a start or end category ( S , E ) with probability p se or a special tokendeletion category ( D ; explained in SS5) with probability p del , or a standard CCG category C:']"
CC952,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",related work,,#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) .,"[""#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.', 'While CCM is designed to learn which tag pairs make for likely contexts, without regard for the constituents themselves, our model attempts to learn the relationships between context categories and the types of the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels.']",0,"[""#AUTHOR_TAG 's CCM is an unlabeled bracketing model that generates the span of part-of-speech tags that make up each constituent and the pair of tags surrounding each constituent span ( as well as the spans and contexts of each non-constituent ) ."", 'They found that modeling constituent context aids in parser learning because it is able to capture the observation that the same contexts tend to appear repeatedly in a corpus, even with different constituents.']"
CC953,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",introduction,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.","In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .","['In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags.', 'In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', 'However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.', 'Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios.']",5,"['In order to estimate the parameters of our model , we develop a blocked sampler based on that of #AUTHOR_TAG to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .', 'However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior.']"
CC954,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,building a large annotated corpus of english the penn treebank,"['Mitchell P Marcus', 'Beatrice Santorini', 'Mary Ann Marcinkiewicz']",experiments,"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.","We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .","['In our evaluation we compared our supertagcontext approach to (our reimplementation of) the best-performing model of our previous work (Garrette et al., 2015), which SCM extends.', 'We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .']",5,"['We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( #AUTHOR_TAG ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .']"
CC955,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",method,,The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG .,"[""The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG ."", ""Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.""]",1,"[""The right-side context of a non-terminal category -- the probability of generating a category to the right of the current constituent 's category -- corresponds directly to the category transitions used for the HMM supertagger of #AUTHOR_TAG ."", ""Thus, the right-side context prior mean th RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.""]"
CC956,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",experiments,,We use the same splits as #AUTHOR_TAG .,"['Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.', 'We use the same splits as #AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.', 'In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Giga- word 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999).', 'For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k).', 'For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences).']",5,"['Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set.', 'We use the same splits as #AUTHOR_TAG .', 'Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\\X)/X rather than introducing special conjunction rules.']"
CC957,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,online learning of relaxed ccg grammars for parsing to logical form,"['Luke S Zettlemoyer', 'Michael Collins']",experiments,"We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar--for example allowing flexible word order, or insertion of lexical items-- with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).","This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .","['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'To accomplish this, we implemented a parsing backoff strategy.', 'The parser first tries to find a valid parse that has either s dcl or np at its root.', 'If that fails, then it searches for a parse with any root.', 'If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D →u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D .', 'Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.', 'Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.']",1,"['When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be ""wrong"".', 'Thus, it is important that we make a best effort to find a parse.', 'The parser first tries to find a valid parse that has either s dcl or np at its root.', 'If that fails, then it searches for a parse with any root.', 'If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without).', 'This is similar to the ""deletion"" strategy employed by #AUTHOR_TAG , but we do it directly in the grammar .', 'We add unary rules of the form D -u for every potential supertag u in the tree.', 'Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t- D , v and t- v, D .', 'Recall that in SS3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary.']"
CC958,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,weaklysupervised bayesian learning of a ccg supertagger,"['Dan Garrette', 'Chris Dyer', 'Jason Baldridge', 'Noah A Smith']",introduction,,We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .,"['Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures.', 'In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015).', 'Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable.', 'We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .']",2,['We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( #AUTHOR_TAG ) .']
CC959,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,typesupervised hidden markov models for partofspeech tagging with incomplete tag dictionaries,"['Dan Garrette', 'Jason Baldridge']",method,"Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the min-greedy algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original min-greedy algorithm for both English and Italian data.","We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4","['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4', 'This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags.', 'These counts are then combined with estimates of the �openness� of each tag in order to assess its likelihood of appearing with new words.']",5,"['We employ the same procedure as our previous work for setting the terminal production prior distributions _TERM-0(w) by estimating word-given- category relationships from the weak supervision : the tag dictionary and raw corpus ( #AUTHOR_TAG ; Garrette et al. , 2015 ) .4']"
CC960,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",method,,"Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .","['Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid.', 'Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .']",4,"['Since we are not generating from the model , this does not introduce difficulties ( #AUTHOR_TAG ) .']"
CC961,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,widecoverage efficient statistical parsing with ccg and loglinear models,"['Stephen Clark', 'James R Curran']",,,We further add rules for combining with punctuation to the left and right and allow for the merge rule X â X X of #AUTHOR_TAG .,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']",5,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules.', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X â\x86\x92 X X of #AUTHOR_TAG .']"
CC962,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.","To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .","['To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .', 'For a sentence w, the strategy is to use the Inside algorithm (Lari and Young, 1990) to inductively compute, for each potential non-terminal position spanning words w i through w j−1 and category t, going ""up"" the tree, the probability of generating w i , . . .', ', w j−1 via any arrangement of productions that is rooted by y ij = t.']",5,"['To sample from our proposal distribution , we use a blocked Gibbs sampler based on the one proposed by Goodman ( 1998 ) and used by #AUTHOR_TAG that samples entire parse trees .']"
CC963,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,bayesian inference for pcfgs via markov chain monte carlo,"['Mark Johnson', 'Thomas Griffiths', 'Sharon Goldwater']",,"This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.",Our strategy is based on the approach presented by #AUTHOR_TAG .,"['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']",5,"['We wish to infer the distribution over CCG parses, given the model we just described and a corpus of sentences.', 'Since there is no way to analytically compute these modes, we resort to Gibbs sampling to find an approximate solution.', 'Our strategy is based on the approach presented by #AUTHOR_TAG .', 'At a high level, we alternate between resampling model parameters (_ROOT, _BIN, _ ,_ ,_,_ ,_ ) given the current set of parse trees and resampling those trees given the current model parameters and observed word sequences.', 'To efficiently sample new model parameters, we exploit Dirichlet-multinomial conjugacy.', 'By repeating these alternating steps and accumu- lating the productions, we obtain an approximation of the required posterior quantities.']"
CC964,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a generative constituentcontext model for improved grammar induction,"['Dan Klein', 'Christopher D Manning']",introduction,,"One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .","['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts.', 'For example, the part-of-speech (POS) sequence ADJ NOUN frequently occurs between the tags DET and VERB.', 'This DET-VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN.', 'From this, we might deduce that DET-VERB is a likely context for a noun phrase.', 'CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability.', 'However, since there is nothing intrinsic about the POS pair DET-VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data.', 'Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000;Steedman and Baldridge, 2011) categories reflect universal grammatical properties.', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'For example, a category might encode that ""this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence"" instead of simply VERB.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']",0,"['One important example is the constituentcontext model ( CCM ) of #AUTHOR_TAG , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .', 'CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents.', 'CCG has proven useful as a framework for grammar induction due to its ability to incorporate linguistic knowledge to guide parser learning by, for example, specifying rules in lexical-expansion algorithms (Bisk and Hockenmaier, 2012;2013) or encoding that information as priors within a Bayesian framework (Garrette et al., 2015).']"
CC965,K15-1003,A Supertag-Context Model for Weakly-Supervised CCG Parser Learning,a ccg parsing with a supertagfactored model,"['Mike Lewis', 'Mark Steedman']",,,"We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .","['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'The direction of the slash operator gives the behavior of the function.', 'A category (s\\np)/pp might describe an intransitive verb with a prepositional phrase complement; it combines on the right (/) with a constituent with category pp, and then on the left (\\) with a noun phrase (np) that serves as its subject.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .', 'We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007).']",5,"['Categories of adjacent constituents can be combined using one of a set of combination rules to form categories of higher-level constituents, as seen in Figure 1.', 'We follow #AUTHOR_TAG in allowing a small set of generic , linguistically-plausible unary and binary grammar rules .']"
CC966,N01-1003,SPoT,discriminative reranking for natural language parsing,['Michael Collins'],,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .","['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones.', 'In total, we used 3,291 features in training the SPR.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .', 'The motivation for the features was to capture declaratively decisions made by the randomized SPG.', 'We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.']",1,"['Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features.', 'Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by #AUTHOR_TAG .']"
CC967,N01-1003,SPoT,clause aggregation using linguistic knowledge,['James Shaw'],related work,"By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar.",Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .,"['Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.', 'Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning.']",0,"['Previous work in sentence planning in the natural language generation ( NLG ) community uses hand-written rules to approximate the distribution of linguistic phenomena in a corpus ( see ( #AUTHOR_TAG ) for a recent example with further references ) .', 'This approach is difficult to scale due to the nonrobustness of rules and unexpected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly.']"
CC968,N01-1003,SPoT,discriminative reranking for natural language parsing,['Michael Collins'],introduction,"This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative--in terms of both simplicity and efficiency--to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation","The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .","['Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into sentences. 1', 'For example, consider the required capabilities of a sentence planner for a mixed-initiative spoken dialog system for travel planning: (D1) System1: Welcome.... What airport would you like to fly out of?', 'User2: I need to go to Dallas.', 'System3: Flying to Dallas.', 'What departure airport was that?', 'User4: from Newark on September the 1st.', 'System5: What time would you like to travel on September the 1st to Dallas from Newark?', ""Utterance System1 requests information about the caller's departure airport, but in User2, the caller takes the initiative to provide information about her destination."", ""In System3, the system's goal is to implicitly confirm the destination (because of the possibility of error in the speech recognition component), and request information (for the second time) of the caller's departure airport."", 'In User4, the caller provides this information but also provides the month and day of travel.', ""Given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and destination cities and the month and day information, as well as to request information about the time of travel."", ""The system's representation of its communicative goals for utterance System5 is in Figure 1."", 'The job of the sentence planner is to decide among the large number of potential realizations of these communicative goals.', 'Some example alternative realizations are in Figure 2. 2 implicit-confirm(orig-city:NEWARK) implicit-confirm(dest-city:DALLAS) implicit-confirm(month:9) implicit-confirm(day-number:1) request(depart-time) In this paper, we present SPoT, for ""Sentence Planner, Trainable"".', 'We also present a new methodology for automatically training SPoT on the basis of feedback provided by human judges.', 'In order to train SPoT, we reconceptualize its task as consisting of two distinct phases.', 'In the first phase, the sentence-plan-generator (SPG) generates a potentially large sample of possible sentence plans for a given text-plan input.', 'In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer.', 'Our primary contribution is a method for training the SPR.', 'The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .']",1,"['The SPR uses rules automatically learned from training data , using techniques similar to ( #AUTHOR_TAG ; Freund et al. , 1998 ) .']"
CC969,N01-1003,SPoT,clause aggregation using linguistic knowledge,['James Shaw'],,"By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar.","These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .","['As already mentioned, we divide the sentence planning task into two phases.', 'In the first phase, the sentenceplan-generator (SPG) generates 12-20 possible sentence plans for a given input text plan.', ""Each speech act is assigned a canonical lexico-structural representation (called a DSyntS -Deep Syntactic Structure (Mel'čuk, 1988))."", 'The sentence plan is a tree recording how these elementary DSyntS are combined into larger DSyntSs; the DSyntS for the entire input text plan is associated with the root node of the tree.', 'In the second phase, the sentence plan ranker (SPR) ranks sentence plans generated by the SPG, and then selects the top-ranked output as input to the surface realizer, RealPro (Lavoie and Rambow, 1997)  The research presented here is primarily concerned with creating a trainable SPR.', 'A strength of our approach is the ability to use a very simple SPG, as we explain below.', 'The basis of our SPG is a set of clausecombining operations that incrementally transform a list of elementary predicate-argument representations (the DSyntSs corresponding to elementary speech acts, in our case) into a single lexico-structural representation, by combining these representations using the following combining operations.', 'Examples can be found in Figure  ADJECTIVE.', 'This transforms a predicative use of an adjective into an adnominal construction.', 'PERIOD.', 'Joins two complete clauses with a period.', 'These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .']",0,"['The basis of our SPG is a set of clausecombining operations that incrementally transform a list of elementary predicate-argument representations (the DSyntSs corresponding to elementary speech acts, in our case) into a single lexico-structural representation, by combining these representations using the following combining operations.', 'These operations are not domain-specific and are similar to those of previous aggregation components ( Rambow and Korelsky ,1992 ; #AUTHOR_TAG ; Danlos , 2000 ) , although the various MERGE operations are , to our knowledge , novel in this form .']"
CC970,N01-1003,SPoT,sentence planning as description using tree adjoining grammar,"['Matthew Stone', 'Christine Doran']",,"We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential interactions between multiple descriptions in a sentence. At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices.","The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .","['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'The interior nodes are associated with DSyntSs by executing their clausecombing operation on their two daughter nodes.', '(A PE-RIOD node results in a DSyntS headed by a period and whose daughters are the two daughter DSyntSs.)', 'If a clause combination fails, the sp-tree is discarded (for example, if we try to create a relative clause of a structure which already contains a period).', 'As a result, the DSyntS for the entire turn is associated with the root node.', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'The SPG is designed in such a way that if a DSyntS is associated with the root node, it is a valid structure which can be realized.', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', '4 shows the result of applying the soft-merge operation when Args 1 and 2 are implicit confirmations of the origin and destination cities. Figure 8 illustrates the relationship between the sp-tree and the DSyntS for alternative 8.', 'The labels and arrows show the DSyntSs associated with each node in the sp-tree (in Figure 7), and the diagram also shows how structures are composed into larger structures by the clause combining operations.', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.', 'In our approach, we do not need to encode such constraints.', 'Rather, we generate a random sample of possible sentence plans for each text plan, up to a pre-specified maximum number of sentence plans, by randomly selecting among the operations according to some probability distribution. 4']",0,"['The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts  from the input text plan, and with its interior nodes labeled with clause-combining operations 3 .', 'Each node is also associated with a DSyntS: the leaves (which correspond to elementary speech acts from the input text plan) are linked to a canonical DSyntS for that speech act (by lookup in a hand-crafted dictionary).', 'This DSyntS can be sent to RealPro, which returns a sentence (or several sentences, if the DSyntS contains period nodes).', 'Figure 2 shows some of the realizations of alternative sentence plans generated by our SPG for utterance Sys- 3 The sp-tree is inspired by (Lavoie and Rambow, 1998).', 'The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or #AUTHOR_TAG are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .', 'The complexity of most sentence planners arises from the attempt to encode constraints on the application of, and ordering of, the operations, in order to generate a single high quality sentence plan.']"
CC971,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],method,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .","['2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .']",5,"['2The algorithm was implemented by the the authors , following the description in #AUTHOR_TAG .']"
CC972,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .","['The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .']",1,"['The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation-based learner , and the ICA system ( #AUTHOR_TAG ) .']"
CC973,N01-1006,Transformation Based Learning in the Fast Lane,classifier combination for improved lexical disambiguation,"['E Brill', 'J Wu']",experiments,"One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees..","The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .","['The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .']",5,"['The data used in the experiment was selected from the Penn Treebank Wall Street Journal , and is the same used by #AUTHOR_TAG .']"
CC974,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],experiments,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.","â¢ The regular TBL , as described in section 2 ; â¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â¢ The FastTBL algorithm ; â¢ The ICA algorithm ( #AUTHOR_TAG ) .","[""â\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm ( #AUTHOR_TAG ) .""]",1,"[""â\x80¢ The regular TBL , as described in section 2 ; â\x80¢ An improved version of TBL , which makes extensive use of indexes to speed up the rules ' update ; â\x80¢ The FastTBL algorithm ; â\x80¢ The ICA algorithm ( #AUTHOR_TAG ) .""]"
CC975,N01-1006,Transformation Based Learning in the Fast Lane,independence and commitment assumptions for rapid training and execution of rulebased pos taggers,['M Hepple'],,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.",The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .,['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .'],0,['The ICA system ( #AUTHOR_TAG ) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance .']
CC976,N01-1010,Tree-cut and a lexicon based on systematic polysemy,automatic extraction of systematic polysemy using treecut,['N Tomuro'],experiments,"This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins.","Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .","['Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .', 'In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method.']",2,"['Using the tree-cut technique described above , our previous work ( #AUTHOR_TAG ) extracted systematic polysemy from WordNet .']"
CC977,N01-1010,Tree-cut and a lexicon based on systematic polysemy,automatic extraction of systematic polysemy using treecut,['N Tomuro'],introduction,"This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins.","In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .","['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).', 'The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.']",2,"['In this paper, we describes a lexicon organized around systematic polysemy.', 'The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).', 'In our previous work ( #AUTHOR_TAG ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .', 'In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.', 'The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better values (Carletta, 1996) than arbitrary sense groupings on the agreement data.']"
CC978,N01-1024,Knowledge-free induction of inflectional morphologies,knowledgefree induction of morphology using latent semantic analysis,"['P Schone', 'D Jurafsky']",method,"Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stem-and-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (""ally"" stemming to ""all""). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plus-affix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.","In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .","['In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .', 'Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix.', 'The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1.', ""The matrix is structured such that for a given word w's row, the first N columns denote words that -NCS (µ,1)""]",2,"['In order to obtain semantic representations of each word , we apply our previous strategy ( #AUTHOR_TAG ) .']"
CC979,N12-1010,A user modeling-based performance analysis of a wizarded uncertainty-adaptive dialogue system corpus,examining the impacts of dialogue content and system automation on affect models in a spoken tutorial dialogue system,"['J Drummond', 'D Litman']",,"Many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users ' affect. Previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models. Thus, we wish to investigate how more minor differences, like small dialogue content changes and switching from a wizarded system to a fully automated system, influence the performance of our affect detection models. We perform a post-hoc experiment where we use various data sets to train multiple models, and compare against a test set from the most recent version of our dialogue system. Analyzing these results strongly suggests that these differences do impact these models ' performance","Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .","['To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus.', 'As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\'s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (""incorrect runs"").', 'We also included two user-based features, gender and pretest score.', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .', 'To date, however, these features have only decreased the crossvalidation performance of our models.', '8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of ""response appropriateness"" in other domains, while pretest score corresponds to the general notion of domain expertise).', 'Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed.', 'To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.']",2,"['To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus.', 'As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue.', 'The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question\'s name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (""incorrect runs"").', 'We also included two user-based features, gender and pretest score.', 'Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; #AUTHOR_TAG ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .', '8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of ""response appropriateness"" in other domains, while pretest score corresponds to the general notion of domain expertise).', 'To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users.']"
CC980,P00-1004,Translation with Cascaded Finite State Transducers,a polynomialtime algorithm for statistical machine translation,['D Wu'],introduction,,"Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .","['Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .']",0,"['Finite state transducers , which can be learned from bilingual corpora , have been proposed for automatic translation ( Amengual et al. , 2000 ) , as have been bilingual stochastic grammars ( #AUTHOR_TAG ) .']"
CC981,P00-1004,Translation with Cascaded Finite State Transducers,a polynomialtime algorithm for statistical machine translation,['D Wu'],,,It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .,['It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .'],5,['It also shows the structural identity to bilingual grammars as used in ( #AUTHOR_TAG ) .']
CC982,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",method,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .,['It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .'],4,['It can be shown ( #AUTHOR_TAG ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .']
CC983,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .,['#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .'],0,['#AUTHOR_TAG describe an efficient algorithm for accomplishing this in which approximations to Pst ( TIS ) are computed in parallel for all ( new ) features ft by holding all weights in the existing model fixed and optimizing only over a8t .']
CC984,P00-1006,A Maximum Entropy/Minimum Divergence Translation Model,a maximum entropy approach to natural language processing,"['Adam L Berger', 'Stephen A Della Pietra', 'Vincent J Della Pietra']",introduction,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .,['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .'],5,['A statistical technique which has recently become popular for NLP is Maximum Entropy/Minimum Divergence ( MEMD ) modeling ( #AUTHOR_TAG ) .']
CC985,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,memorybased shallow parsing,"['Walter Daelemans', 'Sabine Buchholz', 'Jorn Veenstra']",conclusion,,"As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .","['As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .']",3,"['As ( #AUTHOR_TAG ) show , lexical information improves on NP and VP chunking as well .']"
CC986,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,statistical decisiontree models for parsing,['David M Magerman'],,,"The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by #AUTHOR_TAG , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .']"
CC987,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , #AUTHOR_TAG , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']"
CC988,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,structural ambiguity and lexical relations,"['D Hindle', 'M Rooth']",conclusion,"We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.","It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .","['It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .']",1,"['It is not aimed at handling dependencies , which require heavy use of lexical information ( #AUTHOR_TAG , for PP attachment ) .']"
CC989,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a maximumentropy partial parser for unrestricted text,"['W Skut', 'T Brants']",introduction,,Another approach for partial parsing was presented by #AUTHOR_TAG .,['Another approach for partial parsing was presented by #AUTHOR_TAG .'],0,['Another approach for partial parsing was presented by #AUTHOR_TAG .']
CC990,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , #AUTHOR_TAG , and Sekine ( 1998 ) ) .']"
CC991,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .","['Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .']",1,"['Our results are lower than those of full parsers , e.g. , #AUTHOR_TAG as might be expected since much less structural data , and no lexical data are being used .']"
CC992,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",conclusion,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .","['In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .']",3,"['In a similar vain to Skut and Brants ( 1998 ) and #AUTHOR_TAG , the method extends an existing flat shallow-parsing method to handle composite structures .']"
CC993,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,statistical decisiontree models for parsing,['David M Magerman'],introduction,,"A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .","['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']",0,"['A variety of statistical methods were proposed over the recent years for learning to produce a full parse of free-text sentences ( e.g. , Bod ( 1992 ) , #AUTHOR_TAG , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .']"
CC994,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , Collins ( 1997 ) , and #AUTHOR_TAG , and became a common testbed .']"
CC995,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,three generative lexicalised models for statistical parsing,['M Collins'],,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .","['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .']",1,"['The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by Magerman ( 1995 ) , #AUTHOR_TAG , and Ratnaparkhi ( 1997 ) , and became a common testbed .']"
CC996,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",introduction,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .","['One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .']",0,"['One approach to partial parsing was presented by #AUTHOR_TAG , who extended a shallow-parsing technique to partial parsing .']"
CC997,P00-1007,Incorporating Compositional Evidence in Memory-Based Partial Parsing,a maximumentropy partial parser for unrestricted text,"['W Skut', 'T Brants']",conclusion,,"In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .","['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']",3,"['In a similar vain to #AUTHOR_TAG and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .']"
CC998,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.","To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .","['Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second.', 'The first ordered pairs are more frequent, as are the individual adjectives involved.', 'To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a → b is:']",0,"['Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the second.', 'The first ordered pairs are more frequent, as are the individual adjectives involved.', 'To quantify the relative strengths of these transitive inferences , #AUTHOR_TAG propose to assign a weight to each link .', 'Say the order a, b occurs m times and the pair {a, b} occurs n times in total.', 'Then the weight of the pair a - b is:']"
CC999,P00-1012,The order of prenominal adjectives in natural language generation,distributional clustering of english words,"['Fernando Pereira', 'Naftali Tishby', 'Lilian Lee']",conclusion,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.","More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .","['While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement.', 'Future work will pursue at least two directions for improving the results.', 'First, while semantic information is not available for all adjectives, it is clearly available for some.', 'Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available.', 'More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .', 'Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results.']",3,"['More generally , distributional clustering techniques ( Sch Â¨ utze , 1992 ; #AUTHOR_TAG ) could be applied to extract semantic classes from the corpus itself .']"
CC1000,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.",The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .,"['The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .', 'To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often.']",0,"['The simplest strategy for ordering adjectives is what #AUTHOR_TAG call the direct evidence method .', 'To order the pair {a, b}, count how many times the ordered sequences a, b and b, a appear in the training data and output the pair in the order which occurred more often.']"
CC1001,P00-1012,The order of prenominal adjectives in natural language generation,boosting applied to tagging and pp attachment,"['Steven Abney', 'Robert E Schapire', 'Yoram Singer']",conclusion,Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors.,"In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .","['The second area where the methods described here could be improved is in the way that multiple information sources are integrated.', 'The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data.', 'It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997).', 'In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .']",3,"['In particular , boosting ( Schapire , 1999 ; #AUTHOR_TAG ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .']"
CC1002,P00-1012,The order of prenominal adjectives in natural language generation,ordering among premodifiers,"['James Shaw', 'Vasileios Hatzivassiloglou']",experiments,"We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94 % of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.",#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .,"['One way to think of the direct evidence method is to see that it defines a relation ≺ on the set of English adjectives.', 'Given two adjectives, if the ordered pair a, b appears in the training data more often then the pair b, a , then a ≺ b.', 'If the re-verse is true, and b, a is found more often than a, b , then b ≺ a.', 'If neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', 'That is, if a ≺ c and c ≺ b, we can conclude that a ≺ b.', 'To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'However, the pairs large, new and new, green occur fairly frequently.', 'Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.']",0,"['If the re-verse is true, and b, a is found more often than a, b , then b  a.', 'If neither order appears in the training data, then neither a  b nor b  a and an order must be randomly assigned.', '#AUTHOR_TAG propose to generalize the direct evidence method so that it can apply to unseen pairs of adjectives by computing the transitive closure of the ordering relation .', 'That is, if a  c and c  b, we can conclude that a  b.', 'To take an example from the BNC, the adjectives large and green never occur together in the training data, and so would be assigned a random order by the direct evidence method.', 'However, the pairs large, new and new, green occur fairly frequently.', 'Therefore, in the face of this evidence we can assign this pair the order large, green , which not coincidently is the correct English word order.']"
CC1003,P00-1012,The order of prenominal adjectives in natural language generation,generation that exploits corpusbased statistical knowledge,"['Irene Langkilde', 'Kevin Knight']",method,"We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.","One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .","['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model ."", 'Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the correct morphological form of a word.', 'It also should be straightforwardly applicable to the more specific problem we are addressing here.', 'To determine the correct order for a sequence of prenominal adjectives, we can simply generate all possible orderings and choose the one with the highest probability.', 'This has the advantage of reducing the problem of adjective ordering to the problem of estimating n-gram probabilities, something which is relatively well understood.']",5,"['The problem of generating ordered sequences of adjectives is an instance of the more general problem of selecting among a number of possible outputs from a natural language generation system.', ""One approach to this more general problem , taken by the ` Nitrogen ' generator ( #AUTHOR_TAGa ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .""]"
CC1004,P02-1001,Parameter estimation for probabilistic finite-state transducers,translation with finitestate devices,"['Kevin Knight', 'Yaser Al-Onaizan']",introduction,,"Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) .","['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"[""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( Pereira and Riley , 1997 ) and machine translation ( #AUTHOR_TAG ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1005,P02-1001,Parameter estimation for probabilistic finite-state transducers,a rational design for a weighted finitestate transducer library,"['M Mohri', 'F Pereira', 'M Riley']",,,"fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )","['10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )']",0,"['10 Traditionally log(strength) values are called weights, but this paper uses ""weight"" to mean something else.', 'fÎ¸ on demand ( #AUTHOR_TAG ) can pay off here , since only part of fÎ¸ may be needed subsequently . )']"
CC1006,P02-1001,Parameter estimation for probabilistic finite-state transducers,rational series and their languages,"['Jean Berstel', 'Christophe Reutenauer']",,"I. Rational Series.- 1 Semirings.- 2 Formal Series.- 3 The Topology of K""X"".- 4 Rational Series.- 5 Recognizable Series.- 6 The Fundamental Theorem.- Exercises for Chapter I.- Notes to Chapter I.- II. Minimization.- 1 Syntactic Ideals.- 2 Reduced Linear Representations.- 3 The Reduction Algorithm.- Exercises for Chapter II.- Notes to Chapter II.- III. Series and Languages.- 1 The Theorem of Kleene.- 2 Series and Rational Languages.- 3 Supports.- 4 Iteration.- 5 Complementation.- Exercises for Chapter III.- Notes to Chapter III.- IV. Rational Series in One Variable.- 1 Rational Functions.- 2 The Exponential Polynomial.- 3 A Theorem of Polya.- 4 A Theorem of Skolem, Mahler and Lech.- Notes to Chapter IV.- V. Changing the Semiring.- 1 Rational Series over a Principal Ring.- 2 Positive Rational Series.- 3 Fatou Extensions.- Exercises for Chapter V.- Notes to Chapter V.- VI. Decidability.- 1 Problems of Supports.- 2 Growth.- Exercises for Chapter VI.- Notes to Chapter VI.- VII. Noncommutative Polynomials.- 1 The Weak Algorithm.- 2 Continuant Polynomials.- 3 Inertia.- 4 Gauss's Lemma.- Exercises for Chapter VII.- Notes to Chapter VII.- VIII. Codes and Formal Series.- 1 Codes.- 2 Completeness.- 3 The Degree of a Code.- 4 Factorization.- Exercises for Chapter VIII.- Notes to Chapter VIII.- References.",#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .,"['The denominator of equation ( 1) is the total probability of all accepting paths in x i • f • y i .', 'But while computing this, we will also compute the numerator.', 'The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.', 'We will enforce an invariant: the weight of any pathset Π must be ( π∈Π P (π), π∈Π P (π) val(π)) ∈ R ≥0 × V , from which (1) is trivial to compute.', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', 'Multiplication and addition are replaced by binary operations ⊗ and ⊕ on K. Thus ⊗ is used to combine arc weights into a path weight and ⊕ is used to combine the weights of alternative paths.', 'To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * = ∞ i=0 k i .', 'The usual finite-state algorithms work if (K, ⊕, ⊗, * ) has the structure of a closed semiring. 15', 'rdinary probabilities fall in the semiring (R ≥0 , +, ×, * ). 16', 'Our novel weights fall in a novel 14 Formal derivation of (1):']",5,"['But while computing this, we will also compute the numerator.', '#AUTHOR_TAG give a sufficiently general finite-state framework to allow this : weights may fall in any set K ( instead of R ) .', 'To sum over infinite sets of cyclic paths we also need a closure operation * , interpreted as k * =  i=0 k i .', 'The usual finite-state algorithms work if (K, , , * ) has the structure of a closed semiring. 15', 'Our novel weights fall in a novel 14 Formal derivation of (1):']"
CC1007,P02-1001,Parameter estimation for probabilistic finite-state transducers,an inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process,['L E Baum'],,,"For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .","['• In many cases of interest, T i is an acyclic graph. 20', ""hen Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations."", 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .', 'But notice that it has no backward pass.', 'In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.', 'This is slower because our ⊕ and ⊗ are vector operations, and the vectors rapidly lose sparsity as they are added together.']",0,"['* In many cases of interest, T i is an acyclic graph. 20', ""hen Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of  and  operations."", 'For HMMs ( footnote 11 ) , Ti is the familiar trellis , and we would like this computation of ti to reduce to the forwardbackward algorithm ( #AUTHOR_TAG ) .', 'But notice that it has no backward pass.', 'In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.', 'This is slower because our  and  are vector operations, and the vectors rapidly lose sparsity as they are added together.']"
CC1008,P02-1001,Parameter estimation for probabilistic finite-state transducers,a rational design for a weighted finitestate transducer library,"['M Mohri', 'F Pereira', 'M Riley']",introduction,,"The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .","['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', 'Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).', 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"['The availability of toolkits for this weighted case ( #AUTHOR_TAG ; van Noord and Gerdemann , 2001 ) promises to unify much of statistical NLP .', 'Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding, 1 including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).', 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1009,P02-1001,Parameter estimation for probabilistic finite-state transducers,learning string edit distance,"['E Ristad', 'P Yianilos']",introduction,"In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. In this report, we provide a stochastic model for string-edit distance. Our stochastic model allows us to learn a string-edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string-edit distance with nearly one-fifth the error rate of the untrained Levenshtein distance. Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes.","For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .","['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .']",0,"['For example , the forward-backward algorithm ( Baum , 1972 ) trains only Hidden Markov Models , while ( #AUTHOR_TAG ) trains only stochastic edit distance .']"
CC1010,P02-1001,Parameter estimation for probabilistic finite-state transducers,iterative methods for solving linear systems,['Anne Greenbaum'],,"bthkhady`  withiithamchamsamhrabhaaphlechlyrabbechingesnmii`yuudwykan 2 aebbaihy + khuue` withiithamcham`yaangningkabwithiipriphuumiy`yaikhrl`f bthkhwaamwichaakaarnii`phipraayaenwkhidthawaipaelaethkhnikhphuuenthaankh`ngwithiithamcham`yaangning aidaek withiicchaaokhbii withiiekaas-aichedl aelawithiiph`nprnekinsuuebenuue`ng n`kcchaaknanyangphicchaarnaawithiithamchamthiiphathnaat`y`dcchaakwithiidangklaaw aidaek withiiph`nprnekinaebberng withiithamchamthiimiithaancchaakekrediiynt eaelawithiithamchamkamlangs`ngn`ysud samhrabwithiipriphuumiy`yaikhrl`fnanmiitnaebbmaacchaakwithiikh`ncchuuektekrediiynt withiidangklaawcchasraangthaanhlakechingtangchaakkh`ngpriphuumiaebbyukhlidcchaakemthrikchsamprasiththiodyphicchaarnaacchaakekrediiyntkh`ngfangkchankamlangs`ngthiis`dkhl`ng thaanhlakdangklaawprak`bdwyewket`rthiimiithisthaangthiithamaihphlechlykhaapramaanekhaaaiklphlechlycchringaiderwthiisud klaawodysrupaidwaa withiithamcham`yaangning 4 withiiaerkthiiklaawmaanancchakaarantiikaarluuekhaakh`nglamdabkh`ngphlechlyodypramaansuuphlechlycchringemuue`aichkabrabbthiimiiemthrikchsamprasiththi`yuuainruupaebbechphaaa echn emthrikchaenwthaeyngmumkhmaeth emthrikchldth`naimaid aelaemthrikchaebbae`l odyt`ngkamhndtawaepresrimthiiehmaaasm swnwithiithamchamthiimiithaancchaakekrediiyntaelawithiithamchamkamlangs`ngn`ysudaichaidkabrabbthiimiiemthrikchsamprasiththimiikhaalamdabchanetm samhrabwithiikh`ncchuuektekrediiyntaichaidkabrabbthiiemthrikchsamprasiththiepnemthrikchsmmaatrthiiepnbwkaenn`n   khamsamkhay: rabbechingesn withiiph`nprnekinsuuebenuue`ng  withiiph`nprnekinaebberng withiithamchamthiimiithaancchaakekrediiynt  withiikh`ncchuuektekrediiynt     ABSTRACT There are two major types of iterative methods for solving linear systems, namely, stationary iterative methods and Krylov subspace methods. This survey article discusses general ideas and elementary techniques for stationary iterative methods such as Jacobi method, Gauss-Seidel method, and the successive over-relaxation method. Moreover, we investigate further developed methods, namely, the accelerated over-relaxation method, the gradient based iterative method, and the least squares iterative method. On the other hand, Krylov subspace methods have prototypes from the conjugate gradient method. The latter method constructs an orthogonal basis for the Euclidean space from the gradient of the associated quadratic function. Such basis consists of vectors in directions so that the approximated solutions fastest approach to the exact solution. In conclusions, all 1st-4th mentioned stationary iterative methods guarantee the convergence of the sequence of approximated solutions to the exact solution when applying to the system with specific coefficient matrices such as strictly diagonally dominant matrices, irreducible matrices, and L-matrices. Here, the parameters in the methods must be appropriate. The gradient based iterative method and the least squares iterative method can be applied to systems with full-column rank coefficient matrices. The conjugate gradient method is applicable for the system whose coefficient matrix is a positive definite symmetric matrix.Keywords: linear system, successive over-relaxation method, accelerated over-relaxation method, gradient based iterative method, conjugate gradient metho","The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .","['We therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing t i (so they are needed only to construct T i ).', 'This speedup also works for cyclic graphs and for any V .', 'Write w jk as (p jk , v jk ), and let w 1 jk = (p 1 jk , v 1 jk ) denote the weight of the edge from j to k. 19 Then it can be shown that w 0n = (p 0n , j,k p 0j v 1 jk p kn ).', 'The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .']",0,"['The forward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring ( R , + , x , â\x88\x97 ) -- or equivalently , by solving a sparse linear system of equations over R , a much-studied problem at O ( n ) space , O ( nm ) time , and faster approximations ( #AUTHOR_TAG ) .']"
CC1011,P02-1001,Parameter estimation for probabilistic finite-state transducers,rational series and their languages,"['Jean Berstel', 'Christophe Reutenauer']",introduction,"I. Rational Series.- 1 Semirings.- 2 Formal Series.- 3 The Topology of K""X"".- 4 Rational Series.- 5 Recognizable Series.- 6 The Fundamental Theorem.- Exercises for Chapter I.- Notes to Chapter I.- II. Minimization.- 1 Syntactic Ideals.- 2 Reduced Linear Representations.- 3 The Reduction Algorithm.- Exercises for Chapter II.- Notes to Chapter II.- III. Series and Languages.- 1 The Theorem of Kleene.- 2 Series and Rational Languages.- 3 Supports.- 4 Iteration.- 5 Complementation.- Exercises for Chapter III.- Notes to Chapter III.- IV. Rational Series in One Variable.- 1 Rational Functions.- 2 The Exponential Polynomial.- 3 A Theorem of Polya.- 4 A Theorem of Skolem, Mahler and Lech.- Notes to Chapter IV.- V. Changing the Semiring.- 1 Rational Series over a Principal Ring.- 2 Positive Rational Series.- 3 Fatou Extensions.- Exercises for Chapter V.- Notes to Chapter V.- VI. Decidability.- 1 Problems of Supports.- 2 Growth.- Exercises for Chapter VI.- Notes to Chapter VI.- VII. Noncommutative Polynomials.- 1 The Weak Algorithm.- 2 Continuant Polynomials.- 3 Inertia.- 4 Gauss's Lemma.- Exercises for Chapter VII.- Notes to Chapter VII.- VIII. Codes and Formal Series.- 1 Codes.- 2 Completeness.- 3 The Degree of a Code.- 4 Factorization.- Exercises for Chapter VIII.- Notes to Chapter VIII.- References.","4To prove ( 1 ) â ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .","['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .', 'A full proof is straightforward, as are proofs of (3)_(2), (2)_(1).']",5,"['4To prove ( 1 ) â\x87\x92 ( 3 ) , express f as an FST and apply the well-known Kleene-Sch Â¨ utzenberger construction ( #AUTHOR_TAG ) , taking care to write each regexp in the construction as a constant times a probabilistic regexp .']"
CC1012,P02-1001,Parameter estimation for probabilistic finite-state transducers,a systolic array algorithm for the algebraic path problem shortest paths matrix inversion,['G¨unter Rote'],,"It is shown how the Gauss-Jordan Elimination algorithm for the Algebraic Path Problem can be implemented on a hexagonal systolic array of a quadratic number of simple processors in linear time. Special instances of this general algorithm include parallelizations of the Warshall-Floyd Algorithm, which computes the shortest distances in a graph or the transitive closure of a relation, and of the Gauss-Jordan Elimination algorithm for computing the inverse of a real matrix.ZusammenfassungEs wird dargestellt, wie man den gaus-Jordanschen Eliminationsalgorithmus fur das algebraische Wegproblem auf einem hexagonalen systolischen Feld (systolic array) mit einer quadratischen Anzahl einfacher Prozessoren in linearer Zeit ausfuhren kann. Zu den Anwendungsbeispielen dieses allgemeinen Algorithmus gehort der Warshall-Floyd-Algorithmus zur Berechnung der kurzesten Wegen in einem Graphen oder zur Bestimmung der transitiven Hulle einer Relation sowie der Gauss-Jordansche Eliminationsalgorithmus zur Inversion reeller Matrizen.",Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .,"['Division and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ).', 'Division is commonly used in defining f θ (for normalization). 19', 'Multiple edges from j to k are summed into a single edge.', '(Mohri, 2002).', 'Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .']",3,"['Division and subtraction are also possible: -(p, v) = (-p, -v) and (p, v) -1 = (p -1 , -p -1 vp -1 ).', 'Multiple edges from j to k are summed into a single edge.', 'Efficient hardware implementation is also possible via chip-level parallelism ( #AUTHOR_TAG ) .']"
CC1013,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],,,"Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .","['13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a ∈ Σ; their weights are normalized to sum to 1.', 'Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a.', 'Then the predicted vector given θ is j,a dj,a • (expected feature counts on a randomly chosen arc in Dj,a).', 'Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .', 'The difficult case is global conditional normalization.', 'It arises, for example, when training a joint model of the form']",1,"['13 For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a  S; their weights are normalized to sum to 1.', 'Per-state joint normalization ( #AUTHOR_TAGb , Â§ 8.2 ) is similar but drops the dependence on a .', 'The difficult case is global conditional normalization.']"
CC1014,P02-1001,Parameter estimation for probabilistic finite-state transducers,maximum likelihood from incomplete data via the em algorithm,"['A P Dempster', 'N M Laird', 'D B Rubin']",,,The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .,"['The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .', 'Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f θ , which FST paths stand a chance of having been the path used?', '(Guessing the path also guesses the exact input and output.)', 'The M step updates θ to make those paths more likely.', 'EM alternates these steps and converges to a local optimum.', ""The M step's form depends on the parameterization and the E step serves the M step's needs.""]",5,"['The EM algorithm ( #AUTHOR_TAG ) can maximize these functions .', 'Roughly, the E step guesses hidden information: if (x i , y i ) was generated from the current f th , which FST paths stand a chance of having been the path used?', '(Guessing the path also guesses the exact input and output.)', 'The M step updates th to make those paths more likely.', 'EM alternates these steps and converges to a local optimum.', ""The M step's form depends on the parameterization and the E step serves the M step's needs.""]"
CC1015,P02-1001,Parameter estimation for probabilistic finite-state transducers,compilation of weighted finitestate transducers from decision trees,"['Richard Sproat', 'Michael Riley']",introduction,"We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996).","For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .","['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']",0,"['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat , 1996 ) , ( 3 ) by compilation of decision trees ( #AUTHOR_TAG ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']"
CC1016,P02-1001,Parameter estimation for probabilistic finite-state transducers,algebraic structures for transitive closure,['D J Lehmann'],,"AbstractClosed semi-rings and the closure of matrices over closed semi-rings are defined and studied. Closed semi-rings are structures weaker than the structures studied by Conway [3] and Aho, Hopcroft and Ullman [1]. Examples of closed semi-rings and closure operations are given, including the case of semi-rings on which the closure of an element is not always defined. Two algorithms are proved to compute the closure of a matrix over any closed semi-ring; the first one based on Gauss-Jordan elimination is a generalization of algorithms by Warshall, Floyd and Kleene; the second one based on Gauss elimination has been studied by Tarjan [11, 12], from the complexity point of view in a slightly different framework. Simple semi-rings, where the closure operation for elements is trivial, are defined and it is shown that the closure of an n x n-matrix over a simple semi-ring is the sum of its powers of degree less than n. Dijkstra semi-rings are defined and it is shown that the rows of the closure of a matrix over a Dijkstra semi-ring, can be computed by a generalized version of Dijkstra's algorithm","Now for some important remarks on efficiency : â¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).","['Now for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', 'It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.', 'If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph Ti, Tarjan (1981b) shows how to partition into �hard� subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']",0,"['Now for some important remarks on efficiency : â\x80¢ Computing ti is an instance of the well-known algebraic path problem ( #AUTHOR_TAG ; Tar an , 1981a ) . Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and un- weighted).', 'It is wasteful to compute ti as suggested earlier, by minimizing (__xi)_f_(yi__), since then the real work is done by an _-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.', 'If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph Ti, Tarjan (1981b) shows how to partition into hard subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.']"
CC1017,P02-1001,Parameter estimation for probabilistic finite-state transducers,conditional random fields probabilistic models for segmenting and labeling sequence data,"['J Lafferty', 'A McCallum', 'F Pereira']",introduction,"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.","Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .']",0,"['Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; Eisner , 2001b ; #AUTHOR_TAG ) .']"
CC1018,P02-1001,Parameter estimation for probabilistic finite-state transducers,a gaussian prior for smoothing maximum entropy models,"['Stanley F Chen', 'Ronald Rosenfeld']",,"Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.","In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .","['Maximum-posterior estimation tries to maximize P (θ) • i f θ (x i , y i ) where P (θ) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .']",5,"['Maximum-posterior estimation tries to maximize P (th) * i f th (x i , y i ) where P (th) is a prior probability.', 'In a log-linear parameterization , for example , a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting ( #AUTHOR_TAG ) .']"
CC1019,P02-1001,Parameter estimation for probabilistic finite-state transducers,hidden markov models with finite state supervision in,['E Ristad'],,"In this chapter we provide a supervised training paradigm for hidden Markov models (HMMs). Unlike popular ad-hoc approaches, our paradigm is completely general, need not make any simplifying assumptions about independence, and can take better advantage of the information contained in the training corpus.","For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11","['As training data we are given a set of observed (input, output) pairs, (xi, yi).', 'These are assumed to be independent random samples from a joint dis- tribution of the form f_�(x, y); the goal is to recover the true _�.', 'Samples need not be fully observed (partly supervised training): thus xi _ __, yi _ �_ may be given as regular sets in which input and output were observed to fall.', 'For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']",0,"['For example , in ordinary HMM training , xi = E * and represents a completely hidden state sequence ( cfXXX #AUTHOR_TAG , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence .11']"
CC1020,P02-1001,Parameter estimation for probabilistic finite-state transducers,speech recognition by composition of weighted finite automata,"['Fernando C N Pereira', 'Michael Riley']",introduction,"We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.","Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) .","['The availability of toolkits for this weighted case (Mohri et al., 1998;van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.', ""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']",0,"[""Such tools make it easy to run most current approaches to statistical markup , chunking , normalization , segmentation , alignment , and noisy-channel decoding , ' including classic models for speech recognition ( #AUTHOR_TAG ) and machine translation ( Knight and Al-Onaizan , 1998 ) ."", 'Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.']"
CC1021,P02-1001,Parameter estimation for probabilistic finite-state transducers,generic epsilonremoval and input epsilonnormalization algorithms for weighted transducers,['M Mohri'],,,"It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .","['• Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'Let T i = x i •f •y i .', 'Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted).', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).', 'For a general graph T i , Tarjan (1981b) shows how to partition into ""hard"" subgraphs that localize the cyclicity or irreducibility, then run the O(n 3 ) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.', 'The overhead of partitioning and recombining is essentially only O(m).']",0,"['* Computing t i is an instance of the well-known algebraic path problem (Lehmann, 1977;Tarjan, 1981a).', 'Then t i is the total semiring weight w 0n of paths in T i from initial state 0 to final state n (assumed WLOG to be unique and unweighted).', 'It is wasteful to compute ti as suggested earlier , by minimizing ( cxxi ) of o ( yixE ) , since then the real work is done by an c-closure step ( #AUTHOR_TAG ) that implements the all-pairs version of algebraic path , whereas all we need is the single-source version .', 'If n and m are the number of states and edges, 19 then both problems are O(n 3 ) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tarjan, 1981b).']"
CC1022,P02-1001,Parameter estimation for probabilistic finite-state transducers,conditional random fields probabilistic models for segmenting and labeling sequence data,"['J Lafferty', 'A McCallum', 'F Pereira']",introduction,"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.","Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .","['• An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .', 'Also, in the conditional case such per-state normalization is only correct if all states accept all input suffixes (since ""dead ends"" leak probability mass). 8', ' A better-founded approach is global normalization, which simply divides each f (x, y) by']",0,"['* An easy approach is to normalize the options at each state to make the FST Markovian.', 'Unfortunately, the result may differ for equivalent FSTs that express the same weighted relation.', 'Undesirable consequences of this fact have been termed ""label bias"" ( #AUTHOR_TAG ) .', ' A better-founded approach is global normalization, which simply divides each f (x, y) by']"
CC1023,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],introduction,,"A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .","['A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .', 'A leisurely journal-length version with more details has been prepared and is available.']",2,"['A brief version of this work, with some additional material, first appeared as ( #AUTHOR_TAGa ) .']"
CC1024,P02-1001,Parameter estimation for probabilistic finite-state transducers,regular approximation of contextfree grammars through transformation,"['M Mohri', 'M-J Nederhof']",introduction,We present an algorithm for approximating context-free languages with regular languages. The algorithm is based on a simple transformation that applies to any context-free grammar and guarantees that the result can be compiled into a finite automaton. The resulting grammar contains at most one new nonterminal for any nonterminal symbol of the input grammar. The result thus remains readable and if necessary modifiable. We extend the approximation algorithm to the case of weighted context-free grammars. We also report experiments with several grammars showing that the size of the minimal deterministic automata accepting the resulting approximations is of practical use for applications such as speech recognition.,"A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .","['w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7', 'here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.""]",0,"['A more subtle example is weighted FSAs that approximate PCFGs ( Nederhof , 2000 ; #AUTHOR_TAG ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .']"
CC1025,P02-1001,Parameter estimation for probabilistic finite-state transducers,an efficient compiler for weighted rewrite rules,"['Mehryar Mohri', 'Richard Sproat']",introduction,"Context-dependent rewrite rules are used in many areas of natural language and speech processing. Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs). We describe a new algorithm for compiling rewrite rules into FSTs. We show the algorithm to be simpler and more efficient than existing algorithms. Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights. We have extended the algorithm to allow for this.","For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .","['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']",0,"['For defining conditional relations , a good regexp language is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig. 1c , ( 2 ) by compilation of weighted rewrite rules ( #AUTHOR_TAG ) , ( 3 ) by compilation of decision trees ( Sproat and Riley , 1996 ) , ( 4 ) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation ( Gerdemann and van Noord , 1999 ) ,5 ( 5 ) by conditionalization of a joint relation as discussed below .']"
CC1026,P02-1001,Parameter estimation for probabilistic finite-state transducers,maximum entropy markov models for information extraction and segmentation,"['A McCallum', 'D Freitag', 'F Pereira']",introduction,"Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ's.","Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .']",0,"['The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( #AUTHOR_TAG ; Eisner , 2001b ; Lafferty et al. , 2001 ) .']"
CC1027,P02-1001,Parameter estimation for probabilistic finite-state transducers,practical experiments with regular approximation of contextfree languages,['Mark-Jan Nederhof'],introduction,,"A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .","['w,x,y P (v|w)P (w, x)P (y|x)P (z|y), implemented by composing 4 machines. 6,7', 'here are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).', 'Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into : /2.7 −→ arcs).', 'A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .', ""These are parameterized by the PCFG's parameters, but add or remove strings of the PCFG to leave an improper probability distribution.""]",0,"['A more subtle example is weighted FSAs that approximate PCFGs ( #AUTHOR_TAG ; Mohri and Nederhof , 2001 ) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation .']"
CC1028,P02-1001,Parameter estimation for probabilistic finite-state transducers,an inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process,['L E Baum'],introduction,,"For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .","['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.', 'Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.', 'For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .']",0,"['Unfortunately, there is a stumbling block: Where do the weights come from?', 'After all, statistical models require supervised or unsupervised training.', 'For example , the forward-backward algorithm ( #AUTHOR_TAG ) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996 ) trains only stochastic edit distance .']"
CC1029,P02-1001,Parameter estimation for probabilistic finite-state transducers,expectation semirings flexible em for finitestate transducers,['Jason Eisner'],introduction,,"Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .","['o:e −→, and a:ae −→ share a contextual ""vowel-fronting"" feature, then their weights rise and fall together with the strength of that feature.', 'The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']",0,"['The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.', 'Such approaches have been tried recently in restricted cases ( McCallum et al. , 2000 ; #AUTHOR_TAGb ; Lafferty et al. , 2001 ) .']"
CC1030,P02-1001,Parameter estimation for probabilistic finite-state transducers,inducing features of random fields,"['S Della Pietra', 'V Della Pietra', 'J Lafferty']",,"We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.","The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and","['• If arc probabilities (or even λ, ν, µ, ρ) have loglinear parameterization, then the E step must compute c = i ec f (x i , y i ), where ec(x, y) denotes the expected vector of total feature counts along a random path in f θ whose (input, output) matches (x, y).', 'The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (Σ * , ∆ * ).', 'If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute). 13']",5,"['The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della #AUTHOR_TAG ; Chen and', 'For globally normalized, joint models, the predicted vector is ec f (S * ,  * ).']"
CC1031,P02-1001,Parameter estimation for probabilistic finite-state transducers,speech recognition by composition of weighted finite automata,"['Fernando C N Pereira', 'Michael Riley']",introduction,"We present a general framework based on weighted finite automata and weighted finite-state transducers for describing and implementing speech recognizers. The framework allows us to represent uniformly the information sources and data structures used in recognition, including context-dependent units, pronunciation dictionaries, language models and lattices. Furthermore, general but efficient algorithms can used for combining information sources in actual recognizers and for optimizing their application. In particular, a single composition algorithm is used both to combine in advance information sources such as language models and dictionaries, and to combine acoustic observations and information sources dynamically during recognition.","A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .","['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .', 'The general form is illustrated by 3 Conceptually, the parameters represent the probabilities of reading another a (λ); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to rather than x (ρ). 4 To prove (1)⇒(3), express f as an FST and apply the well-known Kleene-Schützenberger construction (Berstel and Reutenauer, 1988), taking care to write each regexp in the construction as a constant times a probabilistic regexp.', 'A full proof is straightforward, as are proofs of (3)⇒( 2), ( 2)⇒(1).']",0,"['A central technique is to define a joint relation as a noisy-channel model , by composing a joint relation with a cascade of one or more conditional relations as in Fig. 1 ( #AUTHOR_TAG ; Knight and Graehl , 1998 ) .']"
CC1032,P05-3005,Dynamically generating a protein entity dictionary using online resources,dr introducing refseq and locuslink curated human genome resources at the ncbi trends genet,"['Pruitt KD', 'Katz KS', 'H Sicotte', 'Maglott']",,,"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and","['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].', 'The following provides a brief description of each of the database.']",5,"['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq #AUTHOR_TAG , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) [ 14 ] , and']"
CC1033,P05-3005,Dynamically generating a protein entity dictionary using online resources,sgd saccharomyces genome database nucleic acids res,"['Cherry JM', 'C Adler', 'C Ball', 'Chervitz SA', 'Dwight SS', 'Hester ET', 'Y Jia', 'G Juvik', 'T Roe', 'M Schroeder']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) #AUTHOR_TAG , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1034,P05-3005,Dynamically generating a protein entity dictionary using online resources,boddy wj et al the mouse genome database mgd integrating biology with the genome nucleic acids res,"['Bult CJ', 'Blake JA', 'Richardson JE', 'Kadin JA', 'Eppig JT', 'Baldarelli RM', 'K Barsanti', 'M Baya', 'Beal JS']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) #AUTHOR_TAG , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1035,P05-3005,Dynamically generating a protein entity dictionary using online resources,al wormbase a multispecies resource for nematode biology and genomics nucleic acids res,"['Harris TW', 'N Chen', 'F Cunningham', 'M TelloRuiz', 'I Antoshechkin', 'C Bastiani', 'T Bieri', 'D Blasiar', 'K Bradnam', 'Chan J et']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase #AUTHOR_TAG , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1036,P05-3005,Dynamically generating a protein entity dictionary using online resources,the unified medical language system umls integrating biomedical terminology,['O Bodenreider'],,"The Unified Medical Language System (http://umlsks.nlm.nih.gov) is a repository of biomedical vocabularies developed by the US National Library of Medicine. The UMLS integrates over 2 million names for some 900,000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. Vocabularies integrated in the UMLS Metathesaurus include the NCBI taxonomy, Gene Ontology, the Medical Subject Headings (MeSH), OMIM and the Digital Anatomist Symbolic Knowledge Base. UMLS concepts are not only inter-related, but may also be linked to external resources such as GenBank. In addition to data, the UMLS includes tools for customizing the Metathesaurus (MetamorphoSys), for generating lexical variants of concept names (lvg) and for extracting UMLS concepts from text (MetaMap). The UMLS knowledge sources are updated quarterly. All vocabularies are available at no fee for research purposes within an institution, but UMLS users are required to sign a license agreement. The UMLS knowledge sources are distributed on CD-ROM and by FTP.",The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .,"['The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .', 'It contains three knowledge sources: the Metathesaurus (META), the SPECIALIST lexicon, and the Semantic Network.', 'The META provides a uniform, integrated platform for over 60 biomedical vocabularies and classifications, and group different names for the same concept.', 'The SPECIALIST lexicon contains syntactic information for many terms, component words, and English words, including verbs, which do not appear in the META.', 'The Semantic Network contains information about the types or categories (e.g., ""Disease or Syndrome"", ""Virus"") to which all META concepts have been assigned.']",0,['The UMLS -- the Unified Medical Language System ( UMLS ) has been developed and maintained by National Library of Medicine ( NLM ) #AUTHOR_TAG .']
CC1037,P05-3005,Dynamically generating a protein entity dictionary using online resources,mining the biomedical literature in the genomic era an overview,"['H Shatkay', 'R Feldman']",introduction,"The past decade has seen a tremendous growth in the amount of experimental and computational biomedical data, specifically in the areas of genomics and proteomics. This growth is accompanied by an accelerated increase in the number of biomedical publications discussing the findings. In the last few years, there has been a lot of interest within the scientific community in literature-mining tools to help sort through this abundance of literature and find the nuggets of information most relevant and useful for specific analysis tasks. This paper provides a road map to the various literature-mining methods, both in general and within bioinformatics. It surveys the disciplines involved in unstructured-text analysis, categorizes current work in biomedical literature mining with respect to these disciplines, and provides examples of text analysis methods applied towards meeting some of the current challenges in bioinformatics.","With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .","['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', 'Such task is called biological entity tagging.', 'Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).']",0,"['With the use of computers in storing the explosive amount of biological information , natural language processing ( NLP ) approaches have been explored to make the task of managing information recorded in free text more feasible #AUTHOR_TAG .', 'One requirement for NLP is the ability to accurately recognize terms that represent biological entities in free text.', 'Another requirement is the ability to associate these terms with corresponding biological entities (i.e., records in biological databases) in order to be used by other automated systems for literature mining.', 'Such task is called biological entity tagging.', 'Biological entity tagging is not a trivial task because of several characteristics associated with biological entity names, namely: synonymy (i.e., different terms refer to the same entity), ambiguity (i.e., one term is associated with different entities), and coverage (i.e., entity terms or entities are not present in databases or knowledge bases).']"
CC1038,P05-3005,Dynamically generating a protein entity dictionary using online resources,online mendelian inheritance in man omim a knowledgebase of human genes and genetic disorders nucleic acids res,"['A Hamosh', 'Scott AF', 'Amberger JS', 'Bocchini CA', 'McKusick VA']",,"Online Mendelian Inheritance in Man (OMIM) is a comprehensive, authoritative and timely knowledgebase of human genes and genetic disorders compiled to support human genetics research and education and the practice of clinical genetics. Started by Dr Victor A. McKusick as the definitive reference Mendelian Inheritance in Man, OMIM (http://www.ncbi.nlm.nih.gov/omim/) is now distributed electronically by the National Center for Biotechnology Information, where it is integrated with the Entrez suite of databases. Derived from the biomedical literature, OMIM is written and edited at Johns Hopkins University with input from scientists and physicians around the world. Each OMIM entry has a full-text summary of a genetically determined phenotype and/or gene and has numerous links to other genetic databases such as DNA and protein sequence, PubMed references, general and locus-specific mutation databases, HUGO nomenclature, MapViewer, GeneTests, patient support groups and many others. OMIM is an easy and straightforward portal to the burgeoning information in human genetics.","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) #AUTHOR_TAG , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1039,P05-3005,Dynamically generating a protein entity dictionary using online resources,godzik a clustering of highly homologous sequences to reduce the size of large protein databases bioinformatics,"['W Li', 'L Jaroszewski']",,"We present a fast and flexible program for clustering large protein databases at different sequence identity levels. It takes less than 2 h for the all-against-all sequence comparison and clustering of the non-redundant protein database of over 560,000 sequences on a high-end PC. The output database, including only the representative sequences, can be used for more efficient and sensitive database searches.","Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .","['PIR Resources -There are three databases in PIR: the Protein Sequence Database (PSD), iProClass, and PIR-NREF.', 'PSD database includes functionally annotated protein sequences.', 'The iProClass database is a central point for exploration of protein information, which provides summary descriptions of protein family, function and structure for all protein sequences from PIR, Swiss-Prot, and TrEMBL (now UniProt).', 'Additionally, it links to over 70 biological databases in the world.', 'The PIR-NREF database is a comprehensive database for sequence searching and protein identification.', 'It contains non-redundant protein sequences from PSD, Swiss-Prot, TrEMBL, RefSeq, GenPept, and PDB.', 'Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .', 'NCBI resources -three data sources from NCBI were used in this study: GenPept, RefSeq, and Entrez GENE.', 'GenPept entries are those translated from the GenBanknucleotide sequence database.', 'RefSeq is a comprehensive, integrated, non-redundant set of sequences, including genomic DNA, transcript (RNA), and protein products, for major research organisms.', ""Entrez GENE provides a unified query environment for genes defined by sequence and/or in NCBI's Map Viewer."", 'It records gene names, symbols, and many other attributes associated with genes and the products they encode.']",5,"['The PIR-NREF database is a comprehensive database for sequence searching and protein identification.', 'Three UniRef tables UniRef100 , UniRef90 and UniRef50 ) are available for download : UniRef100 combines identical sequences and sub-fragments into a single UniRef entry ; and UniRef90 and UniRef50 are built by clustering UniRef100 sequences into clusters based on the CD-HIT algorithm #AUTHOR_TAG such that each cluster is composed of sequences that have at least 90 % or 50 % sequence similarity , respectively , to the representative sequence .']"
CC1040,P05-3005,Dynamically generating a protein entity dictionary using online resources,kwitek a et al rat genome database rgd mapping disease onto the genome nucleic acids res,"['S Twigger', 'J Lu', 'M Shimoyama', 'D Chen', 'D Pasko', 'H Long', 'J Ginster', 'Chen CF', 'R Nigam']",,,"Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) #AUTHOR_TAG , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1041,P05-3005,Dynamically generating a protein entity dictionary using online resources,suzek be et al the protein information resource nucleic acids res,"['Wu CH', 'Yeh LS', 'H Huang', 'L Arminski', 'J Castro-Alvear', 'Y Chen', 'Z Hu', 'P Kourtesis', 'Ledley RS']",,,"The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and","['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and', 'Additionally, several model organism databases or nomenclature databases were used.', 'Correspondences among records from these databases are identified using the rich cross-reference information provided by the iProClass database of PIR [14].', 'The following provides a brief description of each of the database.']",5,"['The system utilizes several large size biological databases including three NCBI databases ( GenPept [ 11 ] , RefSeq [ 12 ] , and Entrez GENE [ 13 ] ) , PSD database from Protein Information Resources ( PIR ) #AUTHOR_TAG , and']"
CC1042,P05-3005,Dynamically generating a protein entity dictionary using online resources,the flybase database of the drosophila genome projects and community literature nucleic acids res,['F Consortium'],,"FlyBase (http://flybase.bio.indiana.edu/) provides an integrated view of the fundamental genomic and genetic data on the major genetic model Drosophila melanogaster and related species. FlyBase has primary responsibility for the continual reannotation of the D. melanogaster genome. The ultimate goal of the reannotation effort is to decorate the euchromatic sequence of the genome with as much biological information as is available from the community and from the major genome project centers. A complete revision of the annotations of the now-finished euchromatic genomic sequence has been completed. There are many points of entry to the genome within FlyBase, most notably through maps, gene products and ontologies, structured phenotypic and gene expression data, and anatomy.","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase #AUTHOR_TAG , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .']"
CC1043,P05-3005,Dynamically generating a protein entity dictionary using online resources,enzyme nomenclature functional or structural rna,['P Gegenheimer'],,"Altman and colleagues (this issue) call attention to the inability of current standardized enzyme nomenclature to distinguish between enzymatic activities that reside in nonhomologous macromolecules+ This issue is highlighted by the fact that the pre-tRNA 59-maturation activities of bacteria and plant chloroplasts present the first instance (of which I am aware) of two naturally occurring enzymes that cannot be evolutionarily related, but which catalyze an identical reaction+ (In the classic example of convergent evolution between the trypsin family and subtilisin, the enzymes do not have an identical substrate specificity+) Altman and colleagues propose that a single trivial name be used only for members of a family of homologous macromolecules; in other words, that different trivial names be given to enzymes that catalyze the same precursor-product conversion but do so with different catalytic mechanisms, or which are not members of a single family of homologous macromolecules+ I am not convinced that there is a problem needing solution+ The current proposal seems to run counter to the rationale behind current EC nomenclature, and could create more confusion than it would alleviate+ One can distinguish between a function-based nomenclature based on the biochemical reaction catalyzed--the substrate-product conversion--and a structure-based nomenclature based on the physical nature of the catalyst+ For a classical enzymologist, the reaction type being catalyzed is paramount: It is the reaction that one uses to purify the enzyme+ One identifies the enzyme based on its activity,whereas its physical structure may initially be of secondary importance+ The value of function-based nomenclature is precisely that it allows the biochemical reaction (the substrate- product conversion) to be described, specified, and studied concomitant with continuing purification and analysis of the corresponding enzyme+ Further, as more is learned about the enzyme's structure and catalytic mechanism, it is not necessary to rename it+ Indeed, the utility of function-based nomenclature is exemplified by the history of bacterial RNase P purification and characterization+","Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .","['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .']",5,"['Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) #AUTHOR_TAG .']"
CC1044,P07-1007,Estimating class priors in domain adaptation for word sense disambiguation,an empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation,"['Y K Lee', 'H T Ng']",,"In this paper, we evaluate a variety  of knowledge sources and supervised  learning algorithms for word sense  disambiguation on SENSEVAL-2 and  SENSEVAL-1 data. Our knowledge  sources include the part-of-speech of  neighboring words, single words in the  surrounding context, local collocations,  and syntactic relations. The learning algorithms  evaluated include Support Vector  Machines (SVM), Naive Bayes, AdaBoost,  and decision tree algorithms. We  present empirical results showing the relative  contribution of the component knowledge  sources and the different learning  algorithms. In particular, using all of  these knowledge sources and SVM (i.e.,  a single learning algorithm) achieves accuracy  higher than the best official scores  on both SENSEVAL-2 and SENSEVAL-1  test data",These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .,"['For our experiments, we use naive Bayes as the learning algorithm.', 'The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .']",2,"['The knowledge sources we use include parts-of-speech, local collocations, and surrounding words.', 'These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( #AUTHOR_TAG ) .']"
CC1045,P07-1068,Advanced Machine Learning Models for Coreference Resolution,the nonutility of predicateargument frequencies for pronoun interpretation,"['A Kehler', 'D Appelt', 'L Taylor', 'A Simma']",introduction,,"While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['While these approaches have been reasonably successful ( see Mitkov ( 2002 ) ) , #AUTHOR_TAG speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .']"
CC1046,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['X Luo', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'S Roukos']",introduction,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.","['Many ACE participants have also adopted a corpus-based approach to SC deter- mination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)).', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Un- like them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowl- edge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC clas- sifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and an- notated with their SCs.', 'This provides us with a train- ing set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",1,"['More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., #AUTHOR_TAG ); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']"
CC1047,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a corpusbased evaluation of centering and pronoun resolution,['J Tetreault'],introduction,,"In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches , coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process ( e.g. , Mitkov ( 1998 ) , #AUTHOR_TAG ) .', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']"
CC1048,P07-1068,Advanced Machine Learning Models for Coreference Resolution,bbn pronoun coreference and entity type corpus linguistica data consortium,"['R Weischedel', 'A Brunstein']",introduction,,"Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.","['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006)).', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report per- formance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",5,"['Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus ( #AUTHOR_TAG ) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.']"
CC1049,P07-1068,Advanced Machine Learning Models for Coreference Resolution,anaphora resolution,['R Mitkov'],introduction,"In anaphora resolution for English, animacy identification can play an integral role in the application of agreement restrictions between pronouns and candidates, and as a result, can improve the accuracy of anaphora resolution systems. In this paper, two methods for animacy identification are proposed and evaluated using intrinsic and extrinsic measures. The first method is a rule-based one which uses information about the unique beginners in WordNet to classify NPs on the basis of their animacy. The second method relies on a machine learning algorithm which exploits a WordNet enriched with animacy information for each sense. The effect of word sense disambiguation on the two methods is also assessed. The intrinsic evaluation reveals that the machine learning method reaches human levels of performance. The extrinsic evaluation demonstrates that animacy identification can be beneficial in anaphora resolution, especially in the cases where animate entities are identified with high precision","While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al. (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al. (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).']",0,"['While these approaches have been reasonably successful ( see #AUTHOR_TAG ) , Kehler et al. ( 2004 ) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance .']"
CC1050,P07-1068,Advanced Machine Learning Models for Coreference Resolution,libsvm a library for support vector machines software available at httpwwwcsientuedutw∼cjlinlibsvm,"['C-C Chang', 'C-J Lin']",introduction,,"Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.","['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.�s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']",5,"['In addition, we train an SVM classifier for SC determination by combining the output of five clas- sification methods: DL, 1-NN, ME, NB, and Soon et al.s method as described in the introduction,8 with the goal of examining whether SC classifica- tion accuracy can be improved by combining the output of individual classifiers in a supervised man- ner.', 'Specifically, we (1) use 80% of the instances generated from the BBN Entity Type Corpus to train the four classifiers; (2) apply the four classifiers and Soon et al.�s method to independently make predictions for the remaining 20% of the instances; and (3) train an SVM classifier (using the LIBSVM package ( #AUTHOR_TAG )) on these 20% of the instances, where each instance, i, is represented by a PER ORG GPE FAC LOC OTH Training Test 19.8 9.6 11.4 1.6 1.2 56.3 19.5 9.0 9.6 1.8 1.1 59.0 set of 31 binary features.', 'More specifically, let L = i li\x0e ; li2 ; li3 ; li4 ; li5 be the set of predictions that we obtained for i in step (2).', 'To represent i, we generate one feature from each non-empty subset of Li.']"
CC1051,P07-1068,Advanced Machine Learning Models for Coreference Resolution,an algorithm that learns what’s in a name,"['D M Bikel', 'R Schwartz', 'R M Weischedel']",introduction,"In this paper, we present IdentiFinderTM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder's performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.","( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ .","[""( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", 'If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type.', 'However, if NPi is determined to be a LOCATION, we create a feature with value GPE (because most of the MUC LOCA- TION NEs are ACE GPE NEs).', 'Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types).']",5,"[""( 4 ) NE : We use BBN 's IdentiFinder ( #AUTHOR_TAG ) , a MUC-style NE recognizer to determine the NE type of NPZ ."", 'If NPi is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type.', 'Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types).']"
CC1052,P07-1068,Advanced Machine Learning Models for Coreference Resolution,comparing knowledge sources for nominal anaphora resolution,"['K Markert', 'M Nissim']",introduction,"We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora  and definite noun phrase coreference. Specifically, we compare an algorithm that relies on links  encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora  by means of shallow lexico-semantic patterns. As corpora we use the British National  Corpus (BNC), as well as the Web, which has not been previously used for this task. Our  results show that (a) the knowledge encoded in WordNet is often insufficient, especially for  anaphor-antecedent relations that exploit subjective or context-dependent knowledge; (b) for  other-anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite  NP coreference, the Web-based method yields results comparable to those obtained using  WordNet over the whole dataset and outperforms the WordNet-based method on subsets of the  dataset; (d) in both case studies, the BNC-based method is worse than the other methods because  of data sparseness. Thus, in our studies, the Web-based method alleviated the lexical knowledge  gap often encountered in anaphora resolution, and handled examples with context-dependent relations  between anaphor and antecedent. Because it is inexpensive and needs no hand-modelling  of lexical knowledge, it is a promising knowledge source to integrate in anaphora resolution systems","However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .","['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]",0,"['However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , Soon et al. ( 2001 ) , #AUTHOR_TAG ) .']"
CC1053,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised models for named entity classification,"['M Collins', 'Y Singer']",introduction,"This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of la-beled examples should be required to train a classi-fier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple ""seed "" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).","We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .","['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']",5,"['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in #AUTHOR_TAG , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .']"
CC1054,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a modeltheoretic coreference scoring scheme,"['M Vilain', 'J Burger', 'J Aberdeen', 'D Connolly', 'L Hirschman']",,,"We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .","['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .', 'Following Ponzetto and Strube (2006), we consider an anaphoric reference, NP i , correctly resolved if NP i and its closest antecedent are in the same coreference chain in the resulting partition.', 'In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.']",5,"['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics : ( 1 ) the Fmeasure score as computed by the commonly-used MUC scorer ( #AUTHOR_TAG ) , and ( 2 ) the accuracy on the anaphoric references , computed as the fraction of anaphoric references correctly resolved .']"
CC1055,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.","['After training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj.']",4,"['After training, the decision tree classifier is used to select an antecedent for each NP in a test text.', 'Following #AUTHOR_TAG , we select as the antecedent of each NP, NPj, the closest preceding NP that is classified as coreferent with NPj.', 'If no such NP exists, no antecedent is selected for NPj.']"
CC1056,P07-1068,Advanced Machine Learning Models for Coreference Resolution,factorizing complex models a case study in mention detection,"['R Florian', 'H Jing', 'N Kambhatla', 'I Zitouni']",introduction,"As natural language understanding research advances towards deeper knowledge modeling, the tasks become more and more complex: we are interested in more nuanced word characteristics, more linguistic properties, deeper semantic and syntactic features. One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities---mentions---and labeling them with three types of information: entity type, entity subtype and mention type. In this article, we investigate three methods of assigning these related tags and compare them on several data sets. A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation.","Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .","['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .', 'Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location).', 'Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs.', 'This provides us with a training set that is approximately five times bigger than that of ACE.', 'More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004)); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.']",1,"['Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task ( e.g. , #AUTHOR_TAG ) .']"
CC1057,P07-1068,Advanced Machine Learning Models for Coreference Resolution,coreference resolution using competitive learning approach,"['X Yang', 'G Zhou', 'J Su', 'C L Tan']",,"In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the single-candidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model.","Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .","['Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .']",1,"['Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\\x0e, NPi+2, � � �, NPj positional features that have been employed by high- performing resolvers such as Ng and Cardie (2002) and #AUTHOR_TAG , as described below .', 'Our baseline coreference system uses the C4.5 deci- sion tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.']"
CC1058,P07-1068,Advanced Machine Learning Models for Coreference Resolution,c45 programs for machine learning,['J R Quinlan'],,,Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .,"['Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . .', '., NP j−1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']",5,"['Our baseline coreference system uses the C4 .5 decision tree learner ( #AUTHOR_TAG ) to acquire a classifier on the training texts for determining whether two NPs are coreferent .', '., NP j-1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']"
CC1059,P07-1068,Advanced Machine Learning Models for Coreference Resolution,improving machine learning approaches to coreference resolution,"['V Ng', 'C Cardie']",,"We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.","Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.","['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+\x0e, NPi+2, � � �, NPj.', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.']",1,"['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Each instance is represented by 33 lexical, grammatical, semantic, andpositional features that have been employed by high- performing resolvers such as #AUTHOR_TAG and Yang et al. (2003), as described below.']"
CC1060,P07-1068,Advanced Machine Learning Models for Coreference Resolution,exploiting semantic role labeling wordnet and wikipedia for coreference resolution,"['S P Ponzetto', 'M Strube']",,"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.","Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .","['As in SC induction, we use the ACE Phase 2 coreference corpus for evaluation purposes, acquiring the coreference classifiers on the 422 training texts and evaluating their output on the 97 test texts.', 'We report performance in terms of two metrics: (1) the Fmeasure score as computed by the commonly-used MUC scorer (Vilain et al., 1995), and (2) the accuracy on the anaphoric references, computed as the fraction of anaphoric references correctly resolved.', 'Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .', 'In all of our experiments, we use NPs automatically extracted by an in-house NP chunker and IdentiFinder.']",5,"['Following #AUTHOR_TAG , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .']"
CC1061,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised learning of contextual role knowledge for coreference resolution,"['D Bean', 'E Riloff']",introduction,"We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.","As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see #AUTHOR_TAG ) .']"
CC1062,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",introduction,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .","['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]",0,"['Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.', 'However , learning-based resolvers have not been able to benefit from having an SC agreement feature , presumably because the method used to compute the SC of an NP is too simplistic : while the SC of a proper name is computed fairly accurately using a named entity ( NE ) recognizer , many resolvers simply assign to a common noun the first ( i.e. , most frequent ) WordNet sense as its SC ( e.g. , #AUTHOR_TAG , Markert and Nissim ( 2005 ) ) .', ""It is not easy to measure the accuracy of this heuristic, but the fact that the SC agreement feature is not used by Soon et al.'s decision tree coreference classifier seems to suggest that the SC values of the NPs are not computed accurately by this first-sense heuristic.""]"
CC1063,P07-1068,Advanced Machine Learning Models for Coreference Resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'D Lim']",,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .","['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', '., NP j−1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']",5,"['Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent.', 'Following previous work ( e.g. , #AUTHOR_TAG and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .', 'Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003), as described below.']"
CC1064,P07-1068,Advanced Machine Learning Models for Coreference Resolution,unsupervised word sense disambiguation rivaling supervised methods,['D Yarowsky'],introduction,"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%","We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .","['Learning algorithms.', 'We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .', 'We apply add-one smoothing to smooth the class posteriors.']",4,"['We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( #AUTHOR_TAG ) and NE classification ( Collins and Singer , 1999 ) .']"
CC1065,P07-1068,Advanced Machine Learning Models for Coreference Resolution,exploiting semantic role labeling wordnet and wikipedia for coreference resolution,"['S P Ponzetto', 'M Strube']",introduction,"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.","As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( #AUTHOR_TAG ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']"
CC1066,P07-1068,Advanced Machine Learning Models for Coreference Resolution,using semantic relations to refine coreference decisions,"['H Ji', 'D Westbrook', 'R Grishman']",introduction,We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages - English and Chinese.,"As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .","['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al. (2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.', 'In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']",0,"['In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution -the problem of determining which NPs refer to the same real-world entity in a document.', 'In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).', 'As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , #AUTHOR_TAG ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( Ponzetto and Strube , 2006 ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .']"
CC1067,P07-1068,Advanced Machine Learning Models for Coreference Resolution,automatic retrieval and clustering of similar words,['D Lin'],introduction,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .,"['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1, while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992)).', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]",4,"['( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see #AUTHOR_TAGa ) ) .', ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"
CC1068,P07-1068,Advanced Machine Learning Models for Coreference Resolution,automatic acquisition of hyponyms from large text corpora,['M Hearst'],introduction,"We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction  Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects..","These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .","['(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation.', 'Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]",4,"['Again, this represents our attempt to coarsely model subcategorization.', ""(4) NE: We use BBN's IdentiFinder (Bikel et al., 1999)  (5) WN CLASS: For each keyword w shown in the right column of Table 1, we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value."", 'These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs .2 ( 6 ) INDUCED CLASS : Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP , we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics ( e.g. , #AUTHOR_TAG ) .', 'Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations.', 'We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type.', '(7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a)).', ""Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP."", ""To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.""]"
CC1069,P10-2059,Classification of Feedback Expressions in Multimodal Data,contextual recognition of head gestures,"['Louis-Philippe Morency', 'Candace Sidner', 'Christopher Lee', 'Trevor Darrell']",introduction,"Head pose and gesture offer several key conversational grounding cues and are used extensively in face-to-face interaction among people. We investigate how dialog context from an embodied conversational agent (ECA) can improve visual recognition of user gestures. We present a recogntion framework which (1) extracts contextual features from an ECA's dialog manager, (2) computes a predicition of head nod and head shakes, and (3) integrates the contextual predictions with the visual observation of a vision-based head gesture recognizer. We found a subset of lexical, punctuation and timing features that are easily available in most ECA architectures and can be used to learn how to predict user feedback. Using a discriminative approach to contextual prediction and multi-modal integration, we were able to improve the performancae of head gesture detection even when the topic of the test set was significantly different than the training set","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; #AUTHOR_TAG ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"
CC1070,P10-2059,Classification of Feedback Expressions in Multimodal Data,the mumin coding scheme for the annotation of feedback turn management and sequencing multimodal corpora for modelling human multimodal behaviour,"['Jens Allwood', 'Loredana Cerrato', 'Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",,,All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .,"['All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .', 'The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.', 'In this study, we do not deal with functional classification of the gestures in themselves, but rather with how gestures contribute to the semantic interpretations of linguistic expressions.', 'Therefore, only a subset of the MUMIN attributes has been used, i.e.', 'Smile, Laughter, Scowl, FaceOther for facial expressions, and Nod, Jerk, Tilt, SideTurn, Shake, Waggle, Other for head movements.']",5,"['All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( #AUTHOR_TAG ) .', 'The MU-MIN scheme is a general framework for the study of gestures in interpersonal communication.']"
CC1071,P10-2059,Classification of Feedback Expressions in Multimodal Data,turnyielding cues in taskoriented dialogue,"['Agustin Gravano', 'Julia Hirschberg']",introduction,,"Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. ( 2009 ) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while #AUTHOR_TAG examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.']"
CC1072,P10-2059,Classification of Feedback Expressions in Multimodal Data,a conversation robot using head gesture recognition as paralinguistic information,"['Shinya Fujie', 'Y Ejiri', 'K Nakajima', 'Y Matsusaka', 'Tetsunor Kobayashi']",introduction,"A conversation robot that recognizes user's head gestures and uses its results as para-linguistic information is developed. In the conversation, humans exchange linguistic information, which can be obtained by transcription of the utterance, and para-linguistic information, which helps the transmission of linguistic information. Para-linguistic information brings a nuance that cannot be transmitted by linguistic information, and the natural and effective conversation is realized. We recognize user's head gestures as the para-linguistic information in the visual channel. We use the optical flow over the head region as the feature and model them using HMM for the recognition. In actual conversation, while the user performs a gesture, the robot may perform a gesture, too. In this situation, the image sequence captured by the camera mounted on the eyes of the robot includes sways caused by the movement of the camera. To solve this problem, we introduced two artifices. One is for the feature extraction: the optical flow of the body area is used to compensate the swayed images. The other is for the probability models: mode-dependent models are prepared by the MLLR model adaptation technique, and the models are switched according to the motion mode of the robot. Experimental results show the effectiveness of these techniques.","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( #AUTHOR_TAG ; Morency et al. , 2005 ; Morency et al. , 2007 ; Morency et al. , 2009 ) .']"
CC1073,P10-2059,Classification of Feedback Expressions in Multimodal Data,linguistic functions of head movements in the context of speech,['Evelyn McClave'],introduction,,Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']",0,['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena ( see #AUTHOR_TAG for an overview ) .']
CC1074,P10-2059,Classification of Feedback Expressions in Multimodal Data,coefficient kappa some uses misuses and alternatives,"['Robert L Brennan', 'Dale J Prediger']",introduction,"This paper considers some appropriate and inappropriate uses of coefficient kappa and alternative kappa-like statistics. Discussion is restricted to the descriptive characteristics of these statistics for measuring agreement with categorical data in studies of reliability and validity. Special consideration is given to assumptions about whether marginals are fixed a priori, or free to vary. In reliability studies, when marginals are fixed, coefficient kappa is found to be appropriate. When either or both of the marginals are free to vary, however, it is suggested that the ""chance"" term in kappa be replaced by 1/n, where n is the number of categories. In validity studies, we suggest considering whether one wants an index of improvement beyond ""chance"" or beyond the best a priori strategy employing base rates. In the former case, considerations are similar to those in reliability studies with the marginals for the criterion measure considered as fixed. In the latter case, it is suggested that the largest marginal proportion for the criterion measure be used in place of the ""chance"" term in kappa. Similarities and differences among these statistics are discussed and illustrated with synthetic data.","Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 .","['In general, dialogue act, agreement and turn anno- tations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']",5,"[""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( #AUTHOR_TAG ) 2 .""]"
CC1075,P10-2059,Classification of Feedback Expressions in Multimodal Data,the hcrc map task corpus language and speech,"['Anne H Anderson', 'Miles Bader', 'Ellen Gurman Bard', 'Elizabeth Boyle', 'Gwyneth Doherty', 'Simon Garrod', 'Stephen Isard', 'Jacqueline Kowtko', 'Jan McAllister', 'Jim Miller', 'Catherine Sotillo', 'Henry S Thompson', 'Regina Weinert']",introduction,,"Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Louwerse et al. ( 2006 ) and Louwerse et al. ( 2007 ) study the relation between eye gaze , facial expression , pauses and dialogue structure in annotated English map-task dialogues ( #AUTHOR_TAG ) and find correlations between the various modalities both within and across speakers .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"
CC1076,P10-2059,Classification of Feedback Expressions in Multimodal Data,data mining practical machine learning tools and techniques,"['Ian H Witten', 'Eibe Frank']",,"As with any burgeoning technology that enjoys commercial attention, the use of data mining is surrounded by a great deal of hype. Exaggerated reports tell of secrets that can be uncovered by setting algorithms loose on oceans of data. But there is no magic in machine learning, no hidden power, no alchemy. Instead there is an identifiable body of practical techniques that can extract useful information from raw data. This book describes these techniques and shows how they work. The book is a major revision of the first edition that appeared in 1999. While the basic core remains the sam","These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .","['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']",5,"['These two sets of data were used for automatic dialogue act classification , which was run in the Weka system ( #AUTHOR_TAG ) .', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes (HNB) (Zhang et al., 2005).', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']"
CC1077,P10-2059,Classification of Feedback Expressions in Multimodal Data,combining lexical syntactic and prosodic cues for improved online dialog act tagging,"['Vivek Kumar Rangarajan Sridhar', 'Srinivas Bangaloreb', 'Shrikanth Narayanan']",introduction,,"#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', '#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['#AUTHOR_TAG obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical , syntactic and prosodic cues , while Gravano and Hirschberg ( 2009 ) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues .']"
CC1078,P10-2059,Classification of Feedback Expressions in Multimodal Data,a coefficient of agreement for nominal scales,['Jacob Cohen'],introduction,"CONSIDER Table 1. It represents in its formal characteristics a situation which arises in the clinical-social-personality areas of psychology, where it frequently occurs that the only useful level of measurement obtainable is nominal scaling (Stevens, 1951, pp. 2526), i.e. placement in a set of k unordered categories. Because the categorizing of the units is a consequence of some complex judgment process performed by a &dquo;two-legged meter&dquo; (Stevens, 1958), it becomes important to determine the extent to which these judgments are reproducible, i.e., reliable. The procedure which suggests itself is that of having two (or more) judges independently categorize a sample of units and determine the degree, significance, and","Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 .","['In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator.', 'However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement.', 'A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 ."", 'Anvil divides the annotations in slices and compares each slice.', 'We used slices of 0.04 seconds.', 'The inter-coder agreement figures obtained for the three types of annotation are given in Table 2.']",5,"['A measure was derived for each annotated feature using the agreement analysis facility provided in ANVIL.', ""Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( #AUTHOR_TAG ) 1 and corrected kappa ( Brennan and Prediger , 1981 ) 2 .""]"
CC1079,P10-2059,Classification of Feedback Expressions in Multimodal Data,head gestures for perceptual interfaces the role of context in improving recognition,"['Louis-Philippe Morency', 'Candace Sidner', 'Christopher Lee', 'Trevor Darrell']",introduction,"AbstractHead pose and gesture offer several conversational grounding cues and are used extensively in face-to-face interaction among people. To accurately recognize visual feedback, humans often use contextual knowledge from previous and current events to anticipate when feedback is most likely to occur. In this paper we describe how contextual information can be used to predict visual feedback and improve recognition of head gestures in human-computer interfaces. Lexical, prosodic, timing, and gesture features can be used to predict a user's visual feedback during conversational dialog with a robotic or virtual agent. In non-conversational interfaces, context features based on user-interface system events can improve detection of head gestures for dialog box confirmation or document browsing. Our user study with prototype gesture-based components indicate quantitative and qualitative benefits of gesture-based confirmation over conventional alternatives. Using a discriminative approach to contextual prediction and multi-modal integration, performance of head gesture detection was improved with context features even when the topic of the test set was significantly different than the training set","Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",0,"['Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; #AUTHOR_TAG ; Morency et al. , 2009 ) .', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions.', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']"
CC1080,P10-2059,Classification of Feedback Expressions in Multimodal Data,intercoder agreement for computational linguistics,"['Ron Artstein', 'Massimo Poesio']",introduction,,"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971).","['It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971)."", 'Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.']",0,"[""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see #AUTHOR_TAG , it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent (Fleiss, 1971).""]"
CC1081,P10-2059,Classification of Feedback Expressions in Multimodal Data,detecting action meetings in meetings,"['Gabriel Murray', 'Steve Renals']",introduction,,Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example, Jokinen and Ragni (2007) and Jokinen et al. (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels.', 'Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .']",0,['Related are also the studies by Rieks op den Akker and Schulz ( 2008 ) and #AUTHOR_TAG : both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus .']
CC1082,P10-2059,Classification of Feedback Expressions in Multimodal Data,clustering experiments on the communicative prop erties of gaze and gestures,"['Kristiina Jokinen', 'Anton Ragni']",introduction,,"For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .","['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus.']",0,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'For example , #AUTHOR_TAG and Jokinen et al. ( 2008 ) find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .']"
CC1083,P10-2059,Classification of Feedback Expressions in Multimodal Data,distinguishing the communicative functions of gestures,"['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",introduction,"This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task.","For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .","['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .', 'Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multi- modal corpus.']",0,"['Several authors in communication studies have pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview).', 'Others have looked at the application of machine learning algorithms to annotated multimodal corpora.', 'For example , Jokinen and Ragni ( 2007 ) and #AUTHOR_TAG find that machine learning algorithms can be trained to recognise some of the functions of head movements , while Reidsma et al. ( 2009 ) show that there is a dependence between focus of attention and assignment of dialogue act labels .']"
CC1084,P10-2059,Classification of Feedback Expressions in Multimodal Data,measuring nominal scale agreement among many raters,['Joseph L Fleiss'],introduction,,"Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) .","['It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions.', 'For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained.', ""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) ."", 'Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element.']",0,"[""Table 1    Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over are excellent ( #AUTHOR_TAG ) .""]"
CC1085,P10-2059,Classification of Feedback Expressions in Multimodal Data,distinguishing the communicative functions of gestures,"['Kristiina Jokinen', 'Costanza Navarretta', 'Patrizia Paggio']",,"This paper deals with the results of a machine learning experiment conducted on annotated gesture data from two case studies (Danish and Estonian). The data concern mainly facial displays, that are annotated with attributes relating to shape and dynamics, as well as communicative function. The results of the experiments show that the granularity of the attributes used seems appropriate for the task of distinguishing the desired communicative functions. This is a promising result in view of a future automation of the annotation task.","These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.","['The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.']",1,"['The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators.', 'The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions.', 'In the case of gestures we also measured agreement on gesture segmentation.', 'The figures obtained are given in Table 3.', 'These results are slightly worse than those obtained in previous studies using the same annotation scheme ( #AUTHOR_TAG ) , but are still sat -isfactory given the high number of categories provided by the scheme.']"
CC1086,P10-2059,Classification of Feedback Expressions in Multimodal Data,feedback in head gesture and speech to appear in,"['Patrizia Paggio', 'Costanza Navarretta']",introduction,,"The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .","['Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper.', 'Sridhar et al. (2009) obtain promising results in dialogue act tagging of the Switchboard-DAMSL corpus using lexical, syntactic and prosodic cues, while Gravano and Hirschberg (2009) examine the relation between particular acoustic and prosodic turn-yielding cues and turn taking in a large corpus of task-oriented dialogues.', 'Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and gestures.', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information.', 'We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions.', 'The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .', 'The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category.']",1,"['Louwerse et al. (2006) and Louwerse et al. (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al., 1991) and find correlations between the various modalities both within and across speakers.', 'Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al., 2004;Morency et al., 2005;Morency et al., 2007;Morency et al., 2009).', 'In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features.', 'The results , which partly confirm those obtained on a smaller dataset in #AUTHOR_TAG , must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most of the studies mentioned earlier for both head movements and face expressions .']"
CC1087,P10-2059,Classification of Feedback Expressions in Multimodal Data,hidden naive bayes,"['Harry Zhang', 'Liangxiao Jiang', 'Jiang Su']",,,The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .,"['The multimodal data we obtained by combining the linguistic annotations from DanPASS with the gesture annotation created in ANVIL, resulted into two different groups of data, one containing all Yes and No expressions, and the other the subset of those that are accompanied by a face expression or a head movement, as shown in Table 4.', 'These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (Witten and Frank, 2005).', 'We experimented with various Weka classifiers, comprising Hidden Naive Bayes, SMO, ID3, LADTree and Decision Table .', 'The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .', 'Therefore, here we show the results of this classifier.', 'Ten-folds crossvalidation was applied throughout.']",5,['The best results on most of our data were obtained using Hidden Naive Bayes ( HNB ) ( #AUTHOR_TAG ) .']
CC1088,P10-2059,Classification of Feedback Expressions in Multimodal Data,gesture generation by imitation  from human behavior to computer character animation,['Michael Kipp'],introduction,"In an e ort to extend traditional human-computer interfaces research has introduced embodied agents to utilize the modalities of everyday human-human communication, like facial expression, gestures and body postures. However, giving computer agents a human-like body introduces new challenges. Since human users are very sensitive and critical concerning bodily behavior the agents must act naturally and individually in order to be believable. This dissertation focuses on conversational gestures. It shows how to generate conversational gestures for an animated embodied agent based on annotated text input. The central idea is to imitate the gestural behavior of a human individual. Using TV show recordings as empirical data, gestural key parameters are extracted for the generation of natural and individual gestures. The gesture generation task is solved in three stages: observation, modeling and generation. For each stage, a software module was developed. For observation, the video annotation research tool ANVIL was created. It allows the eAEcient transcription of gesture, speech and other modalities on multiple layers. ANVIL is application-independent by allowing users to de ne their own annotation schemes, it provides various import/export facilities and it is extensible via its plugin interface. Therefore, the tool is suitable for a wide variety of research elds. For this work, selected clips of the TV talk show  Das Literarische Quartett"" were transcribed and analyzed, arriving at a total of 1,056 gestures. For the modeling stage, the NOVALIS module was created to compute individual gesture pro les from these transcriptions with statistical methods. A gesture pro le models the aspects handedness, timing and function of gestures for a single human individual using estimated conditional probabilities. The pro les are based on a shared lexicon of 68 gestures, assembled from the data. Finally, for generation, the NOVA generator was devised to create gestures based on gesture pro les in an overgenerate-andlter approach. Annotated text input is processed in a graph-based representation in multiple stages where semantic data is added, the location of potential gestures is determined by heuristic rules, and gestures are added and ltered based on a gesture pro le. NOVA outputs a linear, player-independent action script in XML.",Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .,"['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'This subset comprises the categories Accept, Decline, RepeatRephrase and Answer.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.', 'Finally, the two turn management categories Turn-Take and TurnElicit were also coded.']",5,"['As already mentioned, all words in DanPASS are phonetically and prosodically annotated.', 'In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are marked with onset or offset hesitation, or both.', 'For this study, we added semantic labels -including dialogue acts -and gesture annotation.', 'Both kinds of annotation were carried out using ANVIL ( #AUTHOR_TAG ) .', 'To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources.', 'Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant.']"
CC1089,P10-2059,Classification of Feedback Expressions in Multimodal Data,praat doing phonetics by computer retrieved,"['Paul Boersma', 'David Weenink']",introduction,,The Praat tool was used ( #AUTHOR_TAG ) .,"['Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Gr�nnum (2006).', 'The Praat tool was used ( #AUTHOR_TAG ) .']",5,['The Praat tool was used ( #AUTHOR_TAG ) .']
CC1090,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,linking semantic and knowledge representations in a multidomain dialogue system,"['Myroslava O Dzikovska', 'James F Allen', 'Mary D Swift']",experiments,"We describe a two-layer architecture for supporting semantic interpretation and domain reasoning in dialogue systems. Building system that supports both semantic interpretation and domain reasoning in a transparent and well-integrated manner is an unresolved problem because of the diverging requirements of the semantic representations used in contextual interpretation versus the knowledge representations used in domain reasoning. We propose an architecture that provides both portability and efficiency in natural language interpretation by maintaining separate semantic and domain knowledge representations, and integrating them via an ontology mapping procedure. The ontology mapping is used to obtain representations of utterances in a form most suitable for domain reasoners and to automatically specialize the lexicon. The use of a linguistically motivated parser for producing semantic representations for complex natural language sentences facilitates building portable semantic interpretation components as well as connections with domain reasoners. Two evaluations demonstrate the effectiveness of our approach: we show that a small number of mapping rules are sufficient for customizing the generic semantic representation to a new domain, and that our automatic lexicon specialization technique improves parser speed and accuracy.","The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .","['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer ."", 'At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008).']",2,"[""The diagnoser , based on #AUTHOR_TAGb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .""]"
CC1091,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,developing pedagogically effective tutorial dialogue tactics experiments and a testbed,"['Kurt VanLehn', 'Pamela Jordan', 'Diane Litman']",introduction,"Although effective tutorial dialogue strategies are well understood, tutorial tactics that govern brief episodes of tutoring, such as a single step, are not. Because better tactics seem to be crucial for further improving pedagogical effectiveness, we have begun investigating the effects of varying tutorial tactics. In this paper we describe two planned experiments and the testbed we have created to support this experimentation.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; #AUTHOR_TAG ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1092,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,controlling content realization with functional unification grammars in,"['Michael Elhadad', 'Jacques Robin']",experiments,"Standard Functional Unification Grammars (FUGs) provide a structurally guided top-down control regime for sentence generation. When using FUGs to perform content realization as a whole, including lexical choice, this regime is no longer appropriate for two reasons: (1) the unification of non-lexicalized semantic input with an integrated lexico-grammar requires mapping ""floating"" semantic elements which can trigger extensive backtracking and (2) lexical choice requires accessing external constraint sources on demand to preserve the modularity between conceptual and linguistic knowledge.","The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .","[""The strategy decision made by the tutorial planner, together with relevant semantic content from the student's answer (e.g., part of the answer to confirm), is passed to content planning and generation."", 'The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .', 'Templates are used to generate some stock phrases such as ""When you are ready, go on to the next slide.""']",5,"['The system uses a domain-specific content planner to produce input to the surface realizer based on the strategy decision , and a FUF/SURGE ( #AUTHOR_TAG ) generation system to produce the appropriate text .']"
CC1093,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,cohesion and learning in a tutorial spoken dialog system,"['Arthur Ward', 'Diane Litman']",conclusion,"Two measures of lexical cohesion were developed and applied to a corpus of human-computer tutoring dialogs. For both measures, the amount of cohesion in the tutoring dialog was found to be significantly correlated to learning for students with below-mean pretest scores, but not for those with above-mean pre-test scores, even though both groups had similar amounts of cohesion. We also find that only cohesion between tutor and student is significant: the cohesiveness of tutor, or of student, utterances is not. These results are discussed in light of previous work in textual cohesion and recall.",Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .,"['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010)."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']",3,['Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( #AUTHOR_TAG ) .']
CC1094,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,a natural language tutorial dialogue system for physics,"['Pamela Jordan', 'Maxim Makatchev', 'Umarani Pappuswamy', 'Kurt VanLehn', 'Patricia Albacete']",introduction,Abstract : We describe the WHY2-ATLAS intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue. We focus on the issue of analyzing and responding to multi-sentential explanations. We explore approaches for achieving a deeper understanding of these explanations and dialogue management approaches and strategies for providing appropriate feedback on them.,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( #AUTHOR_TAG ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1095,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,information state and dialogue management in the trindi dialogue move engine toolkit,"['Staffan Larsson', 'David Traum']",experiments,"We introduce an architecture and toolkit for building dialogue managers currently being developed in the TRINDI project, based on the notions of information state and dialogue move engine. The aim is to provide a framework for experimenting with implementations of different theories of information state, information state update and dialogue control. A number of dialogue managers are currently being built using the toolkit, and we present overviews of two of them. We believe that this framework will make implementation of dialogue processing theories easier, also facilitating comparison of different types of dialogue systems, thus helping to achieve a prerequisite for arriving at a best practice for the development of the dialogue management component of a spoken dialogue system.",Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .,"['Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .', 'The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts of the answer.', 'Once the complete answer has been accumulated, the system accepts it and moves on.', 'Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student.']",5,['Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( #AUTHOR_TAG ) .']
CC1096,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,contentlearning correlations in spoken tutoring dialogs at word turn and discourse levels,"['Amruta Purandare', 'Diane Litman']",introduction,"We study correlations between dialog content and learning in a corpus of human-computer tutoring dialogs. Using an online encyclopedia, we first extract domainspecific concepts discussed in our dialogs. We then extend previously studied shallow dialog metrics by incorporating content at three levels of granularity (word, turn and discourse) and also by distinguishing between students' spoken and written contributions. In all experiments, our content metrics show strong correlations with learning, and outperform the corresponding shallow baselines. Our word-level results show that although verbosity in student writings is highly associated with learning, verbosity in their spoken turns is not. On the other hand, we notice that content along with conciseness in spoken dialogs is strongly correlated with learning. At the turn-level, we find that effective tutoring dialogs have more content-rich turns, but not necessarily more or longer turns. Our discourse-level analysis computes the distribution of content across larger dialog units and shows high correlations when student contributions are rich but unevenly distributed across dialog segments. Copyright (c) 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; #AUTHOR_TAG ; Steinhauser et al. , 2007 ) .']"
CC1097,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the impact of interpretation problems on tutorial dialogue,"['Myroslava O Dzikovska', 'Johanna D Moore', 'Natalie Steinhauser', 'Gwendolyn Campbell']",conclusion,"Supporting natural language input may improve learning in intelligent tutoring systems. However, interpretation errors are unavoidable and require an effective recovery policy. We describe an evaluation of an error recovery policy in the BEETLE II tutorial dialogue system and discuss how different types of interpretation problems affect learning gain and user satisfaction. In particular, the problems arising from student use of non-standard terminology appear to have negative consequences. We argue that existing strategies for dealing with terminology problems are insufficient and that improving such strategies is important in future ITS research.","The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) .","['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']",3,"['In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment.', ""The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( #AUTHOR_TAG ) ."", 'Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006).', 'Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.']"
CC1098,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,learning to assess lowlevel conceptual understanding,"['Rodney D Nielsen', 'Wayne Ward', 'James H Martin']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; #AUTHOR_TAG ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.']"
CC1099,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,generalizing tutorial dialogue results,"['Diane Litman', 'Johanna Moore', 'Myroslava Dzikovska', 'Elaine Farrow']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; #AUTHOR_TAG ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1100,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,dealing with interpretation errors in tutorial dialogue,"['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow', 'Johanna D Moore', 'Natalie B Steinhauser', 'Gwendolyn C Campbell']",experiments,"We describe an approach to dealing with interpretation errors in a tutorial dialogue system. Allowing students to provide explanations and generate contentful talk can be helpful for learning, but the language that can be understood by a computer system is limited by the current technology. Techniques for dealing with understanding problems have been developed primarily for spoken dialogue systems in informationseeking domains, and are not always appropriate for tutorial dialogue. We present a classification of interpretation errors and our approach for dealing with them within an implemented tutorial dialogue system.","At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .","['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue.', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']",5,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( #AUTHOR_TAG ) .']"
CC1101,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,resolving pronominal reference to abstract entities,['Donna K Byron'],experiments,"This paper describes PHORA, a technique for resolving pronominal reference to either individual or abstract entities. It defines processes for evoking abstract referents from discourse and for resolving both demonstrative and personal pronouns. It successfully interprets 72% of test pronouns, compared to 37% for a leading technique without these features.","The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output .","['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output ."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",1,"[""The contextual interpreter then uses a reference resolution approach similar to #AUTHOR_TAG , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output ."", 'The interpreter also performs basic ellipsis resolution.']"
CC1102,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the beetle and beediff tutoring systems,"['Charles B Callaway', 'Myroslava Dzikovska', 'Elaine Farrow', 'Manuel Marques-Pita', 'Colin Matheson', 'Johanna D Moore']",introduction,"We describe two tutorial dialogue systems that adapt techniques from task-oriented dialogue systems to tutorial dialogue. Both systems employ the same reusable deep natural language understanding and generation components to interpret students' written utterances and to automatically generate adaptive tutorial responses, with separate domain reasoners to provide the necessary knowledge about the correctness of student answers and hinting strategies. We focus on integrating the domain-independent language processing components with domain-specific reasoning and tutorial components in order to improve the dialogue interaction, and present a preliminary analysis of BeeDiff's evaluation. Index Terms: tutoring systems, dialogue, deep processing",The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .,"['The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .', 'It uses a deep parser and generator, together with a domain reasoner and a diagnoser, to produce detailed analyses of student utterances and generate feedback automatically.', 'This allows the system to consistently apply the same tutorial policy across a range of questions.', 'To some extent, this comes at the expense of being able to address individual student misconceptions.', ""However, the system's modular setup and extensibility make it a suitable testbed for both computational linguistics algorithms and more general questions about theories of learning.""]",0,['The BEETLE II system architecture is designed to overcome these limitations ( #AUTHOR_TAG ) .']
CC1103,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,learning to assess lowlevel conceptual understanding,"['Rodney D Nielsen', 'Wayne Ward', 'James H Martin']",experiments,,"At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .","['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer."", 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .']",3,"['The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness.', ""The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer."", 'At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to #AUTHOR_TAG .']"
CC1104,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,simulated tutors in immersive learning environments empiricallyderived design principles,"['N B Steinhauser', 'L A Butler', 'G E Campbell']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; #AUTHOR_TAG ) .', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']"
CC1105,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,towards tutorial dialog to support selfexplanation adding natural language understanding to a cognitive tutor,"['V Aleven', 'O Popescu', 'K R Koedinger']",introduction,"Self-explanation is an effective metacognitive strategy, as a number of cognitive science studies have shown. In a previous study we showed that self-explanation can be supported effectively in a cognitive tutor for geometry problem solving. In that study, students explained their own problem-solving steps by selecting from a menu the name of a problem-solving principle that justifies the step. They learned with greater understanding, as compared to students who did not explain their reasoning. Currently, we are working toward testing the hypothesis that students will learn even better when they provide explanations in their own words rather than selecting them from a menu. We have implemented a prototype of a cognitive tutor that understands students' explanations and provides feedback. The tutor uses a knowledge-based approach to natural language understanding. We are entering a phase of pilot testing, both for the purpose of assessing the coverage of the natural language understanding component and for gaining insight into the kinds of dialog strategies that are needed.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; #AUTHOR_TAG ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1106,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,linking semantic and knowledge representations in a multidomain dialogue system,"['Myroslava O Dzikovska', 'James F Allen', 'Mary D Swift']",experiments,"We describe a two-layer architecture for supporting semantic interpretation and domain reasoning in dialogue systems. Building system that supports both semantic interpretation and domain reasoning in a transparent and well-integrated manner is an unresolved problem because of the diverging requirements of the semantic representations used in contextual interpretation versus the knowledge representations used in domain reasoning. We propose an architecture that provides both portability and efficiency in natural language interpretation by maintaining separate semantic and domain knowledge representations, and integrating them via an ontology mapping procedure. The ontology mapping is used to obtain representations of utterances in a form most suitable for domain reasoners and to automatically specialize the lexicon. The use of a linguistically motivated parser for producing semantic representations for complex natural language sentences facilitates building portable semantic interpretation components as well as connections with domain reasoners. Two evaluations demonstrate the effectiveness of our approach: we show that a small number of mapping rules are sufficient for customizing the generic semantic representation to a new domain, and that our automatic lexicon specialization technique improves parser speed and accuracy.","The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output .","['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output ."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",5,"['We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances.', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( #AUTHOR_TAGa ) to produce a domain-specific semantic representation of the student 's output .""]"
CC1107,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,the beetle and beediff tutoring systems,"['Charles B Callaway', 'Myroslava Dzikovska', 'Elaine Farrow', 'Manuel Marques-Pita', 'Colin Matheson', 'Johanna D Moore']",experiments,"We describe two tutorial dialogue systems that adapt techniques from task-oriented dialogue systems to tutorial dialogue. Both systems employ the same reusable deep natural language understanding and generation components to interpret students' written utterances and to automatically generate adaptive tutorial responses, with separate domain reasoners to provide the necessary knowledge about the correctness of student answers and hinting strategies. We focus on integrating the domain-independent language processing components with domain-specific reasoning and tutorial components in order to improve the dialogue interaction, and present a preliminary analysis of BeeDiff's evaluation. Index Terms: tutoring systems, dialogue, deep processing",Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .,['Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .'],3,['Other factors such as student confidence could be considered as well ( #AUTHOR_TAG ) .']
CC1108,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,autotutor a simulation of a human tutor cognitive systems research,"['A C Graesser', 'P Wiemer-Hastings', 'P WiemerHastings', 'R Kreuz']",introduction,,"Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; #AUTHOR_TAG ; Aleven et al. , 2001 ; Buckley and Wolska , 2007 ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1109,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,towards modelling and using common ground in tutorial dialogue,"['Mark Buckley', 'Magdalena Wolska']",introduction,"In order to avoid miscommunication par-ticipants in dialogue continuously attempt to align their mutual knowledge (the ""common ground""). A setting that is per-haps most prone to misalignment is tu-toring. We propose a model of common ground in tutoring dialogues which explic-itly models the truth and falsity of do-main level contributions and show how it can be used to detect and repair stu-dents ' false conjectures and facilitate stu-dent modelling.","Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .","['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .', 'However, most existing systems use pre-authored tutor responses for addressing student errors.', 'The advantage of this approach is that tutors can devise remediation dialogues that are highly tailored to specific misconceptions many students share, providing step-by-step scaffolding and potentially suggesting additional problems.', 'The disadvantage is a lack of adaptivity and generality: students often get the same remediation for the same error regardless of their past performance or dialogue context, as it is infeasible to author a different remediation dialogue for every possible dialogue state.', 'It also becomes more difficult to experiment with different tutorial policies within the system due to the inherent completixites in applying tutoring strategies consistently across a large number of individual hand-authored remediations.']",0,"['Over the last decade there has been a lot of interest in developing tutorial dialogue systems that understand student explanations ( Jordan et al. , 2006 ; Graesser et al. , 1999 ; Aleven et al. , 2001 ; #AUTHOR_TAG ; Nielsen et al. , 2008 ; VanLehn et al. , 2007 ) , because high percentages of selfexplanation and student contentful talk are known to be correlated with better learning in humanhuman tutoring ( Chi et al. , 1994 ; Litman et al. , 2009 ; Purandare and Litman , 2008 ; Steinhauser et al. , 2007 ) .']"
CC1110,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,targeted help for spoken dialogue systems intelligent feedback improves naive users’ performance,"['Beth Ann Hockey', 'Oliver Lemon', 'Ellen Campana', 'Laura Hiatt', 'Gregory Aist', 'James Hieronymus', 'Alexander Gruenstein', 'John Dowding']",experiments,"We present experimental evidence that providing naive users of a spoken dialogue system with immediate help messages related to their out-of-coverage utterances improves their success in using the system. A grammar-based recognizer and a Statistical Language Model (SLM) recognizer are run simultaneously. If the grammar-based recognizer suceeds, the less accurate SLM recognizer hypothesis is not used. When the grammar-based recognizer fails and the SLM recognizer produces a recognition hypothesis, this result is used by the Targeted Help agent to give the user feedback on what was recognized, a diagnosis of what was problematic about the utterance, and a related in-coverage example. The in-coverage example is intended to encourage alignment between user inputs and the language model of the system. We report on controlled experiments on a spoken dialogue system for command and control of a simulated robotic helicopter.",Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Since the system accepts unrestricted input, interpretation errors are unavoidable.', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'I don\'t know the word power.""', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']",2,"['The tutorial policy makes a high-level decision as to which strategy to use (for example, ""acknowledge the correct part and give a high specificity hint"") based on the answer analysis and dialogue context.', 'At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy .', 'Our recovery policy is modeled on the TargetedHelp ( #AUTHOR_TAG ) policy used in task-oriented dialogue .', 'If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, ""I\'m sorry, I\'m having a problem understanding.', 'The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.']"
CC1111,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,deep linguistic processing for spoken dialogue systems,"['James Allen', 'Myroslava Dzikovska', 'Mehdi Manshadi', 'Mary Swift']",experiments,"We describe a framework for deep linguistic processing for natural language understanding in task-oriented spoken dialogue systems. The goal is to create domaingeneral processing techniques that can be shared across all domains and dialogue tasks, combined with domain-specific optimization based on an ontology mapping from the generic LF to the application ontology. This framework has been tested in six domains that involve tasks such as interactive planning, coordination operations, tutoring, and learning.",We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .,"['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .', 'The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels.', ""The contextual interpreter then uses a reference resolution approach similar to Byron (2002), and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output."", 'Utterance content is represented as a set of extracted objects and relations between them.', 'Negation is supported, together with a heuristic scoping algorithm.', 'The interpreter also performs basic ellipsis resolution.', 'For example, it can determine that in the answer to the question ""Which bulbs will be on and which bulbs will be off in this diagram?"",', '""off"" can be taken to mean ""all bulbs in the di-agram will be off.""', 'The resulting output is then passed on to the domain reasoning and diagnosis components.']",5,['We use the TRIPS dialogue parser ( #AUTHOR_TAG ) to parse the utterances .']
CC1112,P10-4003,BEETLE II: a system for tutoring and computational linguistics experimentation,interpretation and generation in a knowledgebased tutorial system,"['Myroslava O Dzikovska', 'Charles B Callaway', 'Elaine Farrow']",experiments,,"The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .","['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .', 'At present, the knowledge base represents 14 object types and supports the curriculum containing over 200 questions and 40 different circuits.']",5,"['The system uses a knowledge base implemented in the KM representation language ( Clark and Porter , 1999 ; #AUTHOR_TAG ) to represent the state of the world .']"
CC1113,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,tree edit distance for textual entailment,"['Milen Kouleykov', 'Bernardo Magnini']",introduction,,"All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .","['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']",0,"['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and Moldovan , 2005 ) , or adopting transformation-based techniques ( #AUTHOR_TAG ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']"
CC1114,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,statistical phrasebased translation,"['Philipp Koehn', 'Franz Josef Och', 'Daniel Marcu']",,"This work summarizes a comparison between two ap-proaches to Statistical Machine Translation (SMT), namely Ngram-based and Phrase-based SMT. In both approaches, the translation process is based on bilingual units related by word-to-word alignments (pairs of source and target words), while the main differences are based on the extraction process of these units and the sta-tistical modeling of the translation context. The study has been carried out on two different translation tasks (in terms of translation difculty and amount of available training data), and allowing for distortion (reordering) in the decoding pro-cess. Thus it extends a previous work were both approaches were compared under monotone conditions. We nally report comparative results in terms of trans-lation accuracy, computation time and memory size. Re-sults show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1",They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",0,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language ( #AUTHOR_TAG ) .', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']"
CC1115,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,dirt  discovery of inference rules from text,"['Dekang Lin', 'Patrick Pantel']",introduction,"In this paper, we propose an unsupervised method for discovering inference rules from text, such as ""X is author of Y  X wrote Y"", ""X solved Y  X found a solution to Y"", and ""X caused Y  Y is triggered by X"". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.","ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are fre- quently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']",0,"['ones , DIRT ( #AUTHOR_TAG ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']"
CC1116,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a semantic approach to recognizing textual entailment,['Marta Tatu andDan Moldovan'],introduction,"Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system. 1 Recognizing Textual Entailment While communicating, humans use different expressions to convey the same meaning. Therefore, numerous NLP applications, such as, Question Answering, Information Extraction, or Summarization require computational models of language that recognize if two texts semantically overlap. Trying to capture the major inferences needed to understand equivalent semantic expressions, the PASCAL Network proposed the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2005). Given two text fragments, the task is to determine if the meaning of one text (the entailed hypothesis, H) can be inferred from the meaning of the other text (the entailing text, T). Given the wide applicability of this task, there is an increased interest in creating systems which detect the semantic entailment between two texts. The systems that participated in the Pascal RTE challenge competition exploit various inference elements which, later, they combine within statistical models, scoring methods, or machine learning frameworks. Several systems (Bos and Markert, 2005; Herrera et al., 2005; Jijkoun and de Rijke, 2005; Kouylekov and Magnini, 2005; Newman et al., 2005) measured the word overlap between the two text strings. Using either statistical or Word-Net's relations, almost all systems considered lexical relationships that indicate entailment. The degree of similarity between the syntactic parse trees of the two texts was also used as a clue for entailment by several systems (Herrera et al., 2005; Kouylekov and Magnini, 2005; de Salvo Braz et al., 2005; Raina et al., 2005). Several groups used logic provers to show the entailment between T and H (Bayer e","All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .","['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']",0,"['All current approaches to monolingual TE , either syntactically oriented ( Rus et al. , 2005 ) , or applying logical inference ( Tatu and #AUTHOR_TAG ) , or adopting transformation-based techniques ( Kouleykov and Magnini , 2005 ; Bar-Haim et al. , 2008 ) , incorporate different types of lexical knowledge to support textual inference .', 'Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.', 'WordNet, the most widely used resource in TE, provides all the three types of information.', 'Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.', 'Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.', 'Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.']"
CC1117,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,creating a bilingual entailment corpus through translations with mechanical turk 100 for a 10day rush,"['Matteo Negri', 'Yashar Mehdad']",experiments,"This paper reports on experiments in the creation of a bi-lingual Textual Entailment corpus, using non-experts' workforce under strict cost and time limitations ($100, 10 days). To this aim workers have been hired for translation and validation tasks, through the Crowd-Flower channel to Amazon Mechanical Turk. As a result, an accurate and reliable corpus of 426 English/Spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing. Focusing on two orthogonal dimensions (i.e. reliability of annotations made by non experts, and overall corpus creation costs), we summarize the methodology we adopted, the achieved results, the main problems encountered, and the lessons learned.","Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .","['The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.', 'It consists of 1600 pairs derived from the RTE3 development and test sets (800+800).', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .', ""The method relies on translation-validation cycles, defined as separate jobs routed to MTurk's workforce."", 'Translation jobs return one Spanish version for each hypothesis.', 'Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.', 'At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.', 'Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus.', 'The validation, carried out by a Spanish native speaker on 100 randomly selected pairs after two translation-validation cycles, showed the good quality of the collected material, with only 3 minor ""errors"" consisting in controversial but substantially acceptable translations reflecting regional Spanish variations.']",5,"['The dataset used for our experiments is an English-Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.', 'Translations have been generated by the CrowdFlower3 channel to Amazon Mechanical Turk4 ( MTurk ) , adopting the methodology proposed by ( #AUTHOR_TAG ) .']"
CC1118,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .","['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'For instance, the reliance of cur- rent monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal ex- pressions recognizers and normalizers) has to con- front, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the ex- isting ones, and the burden of integrating language- specific components into the same cross-lingual ar- chitecture.']",0,"['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( #AUTHOR_TAG ) as an extension of Textual Entailment ( Dagan and Glickman , 2004 ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.']"
CC1119,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :","['This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques.', 'Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system�s behaviour.', 'Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.', 'Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']",1,"['Along this direction, we start from the acquisition and use of lexical knowl- edge, which represents the basic building block of any TE system.', 'Using the basic solution proposed by ( #AUTHOR_TAG ) as a term of comparison , we experiment with different sources of multilingual lexical knowledge to address the following questions :']"
CC1120,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a systematic comparison of various statistical alignment models,"['Franz Josef Och', 'Hermann Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.","We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .","['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', 'Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",5,"['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'There are several methods to build phrase tables.', 'We run TreeTagger ( Schmid , 1994 ) for tokenization , and used the Giza + + ( #AUTHOR_TAG ) to align the tokenized corpora at the word level .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']"
CC1121,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,tracking and summarizing news on a daily basis with columbias newsblaster,"['Kathleen R McKeown', 'Regina Barzilay', 'David Evans', 'Vasileios Hatzivassiloglou', 'Judith L Klavans', 'Ani Nenkova', 'Carl Sable', 'Barry Schiffman', 'Sergey Sigelman']",,"Recently, there have been significant advances in several areas of language technology, including clustering, text categorization, and summarization. However, efforts to combine technology from these areas in a practical system for information access have been limited. In this paper, we present Columbia's Newsblaster system for online news summarization. Many of the tools developed at Columbia over the years are combined together to produce a system that crawls the web for news articles, clusters them on specific topics and produces multidocument summaries for each cluster.","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( #AUTHOR_TAG ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .']"
CC1122,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,paraphrasing with bilingual parallel corpora,"['Colin Bannard', 'Chris Callison-Burch']",introduction,"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.","Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .","['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']",0,"['Some previous works ( #AUTHOR_TAG ; Zhao et al. , 2009 ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']"
CC1123,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,making largescale support vector machine learning practical,['Thorsten Joachims'],,"Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.","To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .","['To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .']",5,"['To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( #AUTHOR_TAG ) , using each score as a feature .']"
CC1124,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,lexical selection and paraphrase in a meaning text generation model,"['Lidija Iordanskaja', 'Richard Kittredge', 'Alain Polg re']",,"We introduce a computationally tractable model for language generation based on the Meaning-Text Theory of Mel'cuk et al., in which the lexicon plays a central role. To illustrate the descriptive scope and paraphrase capabilities of the model, we show how the lexicon influences the set of choices at four different points during the multi-stage realization process: (1) semantic net simplification, (2) determination of root lexical node for the deep syntactic dependency tree, (3) possible application of deep paraphrase rules using lexical functions, and (4) surface syntactic realization. We also show some of the ways in which the theme/rheme specifications within the semantic net influence lexical and syntactic choices during realization. Examples are taken primarily from an implemented system which generates paragraph-length reports about the usage of operating systems.","They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( #AUTHOR_TAG ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"
CC1125,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,verbocean mining the web for finegrained semantic verb relations,"['Timothy Chklovski', 'Patrick Pantel']",introduction,"Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of sta- tistically learned inference rules, that is often inte- grated as a source of lexical paraphrases and entail- ment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similar- ity/relatedness scores.']",0,"['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( #AUTHOR_TAG ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.']"
CC1126,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,multiwordnet developing and aligned multilingual database,"['Emanuele Pianta', 'Luisa Bentivogli', 'Christian Girardi']",introduction,,"Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .","['Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources.', 'As regards the first issue, it�s worth noting that in the monolingual scenario simple �bag of words� (or �bag of n- grams�) approaches are per se sufficient to achieve results above baseline.', 'In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lex- ical matches between texts and hypotheses in different languages.', 'This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.', 'However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources are available only for English.', 'Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .', 'As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet�s synsets, thus making the coverage issue even more problematic than for TE.', 'As regards Wikipedia, the cross- lingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.', 'However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.', 'In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g bilingual dictionaries).']",0,"['Multilingual lexical databases aligned with the English WordNet ( e.g. MultiWordNet ( #AUTHOR_TAG ) ) have been created for several languages , with different degrees of coverage .']"
CC1127,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,inference rules and their application to recognizing textual entailment,"['Georgiana Dinu', 'Rui Wang']",,"In this paper, we explore ways of improv-ing an inference rule collection and its ap-plication to the task of recognizing textual entailment. For this purpose, we start with an automatically acquired collection and we propose methods to refine it and ob-tain more rules using a hand-crafted lex-ical resource. Following this, we derive a dependency-based structure representa-tion from texts, which aims to provide a proper base for the inference rule appli-cation. The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible im-provements.","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( #AUTHOR_TAG ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']"
CC1128,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,paraphrasing with bilingual parallel corpora,"['Colin Bannard', 'Chris Callison-Burch']",,"Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.",One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",0,"['One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus ( #AUTHOR_TAG ) .', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.']"
CC1129,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",introduction,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010;Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( Baker et al. , 1998 ) , and Wikipedia ( #AUTHOR_TAG ; Kouylekov et al. , 2009 ) .', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.']"
CC1130,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,probabilistic textual entailment generic applied modeling of language variability,"['Ido Dagan', 'Oren Glickman']",introduction,,"Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .","['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.', 'For instance, the reliance of cur- rent monolingual TE systems on lexical resources (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognizers and normalizers) has to confront, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating language- specific components into the same cross-lingual architecture.']",0,"['Cross-lingual Textual Entailment ( CLTE ) has been proposed by ( Mehdad et al. , 2010 ) as an extension of Textual Entailment ( #AUTHOR_TAG ) that consists in deciding , given two texts T and H in different languages , if the meaning of H can be inferred from the meaning of T .', 'The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.']"
CC1131,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,the berkeley framenet project,"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']",introduction,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work","These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .","['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).', 'These include, just to mention the most popular ones , DIRT ( Lin and Pantel , 2001 ) , VerbOcean ( Chklovski and Pantel , 2004 ) , FrameNet ( #AUTHOR_TAG ) , and Wikipedia ( Mehdad et al. , 2010 ; Kouylekov et al. , 2009 ) .', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.']"
CC1132,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,fluency adequacy or hter exploring different human judgments with a tunable mt metric,"['Matthew Snover', 'Nitin Madnani', 'Bonnie Dorr', 'Richard Schwartz']",,"Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, mea-sure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT met-ric: TER-Plus, which extends the Transla-tion Edit Rate evaluation metric with tun-able parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest aver-age rank in terms of Pearson and Spear-man correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating sig-nificant differences between the types of human judgments.","After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']",0,"['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction , pruning techniques ( #AUTHOR_TAG ) can be applied to increase the precision of the extracted paraphrases .']"
CC1133,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,extending the meteor machine translation evaluation metric to the phrase level,"['Michael Denkowski', 'Alon Lavie']",,"This paper presents METEOR-NEXT, an ex-tended version of the METEOR metric de-signed to have high correlation with post-editing measures of machine translation qual-ity. We describe changes made to the met-ric's sentence aligner and scoring scheme as well as a method for tuning the metric's pa-rameters to optimize correlation with human-targeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves cor-relation with HTER over baseline metrics, in-cluding earlier versions of METEOR, and ap-proaches the correlation level of a state-of-the-art metric, TER-plus (TERp).","They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .","['Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.', 'They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .', 'One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).', 'With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.', 'After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.']",4,"['They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( #AUTHOR_TAG ) , and TE ( Dinu and Wang , 2009 ) .']"
CC1134,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,extracting paraphrase patterns from bilingual parallel corpora,"['Shiqi Zhao', 'Haifeng Wang', 'Ting Liu', 'Sheng Li']",introduction,"Paraphrase patterns are semantically equivalent patterns, which are useful in both paraphrase recognition and generation. This paper presents a pivot approach for extracting paraphrase patterns from bilingual parallel corpora, whereby the paraphrase patterns in English are extracted using the patterns in another language as pivots. We make use of log-linear models for computing the paraphrase likelihood between pattern pairs and exploit feature functions based on maximum likelihood estimation (MLE), lexical weighting (LW), and monolingual word alignment (MWA). Using the presented method, we extract more than 1 million pairs of paraphrase patterns from about 2 million pairs of bilingual parallel sentences. The precision of the extracted paraphrase patterns is above 78%. Experimental results show that the presented method significantly outperforms a well-known method called discovery of inference rules from text (DIRT). Additionally, the log-linear model with the proposed feature functions are effective. The extracted paraphrase patterns are fully analyzed. Especially, we found that the extracted paraphrase patterns can be classified into five types, which are useful in multiple natural language processing (NLP) applications.","Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .","['Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.', 'As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.', 'Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']",0,"['Some previous works ( Bannard and Callison-Burch , 2005 ; #AUTHOR_TAG ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .']"
CC1135,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",experiments,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) .","[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']",1,"[""For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( #AUTHOR_TAG ) ."", 'Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.']"
CC1136,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,a syntaxbased statistical translation model,"['Kenji Yamada', 'Kevin Knight']",conclusion,,"One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .","['Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.', 'On one side, we plan to explore alternative ways to build phrase and paraphrase tables.', 'One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .', 'Another interesting direction is to investigate the potential of paraphrase patterns (i.e.', 'patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).', 'On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.', 'As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.', '1343']",3,"['One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( #AUTHOR_TAG ) .']"
CC1137,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,jlsi a tool for latent semantic indexing software available at httptccitcitresearchtextectoolsresourcesjlsihtml,['Claudio Giuliano'],experiments,,We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .,"['Wikipedia (WIKI).', 'We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .', 'Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).', 'In this way we obtained 13760 word pairs.']",5,['We performed Latent Semantic Analysis ( LSA ) over Wikipedia using the jLSI tool ( #AUTHOR_TAG ) to measure the relatedness between words in the dataset .']
CC1138,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,syntacticsemantic structures for textual entailment recognition,"['Yashar Mehdad', 'Alessandro Moschitti', 'Fabio Massimo Zanzotto']",experiments,"In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.","Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .","['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', '63.50%).', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.', ""Finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."", 'This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.']",1,"['The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.', 'This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.', 'Second , in line with the findings of ( #AUTHOR_TAG ) , the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset ( i.e. 63.50 % ) .', 'Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).', 'In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.', ""Finally, it's worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%)."", 'This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.']"
CC1139,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,moses open source toolkit for statistical machine translation,"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']",,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.","Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .","['Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.', 'They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).', 'There are several methods to build phrase tables.', 'The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.', 'In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .', 'Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.', 'In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.', 'The resulting 1 http://www.statmt.org/wmt10/', 'phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.']",5,"['In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .', 'Subsequently , we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit ( #AUTHOR_TAG ) .']"
CC1140,P11-1134,Optimizing textual entailment recognition using particle swarm optimization,recognizing textual entailment rational evaluation and approaches,"['Ido Dagan', 'Bill Dolan', 'Bernardo Magnini', 'Dan Roth']",introduction,"The goal of identifying textual entailment - whether one piece of text can be plausibly inferred  from another - has emerged in recent years as a generic core problem in natural language  understanding. Work in this area has been largely driven by the PASCAL Recognizing  Textual Entailment (RTE) challenges, which are a series of annual competitive meetings.  The current work exhibits strong ties to some earlier lines of research, particularly automatic  acquisition of paraphrases and lexical semantic relationships and unsupervised inference in  applications such as question answering, information extraction and summarization. It has  also opened the way to newer lines of research on more involved inference methods, on  knowledge representations needed to support this natural language understanding challenge  and on the use of learning methods in this context. RTE has fostered an active and growing  community of researchers focused on the problem of applied entailment. This special issue  of the JNLE provides an opportunity to showcase some of the most important work in this  emerging area","Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .","['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']",0,"['Besides WordNet , the RTE literature documents the use of a variety of lexical information sources ( Bentivogli et al. , 2010 ; #AUTHOR_TAG ) .', 'These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia Kouylekov et al., 2009).', 'DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.', 'VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.', 'It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.', 'Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.']"
CC1141,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",introduction,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect",Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .,"['Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments).']",0,"['Such questions are typically answered by designing appropriate priming experiments ( #AUTHOR_TAG ) or other lexical decision tasks .', 'The reaction time of the subjects for recognizing various lexical items under appropriate conditions reveals important facts about their organization in the brain.', '(See Sec. 2 for models of morphological organization and access and related experiments).']"
CC1142,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",the measurement of interrater agreement statistical methods for rates and proportions2212–236,"['Joseph L Fleiss', 'Bruce Levin', 'Myunghee Cho Paik']",method,,We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .,"['Organization and Processing of Compound Verbs in the Mental Lexicon Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'The verb V1 is known as pole and V2 is called as vector.', 'For example, ""ওঠে পড়া "" (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'However, not all V1+V2 combinations are CVs.', 'For example, expressions like, ""নিঠে য়াও ""(take and then go) and "" নিঠে আঠ ়া"" (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as CV.', 'The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units.', 'Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb sequences.', 'In order to do so, presently we have applied three different techniques to collect user data.', 'In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure.', 'Each linguist has received 2000 verb pairs along with their respective example sentences.', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of κ=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', 'This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'We found a significant correlation between the expert annotation and the compositionality score.', 'We observe verb sequences that are annotated as CVs (like, খেঠে খিল , ওঠে পড , কঠে খি ) have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তু ঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)).', 'This reflects that verb sequences which are not CV shows high degree of compositionality.', 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.', 'We followed the same experimental procedure as discussed in (Taft, 2004) for English polymorphemic words.', 'However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.', 'The reaction time (RT) of each subject is recorded.', 'Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.', 'This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']",5,"['We measure the inter annotator agreement using the Fleiss Kappa ( #AUTHOR_TAG ) measure ( x ) where the agreement lies around 0.79 .', 'We found an agreement of k=0.69 among the subjects.']"
CC1143,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",serial verb construction in marathiquot,['R Pandharipande'],related work,,"Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .","['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Similar findings have been proposed by #AUTHOR_TAG that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1144,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],method,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.",We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .,"['Organization and Processing of Compound Verbs in the Mental Lexicon Compound verbs, as discussed above, are special type of verb sequences consisting of two or more verbs acting as a single verb and express a single expression of meaning.', 'The verb V1 is known as pole and V2 is called as vector.', 'For example, ""ওঠে পড়া "" (getting up) is a compound verb where individual words do not entirely reflects the meaning of the whole expression.', 'However, not all V1+V2 combinations are CVs.', 'For example, expressions like, ""নিঠে য়াও ""(take and then go) and "" নিঠে আঠ ়া"" (return back) are the examples of verb sequences where meaning of the whole expression can be derived from the meaning of the individual component and thus, these verb sequences are not considered as CV.', 'The key question linguists are trying to identify for a long time and debating a lot is whether to consider CVs as a single lexical units or consider them as two separate units.', 'Since linguistic rules fails to explain the process, we for the first time tried to perform cognitive experiments to understand the organization and processing of such verb sequences in the human mind.', 'A clear understanding about these phenomena may help us to classify or extract actual CVs from other verb sequences.', 'In order to do so, presently we have applied three different techniques to collect user data.', 'In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'We asked the experts to classify the verb sequences into three classes namely, CV, not a CV and not sure.', 'Each linguist has received 2000 verb pairs along with their respective example sentences.', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We measure the inter annotator agreement using the Fleiss Kappa (Fleiss et al., 1981) measure (κ) where the agreement lies around 0.79.', 'Next, out of the common verb sequences that were annotated by all the three linguists, we randomly choose 300 V1+V2 pairs and presented them to 36 native Bangla speakers.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of κ=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', 'This reflects that it is difficult to classify Bangla verb sequences discretely into the classes of CV and not a CV.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'We found a significant correlation between the expert annotation and the compositionality score.', 'We observe verb sequences that are annotated as CVs (like, খেঠে খিল , ওঠে পড , কঠে খি ) have got low compositionality score (average score ranges between 1-4) on the other hand high compositional values are in general tagged as not a cv (নিঠে য়া (come and get), নিঠে আে (return back), তু ঠল খেঠেনি (kept), গনিঠে পিল (roll on floor)).', 'This reflects that verb sequences which are not CV shows high degree of compositionality.', 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'In order to validate such claim we perform a lexical decision experiment using native Bangla speakers with 92 different verb sequences.', 'We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .', 'However, rather than derived words, the subjects were shown a verb sequence and asked whether they recognize them as a valid combination.', 'The reaction time (RT) of each subject is recorded.', 'Our preliminarily observation from the RT analysis shows that as per our claim, RT of verb sequences having high compositionality value is significantly higher than the RTs for low or noncompositional verbs.', 'This proves our hypothesis that Bangla compound verbs that show less compositionality are stored as a hole in the mental lexicon and thus follows the full-listing model whereas compositional verb phrases are individually parsed.', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']",5,"['In the first technique, we annotated 4500 V1+V2 sequences, along with their example sentences, using a group of three linguists (the expert subjects).', 'Out of this, 1500 verb sequences are unique to each of them and rest 500 are overlapping.', 'We ask each subjects to give a compositionality score of each verb sequences under 1-10 point scale, 10 being highly compositional and 1 for noncompositional.', 'We found an agreement of k=0.69 among the subjects.', 'We also observe a continuum of compositionality score among the verb sequences.', ""We then, compare the compositionality score with that of the expert user's annotation."", 'In other words non CV verbs can directly interpret from their constituent verbs.', 'This leads us to the possibility that compositional verb sequences requires individual verbs to be recognized separately and thus the time to recognize such expressions must be greater than the non-compositional verbs which maps to a single expression of meaning.', 'We followed the same experimental procedure as discussed in ( #AUTHOR_TAG ) for English polymorphemic words .', 'However, we do believe that our experiment is composed of a very small set of data and it is premature to conclude anything concrete based only on the current experimental results.']"
CC1145,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",causal chains and compound verbsquot,['E Bashir'],related work,,"#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".","['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1146,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],related work,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.","Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .","['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']",0,"['Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; Bradley , 1980 ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; #AUTHOR_TAG ) where it has been claimed that words having low surface frequency tends to decompose .']"
CC1147,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical representation of derivational relation,['D Bradley'],related work,,"The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .","['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988).']",0,"['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon ( #AUTHOR_TAG ; Butterworth , 1983 ) .', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance, the derived morphological forms are believed to be represented as a whole, whereas the representation of the inflected forms follows the morphemic model (Caramazza et al., 1988).']"
CC1148,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",masked morphological priming in visual word recognition,"['J Grainger', 'P Cole', 'J Segui']",introduction,"Masked priming studies with adult readers have provided evidence for a form-based morpho-orthographic segmentation mechanism that ""blindly"" decomposes any word with the appearance of morphological complexity. The present studies investigated whether evidence for structural morphological decomposition can be obtained with developing readers. We used a masked primed lexical decision design first adopted by Rastle, Davis, and New (2004), comparing truly suffixed (golden-GOLD) and pseudosuffixed (mother-MOTH) prime-target pairs with nonsuffixed controls (spinach-SPIN). Experiment 1 tested adult readers, showing that priming from both pseudo- and truly suffixed primes could be obtained using our own set of high-frequency word materials. Experiment 2 assessed a group of Year 3 and Year 5 children, but priming only occurred when prime and target shared a true morphological relationship, and not when the relationship was pseudomorphological. This pattern of results indicates that morpho-orthographic decomposition mechanisms do not become automatized until a relatively late stage in reading development.21 page(s","There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; #AUTHOR_TAG ; Drews and Zwitserlood , 1995 ) .', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).']"
CC1149,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",conscious choice and some light verbs in urduquot,['M Butt'],related work,,#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', 'Hook (1981) considers the second verb V2 as an aspectual complex comparable to the auxiliaries.', '#AUTHOR_TAG argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure .', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1150,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",introduction,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect","With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .","['With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', 'Our cross-modal and masked priming experiment on Bangla derivationally suffixed words shows that morphological relatedness between lexical items triggers a significant priming effect, even when the forms are phonologically/orthographically unrelated.', 'These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes.', 'Further, based on the experimental data we have developed a series of computational models that can be used to predict the decomposition of Bangla polymorphemic words.', 'Our evaluation result shows that decomposition of a polymorphemic word depends on several factors like, frequency, productivity of the suffix and the compositionality between the stem and the suffix.']",5,"['With respect to this , we apply the different priming and other lexical decision experiments , described in literature ( #AUTHOR_TAG ; Bentin , S. and Feldman , 1990 ) specifically for derivationally suffixed polymorphemic words and compound verbs of Bangla .', 'These observations are similar to those reported for English and indicate that derivationally suffixed words in Bangla are in general accessed through decomposition of the word into its constituent morphemes.']"
CC1151,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",repetition priming and frequency attenuation in lexical access,"['K I Forster', 'C Davis']",method,,"We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .","['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'The same target word is again probed but with a different audio or visual probe called the control word.', 'The control shows no relationship with the target.', 'For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).']",5,"['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( #AUTHOR_TAG ; Rastle et al. , 2000 ; Marslen-Wilson et al. , 1994 ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .']"
CC1152,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",what can we learn from the morphology of hebrew a maskedpriming investigation of morphological representation,"['R Frost', 'K I Forster', 'A Deutsch']",introduction,"All Hebrew words are composed of 2 interwoven morphemes: a triconsonantal root and a phonological word pattern. the lexical representations of these morphemic units were examined using masked priming. When primes and targets shared an identical word pattern, neither lexical decision nor naming of targets was facilitated. In contrast root primes facilitated both lexical decisions and naming of target words that were derived from these roots. This priming effect proved to be independent of meaning similarity because no priming effects were found when primes and targets were semantically but not morphologically related. These results suggest that Hebrew roots are lexical units whereas word patterns are not. A working model of lexical organization in Hebrew is offered on the basis of these results.","There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; #AUTHOR_TAG ; Grainger , et al. , 1991 ; Drews and Zwitserlood , 1995 ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']"
CC1153,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphology and meaning in the english mental lexicon,"['W D Marslen-Wilson', 'L K Tyler', 'R Waksler', 'L Older']",method,"The authors investigated the lexical entry for morphologically complex words in English. Six experiments, using a cross-modal repetition priming task, asked whether the lexical entry for derivationally suffixed and prefixed words is morphologically structured and how this relates to the semantic and phonological transparency of the surface relationship between stem and affix. There was clear evidence for morphological decomposition of semantically transparent forms. This was independent of phonological transparency, suggesting that morphemic representations are phonologically abstract. Semantically opaque forms, in contrast, behave like monomorphemic words. Overall, suffixed and prefixed derived words and their stems prime each other through shared morphemes in the lexical entry, except for pairs of suffixed forms, which show a cohort-based interference effect","We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .","['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'Here, the prime is morphologically derived form of the target presented auditorily (for cross modal priming) or visually (for masked priming).', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.', 'The same target word is again probed but with a different audio or visual probe called the control word.', 'The control shows no relationship with the target.', 'For example, baYaska (aged) and baYasa (age) is a prime-target pair, for which the corresponding control-target pair could be naYana (eye) and baYasa (age).']",5,"['We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; #AUTHOR_TAG ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .', 'The subjects were asked to make a lexical decision whether the given target is a valid word in that language.']"
CC1154,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",wordnet an electronic lexical database,['C Fellbaum'],introduction,"Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.","Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .","['A clear understanding of the structure and the processing mechanism of the mental lexicon will further our knowledge of how the human brain processes language.', 'Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .']",0,"['Further, these linguistically important and interesting questions are also highly significant for computational linguistics (CL) and natural language processing (NLP) applications.', 'Their computational significance arises from the issue of their storage in lexical resources like WordNet ( #AUTHOR_TAG ) and raises the questions like , how to store morphologically complex words , in a lexical resource like WordNet keeping in mind the storage and access efficiency .']"
CC1155,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",morphological decomposition and the reverse base frequency effect,['M Taft'],introduction,"If recognition of a polymorphemic word always takes place via its decomposition into stem and affix, then the higher the frequency of its stem (i.e., base frequency) the easier the lexical decision response should be when frequency of the word itself (i.e., surface frequency) is controlled. Past experiments have demonstrated such a base frequency effect, but not under all circumstances. Thus, a dual pathway notion has become dominant as an account of morphological processing whereby both decomposition and whole-word access is possible. Two experiments are reported here that demonstrate how an obligatory decomposition account can handle the absence of base frequency effects. In particular, it is shown that the later stage of recombining the stem and affix is harder for high base frequency words than for lower base frequency words when matched on surface frequency, and that this can counterbalance the advantage of easier access to the higher frequency stem. When the combination stage is crucial for discriminating the word items from the nonword items, a reverse base frequency effect emerges, revealing the disadvantage at this stage for high base frequency words. Such an effect is hard for the dual-pathway account to explain, but follows naturally from the idea of obligatory decomposition.","On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English, Hebrew, Italian, French, Dutch, and few other languages (Marslen-Wilson et al., 2008;Frost et al., 1997;Grainger, et al., 1991;Drews and Zwitserlood, 1995).', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['On the other hand , experiments indicate that mental representation and processing of morphologically complex words are not quite language independent ( #AUTHOR_TAG ) .']"
CC1156,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical access and inflectional morphology,"['A Caramazza', 'A Laudanna', 'C Romani']",related work,,"For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .","['Over the last few decades many studies have attempted to understand the representation and processing of morphologically complex words in the brain for various languages.', 'Most of the studies are designed to support one of the two mutually exclusive paradigms: the full-listing and the morphemic model.', 'The full-listing model claims that polymorphic words are represented as a whole in the human mental lexicon (Bradley, 1980;Butterworth, 1983).', 'On the other hand, morphemic model argues that morphologically complex words are decomposed and represented in terms of the smaller morphemic units.', 'The affixes are stripped away from the root form, which in turn are used to access the mental lexicon (Taft and Forster, 1975;Taft, 1981;MacKay, 1978).', 'Intermediate to these two paradigms is the partial decomposition model that argues that different types of morphological forms are processed separately.', 'For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .']",0,"['For instance , the derived morphological forms are believed to be represented as a whole , whereas the representation of the inflected forms follows the morphemic model ( #AUTHOR_TAG ) .']"
CC1157,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",hindi structures intermediate levelquot michigan papers on south and,['P E Hook'],related work,,#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']",0,"['A plethora of works has been done to provide linguistic explanations on the formation of such word, yet none so far has led to any consensus.', '#AUTHOR_TAG considers the second verb V2 as an aspectual complex comparable to the auxiliaries .', 'Butt (1993) argues CV formations in Hindi and Urdu are either morphological or syntactical and their formation take place at the argument structure.', 'Bashir (1993) tried to construct a semantic analysis based on ""prepared"" and ""unprepared mind"".', 'Similar findings have been proposed by Pandharipande (1993) that points out V1 and V2 are paired on the basis of their semantic compa-tibility, which is subject to syntactic constraints.', 'Paul (2004) tried to represent Bangla CVs in terms of HPSG formalism.', 'She proposes that the selection of a V2 by a V1 is determined at the semantic level because the two verbs will unify if and only if they are semantically compatible.', 'Since none of the linguistic formalism could satisfactorily explain the unique phenomena of CV formation, we here for the first time drew our attention towards psycholinguistic and neurolinguistic studies to model the processing of verb-verb combinations in the ML and compare these responses with that of the existing models.']"
CC1158,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",lexical representation of derivational relation,['D Bradley'],related work,,"Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .","['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.', 'If it is below that same threshold of frequency, the parsing route will win, and the word will be accessed via its parts.']",0,"['It has been argued that frequency of a word influences the speed of lexical processing and thus, can serve as a diagnostic tool to observe the nature and organization of lexical representations.', '(Taft, 1975) with his experiment on English inflected words, argued that lexical decision responses of polymorphemic words depends upon the base word frequency.', 'Similar observation for surface word frequency was also observed by ( Bertram et al. , 2000 ; #AUTHOR_TAG ; Burani et al. , 1987 ; Burani et al. , 1984 ; Schreuder et al. , 1997 ; Taft 1975 ; Taft , 2004 ) where it has been claimed that words having low surface frequency tends to decompose .', 'Later, Baayen(2000) proposed the dual processing race model that proposes that a specific morphologically complex form is accessed via its parts if the frequency of that word is above a certain threshold of frequency, then the direct route will win, and the word will be accessed as a whole.']"
CC1159,P13-3018,"DNA, Words and Models, Statistics of Exceptional Words",and orthographic similarity in visual word recognition,"['E Drews', 'P Zwitserlood']",introduction,,"There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .","['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'However, we do not know of any such investigations for Indian languages, which are morphologically richer than many of their Indo-European cousins.', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).', 'Therefore, the findings from experiments in one language cannot be generalized to all languages making it important to conduct similar experimentations in other languages.']",0,"['There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; #AUTHOR_TAG ) .', 'Moreover, Indian languages show some distinct phenomena like, compound and composite verbs for which no such investigations have been conducted yet.', 'On the other hand, experiments indicate that mental representation and processing of morphologically complex words are not quite language independent (Taft, 2004).']"
CC1160,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],method,,"Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .","['2We could just as easily use other symmetric ""association"" measures, such as ¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",0,"['2We could just as easily use other symmetric ""association"" measures, such as C/2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( #AUTHOR_TAGc ) .', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"
CC1161,P97-1063,A word-to-word model of translational equivalence,a statistical approach to machine translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; #AUTHOR_TAG ; Brown et al. , 1993a ) .']"
CC1162,P97-1063,A word-to-word model of translational equivalence,robust word alignment for machine aided translationquot,"['I Dagan', 'K Church', 'SZ W Gale']",method,,Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) .,"['2We could just as easily use other symmetric ""association"" measures, such as ¢2  or the Dice coefficient (Smadja, 1992).', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) ."", 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",0,"['Now, suppose that uk and Uk+z often co-occur within their language.', ""Models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' ( #AUTHOR_TAG ) ."", ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest.""]"
CC1163,P97-1063,A word-to-word model of translational equivalence,using bitextual alignment for translation validation the transcheck systemquot,['E Macklovitch'],introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. ( #AUTHOR_TAG ; Melamed , 1996b ) ), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church, 1991), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1164,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",,,"This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .","['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']",0,"['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( #AUTHOR_TAG ; Wu & Xia , 1994 ; Chen , 1996 ) .']"
CC1165,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993; #AUTHOR_TAG ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1166,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],introduction,,"Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade, researchers at IBM have devel- oped a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computation- ally expensive to apply.', 'Table look-up using an ex- plicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g. (Macklovitch, 1994; #AUTHOR_TAGb )), concordancing for bilingual lexicography (Catizone et al., 1993; Gale & Church , 1991 ), computer- assisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1167,P97-1063,A word-to-word model of translational equivalence,how to compile a bilingual collocational lexicon automaticallyquot,['F Smadja'],method,,"2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .","['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",1,"['2We could just as easily use other symmetric ""association"" measures, such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( #AUTHOR_TAG ) .', 'Now, suppose that uk and Uk+z often co-occur within their language.', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']"
CC1168,P97-1063,A word-to-word model of translational equivalence,a statistical approach to language translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( #AUTHOR_TAG ; Brown et al. , 1990 ; Brown et al. , 1993a ) .']"
CC1169,P97-1063,A word-to-word model of translational equivalence,a geometric approach to mapping bitext correspondencequot,['I D Melamed'],introduction,,"The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']",0,"['A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other ( Gale & Church , 1991 ; #AUTHOR_TAGa ) .', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']"
CC1170,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",,,"We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .","['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']",5,"['We induced a two-class word-to-word model of translational equivalence from 13 million words of the Canadian Hansards , aligned using the method in ( #AUTHOR_TAG ) .']"
CC1171,P97-1063,A word-to-word model of translational equivalence,robust word alignment for machine aided translationquot,"['I Dagan', 'K Church', 'SZ W Gale']",method,,"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",0,"['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; #AUTHOR_TAG ; Chen , 1996 ) .']"
CC1172,P97-1063,A word-to-word model of translational equivalence,semiautomatic acquisition of domainspecific translation lexiconsquot,['personal communication Nasr'],,,#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .,"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al.'s Model 2 on 74 million words of the Canadian Hansards."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']",0,"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', '#AUTHOR_TAG reported that the translation lexicon that our model induced from this tiny bitext accounted for 30 % of the word types with precision between 84 % and 90 % .', 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.']"
CC1173,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",introduction,,"Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).","['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other Melamed, 1996a).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993;Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995;.']",0,"['Co-occurrence With the exception of (Fung, 1998b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts  ( #AUTHOR_TAG ; Kumano & Hirakawa, 1994;Fung, 1998a;Melamed, 1995).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.']"
CC1174,P97-1063,A word-to-word model of translational equivalence,deriving translation data from bilingual textsquot,"['R Catizone', 'G Russell', 'S Warwick']",introduction,,"Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).","['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade, researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993a).', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications , including ""crummy"" MT on the World Wide Web ( Church & Hovy , 1993 ) , certain machine-assisted translation tools ( e.g. ( Macklovitch , 1994 ; Melamed , 1996b ) ) , concordancing for bilingual lexicography ( #AUTHOR_TAG ; Gale & Church , 1991 ) , computerassisted language learning , corpus linguistics ( Melby . 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']"
CC1175,P97-1063,A word-to-word model of translational equivalence,how to compile a bilingual collocational lexicon automaticallyquot,['F Smadja'],conclusion,,Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .,"['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy .', ""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) ."", 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']",3,"[""Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( #AUTHOR_TAG ) .""]"
CC1176,P97-1063,A word-to-word model of translational equivalence,accurate methods for the statistics of surprise and coincidencequot,['T Dunning'],method,,"For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .","['Our translation model consists of the hidden parameters A+ and A-, and likelihood ratios L(u, v).', 'The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'L(u,v) represents the likelihood that u and v can be mutual translations.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .', ""When the L(u, v) are re-estimated, the model's hidden parameters come into play.""]",5,"['The two hidden parameters are the probabilities of the model generating true and false positives in the data.', 'For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( â\x80\x9e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( #AUTHOR_TAG ) 2 .']"
CC1177,P97-1063,A word-to-word model of translational equivalence,automatic evaluation and uniform filter cascades for inducing nbest translation lexiconsquot,['I D Melamed'],introduction,"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora.","With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .","['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .', 'A bitext comprises a pair of texts in two languages, where each text is a translation of the other.', 'Word co-occurrence can be defined in various ways.', 'The most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments Si and Ti are translations of each other (Gale & Church, 1991; Melamed, 1996a).', 'Then, two word tokens (u, v) are said to co-occur in the aligned segment pair i if u E Si and v E Ti.', 'The co-occurrence relation can also be based on distance in a bitext space, which is a more general representations of bitext correspondence (Dagan et al., 1993; Resnik & Melamed, 1997), or it can be restricted to words pairs that satisfy some matching predicate, which can be extrinsic to the model (Melamed, 1995; Melamed, 1997).']",0,"['With the exception of ( Fung , 1995b ) , previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts ( Gale & Church , 1991 ; Kumano & Hirakawa , 1994 ; Fung , 1995a ; #AUTHOR_TAG ) .']"
CC1178,P97-1063,A word-to-word model of translational equivalence,building probabilistic models for natural language,['S Chen'],method,"Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science","It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",1,"['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( Brown et al. , 1990 ; Dagan et al. , 1993 ; #AUTHOR_TAG ) .']"
CC1179,P97-1063,A word-to-word model of translational equivalence,maximum likelihood from incomplete data via the em algorithmquot,"['A P Dempster', 'N M Laird', 'D B Rubin']",,,"By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .","[""One advantage that Brown et al.'s Model i has over our word-to-word model is that their objective function has no local maxima."", 'By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .', 'In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel ) in a non-monotonic fashion.', 'We have adopted the simple heuristic that the model ""has converged"" when this probability stops increasing.']",0,"['By using the EM algorithm ( #AUTHOR_TAG ) , they can guarantee convergence towards the globally optimum parameter set .']"
CC1180,P97-1063,A word-to-word model of translational equivalence,measuring semantic entropyquot,['I D Melamed'],conclusion,,"Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .","['Even better accuracy can be achieved with a more fine-grained link class structure.', 'Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).', 'If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.', 'In this manner, the model can account for a wider range of translation phenomena.']",3,"['Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( #AUTHOR_TAG ) .', 'Another interesting extension is to broaden the definition of a ""word"" to include multi-word lexical units (Smadja, 1992).']"
CC1181,P97-1063,A word-to-word model of translational equivalence,measuring semantic entropyquot,['I D Melamed'],method,,"For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .","['In the basic word-to-word model, the hidden parameters A + and A-depend only on the distributions of link frequencies generated by the competitive linking algorithm.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']",0,"['More accurate models can be induced by taking into account various features of the linked tokens.', 'For example , frequent words are translated less consistently than rare words ( #AUTHOR_TAG ) .']"
CC1182,P97-1063,A word-to-word model of translational equivalence,a statistical approach to machine translationquot,"['P F Brown', 'J Cocke', 'S Della Pietra', 'V Della Pietra', 'F Jelinek', 'R Mercer', 'P Roossin']",method,,"It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .","['1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1.', 'This step significantly reduces the computational burden of the algorithm.', 'It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .', 'To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.']",1,"['It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( #AUTHOR_TAG ; Dagan et al. , 1993 ; Chen , 1996 ) .']"
CC1183,P97-1063,A word-to-word model of translational equivalence,a program for aligning sentences in bilingual corporaquot,"['W Gale', 'K W Church']",method,,"2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) .","[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'co-occur is called a direct association.', 'Now, suppose that uk and Uk+z often co-occur within their language.', 'Then vk and uk+l will also co-occur more often than expected by chance.', 'The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk.', 'Models of translational equivalence that are ignorant of indirect associations have ""a tendency ... to be confused by collocates"" (Dagan et al., 1993).', 'Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c).', ""The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest."", 'The competitive linking algorithm implements this heuristic:']",1,"[""2We could just as easily use other symmetric `` association '' measures , such as 02 ( #AUTHOR_TAG ) or the Dice coefficient ( Smadja , 1992 ) ."", 'Now, suppose that uk and Uk+z often co-occur within their language.']"
CC1184,P97-1063,A word-to-word model of translational equivalence,the mathematics of statistical machine translation parameter estimationquot,"['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']",introduction,,"Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .","['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .', 'However, the IBM models, which attempt to capture a broad range of translation phenomena, are computationally expensive to apply.', 'Table look-up using an explicit translation lexicon is sufficient and preferable for many multilingual NLP applications, including ""crummy"" MT on the World Wide Web (Church & I-Iovy, 1993), certain machine-assisted translation tools (e.g.', '(Macklovitch, 1994;Melamed, 1996b)), concordancing for bilingual lexicography (Catizone et al., 1993;, computerassisted language learning, corpus linguistics (Melby. 1981), and cross-lingual information retrieval (Oard &Dorr, 1996).']",0,"['Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; #AUTHOR_TAGa ) .']"
CC1185,P97-1063,A word-to-word model of translational equivalence,building probabilistic models for natural language,['S Chen'],,"Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation. In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.Engineering and Applied Science","This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .","['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']",0,"['Though some have tried, it is not clear how to extract such accurate lexicons from other published translation models.', 'Part of the difficulty stems from the implicit assumption in other models that each word has only one sense.', 'Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.', 'The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.', 'This imbalance foils thresholding strategies , clever as they might be ( Gale & Church , 1991 ; Wu & Xia , 1994 ; #AUTHOR_TAG ) .', 'The likelihoods in the word-to-word model remain unnormalized, so they do not compete.']"
CC1186,P97-1063,A word-to-word model of translational equivalence,the mathematics of statistical machine translation parameter estimationquot,"['P F Brown', 'V J Della Pietra', 'S A Della Pietra', 'R L Mercer']",method,,This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .,"['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .', 'for their models (Brown et al., 1993b).', 'When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.']",1,"['To account for this difference, we can estimate separate values of X + and A-for different ranges of n(u,v).', 'Similarly, the hidden parameters can be conditioned on the linked parts of speech.', 'Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.', 'Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.', 'This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( #AUTHOR_TAGb ) .']"
CC1187,P97-1063,A word-to-word model of translational equivalence,line em up advances in alignment technology and their impact on translation support toolsquot,"['E Macklovitch', 'M-L Hannan']",,"We present a quantitative evaluation of one well-known word-alignment algorithm, as well as an analysis of frequent errors in terms of this model's underlying assumptions. Despite error rates that range from 22% to 32%, we argue that this technology can be put to good use in certain automated aids for human translators. We support our contention by pointing to several successful applications and outline ways in which text alignments below the sentence level would allow us to improve the performance of other translation support tools.","The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards .","['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', 'Recall drops when there is tess training data, because the model refuses to make predictions that it cannot make with confidence.', 'For many applications, this is the desired behavior.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.', 'We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.', 'The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.', 'Therefore, we ignored English words that were linked to nothing.']",1,"['The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.', 'Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.', ""The most detailed evaluation of link tokens to date was performed by ( #AUTHOR_TAG ) , who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards ."", 'These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.']"
CC1188,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a wordclass approach to labeling pscfg rules for machine translation,"['Andreas Zollmann', 'Stephan Vogel']",experiments,"In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.","11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .","['For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.', 'The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.', 'For the hyperparameters, we set Į to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .', 'Thus, to be convenient, we only conduct experiments with the SAMT system.']",4,"['For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.', 'For the hyperparameters, we set I to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.', 'We built an s2t translation system with the achieved U-trees after the 1000th iteration.', 'We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large. 11', '11 From ( #AUTHOR_TAG ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .']"
CC1189,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .","['Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules 7 in terms of frontier nodes (Galley et al., 2004).', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']",4,"['Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently , #AUTHOR_TAG designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.']"
CC1190,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,hierarchical phrasebased translation,['David Chiang'],related work,"Left-to-right (LR) decoding (Watanabe et al., 2006) is promising decoding algorithm for hierarchical phrase-based translation (Hiero) that visits input spans in arbitrary order producing the output translation in left to right order. This leads to far fewer language model calls, but while LR decod-ing is more efficient than CKY decoding, it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower transla-tion quality as a result. This paper in-troduces two improvements to LR decod-ing that make it comparable in translation quality to CKY-based Hiero.",The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system ( #AUTHOR_TAG ) .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.']"
CC1191,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,bayesian synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Miles Osborne']",related work,"We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this field. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over maximum likelihood models.","#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .","['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', '#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['#AUTHOR_TAG , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .']"
CC1192,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,spmt statistical machine translation with syntactified target language phrases,"['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']",experiments,"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.","The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']",5,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( Galley et al. , 2006 ) and ( #AUTHOR_TAG ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']"
CC1193,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,treebased translation without using parse trees,"['Feifei Zhai', 'Jiajun Zhang', 'Yu Zhou', 'Chengqing Zong']",related work,"Parse trees are indispensable to the existing tree- based translation models. However, there exist two major challenges in utilizing parse trees: 1) Fo r most language pairs, it is hard to get parse trees due to the lack of syntactic resources for training. 2) Numero us parse trees are not compatible with word alignment which is generally learned by GIZA++. Therefore, a number of useful translation rules are often excluded. To overcome these two problems, in this paper we make a great effort to bypass the parse trees and induce effective unsupervised trees for treebased translation models. Our unsupervised trees depend only on the word alignment without utilizing any syntactic resource or linguistic pars er. Hence, they are very beneficial for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms th e stringto-tree system using parse trees.",Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['Our previous work ( #AUTHOR_TAG ) designed an EMbased method to construct unsupervised trees for tree-based translation models .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.']"
CC1194,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,binarizing syntax trees to improve syntaxbased machine translation accuracy,"['Wei Wang', 'Kevin Knight', 'Daniel Marcu']",experiments,We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.,"Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .","['To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).', 'Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']",5,"['Then , we binarize the English parse trees using the head binarization approach ( #AUTHOR_TAG ) and use the resulting binary parse trees to build another s2t system .']"
CC1195,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,spmt statistical machine translation with syntactified target language phrases,"['Daniel Marcu', 'Wei Wang', 'Abdessamad Echihabi', 'Kevin Knight']",introduction,"We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; #AUTHOR_TAG ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1196,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a new stringtodependency machine translation algorithm with a target dependency language model,"['Libin Shen', 'Jinxi Xu', 'Ralph Weischedel']",introduction,"In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; #AUTHOR_TAG ; Zhang et al. , 2011b ) .']"
CC1197,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",related work,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.",#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', '#AUTHOR_TAG adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1198,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :","['The base distribution 0 ( | ) P r N is designed to assign prior probabilities to the STSG production rules.', 'Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']",5,"['Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow #AUTHOR_TAG and decompose the prior probability P0 ( r | N ) into two factors as follows :']"
CC1199,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a wordclass approach to labeling pscfg rules for machine translation,"['Andreas Zollmann', 'Stephan Vogel']",related work,"In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features.    Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chinese-to-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.",#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', '#AUTHOR_TAG further labeled the SCFG rules with POS tags and unsupervised word classes .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1200,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .","['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .', 'Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.', 'For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.', 'In addition, it should be noted that the word alignment is fixed 8 , and we only explore the entire space of tree structures in our sampler.', 'Differently,  designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment.', 'We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.', 'This subject will be one of our future work topics.']",5,"['Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( #AUTHOR_TAG ) .']"
CC1201,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,retraining monolingual parser bilingually for syntactic smt,"['Shujie Liu', 'Chi-Ho Li', 'Mu Li', 'Ming Zhou']",related work,"The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks.",#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', '#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,['#AUTHOR_TAG re-trained the linguistic parsers bilingually based on word alignment .']
CC1202,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,statistical significance tests for machine translation evaluation,['Philipp Koehn'],experiments,"If two translation systems differ differ in perfor-mance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling meth-ods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.",The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .,"['The experiments are conducted on Chinese-to-English translation.', 'The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.', 'We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.', 'We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.', 'For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.', 'We use MERT (Och, 2004) to tune parameters.', 'Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']",5,"['The experiments are conducted on Chinese-to-English translation.', 'The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.', 'We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.', 'We train a 5gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.', 'For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.', 'We use MERT (Och, 2004) to tune parameters.', 'Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.', 'The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.', 'The statistical significance test is performed by the re-sampling approach ( #AUTHOR_TAG ) .']"
CC1203,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,scalable inference and training of contextrich syntactic translation models,"['Michel Galley', 'Jonathan Graehl', 'Kevin Knight', 'Daniel Marcu', 'Steve DeNeefe', 'Wei Wang', 'Ignacio Thayer']",experiments,"Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.",The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']",5,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on ( #AUTHOR_TAG ) and ( Marcu et al. 2006 ) .', 'In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.']"
CC1204,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model for learning scfgs with discontiguous rules,"['Abby Levenberg', 'Chris Dyer', 'Phil Blunsom']",related work,"We describe a nonparametric model and corresponding inference algorithm for learning Synchronous Context Free Grammar derivations for parallel text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences--- including discontiguous, many-to-many alignments---and produces competitive translation results. Further, inference is efficient and we present results on significantly larger corpora than prior work.",#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', '#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,['#AUTHOR_TAG employed a Bayesian method to learn discontinuous SCFG rules .']
CC1205,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,two languages are better than one for syntactic parsing,"['David Burkett', 'Dan Klein']",related work,"We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F1 at predicting English side trees and 1.8 F1 at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.",#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', '#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['#AUTHOR_TAG and Burkett et al. ( 2010 ) focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.']"
CC1206,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.",9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .,"['The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM).', 'Actually, the frequent AEs also greatly impair the conventional TM.', 'Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.', 'Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.', 'Our final experiments verify this point and we will conduct a much detailed analysis in future.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']",5,"['The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM).', 'Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.', '9 We only use the minimal GHKM rules ( #AUTHOR_TAG ) here to reduce the complexity of the sampler .']"
CC1207,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",experiments,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .","['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'The system is implemented based on (Galley et al., 2006) and ).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']",5,"['The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).', 'In the system , we extract both the minimal GHKM rules ( #AUTHOR_TAG ) , and the rules of SPMT Model 1 ( Galley et al. , 2006 ) with phrases up to length L = 5 on the source side .']"
CC1208,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",method,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .","['Our first Gibbs operator, Rotate, just works by sampling value of the Ȍparameters, one at a time, and changing the U-tree accordingly.', 'For example, in Figure 3(a), the s-node is currently in the left VWDWHȌ :HVDPSOHWKHȌRIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRIȌLVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state.', 'Otherwise, we change its state to the right state Ȍ , and transform the U-tree to Figure 3 Obviously, towards an s-node for sampling, the two values of Ȍ would define two different U-trees.', 'Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.', ""In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node."", 'For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (Ȍ ) defines only one rule: Using these STSG rules, the two derivations are evaluated as follows (We use the value of Ȍ to denote the corresponding STSG derivation):']",5,"['Using the GHKM algorithm ( #AUTHOR_TAG ) , we can get two different STSG derivations from the two U-trees based on the fixed word alignment .', 'Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.']"
CC1209,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,what’s in a translation rule,"['Michel Galley', 'Mark Hopkins', 'Kevin Knight', 'Daniel Marcu']",introduction,"Abstract : We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; Quirk et al. , 2005 ; #AUTHOR_TAG , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1210,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,dependency treelet translation syntactically informed phrasal smt,"['Chris Quirk', 'Arul Menezes', 'Colin Cherry']",introduction,"We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. 1","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; #AUTHOR_TAG ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1211,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,syntax augmented machine translation via chart parsing,"['Andreas Zollmann', 'Ashish Venugopal']",experiments,"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License.","To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .","['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']",5,"['To create the baseline system , we use the opensource Joshua 4.0 system ( Ganitkevitch et al. , 2012 ) to build a hierarchical phrase-based ( HPB ) system , and a syntax-augmented MT ( SAMT ) 11 system ( #AUTHOR_TAG ) respectively .']"
CC1212,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,learning accurate compact and interpretable tree annotation,"['Slav Petrov', 'Leon Barrett', 'Romain Thibaux', 'Dan Klein']",experiments,"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple Xbar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2 % on the Penn Treebank, higher than fully lexicalized systems.","To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .","['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']",5,"['To build the above s2t system , we first use the parse tree , which is generated by parsing the English side of the bilingual data with the Berkeley parser ( #AUTHOR_TAG ) .', 'Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.']"
CC1213,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,binarizing syntax trees to improve syntaxbased machine translation accuracy,"['Wei Wang', 'Kevin Knight', 'Daniel Marcu']",method,We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.,"This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .","['is the probability of producing the target tree fragment frag.', 'To generate frag,  used a geometric prior to decide how many child nodes to assign each node.', 'Differently, we require that each multi-word non-terminal node must have two child nodes.', 'This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']",4,"['This is because the binary structure has been verified to be very effective for tree-based translation ( #AUTHOR_TAG ; Zhang et al. , 2011a ) .']"
CC1214,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,treetostring alignment template for statistical machine translation,"['Yang Liu', 'Qun Liu', 'Shouxun Lin']",introduction,"We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.","Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .","['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']",0,"['In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).', 'Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( #AUTHOR_TAG , 2009 ; Quirk et al. , 2005 ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .']"
CC1215,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,restructuring relabeling and realigning for syntaxbased machine translation,"['Wei Wang', 'Jonathan May', 'Kevin Knight', 'Daniel Marcu']",introduction,,This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .,"['However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.', '2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.', 'This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']",0,['This indicates that parse trees are usually not the optimal choice for training tree-based translation models ( #AUTHOR_TAG ) .']
CC1216,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,joint parsing and alignment with weakly synchronized grammars,"['David Burkett', 'John Blitzer', 'Dan Klein']",related work,"Syntactic machine translation systems extract rules from bilingual, word-aligned, syntacti-cally parsed text, but current systems for pars-ing and word alignment are at best cascaded and at worst totally independent of one an-other. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we de-velop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 F1 for English pars-ing, 2.1 F1 for Chinese parsing, and 5.5 F1 for word alignment over each task's indepen-dent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chi-nese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.",Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,['Burkett and Klein ( 2008 ) and #AUTHOR_TAG focused on joint parsing and alignment .']
CC1217,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,transforming trees to improve syntactic convergence,"['David Burkett', 'Dan Klein']",related work,"We describe a transformation-based learning method for learning a sequence of mono-lingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Transla-tion Treebank, we show how our method au-tomatically discovers transformations that ac-commodate differences in English and Chi-nese syntax. Furthermore, when transforma-tions are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU im-provement over baseline trees.",#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', '#AUTHOR_TAG utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation .', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', 'Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1218,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a bayesian model of syntaxdirected tree to string grammar induction,"['Trevor Cohn', 'Phil Blunsom']",method,"Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.","Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']",4,"['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( Blunsom et al. , 2009 ) and ( #AUTHOR_TAG ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']"
CC1219,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a gibbs sampler for phrasal synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']",method,"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.","Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .","['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']",4,"['P(str | frag) in Equation (4) is the probability of generating the source string, which contains several source words and variables.', 'Inspired by ( #AUTHOR_TAG ) and ( Cohn and Blunsom , 2009 ) , we define P ( str | frag ) as follows : where csw is the number of words in the source string .']"
CC1220,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,a gibbs sampler for phrasal synchronous grammar induction,"['Phil Blunsom', 'Trevor Cohn', 'Chris Dyer', 'Miles Osborne']",,"We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches.","In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .","['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .', 'This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.', 'Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.']",1,"['In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( #AUTHOR_TAG ) 5 .']"
CC1221,Q13-1020,Unsupervised Tree Induction for Tree-based Translation,syntax augmented machine translation via chart parsing,"['Andreas Zollmann', 'Ashish Venugopal']",related work,"We present translation results on the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop's baseline system. Our translation system is available open-source under the GNU General Public License.",#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']",1,"['For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.', 'Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models.', 'Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.', 'The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).', 'This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.', 'Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.', 'Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.', 'They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment.', 'adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.', 'Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment.', 'Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.', 'Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models.', '#AUTHOR_TAG substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .', 'Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.']"
CC1222,W00-1017,WIT,implementation of coordinative nodding behavior on spoken dialogue systems,"['Jun-ichi Hirasawa', 'Noboru Miyazaki', 'Mikio Nakano', 'Takeshi Kawabata']",,"This paper proposes a mechanism that contributes to the implementation of a spoken dialogue system with which a user can communicate e ortlessly. In a dialogue, exchanges between participants promote the establishment of shared information and this leads to e ortless communication. This is called  dialogue coordination"". In particular, revealing the respondent's internal state, such as through nodding and back-channel feedback, promotes the establishment of shared information. This is called  manifestation"", which is one aspect of coordinative behavior, and a mechanism for handling manifestation is introduced. In a human-human dialogue, the listener's manifestative behavior often occurs during a speaker's utterance. However, systems using conventional speech recognition technologies cannot respond during the speaker's utterance. In order to solve this problem, the proposed mechanism, ISTAR protocol transmission, utilizes the intermediate speech recognition results without waiting for the end of the speaker's utterance. This realizes a system with exible manifestative behavior.","This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .","['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan (Kobayashi et al., 1992).', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']",0,"['As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search ( #AUTHOR_TAG ) using the ISTAR ( Incremental Structure Transmitter And Receiver ) protocol , which conveys word graph information as well as word hypotheses .', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']"
CC1223,W00-1017,WIT,the philips automatic train timetable information system,"['Harald Aust', 'Martin Oerder', 'Frank Seide', 'Volker Steinbiss']",introduction,,"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( #AUTHOR_TAG ; Allen et al. , 1996 ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']"
CC1224,W00-1017,WIT,gus a frame driven dialog system,"['Daniel G Bobrow', 'Ronald M Kaplan', 'Martin Kay', 'Donald A Norman', 'Henry Thompson', 'Teny Winograd']",conclusion,,"There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .","['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .', 'Incorporating such techniques would deo crease the system developer workload.', 'However, there has been no work on domain-independent response generation for robust spoken dialogue systems that can deal with utterances that might include pauses in the middle of a sentence, which WIT handles well.', 'Therefore incorporating those techniques remains as a future work.']",3,"['There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( #AUTHOR_TAG ; Chu-Carroll , 1999 ) .']"
CC1225,W00-1017,WIT,mimic an adaptive mixed initiative spoken dialogue system for information queries,['Junnifer Chu-Carroll'],,"This paper describes MIMIC, an adaptive mixed initiative spoken dialogue system that provides movie showtime information. MIMIC improves upon previous dialogue systems in two respects. First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumulative effect of information dynamically extracted from user utterances during the dialogue. Second, MIMIC's dialogue management architecture decouples its initiative module from the goal and response strategy selection processes, providing a general framework for developing spoken dialogue systems with different adaptation behavior.",They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .,"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']",1,"['Functions defined here decide what string should be spoken and send that string to the speech output module based on the current dialogue state.', 'They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of #AUTHOR_TAG .']"
CC1226,W00-1017,WIT,asj continuous speech corpus for research,"['Tetsunori Kobayashi', 'Shuichi Itahashi', 'Satoru Hayamizu', 'Toshiyuld Takezawa']",,,Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .,"['word hypotheses.', 'As the recogn/fion engine, either VoiceRex, developed by NTI"" (Noda et al., 1998), or HTK from Entropic Research can be used.', 'Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .', 'This recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998) using the ISTAR (Incremental Structure Transmitter And Receiver) protocol, which conveys word graph information as well as word hypotheses.', 'This incremental output allows the language understanding module to process recognition results before the speech interval ends, and thus real-time responses are possible.', 'This module continuously runs and outputs recognition results when it detects a speech interval.', 'This enables the language generation module to react immediately to user interruptions while the system is speaking.', 'The language model for speech recognition is a network (regular) grammar, and it allows each speech interval to be an arbitrary number of phrases.', 'A phrase is a sequence of words, which is to be defined in a domain-dependent way.', 'Sentences can be decomposed into a couple of phrases.', 'The reason we use a repetition of phrases instead of a sentence grammar for the language model is that the speech recognition module of a robust spoken dialogue system sometimes has to recognize spontaneously spoken utterances, which include self-repairs and repetition.', 'In Japanese, bunsetsu is appropriate for defining phrases.', 'A bunsetsu consists of one content word and a number (possibly zero) of function words.', 'In the meeting room reservation system we have developed, examples of defined phrases are bunsetsu to specify the room to be reserved and the time of the reservation and bunsetsu to express affirmation and negation.']",5,['Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( #AUTHOR_TAG ) .']
CC1227,W00-1017,WIT,learning to predict problematic situations in a spoken dialogue system experiments with how may i help you,"['Marilyn Walker', 'Irene Langkilde', 'Jerry Wright Allen Gorin', 'Diane Litman']",introduction,"Current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system. Our expectation is that the ability to predict problematic dialogues will allow the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We train a problematic dialogue classifier using automatically-obtainable features that can identify problematic dialogues significantly better (23%) than the baseline. A classifier trained with only automatic features from the first exchange in the dialogue can predict problematic dialogues 7% more accurately than the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline.","The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; #AUTHOR_TAG ) .']"
CC1228,W00-1017,WIT,jupiter a telephonebased conversational interface for weather information,"['Victor Zue', 'Stephanie Seneff', 'James Glass', 'Joseph Polifroni', 'Christine Pao', 'Timothy J Hazen', 'Lee Hetherington']",introduction,,"The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; #AUTHOR_TAG ; Walker et al. , 2000 ) .']"
CC1229,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",experiments,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', ""The weather information system can answer the user's questions about weather forecasts in Japan."", 'The vocabulary size is around 500, and the number of phrase structure rules is 31.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']",2,"['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( #AUTHOR_TAGb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']"
CC1230,W00-1017,WIT,a robust system for natural spoken dialogue,"['James F Allen', 'Bradford W Miller', 'Eric K Ringger', 'Teresa Sikorsld']",introduction,"This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains. It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors. Robustness is achieved by a combination of statistical error post-correction, syntactically- and semantically-driven robust parsing, and extensive use of the dialogue context. We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training.Comment: uuencoded, gzipped PostScript. Includes extra Appendi","The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .","['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .', 'One of the next research goals is to make these systems task-portable, that is, to simplify the process of porting to another task domain.']",0,"['The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; #AUTHOR_TAG ; Zue et al. , 2000 ; Walker et al. , 2000 ) .']"
CC1231,W00-1017,WIT,a grammar and a parser for spontaneous speech,"['Mikio Nakano', 'Akira Shimazu', 'Kiyoshi Kogure']",conclusion,"This paper classifies distinctive phenomena occurring in Japanese spontaneous speech, and proposes a grammar and processing techniques for handling them. Parsers using a grammar for written sentences cannot deal with spontaneous speech because in spontaneous speech there are phenomena that do not occur in written sentences. A grammar based on analysis of transcripts of dialogues was therefore developed. It has two distinctive features: it uses short units as input units instead of using sentences in grammars for written sentences, and it covers utterances including phrases peculiar to spontaneous speech. Since the grammar is an augmentation of a grammar for written sentences, it can also be used to analyze complex utterances. Incorporating the grammar into the distributed natural language processing model described elsewhere enables the handling of utterances including variety of phenomena peculiar to spontaneous speech.","For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .","['Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems.', 'Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information.', 'For example, it is possible to represent a discourse stack whose depth is limited.', 'Recording some dialogue history is also possible.', 'Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .', 'The language generation module features Common Lisp functions, so there is no limitation on the description.', 'Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997).', 'It is also possible to build a simple finite-state-model-based dialogue system using WIT.', 'States can be represented by dialogue phases in WIT.']",3,"['Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered.', 'For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( #AUTHOR_TAG ; Nakano and Shimazu , 1999 ) .']"
CC1232,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .","['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .', 'Thus the system can respond immediately after user pauses when the user has the initiative.', 'When the system holds the initiative, it can immediately react to an interruption by the user because user utterances are understood in an incremental way (Dohsaka and Shimazu, 1997).']",0,"['Since the language generation module works in parallel with the language understanding module , utterance generation is possible even while the system is listening to user utterances and that utterance understanding is possible even while it is speaking ( #AUTHOR_TAGa ) .']"
CC1233,W00-1017,WIT,an efficient dialogue control method under systems limited knowledge,"['Kohji Dohsaka', 'Norihito Yasuda', 'Noboru Miyazaki', 'Mikio Nakano', 'Kiyoald Aikawa']",experiments,"This paper presents a novel method that controls a dialogue between a spoken dialogue system and a user efficiently so that the system responds as helpfully as possible within the limits of its knowledge. Due to speech recognition errors, a system and user must engage in a ""confirmation dialogue"" to clarify a user's request. Although a confirmation dialogue is unavoidable, it should be as concise as possible. Previous methods do not sufficiently allow for the effect of the limits of the system's knowledge on the efficiency of dialogue. The result is unnecessarily long dialogues to confirm a user's request minutely even if the request is beyond the system's knowledge. This paper describes a method that controls a dialogue efficiently so as to avoid an unnecessary confirmation dialogue and presents a computational efficiency criterion for dialogue control within the limits of the system's knowledge.","WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .","['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files.', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', ""The weather information system can answer the user's questions about weather forecasts in Japan."", 'The vocabulary size is around 500, and the number of phrase structure rules is 31.', 'The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.']",2,"['WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( #AUTHOR_TAG ) .', 'A sample dialogue between this system and a naive user is shown in Figure 2.', 'This system employs HTK as the speech recognition engine.', 'The vocabulary size is around 500, and the number of phrase structure rules is 31.']"
CC1234,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",introduction,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.",WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .,"['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'In addition, WIT compiles domain-dependent system specifications into internal knowledge sources so that building systems is easier.', 'Although WIT requires more domaindependent specifications than finite-state-modelbased toolkits, WIT-based systems are capable of taking full advantage of language processing technology.', 'WIT has been implemented and used to build several spoken dialogue systems.']",5,"['This paper presents WIT 1, which is a toolkit IWIT is an acronym of Workable spoken dialogue lnter-for building spoken dialogue systems that integrate speech recognition, language understanding and generation, and speech output.', 'WIT features an incremental understanding method ( #AUTHOR_TAGb ) that makes it possible to build a robust and real-time system .', 'WIT has been implemented and used to build several spoken dialogue systems.']"
CC1235,W00-1017,WIT,constraint projection an efficient treatment of disjunctive feature descriptions,['Mikio Nakano'],,"Unification of disjunctive feature descriptions is important for efficient unification-based parsing. This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints. Constraint projection is a generalization of constraint unification, and is more efficient because constraint projection has a mechanism for abandoning information irrelevant to a goal specified by a list of variables.",Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .,"['The domain-dependent knowledge used in this module consists of a unification-based lexicon and phrase structure rules.', 'Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .', 'When a phrase boundary is detected, the feature structure for a phrase is computed using some built-in rules from the feature structure rules for the words in the phrase.', 'The phrase structure rules specify what kind of phrase sequences can be considered as sentences, and they also enable computing the semantic representation for found sentences.', ""Two kinds of sentenees can be considered; domain-related ones that express the user's intention about the reser-vafion and dialogue-related ones that express the user's attitude with respect to the progress of the dialogue, such as confirmation and denial."", 'Considering the meeting room reservation system, examples of domain-related sentences are ""I need to book Room 2 on Wednesday"", ""I need to book Room 2"", and ""Room 2"" and dialogue-related ones are ""yes"", ""no"", and ""Okay"".']",5,['Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( #AUTHOR_TAG ) .']
CC1236,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",experiments,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.",The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .,"['These roles are similar to DCG (Pereira and Warren, 1980) rules; they can include logical variables and these variables can be bound when these rules are applied.', 'It is possible to add to the rules constraints that stipulate relationships that must hold among variables (Nakano,199 I), but we do not explain these constraints in detail in this paper.', 'The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state."", 'The command is one of the following:']",5,"['The priorities are used for disambiguating interpretation in the incremental understanding method ( #AUTHOR_TAGb ) .', ""When the command on the right-hand side of the arrow is a frame operation command, phrases to which this rule can be applied can be considered a sentence, and the sentence's semantic representation is the command for updating the dialogue state.""]"
CC1237,W00-1017,WIT,europa a generic framework for developing spoken dialogue systems,"['Munehiko Sasajima', 'Yakehide Yano', 'Yasuyuld Kono']",introduction,"Voice interfaces are not popular since they are neither useful nor user-friendly for non-specialist users. In this paper, EUROPA, a new framework for developing spoken dialogue systems, is introduced. In developing EUROPA, the authors focused on three points : (1) acceptance of spoken language, (2) portability in terms of domain and task, and (3) practical performance of the applied system. The framework is applied to prototyping a car navigation system called MINOS. MINOS is built on a portable PC, can process over 700 words of recognition vocabulary, and is able to respond to a user's question within a few seconds.","To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .","['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'One is the CSLU Toolkit (Sutton et al., 1998), which enables rapid prototyping of a spoken dialogue system that incorporates a finite-state dialogue model.', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.', 'However, it limits system functions; it is not easy to employ the advanced language processing techniques developed in the realm of computational linguistics.', 'Another is GALAXY-II (Seneffet al., 1998), *Mikio Nakano is currently a visiting scientist at MIT Laboratory for Computer Science. which enables modules in a dialogue system to communicate with each other.', 'It consists of the hub and several servers, such as the speech recognition server and the natural language server, and the hub communicates with these servers.', 'Although it requires more specifications than finitestate-model-based toolkits, it places less limitations on system functions.']",0,"['To this end , several toolkits for building spoken dialogue systems have been developed ( Barnett and Singh , 1997 ; #AUTHOR_TAG ) .', 'It decreases the amount of the effort required in building a spoken dialogue system in a user-defined task domain.']"
CC1238,W00-1017,WIT,understanding unsegmented user utterances in realtime spoken dialogue systems,"['Mikio Nakano', 'Noboru Miyazalci', 'Jun-ichi Hirasawa', 'Kohji Dohsaka', 'Takeshi Kawabata']",,"This paper proposes a method for incrementally understanding user utterances whose semantic boundaries are not known and responding in real time even before boundaries are determined. It is an integrated parsing and discourse processing method that updates the partial result of understanding word by word, enabling responses based on the partial result. This method incrementally finds plausible sequences of utterances that play crucial roles in the task execution of dialogues, and utilizes beam search to deal with the ambiguity of boundaries as well as syntactic and semantic ambiguities. The results of a preliminary experiment demonstrate that this method understands user utterances better than an understanding method that assumes pauses to be semantic boundaries.","The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .","['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .', 'ISSS enables the incremental understanding of user utterances that are not segmented into sentences prior to pars-ing by incrementally finding the most plausible sequence of sentences (or significant utterances in the ISSS terms) out of the possible sentence sequences for the input word sequence.', 'ISSS also makes it possible for the language generation module to respond in real time because it can output a partial result of understanding at any point in time.']",5,"['The language understanding :module receives word hypotheses from the speech recognition module and incrementally understands the sequence of the word hypotheses to update the dialogue state, in which the resnlt of understanding and discourse information are represented by a frame (i.e., attribute-value pairs).', 'The understanding module utilizes ISSS ( Incremental Significant-utterance Sequence Search ) ( #AUTHOR_TAGb ) , which is an integrated parsing and discourse processing method .']"
CC1239,W00-1312,Cross-lingual information retrieval using hidden Markov models,phrasal translation and query expansion techniques for crosslanguage information retrievalquot,"['L Ballesteros', 'W B Croft']",related work,"Dictionary methods for cross-language information retrieval give performance below that for mono-lingual retrieval. Failure to translate multi-term phrases has been shown to be one of the factors responsible for the errors associated with dictionary methods. First, we study the importance of phrasal translation for this approach. Second, we explore the role of phrases in query expansion via local context analysis and local feedback and show how they can be used to significantly reduce the error associated with automatic dictionary translation.","Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).","['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']",1,"['Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; #AUTHOR_TAG ; Hull and Grefenstette, 1996).']"
CC1240,W00-1312,Cross-lingual information retrieval using hidden Markov models,using structured queries for disambiguation in crosslanguage information retrievalquot,['D A Hull'],method,,"There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']",1,"['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; #AUTHOR_TAG ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']"
CC1241,W00-1312,Cross-lingual information retrieval using hidden Markov models,using statistical testing in the evaluation of retrieval experimentsquot,['D Hull'],,,The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .,"['The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C.', 'Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']",5,"['Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries.', 'The one-sided t-test ( #AUTHOR_TAG ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .']"
CC1242,W00-1312,Cross-lingual information retrieval using hidden Markov models,translingual information retrieval a comparative evaluationquot,"['J Carbonell', 'Y Yang', 'R Frederlcing', 'R Brown', 'Y Geng', 'D Lee']",related work,Translingual information retrieval TIR con sists of providing a query in one language and searching document collections in one or more di erent languages This paper introduces new TIR methods and reports on comparative TIR experiments with these new methods and with previously reported ones in a realistic setting Methods fall into two categories query trans lation based and statistical IR approaches es tablishing translingual associations The re sults show that using bilingual corpora for au tomated extraction of term equivalences in con text outperforms other methods Translin gual versions of the Generalized Vector Space Model GVSM and Latent Semantic Indexing LSI perform relatively well as does translin gual pseudo relevance feedback PRF All showed relatively small performance loss be tween monolingual and translingual versions Query translation based on a general machine readable bilingual dictionary heretofore the most popular method did not match the per formance of other more sophisticated methods Also the previous very high LSI results in the literature were discon rmed by more realistic relevance based evaluations,"The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .","['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .', 'We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999).']",1,"['The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( #AUTHOR_TAG ) .']"
CC1243,W00-1312,Cross-lingual information retrieval using hidden Markov models,a tutorial on hidden markov models and selected applications in speech recognitionquot,['L Rabiner'],,,â¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .,['â\x80¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .'],5,['â\x80¢ The transition probability a is 0.7 using the EM algorithm ( #AUTHOR_TAG ) on the TREC4 ad-hoc query set .']
CC1244,W00-1312,Cross-lingual information retrieval using hidden Markov models,word sense disambiguation and information retrievalquot,['M Sanderson'],related work,,#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.,"['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']",0,"['While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997.', '#AUTHOR_TAG studied the issue of disambiguation for mono-lingual M.']"
CC1245,W00-1312,Cross-lingual information retrieval using hidden Markov models,using structured queries for disambiguation in crosslanguage information retrievalquot,['D A Hull'],related work,,"Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .","['Another common approach is term translation, e.g., via a bilingual lexicon.', '(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996).', 'While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable.', 'Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .', 'Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.']",0,"['Other studies on the value of disambiguation for cross-lingual IR include Hiemstra and de Jong , 1999 ; #AUTHOR_TAG .']"
CC1246,W00-1312,Cross-lingual information retrieval using hidden Markov models,a comparative study of query and document translation for crosslanguage information retrievalquot,['D W Oard'],related work,"Cross-language retrieval systems use queries in one natural language to guide retrieval of documents that might be written in another. Acquisition and representation of translation knowledge plays a central role in this process. This paper explores the utility of two sources of translation knowledge for cross-language retrieval. We have implemented six query translation techniques that use bilingual term lists and one based on direct use of the translation output from an existing machine translation system; these are compared with a document translation technique that uses output from the same machine translation system. Average precision measures on a TREC collection suggest that arbitrarily selecting a single dictionary translation is typically no less effective than using every translation in the dictionary, that query translation using a machine translation system can achieve somewhat better effectiveness than simpler techniques, and that document translation may result in further improvements in retrieval effectiveness under some conditions.","One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .","['Many approaches to cross-lingual IR have been published.', 'One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .', 'For most languages, there are no MT systems at all.', 'Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.']",1,"['One common approach is using Machine Translation ( MT ) to translate the queries to the language of the documents or translate documents to the language of the queries ( Gey et al , 1999 ; #AUTHOR_TAG ) .']"
CC1247,W00-1312,Cross-lingual information retrieval using hidden Markov models,the effects of query structure and dictionary setups in dictionarybased crosslanguage information retrievalquot,['An Pirkola'],method,,"There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .","['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']",1,"['A second method is to structure the translated query, separating the translations for one term from translations for other terms.', 'This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query.', 'There are several variations of such a method ( Ballesteros and Croft , 1998 ; #AUTHOR_TAG ; Hull 1997 ) .', 'One such method is to treat different translations of the same term as synonyms.', 'Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms.', 'However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other.', 'By contrast, our HMM approach supports translation probabilities.', ""The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function."", 'Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.']"
CC1248,W00-1312,Cross-lingual information retrieval using hidden Markov models,resolving ambiguity for crosslanguage retrievalquot,"['L Ballesteros', 'W B Croft']",method,," However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .","['In this section we compare our approach with two other approaches.', 'One approach is ""simple substitution"", i.e., replacing a query term with all its translations and treating the translated query as a bag of words in mono-lingual retrieval.', 'Suppose we have a simple query Q=(a, b), the translations for a are al, a2, a3, and the translations for b are bl, b2. The translated query would be (at, a2, a3, b~, b2).', 'Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', 'Also, a document matching different translations of one term in the original query may be ranked higher than a document that matches translations of different terms in the original query.', 'That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at andbl.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']",0,"['One approach is ""simple substitution"", i.e., replacing a query term with all its translations and treating the translated query as a bag of words in mono-lingual retrieval.', 'Since all terms are treated as equal in the translated query, this gives terms with more translations (potentially the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common.', ' However , the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur ( #AUTHOR_TAG ) .']"
CC1249,W00-1312,Cross-lingual information retrieval using hidden Markov models,corpusbased stemming using cooccurrence of word variantsquot,"['J Xu', 'W B Croft']",experiments,,A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .,"['For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet (http://www.activa.arrakis.es)', 'containing around 22,000 English words (16,000 English stems) and processed it similarly.', 'Each English word has around 1.5 translations on average.', 'A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .', 'One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.', 'This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon.']",5,['A cooccurrence based stemmer ( #AUTHOR_TAG ) was used to stem Spanish words .']
CC1250,W00-1312,Cross-lingual information retrieval using hidden Markov models,a language modeling approach to information retrievalquot,"['J Ponte', 'W B Croft']",related work,,"Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .","['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']",1,"['Other studies which view lR as a query generation process include Maron and Kuhns , 1960 ; Hiemstra and Kraaij , 1999 ; #AUTHOR_TAG ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']"
CC1251,W00-1312,Cross-lingual information retrieval using hidden Markov models,on relevance probabilistic indexing and information retrievalquot,"['M E Maron', 'K L Kuhns']",related work,,"Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .","['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']",1,"['Other studies which view lR as a query generation process include #AUTHOR_TAG ; Hiemstra and Kraaij , 1999 ; Ponte and Croft , 1998 ; Miller et al , 1999 .', 'Our work has focused on cross-lingual retrieval.']"
CC1252,W00-1312,Cross-lingual information retrieval using hidden Markov models,finding terminology translations from nonparallel corporaquot,"['P Fung', 'K Mckeown']",,"We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance.",Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .,"['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .', 'Since traditional lexicons are more or less static repositories of knowledge, techniques that discover translation from newly published materials can supplement them with corpus-specific vocabularies.']",0,['Another technique is automatic discovery of translations from parallel or non-parallel corpora ( #AUTHOR_TAG ) .']
CC1253,W00-1312,Cross-lingual information retrieval using hidden Markov models,a hidden markov model information retrieval systemquot,"['D Miller', 'T Leek', 'R Schwartz']",,"We present a new method for information retrieval using hidden Markov models (HMMs). We develop a general framework for incorporating multiple word generation mechanisms within the same model. We then demonstrate that an extremely simple realization of this model substantially outperforms standard tf :idf ranking on both the TREC-6 and TREC-7 ad hoc retrieval tasks. We go on to present a novel method for performing blind feedback in the HMM framework, a more complex HMM that models bigram production, and several other algorithmic re nements. Together, these methods form a state-of-the-art retrieval system that ranked among the best on the TREC-7 ad hoc retrieval task.","Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .","['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']",5,"['Following #AUTHOR_TAG , the IR system ranks documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) .', 'Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking documents according to P(Q[D is R) is the same as ranking them according to P(D is RIQ).', 'The approach therefore estimates the probability that a query Q is generated, given the document D is relevant.', '(A glossary of symbols used appears below.)']"
CC1254,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( #AUTHOR_TAG ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1255,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .,"['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .', 'In this case, a full parse tree is represented in a flat form, producing a representation as in the example above.', 'The goal in this case is therefore to accurately predict a collection of ¢ £ ¢ different types of phrases.', 'The chunk types are based on the syntactic category part of the bracket label in the Treebank.', 'Roughly, a chunk contains everything to the left of and including the syntactic head of the constituent of the same name.', 'The phrases are: adjective phrase (ADJP), adverb phrase (ADVP), conjunction phrase (CONJP), interjection phrase (INTJ), list marker (LST), noun phrase (NP), preposition phrase (PP), particle (PRT), subordinated clause (SBAR), unlike coordinated phrase (UCP), verb phrase (VP).', '(See details in (Tjong Kim Sang and Buchholz, 2000).)']",5,['The first is the one used in the chunking competition in CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) .']
CC1256,W01-0706,Exploring evidence for shallow parsing,cooccurrence and transformation in linguistic structure,['Z S Harris'],introduction,,"Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( #AUTHOR_TAG ; Abney , 1991 ; Greffenstette , 1993 ) .']"
CC1257,W01-0706,Exploring evidence for shallow parsing,statistical parsing with a contextfree grammar and word statistics,['E Charniak'],introduction,,"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; #AUTHOR_TAGa ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1258,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']",0,"['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; #AUTHOR_TAG ; Ratnaparkhi , 1997 ) .']"
CC1259,W01-0706,Exploring evidence for shallow parsing,three generative lexicalised models for statistical parsing,['M Collins'],experiments,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).Comment: 8 pages, to appear in Proceedings of ACL/EACL 97","For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']",5,"['For the full parser , we use the one developed by Michael Collins ( Collins , 1996 ; #AUTHOR_TAG ) -- one of the most accurate full parsers around .']"
CC1260,W01-0706,Exploring evidence for shallow parsing,statistical parsing with a contextfree grammar and word statistics,['E Charniak'],introduction,,"However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']",0,"['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( #AUTHOR_TAGb ; Charniak , 1997a ; Collins , 1997 ; Ratnaparkhi , 1997 ) .']"
CC1261,W01-0706,Exploring evidence for shallow parsing,the snow learning architecture,"['A Carleson', 'C Cumby', 'J Rosen', 'D Roth']",experiments,,"SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['SNoW ( #AUTHOR_TAG ; Roth , 1998 ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']"
CC1262,W01-0706,Exploring evidence for shallow parsing,a stochastic parts program and noun phrase parser for unrestricted text,['Kenneth W Church'],introduction,A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.>,"Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( #AUTHOR_TAG ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1263,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",introduction,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; #AUTHOR_TAG ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1264,W01-0706,Exploring evidence for shallow parsing,parsing by chunks,['S P Abney'],introduction,"I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this:    (1)    [I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time]              These chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks.","Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; #AUTHOR_TAG ; Greffenstette , 1993 ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.']"
CC1265,W01-0706,Exploring evidence for shallow parsing,evaluation techniques for automatic semantic extraction comparing semantic and window based approaches,['G Greffenstette'],introduction,"As large on-line corpora become more prevalent, a number of attempts have been made to automatically extract thesaurus-like relations directly from text using knowledge poor methods. In the absence of any specific application, comparing the results of these attempts is difficult. Here we propose an evaluation method using gold standards, i.e., pre-existing hand-compiled resources, as a means of comparing extraction techniques. Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words. The two techniques are very similar except that in one case selective natural language processing, a partial syntactic analysis, is performed. On a 4 megabyte corpus, syntactic contexts produce significantly better results against the gold standards for the most characteristk: words in the corpus, while windows produce better results for rare words.","Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers (Collins, 1997;Charniak, 1997a;Charniak, 1997b;Ratnaparkhi, 1997), significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns -syntactic phrases or words that participate in a syntactic relationship (Church, 1988;Ramshaw and Marcus, 1995;Argamon et al., 1998;Cardie and Pierce, 1998;Munoz et al., 1999;Punyakanok and Roth, 2001;Buchholz et al., 1999;Tjong Kim Sang and Buchholz, 2000).']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences , the alternative is to perform only partial analysis of the syntactic structures in a text ( Harris , 1957 ; Abney , 1991 ; #AUTHOR_TAG ) .', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.']"
CC1266,W01-0706,Exploring evidence for shallow parsing,fastus a finitestate processor for information extraction from realworld text,"['D Appelt', 'J Hobbs', 'J Bear', 'D Israel', 'M Tyson']",introduction,,"First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']",0,"['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( Grishman , 1995 ; #AUTHOR_TAG ) .', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']"
CC1267,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,"Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",5,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim #AUTHOR_TAG ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1268,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",introduction,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; #AUTHOR_TAG ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1269,W01-0706,Exploring evidence for shallow parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .","['Overall, the driving force behind the work on learning shallow parsers was the desire to get better performance and higher reliability.', 'However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']",0,"['However , since work in this direction has started , a significant progress has also been made in the research on statistical learning of full parsers , both in terms of accuracy and processing time ( Charniak , 1997b ; Charniak , 1997a ; Collins , 1997 ; #AUTHOR_TAG ) .']"
CC1270,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",experiments,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .","['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['The shallow parser used is the SNoW-based CSCL parser ( #AUTHOR_TAG ; Munoz et al. , 1999 ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]"
CC1271,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",experiments,,Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .,"['We start by reporting the results in which we compare the full parser and the shallow parser on the ""clean"" WSJ data.', 'Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .', 'The results show that for the tasks of identifying phrases, learning directly, as done by the shallow parser outperforms the outcome from the full parser.', 'Next, we compared the performance of the parsers on the task of identifying atomic phrases 2 .', 'Here, again, the shallow parser exhibits significantly better performance.', 'Table 3 shows the results of extracting atomic phrases.']",5,['Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim #AUTHOR_TAG ) terminology .']
CC1272,W01-0706,Exploring evidence for shallow parsing,text chunking using transformationbased learning,"['L A Ramshaw', 'M P Marcus']",introduction,"Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ""baseNP"" chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; #AUTHOR_TAG ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1273,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",experiments,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",2,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1274,W01-0706,Exploring evidence for shallow parsing,performance structuresa psycholinguistic and linguistic appraisal cognitive psychology,"['J P Gee', 'F Grosjean']",introduction,,"Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .","['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']",0,"['Research on shallow parsing was inspired by psycholinguistics arguments ( #AUTHOR_TAG ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .']"
CC1275,W01-0706,Exploring evidence for shallow parsing,a new statistical parser based on bigram lexical dependencies,['M Collins'],experiments,"This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.  1 Introduction  Lexical information has been shown to be crucial for many parsing decisions, such as prepositional-phrase attachment (for example (Hindle and Rooth 93)). However, early approaches to probabilistic parsing (Pereira and Schabes 92; Magerman and Marcus 91; Briscoe and Carroll 93) conditioned probabil..","For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .","['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.', 'After that, it will choose the candidate parse tree with the highest probability as output.', 'The experiments use the version that was trained (by Collins) on sections 02-21 of the Penn Treebank.', 'The reported results for the full parse tree (on section 23) are recall/precision of 88.1/87.5 (Collins, 1997).']",5,"['We perform our comparison using two state-ofthe-art parsers.', 'For the full parser , we use the one developed by Michael Collins ( #AUTHOR_TAG ; Collins , 1997 ) -- one of the most accurate full parsers around .', 'It represents a full parse tree as a set of basic phrases and a set of dependency relationships between them.', 'Statistical learning techniques are used to compute the probabilities of these phrases and of candidate dependency relations occurring in that sentence.']"
CC1276,W01-0706,Exploring evidence for shallow parsing,errordriven pruning of treebanks grammars for base noun phrase identification,"['C Cardie', 'D Pierce']",introduction,"Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a ""treebank"" corpus; then the grammar is improved by selecting rules with high ""benefit"" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; #AUTHOR_TAG ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1277,W01-0706,Exploring evidence for shallow parsing,a learning approach to shallow parsing,"['M Munoz', 'V Punyakanok', 'D Roth', 'D Zimak']",experiments,"A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally. The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference. Two instantiations of this approach are studied and experimental results for Noun-Phrases (NP) and Subject-Verb (SV) phrases that compare favorably with the best published results are presented. In doing that, we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are better learned using open/close predictors than using inside/outside predictors.} thus contribute to the understanding of how to model shallow parsing tasks as learning problems.","The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .","['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .', 'SNoW (Carleson et al., 1999;Roth, 1998) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example.', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; #AUTHOR_TAG ) .']"
CC1278,W01-0706,Exploring evidence for shallow parsing,learning to resolve natural language ambiguities a unified approach,['D Roth'],experiments,"We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.","SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .","['The shallow parser used is the SNoW-based CSCL parser (Punyakanok and Roth, 2001;Munoz et al., 1999).', 'SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .', 'It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space.', 'Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation value of the target classes.', 'However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference.', 'Indeed, in CSCL (constraint satisfaction with classifiers), SNoW is used to learn several different classifiers -each detects the beginning or end of a phrase of some type (noun phrase, verb phrase, etc.).', ""The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.""]",5,"['SNoW ( Carleson et al. , 1999 ; #AUTHOR_TAG ) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources ( features ) taking part in decisions is very large , of which NLP is a principal example .']"
CC1279,W01-0706,Exploring evidence for shallow parsing,introduction to the conll2000 shared task chunking,"['E F Tjong Kim Sang', 'S Buchholz']",introduction,,would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney�s work (Abney, 1991), who has suggested to �chunk� sentences to base level phrases.', 'For example, the sentence �He reckons the current account deficit will narrow to only $ 1.8 billion in September .�', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'would be chunked as follows ( Tjong Kim #AUTHOR_TAG ) : [ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [NP only $ 1.8 billion ] [PP in ] [NP September] .']"
CC1280,W01-0706,Exploring evidence for shallow parsing,the nyu system for muc6 or where’s syntax in,['R Grishman'],introduction,,"First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .","['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'Second, while training a full parser requires a collection of fully parsed sentences as training corpus, it is possible to train a shallow parser incrementally.', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']",0,"['First , it has been noted that in many natural language applications it is sufficient to use shallow parsing information ; information such as noun phrases ( NPs ) and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization ( #AUTHOR_TAG ; Appelt et al. , 1993 ) .', 'If all that is available is a collection of sentences annotated for NPs, it can be used to produce this level of analysis.', 'This can be augmented later if more information is available.', 'Finally, the hope behind this research direction was that this incremental and modular processing might result in more robust parsing decisions, especially in cases of spoken language or other cases in which the quality of the natural language inputs is low -sentences which may have repeated words, missing words, or any other lexical and syntactic mistakes.']"
CC1281,W01-0706,Exploring evidence for shallow parsing,a memorybased approach to learning shallow natural language patterns,"['S Argamon', 'I Dagan', 'Y Krymolowski']",introduction,"Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; #AUTHOR_TAG ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1282,W01-0706,Exploring evidence for shallow parsing,cascaded grammatical relation assignment,"['S Buchholz', 'J Veenstra', 'W Daelemans']",introduction,"In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; #AUTHOR_TAG ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1283,W01-0706,Exploring evidence for shallow parsing,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],introduction,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .","['Shallow parsing is studied as an alternative to full-sentence parsing.', 'Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'A lot of recent work on shallow parsing has been influenced by Abney\'s work (Abney, 1991), who has suggested to ""chunk"" sentences to base level phrases.', 'For example, the sentence ""He reckons the current account deficit will narrow to only $ 1.8 billion in September .""', 'would be chunked as follows (Tjong Kim Sang and Buchholz, 2000): While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information.', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']",0,"['Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957;Abney, 1991;Greffenstette, 1993).', 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; #AUTHOR_TAG ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .']"
CC1284,W01-0706,Exploring evidence for shallow parsing,the use of classifiers in sequential inference,"['V Punyakanok', 'D Roth']",experiments,"We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.","Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .","['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']",2,"['Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( #AUTHOR_TAG ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .', 'Table 1 shows that it ranks among the top shallow parsers evaluated there 1 .']"
CC1285,W01-0706,Exploring evidence for shallow parsing,building a large annotated corpus of english the penn treebank,"['M P Marcus', 'B Santorini', 'M Marcinkiewicz']",experiments,"Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.","Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .","['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .', 'To train the CSCL shallow parser we had first to convert the WSJ data to a flat format that directly provides the phrase annotations.', 'This is done using the ""Chunklink"" program provided for CoNLL-2000 (Tjong Kim Sang andBuchholz, 2000).']",5,"['Training was done on the Penn Treebank ( #AUTHOR_TAG ) Wall Street Journal data , sections 02-21 .']"
CC1286,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,translating the xtag english grammar to hpsg,"['Yuka Tateisi', 'Kentaro Torisawa', 'Yusuke Miyao', 'Jun’ichi Tsujii']",introduction,,Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .,"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'However, their method depended on translator�s intuitive analy- sis of the original grammar.', 'Thus the transla- tion was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equiva- lence between the original and obtained gram- mars.', 'Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', 'However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above ad- vantages.']",1,"['Tateisi et al. also translated LTAG into HPSG ( #AUTHOR_TAG ) .', 'Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars.', 'However, given the greater ex- pressive power of HPSG, it is impossible to con- vert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capac- ity.']"
CC1287,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hybrid japanese parser with handcrafted grammar and statistics,"['Hiroshi Kanayama', 'Kentaro Torisawa', 'Yutaka Mitsuisi', 'Jun’ichi Tsujii']",introduction,,"There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).","['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; #AUTHOR_TAG ) , and ones on programming/grammar-development environ -  (Sarkar and Wintner, 1999;Doran et al., 2000;Makino et al., 1998).']"
CC1288,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,efficient ltag parsing using hpsg parsers,"['Naoki Yoshinaga', 'Yusuke Miyao', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",introduction,,A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .,"['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001), which is a large-scale FB-LTAG grammar.', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",1,"['A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ( #AUTHOR_TAG ) .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"
CC1289,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,an hpsg parser with cfg filtering,"['Kentaro Torisawa', 'Kenji Nishida', 'Yusuke Miyao', 'Jun’ichi Tsujii']",experiments,,"TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']",1,"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'TNT refers to the HPSG parser ( #AUTHOR_TAG ) , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG ( phase 1 ) and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']"
CC1290,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,headdriven phrase structure grammar,"['Carl Pollard', 'Ivan A Sag']",introduction,,"This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( #AUTHOR_TAG ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.']"
CC1291,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,feature structures based tree adjoining grammars,"['K Vijay-Shanker', 'Aravind K Joshi']",introduction,"We have embedded Tree Adjoining Grammars (TAG) in a  feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG u27s. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986)involving the logical formulation of feature structures","FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Y=).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with PS).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.']"
CC1292,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,typing as a means for validating feature structures,"['Anoop Sarkar', 'Shuly Wintner']",introduction,"We present a method for validating the consistency of feature structure speci cations by imposing a type discipline. A typed system facilitates a great number of compile-time checks: many possible errors can be detected before the grammar is used for parsing. We have constructed a type signature for an existing broad-coverage grammar of English, and implemented a type inference algorithm that operates on the feature structure speci cations in the grammar. The algorithm reports occurrences of incompatibility with the type signature. We have detected a large number of errors in the grammar; four types of errors are described in the paper.","ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .","['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'These works are re- stricted to each closed community, and the rela- tion between them is not well discussed.', 'Investi- gating the relation will be apparently valuable for both communities.']",0,"['ment ( #AUTHOR_TAG ; Doran et al. , 2000 ; Makino et al. , 1998 ) .', 'Investi- gating the relation will be apparently valuable for both communities.']"
CC1293,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,feature structures based tree adjoining grammars,"['K Vijay-Shanker', 'Aravind K Joshi']",introduction,"We have embedded Tree Adjoining Grammars (TAG) in a  feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG u27s. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986)involving the logical formulation of feature structures","This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( Vijay-Shanker , 1987 ; #AUTHOR_TAG ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']"
CC1294,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,parsing strategies with ‘lexicalized’ grammars application to tree adjoining grammars,"['Yves Schabes', 'Anne Abeille', 'Aravind K Joshi']",introduction,"In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988).In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure.We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely.We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set. The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search.We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach.",LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar,['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar'],0,['LTAG ( #AUTHOR_TAG ) is a grammar formalism that provides syntactic analyses for a sentence by composing elementary trees with two opera - Figure 6: Parsing with an HPSG grammar']
CC1295,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,verbmobil a translation system for facetoface dialog,"['M Kay', 'J Gawron', 'P Norvig']",introduction,,"In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']",0,"['In practical context , German , English , and Japanese HPSG-based grammars are developed and used in the Verbmobil project ( #AUTHOR_TAG ) .']"
CC1296,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hpsgstyle underspecified japanese grammar with wide coverage,"['Yutaka Mitsuishi', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",introduction,"This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely. The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However, this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu is attached to the nearest possible one.","Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']",0,"['Our group has developed a wide-coverage HPSG grammar for Japanese ( #AUTHOR_TAG ) , which is used in a high-accuracy Japanese dependency analyzer ( Kanayama et al. , 2000 ) .']"
CC1297,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,adapting hpsgtotag compilation to widecoverage grammars,"['Tilman Becker', 'Patrice Lopez']",introduction,,"Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .","['Figure 1 depicts a brief sketch of the RenTAL system.', 'The system consists of the following four modules: Tree converter, Type hierarchy extractor, Lexicon converter and Derivation translator.', 'The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them.', 'The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries.', 'The derivation translator module takes HPSG parse  (Tateisi et al., 1998).', ""However, their method depended on translator's intuitive analysis of the original grammar."", 'Thus the translation was manual and grammar dependent.', 'The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars.', 'Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .', 'However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar.', 'Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity.', 'Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.']",1,"['Other works ( Kasper et al. , 1995 ; #AUTHOR_TAG ) convert HPSG grammars into LTAG grammars .']"
CC1298,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,hybrid japanese parser with handcrafted grammar and statistics,"['Hiroshi Kanayama', 'Kentaro Torisawa', 'Yutaka Mitsuisi', 'Jun’ichi Tsujii']",introduction,,"Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']",0,"['Our group has developed a wide-coverage HPSG grammar for Japanese ( Mitsuishi et al. , 1998 ) , which is used in a high-accuracy Japanese dependency analyzer ( #AUTHOR_TAG ) .']"
CC1299,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,the logic of typed feature structures,['Bob Carpenter'],introduction,"For those of us who belonged to the ""Bay Area (Computational) Linguistics Community,"" the early eighties were a heady time. Local researchers working on linguistics, computational linguistics, and logic programming were investigating notions of category, type, feature, term, and partial specification that appeared to converge to a powerful new approach for describing (linguistic) objects and their relationships by monotonic accumulation of constraints between their features. The seed notions had almost independently arisen in generalized phrase structure grammar (GPSG) (Gazdar et al. 1985), lexical-functional grammar (LFG) (Bresnan and Kaplan 1982), functionalunification grammar (FUG) (Kay 1985), logic programming (Colmerauer 1978, Pereira and Warren 1980), and terminological reasoning systems (Ait-Kaci 1984). It took, however, a lot of experimental and theoretical work to identify precisely what the core notions were, how particular systems related to the core notions, and what were the most illuminating mathematical accounts of that core. The development of the unificationbased formalism PATR-II (Shieber 1984) was an early step toward the definition of the core, but its mathematical analysis, and the clarification of the connections between the various systems, are only now coming to a reasonable closure. The Logic of Typed Feature Structures is the first monograph that brings all the main theoretical ideas into one place where they can be related and compared in a unified setting. Carpenter's book touches most of the crucial questions of the developments during the decade, provides proofs for central results, and reaches right up to the edge of current research in the field. These contributions alone make it an indispensable compendium for the researcher or graduate student working on constraint-based grammatical formalisms, and they also make it a very useful reference work for researchers in object-oriented databases and logic programming. Having discharged the main obligation of the reviewer of saying who should read the book under review and why, I will now survey each of the book's four parts while raising some more general questions impinging on the whole book as they arise from the discussion of each part.","An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .","['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'A lexical entry for each word expresses the characteristics of the word, such as the subcategorization frame and the grammatical category.', 'An ID grammar rule represents a relation between a mother and its daughters, and is independent of lexical characteristics.', 'Figure 6 illustrates an example of bottom-up parsing with an HPSG grammar.', 'First, lexical entries for ""can"" and ""run"" are unified respectively with the daughter feature structures of']",0,"['An HPSG grammar consists of lexical entries and ID grammar rules , each of which is described with typed feature structures ( #AUTHOR_TAG ) .', 'First, lexical entries for ""can"" and ""run"" are unified respectively with the daughter feature structures of']"
CC1300,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],introduction,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",5,"['We apply our system to the latest version of the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) , which is a large-scale FB-LTAG grammar .']"
CC1301,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,evolution of the xtag system,"['Christy Doran', 'Beth Ann Hockey', 'Anoop Sarkar', 'B Srinivas', 'Fei Xia']",introduction,,"The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Y=).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with PS).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group ( #AUTHOR_TAG ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .']"
CC1302,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,grammar conversion from fbltag to hpsg,"['Naoki Yoshinaga', 'Yusuke Miyao']",introduction,,The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoin-ing Grammar (FB-LTAG 1 ) (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion.', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,['The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar ( #AUTHOR_TAG ) .']
CC1303,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,some experiments on indicators of parsing complexity for lexicalized grammars,"['Anoop Sarkar', 'Fei Xia', 'Aravind Joshi']",experiments,"In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efficiency of parsers for such grammar formalisms.","In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .","['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']",1,"['In Table 2 , lem refers to the LTAG parser ( #AUTHOR_TAG ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper  describes the detailed analysis on the factor of the difference of parsing performance.']"
CC1304,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a study of tree adjoining grammars,['K Vijay-Shanker'],introduction,"Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently","FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with Y=).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with PS).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled U onto an internal node of another tree with the same symbol U (Figure 4).', 'FBLTAG ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) is an extension of the LTAG formalism .', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism, including the XTAG English grammar, a large-scale grammar for English (The XTAG Research Group, 2001).', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeille and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']"
CC1305,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,on building a more efficient grammar by exploiting types,['Dan Flickinger'],introduction,"Modern grammar development platforms often support multiple devices for representing properties of a natural language, giving the grammar writer some freedom in implementing analyses of linguistic phenomena. These design alternatives can have dramatic consequences for efficiency both in processing and in grammar building. In this paper I report on three experiments in making systematic modifications to a broad-coverage grammar of English in order to gain efficiency without loss of linguistic elegance. While the experiments are to some degree both platform-dependant and theory-bound, the kinds of modifications reported should be applicable to any unification-based grammar which makes use of types. The results make a strong case for a more visible role for the linguist in the collaborative effort to achieve greater processing efficiency.","Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .","['There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts .', 'Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994).', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']",0,"['Stanford University is developing the English Resource Grammar , an HPSG grammar for English , as a part of the Linguistic Grammars Online ( LinGO ) project ( #AUTHOR_TAG ) .', 'Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000).']"
CC1306,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,evolution of the xtag system,"['Christy Doran', 'Beth Ann Hockey', 'Anoop Sarkar', 'B Srinivas', 'Fei Xia']",introduction,,"There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environment ( Sarkar and Wintner , 1999 ; #AUTHOR_TAG ; Makino et al. , 1998 ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']"
CC1307,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,an hpsg parser with cfg filtering,"['Kentaro Torisawa', 'Kenji Nishida', 'Yusuke Miyao', 'Jun’ichi Tsujii']",experiments,,"LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .","['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']",0,"['LiLFeS is one of the fastest inference engines for processing feature structure logic , and efficient HPSG parsers have already been built on this system ( Nishida et al. , 1999 ; #AUTHOR_TAG ) .']"
CC1308,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,efficient ltag parsing using hpsg parsers,"['Naoki Yoshinaga', 'Yusuke Miyao', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",experiments,,Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .,"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']",0,"['Table 2 shows the average parsing time with the LTAG and HPSG parsers.', 'In Table 2, lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2).', 'TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2).', 'Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser.', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.', 'Another paper ( #AUTHOR_TAG ) describes the detailed analysis on the factor of the difference of parsing performance .']"
CC1309,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,building a large annotated corpus of english the penn treebank computational linguistics,"['Mitchell Marcus', 'Beatrice Santorini', 'Mary Ann Marcinkiewicz']",experiments,,The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .,"['The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .', 'This result empirically attested the strong equivalence of our algorithm.']",5,"['We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus ( #AUTHOR_TAG ) 6 ( the average length is 6.32 words ) .']"
CC1310,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,lilfes — towards a practical hpsg parsers,"['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",experiments,,The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .,"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']",5,"['The RenTAL system is implemented in LiLFeS ( #AUTHOR_TAG ) 2 .', 'LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999;.', 'We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English.', 'The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words).', 'This result empirically attested the strong equivalence of our algorithm.']"
CC1311,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a study of tree adjoining grammars,['K Vijay-Shanker'],introduction,"Constrained grammatical system have been the object of study in computational linguistics over the last few years, both with respect to their linguistic adequacy and their computational properties. A Tree Adjoining Grammar (TAG) is a tree rewriting system whose linguistic relevance has been extensively studied. A key property of these systems is that a TAG factors recursion from the co-occurrence restrictions. In this thesis, we study some mathematical properties of TAG u27s. We show that TAG u27s have several interesting properties and are a natural generalization of Context Free Grammars. We show the equivalence of the classes of languages generated by TAG u27s with those generated by Head Grammars and a linear version of Indexed Grammars, which have been studied for their linguistic applicability. We define the embedded pushdown automaton, an extention of the pushdown automaton, and prove that they are equivalent to TAG u27s. We show that the class of Tree Adjoining Languages form a substitution closed abstract family of languages, and that each Tree Adjoining Language is a semilinear language. We show that a TAG can be parsed in polynomial time by adapting the Cocke-Kasami-Younger algorithm for CFL u27s. Feature structures, essentially a set of attribute value pairs, have been used in computational linguistics to make statements of equality to capture some linguistic phenomena such as subcategorization and agreement. We embed TAG u27s in a feature structure based framework. We show that the resulting system has several advantages over TAG u27s. We give a mathematical model of this system based on the logical calculus developed by Rounds, Kasper, and Manaster-Ramer. Finally, we propose a restriction of this system and show how parsing of such a system can be done efficiently","This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .","['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .', 'The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar .', 'Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications.', 'Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch.']",0,"['This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar ( FB-LTAG1 ) ( #AUTHOR_TAG ; Vijay-Shanker and Joshi , 1988 ) and Head-Driven Phrase Structure Grammar ( HPSG ) ( Pollard and Sag , 1994 ) by a method of grammar conversion .']"
CC1312,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,lilfes — towards a practical hpsg parsers,"['Takaki Makino', 'Minoru Yoshida', 'Kentaro Torisawa', 'Jun’ichi Tsujii']",introduction,,"There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .', 'These works are re-stricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development ( Sarkar and Wintner , 1999 ; Doran et al. , 2000 ; #AUTHOR_TAG ) .']"
CC1313,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,twostep tag parsing revisited,"['Peter Poller', 'Tilman Becker']",introduction,"Based on the work in (Poller, 1994) and a minor assumption about a normal form for TAGs, we present a highly simplified version of the twostep parsing approach for TAGs which allows for a much easier analysis of run-time and space complexity. It also snggests how restrictions on the grammars might result in improvements in run-time complexity. The main advantage of a two-step parsing system shows in practical applications like Verbmobil (Bub et al., 1997) where the parser must look at multiple hypotheses supplied by a speech recognizer (encoded in a word hypotheses lattice) and filter out illicit hypotheses as early as possible. The first (context-free) step of our parser filters out some illicit hypotheses fast (O(n3 )); the constructed parsing matrix is then reused for the second step, the complete (O(n6 )) TAG parse.","There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).","['Our concern is, however, not limited to the sharing of grammars and lexicons.', 'Strongly equivalent grammars enable the sharing of ideas developed in each formalism.', 'There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).', 'These works are restricted to each closed community, and the relation between them is not well discussed.', 'Investigating the relation will be apparently valuable for both communities.']",0,"['There have been many studies on parsing techniques ( #AUTHOR_TAG ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environment (Sarkar and Wintner, 1999; Doran et al., 2000; Makino et al., 1998).']"
CC1314,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,grammar conversion from fbltag to hpsg,"['Naoki Yoshinaga', 'Yusuke Miyao']",,,The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .,['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .'],0,['The grammar conversion from LTAG to HPSG ( #AUTHOR_TAG ) is the core portion of the RenTAL system .']
CC1315,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],introduction,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .","['Figure 4: Adjunction tions called substitution and adjunction.', 'Elementary trees are classified into two types, initial trees and auxiliary trees (Figure 2).', 'An elementary tree has at least one leaf node labeled with a terminal symbol called an anchor (marked with ¥).', 'In an auxiliary tree, one leaf node is labeled with the same symbol as the root node and is specially marked as a foot node (marked with £).', 'In an elementary tree, leaf nodes with the exception of anchors and the foot node are called substitution nodes (marked with ).', 'Substitution replaces a substitution node with another initial tree (Figure 3).', 'Adjunction grafts an auxiliary tree with the root node and foot node labeled Ü onto an internal node of another tree with the same symbol Ü (Figure 4).', 'FB-LTAG (Vijay-Shanker, 1987;Vijay-Shanker and Joshi, 1988) is an extension of the LTAG formalism.', 'In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .', 'The XTAG group (Doran et al., 2000) at the University of Pennsylvania is also developing Korean, Chinese, and Hindi grammars.', 'Development of a large-scale French grammar (Abeillé and Candito, 2000) has also started at the University of Pennsylvania and is expanded at University of Paris 7.']",0,"['In FB-LTAG, each node in the elementary trees has a feature structure, containing grammatical constraints on the node.', 'Figure 5 shows a result of LTAG analysis, which is described not There are several grammars developed in the FB-LTAG formalism , including the XTAG English grammar , a large-scale grammar for English ( The XTAG Research #AUTHOR_TAG ) .']"
CC1316,W01-1510,Resource sharing among HPSG and LTAG communities by a method of grammar conversion from FB-LTAG to HPSG,a lexicalized tree adjoining grammar for english,['The XTAG Research Group'],experiments,"This paper presents a sizable grammar for English written in the Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeille, Joshi 1988) and feature-based (VijayShankar, Joshi 1988). In this paper, we describe a wide range of phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree's structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us to take full advantage of the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double whquestions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeille (1988c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG'S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeille and Schabes (1989), we describe how to deal with semantic non compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-24. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/527 A Lexicalized Tree Adjoining Grammar For English MS-CIS-90-24 LINC LAB 170 Anne Abeillh Kathleen Bishop Sharon Cote Yves Schabes Department of Computer and Information Science School of Engineering and Applied Science University of Pennsylvania Philadelphia, PA 19104-6389","We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .","['In this paper, we show that the strongly equivalent grammars enable the sharing of ""parsing techniques"", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities.', 'We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .', 'A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser .', 'This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing.', 'We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing.']",5,"['We applied our system to the XTAG English grammar ( The XTAG Research #AUTHOR_TAG ) 3 , which is a large-scale FB-LTAG grammar for English .']"
CC1317,W02-0309,Biomedical text retrieval in languages with a complex morphology,how effective is suffixing,['D Harman'],conclusion,"s and titles from the Cranfield collection (with 225 queries and 1400 documents), comprised the major test collection for this study. The Medlars collection (30 queries and 1033 documents), and the CACM collection (64 queries and 3204 documents) were used to provide information about the variation of stemming performance across different subject areas and test collections. In addition to the standard recall/precision measures, with SMART system averaging (Salton, 1971), several methods more suited to an interactive retrieval environment were adopted. The interactive environment returns lists of the top ranked documents, and allows the users to scan titles of a group of documents a screenful at a time, so that the ranking of individual documents within the screenful is not as important as the total number of relevant titles within a screen. Furthermore, the number of relevant documents in the first few screens is far more important for the user than the number of relevant in the last screenfuls. Three measures were selected which evaluate performance at given rank cutoff points, such as those corresponding to a screenful of document titles. The first measure, the E measure (Van Rijsbergen, 1979), is a weighted combination of recall and precision that evaluates a set of retrieved documents at a given cutoff, ignoring the ranking within that set. The measure may have weights of 0.5, 1.0, and 2.0 which correspond, respectively, to attaching half the importance to recall as to precision, equal importance to both, and double importance to recall. A lower E value indicates a more effective performance. A second measure, the total number of relevant documents retrieved by a given cutoff, was also calculated. Cutoffs of 10 and 30 documents were used, with ten reflecting a minimum number a user might be expected to TABLE 2. Retrieval performance for Cranfteld 225. scan, and 30 being an assumed upper limit of what a user would scan before query modification. The third measure applicable to the interactive environment is the number of queries that retrieve no relevant documents by the given cutoff. This measure is important because many types of query modification techniques, such as relevance feedback, require relevant documents to be in the retrieved set to work well. These measures were all used in Croft (1983) as complementary measures to the standard recall/precision evaluation.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( #AUTHOR_TAG ; Krovetz , 1993 ; Hull , 1996 ) .']"
CC1318,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic parsing of medical expressions,"['R Baud', 'C Lovis', 'A-M Rassinoux', 'J-R Scherrer']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']",0,"['The efforts required for performing morphological analysis vary from language to language.', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; #AUTHOR_TAG ) .']"
CC1319,W02-0309,Biomedical text retrieval in languages with a complex morphology,development of a stemming algorithm,['J Lovins'],introduction,"A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods.","mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( #AUTHOR_TAG ; Porter , 1980 ) demonstrably improve retrieval performance .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.']"
CC1320,W02-0309,Biomedical text retrieval in languages with a complex morphology,stemming algorithms a case study for detailed evaluation,['D A Hull'],conclusion,"The majority of information retrieval experiments are evaluated by measures such as average precision and average recall. Fundamental decisions about the superiority of one retrieval technique over another are made solely on the basis of these measures. We claim that average performance figures need to be validated with a careful statistical analysis and that there is a great deal of additional information that can be uncovered by looking closely at the results of individual queries. This article is a case study of stemming algorithms which describes a number of novel approaches to evaluation and demonstrates their value. (c) 1996 John Wiley & Sons, Inc.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; #AUTHOR_TAG ) .']"
CC1321,W02-0309,Biomedical text retrieval in languages with a complex morphology,an algorithm for suffix stripping,['M Porter'],introduction,"Purpose - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value - The piece provides a useful historical document on information retrieval.","mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'mers ( Lovins , 1968 ; #AUTHOR_TAG ) demonstrably improve retrieval performance .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.']"
CC1322,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing stemming as recall enhancement,"['W Kraaij', 'R Pohlmann']",introduction,"Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision.","Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]",0,"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( Choueka , 1990 ; J Â¨ appinen and Niemist Â¨ o , 1988 ; #AUTHOR_TAG ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure']).""]"
CC1323,W02-0309,Biomedical text retrieval in languages with a complex morphology,the use of morphosemantic regularities in the medical vocabulary for automatic lexical coding methods ofinformation in,['S Wolff'],introduction,The use of morphosemantic regularities in the medical vocabulary for automatic lexical coding. -,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"[""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; #AUTHOR_TAG ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1324,W02-0309,Biomedical text retrieval in languages with a complex morphology,an algorithm for suffix stripping,['M Porter'],conclusion,"Purpose - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. Design/methodology/approach - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Findings - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. Originality/value - The piece provides a useful historical document on information retrieval.","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; #AUTHOR_TAG ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']"
CC1325,W02-0309,Biomedical text retrieval in languages with a complex morphology,automated coding of diagnoses three methods compared,"['P Franz', 'A Zaiss', 'S Schulz', 'U Hahn', 'R Klar']",experiments,"In Germany, new legal requirements have raised the importance of the accurate encoding of admission and discharge diseases for in- and outpatients. In response to emerging needs for computer-supported tools we examined three methods for automated coding of German-language free-text diagnosis phrases. We compared a language-independent lexicon-free n-gram approach with one which uses a dictionary of medical morphemes and refines the query by a mapping to SNOMED codes. Both techniques produced a ranked output of possible diagnoses within a vector space framework for retrieval. The results did not reveal any significant difference: The correct diagnosis was found in approximately 40% for three-digit codes, and 30% for four-digit codes. The lexicon-based method was then modified by substituting the vector space ranking by a heuristic approach that capitalizes on the semantic structure of SNOMED, thus raising the number of correct diagnoses significantly (approximately 50% for three-digit codes, and 40% for four-digit codes). As a result, we claim that lexicon-based retrieval methods do not perform better than the lexicon-free ones, unless conceptual knowledge is added.",Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .,"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']",1,"['), but at high ones its precision decreases almost dramatically.', 'Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( #AUTHOR_TAG ) ) one can not really recommend this method .']"
CC1326,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphologic analysis of compound words,['F Wingert'],introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; Norton and Pacak , 1983 ; Wolff , 1984 ; #AUTHOR_TAG ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1327,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing stemming as recall enhancement,"['W Kraaij', 'R Pohlmann']",conclusion,"Previous research on stemming has shown both positive and negative effects on retrieval performance. This paper describes an experiment in which several linguistic and non-linguistic stemmers are evaluated on a Dutch test collection. Experiments especially focus on the measurement of Recall. Results show that linguistic stemming restricted to inflection yields a significant improvement over full linguistic and non-linquistic stemming, both in average Precision and R-Recall. Best results are obtained with a linguistic stemmer which is enhanced with compound analysis. This version has a significantly better Recall than a system without stemming, without a significant deterioration of Precision.","Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']",0,"['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; #AUTHOR_TAG ; Tzoukermann et al. , 1997 ) .']"
CC1328,W02-0309,Biomedical text retrieval in languages with a complex morphology,automatic text processing the transformation analysis and retrieval ofinformation by computer,['Gerard Salton'],experiments,,"The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .","['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']",5,"['For unbiased evaluation of our approach, we used a home-grown search engine (implemented in the PYTHON script language).', 'It crawls text/HTML files, produces an inverted file index, and assigns salience weights to terms and documents based on a simple tf-idf metric.', 'The retrieval process relies on the vector space model ( #AUTHOR_TAG ) , with the cosine measure expressing the similarity between a query and a document .', 'The search engine produces a ranked output of documents.']"
CC1329,W02-0309,Biomedical text retrieval in languages with a complex morphology,development of a stemming algorithm,['J Lovins'],conclusion,"A stemming algorithm, a procedure to reduce all words with the same stem to a common form, is useful in many areas of computational linguistics and information-retrieval work. While the form of the algorithm varies with its application, certain linguistic problems are common to any stemming procedure. As a basis for evaluation of previous attempts to deal with these problems, this paper first discusses the theoretical and practical attributes of stemming algorithms. Then a new version of a context-sensitive, longest-match stemming algorithm for English is proposed; though developed for use in a library information transfer system, it is of general application. A major linguistic problem in stemming, variation in spelling of stems, is discussed in some detail and several feasible programmed solutions are outlined, along with sample results of one of these methods.","There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( #AUTHOR_TAG ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; Krovetz , 1993 ; Hull , 1996 ) .']"
CC1330,W02-0309,Biomedical text retrieval in languages with a complex morphology,the use of morphosemantic regularities in the medical vocabulary for automatic lexical coding methods ofinformation in,['S Wolff'],introduction,The use of morphosemantic regularities in the medical vocabulary for automatic lexical coding. -,"While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .","['Furthermore, medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language (e.g., German), often referred to as neo-classical compounding (Mc-Cray et al., 1988).', 'While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']",0,"['While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( #AUTHOR_TAG ) .']"
CC1331,W02-0309,Biomedical text retrieval in languages with a complex morphology,medical subject headings,['NLM'],experiments,"Automatically assigning MeSH (Medical Subject Headings) to articles is an active research topic. Recent work demonstrated the feasibility of improving the existing automated Medical Text Indexer (MTI) system, developed at the National Library of Medicine (NLM). Encouraged by this work, we propose a novel data-driven approach that uses semantic distances in the MeSH ontology for automated MeSH assignment. Specifically, we developed a graphical model to propagate belief through a citation network to provide robust MeSH main heading (MH) recommendation. Our preliminary results indicate that this approach can reach high Mean Average Precision (MAP) in some scenarios","This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .","['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'The gain is still not overwhelming.', ""With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'Zäkum ', 'Cäkum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum')."", 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf.', 'Section 3).', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'However, synonym mapping is still incomplete in the current state of our subword dictionary.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .', 'Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995).']",3,"['For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( #AUTHOR_TAG ) ) are incorporated into our system .']"
CC1332,W02-0309,Biomedical text retrieval in languages with a complex morphology,viewing morphology as an inference process,['R Krovetz'],conclusion,"AbstractMorphology is the area of linguistics concerned with the internal structure of words. Information retrieval has generally not paid much attention to word structure, other than to account for some of the variability in word forms via the use of stemmers. We report on our experiments to determine the importance of morphology, and the effect that it has on performance. We found that grouping morphological variants makes a significant improvement in retrieval performance. Improvements are seen by grouping inflectional as well as derivational variants. We also found that performance was enhanced by recognizing lexical phrases. We describe the interaction between morphology and lexical ambiguity, and how resolving that ambiguity will lead to further improvements in performance","There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .","['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories (Krovetz, 1993;Kraaij and Pohlmann, 1996;Tzoukermann et al., 1997).']",0,"['There has been some controversy , at least for simple stemmers ( Lovins , 1968 ; Porter , 1980 ) , about the effectiveness of morphological analysis for document retrieval ( Harman , 1991 ; #AUTHOR_TAG ; Hull , 1996 ) .']"
CC1333,W02-0309,Biomedical text retrieval in languages with a complex morphology,the effectiveness of stemming for natural language access to slovene textual data,"['M Popovic', 'P Willett']",introduction,"There have been several studies of the use of stemming algorithms for conflating morphological variants in freetext retrieval systems. Comparison of stemmed and nonconflated searches suggests that there are no significant increases in the effectiveness of retrieval when stemming is applied to English-language documents and queries. This article reports the use of stemming on Slovene-language documents and queries, and demonstrates that the use of an appropriate stemming algorithm results in a large, and statistically significant, increase in retrieval effectiveness when compared with nonconflated processing; similar comments apply to the use of manual, right-hand truncation. A comparison is made with stemming of English versions of the same documents and queries and it is concluded that the effectiveness of a stemming algorithm is determined by the morphological complexity of the language that it is designed to process.","This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .","['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998).']",0,"['The efforts required for performing morphologi- cal analysis vary from language to language.', 'For English, known for its limited number of inflection patterns, lexicon-free general-purpose stemmers (Lovins, 1968; Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; #AUTHOR_TAG ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; Pirkola , 2001 ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980; Norton and Pacak, 1983; Wolff, 1984; Wingert, 1985; Dujols et al., 1991; Baud et al., 1998).']"
CC1334,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic analysis of compound word forms denoting surgical procedures methods ofinformation in medicine,"['L Norton', 'M Pacak']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( Pacak et al. , 1980 ; #AUTHOR_TAG ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1335,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphosemantic analysis of itis forms in medical language,"['M Pacak', 'L Norton', 'G Dunham']",introduction,,"From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jäppinen and Niemistö, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekçioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']",0,"['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '! ' denotes the string concatenation operator."", 'This has been reported for other languages, too, dependent on the generality of the chosen approach (Jappinen and Niemisto, 1988;Choueka, 1990;Popovic and Willett, 1992;Ekmekcioglu et al., 1995;Hedlund et al., 2001;Pirkola, 2001).', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'From an IR view , a lot of specialized research has already been carried out for medical applications , with emphasis on the lexico-semantic aspects of dederivation and decomposition ( #AUTHOR_TAG ; Norton and Pacak , 1983 ; Wolff , 1984 ; Wingert , 1985 ; Dujols et al. , 1991 ; Baud et al. , 1998 ) .']"
CC1336,W02-0309,Biomedical text retrieval in languages with a complex morphology,towards new measures of information retrieval evaluation,"['W Hersh', 'D Elliot', 'D Hickam', 'S Wolf', 'A Molnar', 'C Leichtenstien']",experiments,"All of the methods currently used to assess information retrieval (IR) systems have limitations in their ability to measure how well users are able to acquire information. We utilized a new approach to assessing information obtained, based on a short-answer test given to senior medical students. Students took the ten-question test and then searched one of two IR systems on the five questions for which they were least certain of their answer Our results showed that pre-searching scores on the test were low but that searching yielded a high proportion of answers with both systems. These methods are able to measure information obtained, and will be used in subsequent studies to assess differences among IR systems.","Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .","['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'The gain is still not overwhelming.', ""With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'Zäkum ', 'Cäkum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum')."", 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf.', 'Section 3).', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'However, synonym mapping is still incomplete in the current state of our subword dictionary.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001)) are incorporated into our system.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']",3,"['Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model.', 'For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers.', 'In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization.', 'However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider-  Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far.', 'A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing.', 'We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5).', 'It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents.', 'Alternatively , we may think of user-centered comparative studies ( #AUTHOR_TAG ) .']"
CC1337,W02-0309,Biomedical text retrieval in languages with a complex morphology,effective use of natural language processing techniques for automatic conflation of multiword terms the role of derivational morphology part of speech tagging and shallow parsing,"['E Tzoukermann', 'J Klavans', 'C Jacquemin']",conclusion,,"Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .","['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'The key for quality improvement seems to be rooted mainly in the presence or absence of some form of dictionary.', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']",0,"['There has been some controversy, at least for simple stemmers (Lovins, 1968;Porter, 1980), about the effectiveness of morphological analysis for document retrieval (Harman, 1991;Krovetz, 1993;Hull, 1996).', 'Empirical evidence has been brought forward that inflectional and/or derivational stemmers augmented by dictionaries indeed perform substantially better than those without access to such lexical repositories ( Krovetz , 1993 ; Kraaij and Pohlmann , 1996 ; #AUTHOR_TAG ) .']"
CC1338,W02-0309,Biomedical text retrieval in languages with a complex morphology,the contribution of morphological knowledge to french mesh mapping for information retrieval,"['P Zweigenbaum', 'S Darmoni', 'N Grabar']",introduction,"MeSH-indexed Internet health directories must provide a mapping from natural language queries to MeSH terms so that both health professionals and the general public can query their contents. We describe here the design of lexical knowledge bases for mapping French expressions to MeSH terms, and the initial evaluation of their contribution to Doc'CISMeF, the search tool of a MeSH-indexed directory of French-language medical Internet resources. The observed trend is in favor of the use of morphological knowledge as a moderate (approximately 5%) but effective factor for improving query to term mapping capabilities.","Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .","['While one may argue that single-word compounds are quite rare in English (which is not the case in the medical domain either), this is certainly not true for German and other basically agglutinative languages known for excessive single-word nominal compounding.', 'This problem becomes even more pressing for technical sublanguages, such as medical German (e.g., �Blut druck mess gera__t� translates to �device for measuring blood pressure�).', 'The problem one faces from an IR point of view is that besides fairly standardized nominal compounds, which already form a regular part of the sublanguage proper, a myriad of ad hoc compounds are formed on the fly which cannot be anticipated when formulating a retrieval query though they appear in relevant documents.', 'Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']",0,"['Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( #AUTHOR_TAG ) , turns out to be infeasible , at least for German and related languages .']"
CC1339,W02-0309,Biomedical text retrieval in languages with a complex morphology,morphological typology of languages for ir,['A Pirkola'],introduction,"This paper presents a morphological classification of languages from the IR perspective. Linguistic typology research has shown that the morphological complexity of every language in the world can be described by two variables, index of synthesis and index of fusion. These variables provide a theoretical basis for IR research handling morphological issues. A common theoretical framework is needed in particular because of the increasing significance of cross-language retrieval research and CLIR systems processing different languages. The paper elaborates the linguistic morphological typology for the purposes of IR research. It studies how the indexes of synthesis and fusion could be used as practical tools in mono- and cross-lingual IR research. The need for semantic and syntactic typologies is discussed. The paper also reviews studies made in different languages on the effects of morphology and stemming in IR.","This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .","['The efforts required for performing morphological analysis vary from language to language.', ""For English, known for its limited number of inflection patterns, lexicon-free general-purpose stem-1 '¡ ' denotes the string concatenation operator."", 'mers (Lovins, 1968;Porter, 1980) demonstrably improve retrieval performance.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.', 'This is particularly true for the medical domain.', 'From an IR view, a lot of specialized research has already been carried out for medical applications, with emphasis on the lexico-semantic aspects of dederivation and decomposition (Pacak et al., 1980;Norton and Pacak, 1983;Wolff, 1984;Wingert, 1985;Dujols et al., 1991;Baud et al., 1998).']",0,"['The efforts required for performing morphological analysis vary from language to language.', 'This has been reported for other languages , too , dependent on the generality of the chosen approach ( J Â¨ appinen and Niemist Â¨ o , 1988 ; Choueka , 1990 ; Popovic and Willett , 1992 ; Ekmekc Â¸ ioglu et al. , 1995 ; Hedlund et al. , 2001 ; #AUTHOR_TAG ) .', 'When it comes to a broader scope of morphological analysis, including derivation and composition, even for the English language only restricted, domain-specific algorithms exist.']"
CC1340,W02-0309,Biomedical text retrieval in languages with a complex morphology,responsa an operational fulltext retrieval system with linguistic components for large corpora,['Y Choueka'],introduction,,"Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .","['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .', ""In order to cope with such variation, morphological analysis is concerned with the reverse processing of inflection (e.g., 'search ed', 'search ing') 1 , derivation (e.g., 'search er' or 'search able') and composition (e.g., German 'Blut hoch druck ' ['high blood pressure'])."", ""The goal is to map all occurring morphological variants to some canonical base forme.g., 'search' in the examples from above.""]",0,"['Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval ( IR ) system ( #AUTHOR_TAG ; J Â¨ appinen and Niemist Â¨ o , 1988 ; Kraaij and Pohlmann , 1996 ) , since they preclude a direct match between the search term proper and its morphological variants in the documents to be retrieved .']"
CC1341,W02-0309,Biomedical text retrieval in languages with a complex morphology,the semantic structure of neoclassical compounds,"['A McCray', 'A Browne', 'D Moore']",introduction,,"Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .","['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .', 'While this is simply irrelevant for general-purpose morphological analyzers, dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting (Wolff, 1984).']",0,"['Furthermore , medical terminology is characterized by a typical mix of Latin and Greek roots with the corresponding host language ( e.g. , German ) , often referred to as neo-classical compounding ( #AUTHOR_TAG ) .']"
CC1342,W02-1601,A synchronization structure of SSTC and its applications in machine translation,finding structural correspondences from bilingual parsed corpus for corpusbased translation,"['H Watanabe', 'S Kurohashi', 'E Aramaki']",,"In this paper, we describe a system and methods for finding structural correspondences from the paired dependency structures of a source sentence and its translation in a target language. The system we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences. We have also developed a GUI system with which a user can check and correct the correspondences retrieved by the system. These structural correspondences will be used as raw translation patterns in a corpus-based translation system.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( #AUTHOR_TAG ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1343,W02-1601,A synchronization structure of SSTC and its applications in machine translation,examplebased machine translation based on the synchronous sstc annotation schema,"['M H Al-Adhaileh', 'E K Tang']",,"In this paper, we describe an Example-Based Machine Translation (EBMT) system for English-Malay translation. Our approach is an example-based approach which relies sorely on example translations kept in a Bilingual Knowledge Bank (BKB). In our approach, a flexible annotation schema called Structured String-Tree Correspondence (SSTC) is used to annotate both the source and target sentences of a translation pair. Each SSTC describes a sentence, a representation tree as well as the correspondences between substrings in the sentence and subtrees in the representation tree. With both the source and target SSTCs established, a translation example in the BKB can then be represented effectively in terms of a pair of synchronous SSTCs. In the process of translation, we first try to build the representation tree for the source sentence (English) based on the example-based parsing algorithm as presented in [1]. By referring to the resultant source parse tree, we then proceed to synthesis the target sentence (Malay) based on the target SSTCs as pointed to by the synchronous SSTCs which encode the relationship between source and target SSTCs.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( #AUTHOR_TAG ) .']"
CC1344,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",introduction,,"In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .","['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.', 'non-projective correspondence).']",0,"['In this paper , a flexible annotation schema called Structured String-Tree Correspondence ( SSTC ) ( #AUTHOR_TAG ) will be introduced to capture a natural language text , its corresponding abstract linguistic representation and the mapping ( correspondence ) between these two .', 'The correspondence between the string and its associated representation tree structure is defined in terms of the sub-correspondence between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation.', 'Such correspondence is defined in a way that is able to handle some non-standard cases (e.g.', 'non-projective correspondence).']"
CC1345,W02-1601,A synchronization structure of SSTC and its applications in machine translation,handling crossed dependencies with the stcg,"['E K Tang', 'Y Zaharin']",,,"These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .","['The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be nonprojective (Boitet & Zaharin, 1988).', 'These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']",0,"['These features are very much desired in the design of an annotation scheme , in particular for the treatment of linguistic phenomena , which are non-standard , e.g. crossed dependencies ( #AUTHOR_TAG ) .', 'crossed dependencies (Tang & Zaharin, 1995).']"
CC1346,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .","['In this section, we stress on the fact that in order to describe Natural Language (NL) in a natural manner, three distinct components need to be expressed by the linguistic formalisms; namely, the text, its corresponding abstract linguistic representation and the mapping (correspondence) between these two.', 'Actually, NL is not only a correspondence between different representation levels, as stressed by MTT postulates, but also a sub-correspondence between them.', 'For instance, between the string in a language and its representation tree structure, it is important to specify the sub-correspondences between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation in NLP.', 'It is well known that many linguistic constructions are not projective (e.g.', 'scrambling, cross serial dependencies, etc.).', 'Hence, it is very much desired to define the correspondence in a way to be able to handle the non-standard cases (e.g.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']",0,"['Actually, NL is not only a correspondence between different representation levels, as stressed by MTT postulates, but also a sub-correspondence between them.', 'For instance, between the string in a language and its representation tree structure, it is important to specify the sub-correspondences between parts of the string (substrings) and parts of the tree structure (subtrees), which can be interpreted for both analysis and generation in NLP.', 'non-projective correspondence), see Figure 1.', 'Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in #AUTHOR_TAG to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .']"
CC1347,W02-1601,A synchronization structure of SSTC and its applications in machine translation,converting a bilingual dictionary into a bilingual knowledge bank based on the synchronous sstc annotation schema,"['M H Al-Adhaileh', 'E K Tang']",,"In this paper, we would like to present an approach to construct a huge Bilingual Knowledge Bank (BKB) from an English Malay bilingual dictionary based on the idea of synchronous Structured String-Tree Correspondence (SSTC). The SSTC is a general structure that can associate an arbitrary tree structure to string in a language as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be non-projective. With this structure, we are able to match linguistic units at different inter levels of the structure (i.e. define the correspondence between substrings in the sentence, nodes in the tree, subtrees in the tree and sub-correspondences in the SSTC). This flexibility makes synchronous SSTC very well suited for the construction of a Bilingual Knowledge Bank we need for the English-Malay MT application.",#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .,"['However, what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", 'S-SSTC very well suited for the construction of a BKB, which is needed for the EBMT applications.', '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']",0,"['However, what has so far been lacking is a schema or a framework to annotate and express such extracted lexical and structural correspondences in a flexible and powerful manner.', ""The proposed S-SSTC annotation schema can fulfill this need, and it is flexible enough to handle different type of relations that may happen between different languages' structures."", '#AUTHOR_TAG presented an approach for constructing a BKB based on the S-SSTC .']"
CC1348,W02-1601,A synchronization structure of SSTC and its applications in machine translation,chartbased transfer rule application in machine translation,"['A Meyers', 'M Kosaka', 'R Grishman']",,"Transfer-based Machine Translation systems require a procedure for choosing the set of transfer rules for generating a target language translation from a given source language sentence. In an MT system with many competing transfer rules, choosing the best set of transfer rules for translation may involve the evaluation of an explosive number of competing sets. We propose a solution to this problem based on current best-first chart parsing algorithms.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( #AUTHOR_TAG ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1349,W02-1601,A synchronization structure of SSTC and its applications in machine translation,nonisomorphic synchronous tags,"['K Harbusch', 'P Poller']",,"Synchronous tree{adjoining grammars (S{TAGs) combine two standard tree{adjoining grammars (TAGs), e.g., for language transduction in Machine Translation (MT). Recent advances show that the restriction to isomorphic derivation trees (IS{TAGs) ensures eecient transduction because only tree{adjoining languages can be formed in each component. As a result IS{TAGs only allow for  triv-ial"" transfer rules, due to the fact that only isomorphic derivations can be synchronized. This means that only very similar constructions in the two languages can be translated into each other. To overcome these limitations and provide a way of realizing more complex translation phenomena, this paper introduces a new formalism, the dynamic link synchronous tree{adjoining grammars or DLS{TAGs. This formalism allows for the synchronization of non{isomorphic derivation trees by introducing the new concept of dynamic links. DLS{TAGs are more powerful than IS-TAGs. More precisely speaking, DLS{TAGs allow for the formulation of a non{tree{adjoining language in one of the two components. This makes the translation problem more diicult but not untractable as outlined in this paper. However , there remain non{isomorphic translation phenomena which cannot be handled by DLS-TAGs as we also show in this paper.","It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .","['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']",0,"['The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeille et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( Shieber , 1994 ) , ( #AUTHOR_TAG ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']"
CC1350,W02-1601,A synchronization structure of SSTC and its applications in machine translation,what is a natural language and how to describe it meaningtext approaches in contrast with generative approaches,['S Kahane'],,"The paper expounds the general conceptions of the Meaning- Text theory about what a natural language is and how it must be de- scribed. In a second part, a formalization of these conceptions - the transductive grammars - is proposed and compared with generative ap- proaches.","From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .","['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .', 'The MTT point of view, even if it has been introduced in different formulations, is more or less accepted by the whole linguistic community.']",0,"['From the Meaning-Text Theory ( MTT ) 1 point of view , Natural Language ( NL ) is considered as a correspondence between meanings and texts ( #AUTHOR_TAG ) .']"
CC1351,W02-1601,A synchronization structure of SSTC and its applications in machine translation,towards memorybased translation,"['S Sato', 'M Nagao']",,,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( #AUTHOR_TAG ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1352,W02-1601,A synchronization structure of SSTC and its applications in machine translation,restricting the weak generative capacity of synchronous tree adjoining grammar,['S Shieber'],,,"In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .","['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .', 'Shieber (1994) cases).', 'Due to lack of space we will only brief on some of these non-standard cases without going into the details.']",0,"['As mentioned earlier, there are some non-standard phenomena exist between different languages, that cause challenges for synchronized formalisms.', 'In this Section , we will describe some example cases , which are drawn from the problem of using synchronous formalisms to define translations between languages ( e.g. #AUTHOR_TAG cases ) .']"
CC1353,W02-1601,A synchronization structure of SSTC and its applications in machine translation,structural matching of parallel texts,"['Y Matsumoto', 'H Ishimoto', 'T Utsuro']",,"This paper describes a nethod for finding structural matching between parallel sentences of two lauguages, (such as Japanese and English). Par- allel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages. Syntactic ambiguities are resolved simultaneously in the matching process. The results serve as a useful source for extracting linguistic and lexical knowledge","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( #AUTHOR_TAG ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1354,W02-1601,A synchronization structure of SSTC and its applications in machine translation,synchronous models of language,"['O Rambow', 'G Satta']",,"In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.Comment: 8 pages uuencoded gzipped ps fil","Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .","['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']",0,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) , such as the relation between syntax and semantic .']"
CC1355,W02-1601,A synchronization structure of SSTC and its applications in machine translation,pilot implementation of a bilingual knowledge bank,"['V Sadler', 'R Vendelmans']",,"A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora, one being a  translation of the other, in which translation units are  cross-codexl between the corpora. A pilot implementation  is described for a corpus of some 20,000 words  each in English, French and Esperanto which has been cross-coded between English and Esperanto and &apos;between Esperanto and French. The aim is to develop a corpus-based general-purpose knowledge sontee for applicatious in machine translation and computer-  aided translation","For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus","[') are governed by the following constraints:  .', 'This means allowing one-to-one, one-to-many and many-to-many, but the mappings do not overlap.', 'Note that these constraints can be used to license only the linguistically meaningful synchronous correspondences between the two SSTCs of the S-SSTC (i.e. between the two languages).', ""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]",0,"[""For instance, when building translation units in EBMT approaches ( Richardson et al. , 2001 ) , ( Aramaki , 2001 ) , ( AlAdhaileh & Tang , 1999 ) , ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , etc. , where S-SSTC can be used to represent the entries of the BKB or when S-SSTC used as an annotation schema to find the translation correspondences (lexical and structural correspondences) for transferrules' extraction from parallel parsed corpus""]"
CC1356,W02-1601,A synchronization structure of SSTC and its applications in machine translation,a bestfirst algorithm for automatic extraction of transfer mappings from bilingual corpora,"['A Menezes', 'S Richardson']",,"Translation systems that automatically extract transfer mappings (rules or examples) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings. We describe an algorithm that uses a best-first strategy and a small alignment grammar to significantly improve the quality of the transfer mappings extracted. For each mapping, frequencies are computed and sufficient context is retained to distinguish competing mappings during translation. Variants of the algorithm are run against a corpus containing 200K sentence pairs and evaluated based on the quality of resulting translations.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e.', 'Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( #AUTHOR_TAG ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1357,W02-1601,A synchronization structure of SSTC and its applications in machine translation,synchronous models of language,"['O Rambow', 'G Satta']",introduction,"In synchronous rewriting, the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings. We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems. We also prove some interesting formal/computational properties of our system.Comment: 8 pages uuencoded gzipped ps fil",Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']",0,"['There is now a consensus about the fact that natural language should be described as correspondences between different levels of representation.', 'Much of theoretical linguistics can be formulated in a very natural manner as stating correspondences ( translations ) between layers of representation structures ( #AUTHOR_TAG ) .']"
CC1358,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"For more details on the proprieties of SSTC , see #AUTHOR_TAG .","['The case depicted in Figure 2, describes how the SSTC structure treats some non-standard linguistic phenomena.', 'The particle ""up"" is featurised into the verb ""pick"" and in discontinuous manner (e.g.', '""up"" (4-5) in ""pick-up"" (1-2+4-5)) in the sentence ""He picks the box up"".', 'For more details on the proprieties of SSTC , see #AUTHOR_TAG .']",0,"['For more details on the proprieties of SSTC , see #AUTHOR_TAG .']"
CC1359,W02-1601,A synchronization structure of SSTC and its applications in machine translation,natural language analysis in machine translation mt based on the stringtree correspondence grammar stcg,['E K Tang'],,"The formalism is argued to be a totally declarative grammar formalism that can associate, to strings in a language, arbitrary tree structures as desired by the grammar writer to be the linguistic representation structures of the strings. More importantly is the facility to specify the correspondence between the string and the associated tree in a very natural manner. These features are very much desired in grammar writing, in particular for the treatment of certain linguistic phenomena which are 'non-standard', namely featurisation, lexicalisation and crossed dependencies [2,3]. Furthermore, a grammar written in this way naturally inherits the desired property of bi-directionality (in fact non-directionality [4]) such that the same grammar can be interpreted for both analysis and generation.",A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .,"['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']",5,"['A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( #AUTHOR_TAG ) and Boitet & Zaharin ( 1988 ) .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"
CC1360,W02-1601,A synchronization structure of SSTC and its applications in machine translation,achieving commercialquality translation with examplebased methods,"['S Richardson', 'W Dolan', 'A Menezes', 'J Pinkham']",,,"For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( Sato , 1991 ) , ( #AUTHOR_TAG ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1361,W02-1601,A synchronization structure of SSTC and its applications in machine translation,restricting the weak generative capacity of synchronous tree adjoining grammar,['S Shieber'],,,"It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .","['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeillé et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'In this case only TAL can be formed in each component.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']",0,"['The idea of parallelized formalisms is widely used one, and one which has been applied in many different ways.', 'The use of synchronous formalisms is motivated by the desire to describe two languages that are closely related to each other but that do not have the same structures.', 'For example, synchronous Tree Adjoining Grammar (S-TAG) can be used to relate TAGs for two different languages, for example, for the purpose of immediate structural translation in machine translation (Abeille et al.,1990), (Harbusch & Poller,1996), or for relating a syntactic TAG and semantic one for the same language (Shieber & Schabes,1990).', 'S-TAG is a variant of Tree Adjoining Grammar (TAG) introduced by (Shieber & Schabes,1990) to characterize correspondences between tree adjoining languages.', 'Considering the original definition of S-TAGs, one can see that it does not restrict the structures that can be produced in the source and target languages.', 'It allows the construction of a non-TAL ( #AUTHOR_TAG ) , ( Harbusch & Poller , 2000 ) .', 'As a result, Shieber (1994) propose a restricted definition for S-TAG, namely, the IS-TAG for isomorphic S-TAG.', 'This isomorphism requirement is formally attractive, but for practical applications somewhat too strict.', 'Also contrastive well-known translation phenomena exist in different languages, which cannot be expressed by IS-TAG, Figure 3 illustrates some examples (Shieber, 1994).']"
CC1362,W02-1601,A synchronization structure of SSTC and its applications in machine translation,examplebased machine translation,['S Sato'],,"Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in re-trieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the pro-posed model, the system searches the transla-tion example combination which has the high-est probability. The proposed model clearly for-malizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental re-sults demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems.","For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .","['Due to these limitations, instead of investigating into the synchronization of two grammars, we propose a flexible annotation schema (i.e. Synchronous Structured String-Tree Correspondence (S-SSTC)) to realize additional power and flexibility in expressing structural correspondences at the level of language sentence pairs.', 'For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']",0,"['For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( #AUTHOR_TAG ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .']"
CC1363,W02-1601,A synchronization structure of SSTC and its applications in machine translation,representation trees and stringtree correspondences,"['C Boitet', 'Y Zaharin']",,,"A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .","['Figure 2 illustrates the sentence ""John picks the box up"" with its corresponding SSTC.', 'It contains a nonprojective correspondence.', 'An interval is assigned to each word in the sentence, i.e. (0-1) for ""John"", (1-2) for ""picks"", (2-3) for ""the"", (3-4) for ""box"" and (4-5) for ""up"".', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']",5,"['It contains a nonprojective correspondence.', 'A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and #AUTHOR_TAG .', 'and its dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree.']"
CC1364,W03-0806,Blueprint for a high performance NLP infrastructure,english gigaword corpus catalogue number ldc2003t05,['Linguistic Data Consortium'],introduction,,"However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .","['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .', 'Recent work (Banko and Brill, 2001;Curran and Moens, 2002) has suggested that some tasks will benefit from using significantly more data.', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['However , the greatest increase is in the amount of raw text available to be processed , e.g. the English Gigaword Corpus ( Linguistic Data #AUTHOR_TAG ) .']"
CC1365,W03-0806,Blueprint for a high performance NLP infrastructure,dialogue interaction with the darpa communicator infrastructure the development of useful software,"['Samuel Bayer', 'Christine Doran', 'Bryan George']",,"To support engaging human users in robust, mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems, the DARPA Communicator program [1] is funding the development of a distributed message-passing infrastructure for dialogue systems which all Communicator participants are using. In this presentation, we describe the features of and requirements for a genuinely useful software infrastructure for this purpose.","There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .","['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']",0,"['Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( #AUTHOR_TAG ) and speech recognition ( Hacioglu and Pellom , 2003 ) .']"
CC1366,W03-0806,Blueprint for a high performance NLP infrastructure,combining labeled and unlabeled data with cotraining,"['Avrim Blum', 'Tom Mitchell']",,,"Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .","['As discussed earlier, there are two main requirements of the system that are covered by ""high performance"": speed and state of the art accuracy.', 'Efficiency is required both in training and processing.', 'Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .', 'Processing text needs to be extremely efficient since many new applications will require very large quantities of text to be processed or many smaller quantities of text to be processed very quickly.']",0,"['Efficient training is required because the amount of data available for training will increase significantly.', 'Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( #AUTHOR_TAG ) .']"
CC1367,W03-0806,Blueprint for a high performance NLP infrastructure,gate – a general architecture for text engineering,"['Hamish Cunningham', 'Yorick Wilks', 'Robert J Gaizauskas']",experiments,"This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering.GATE lies at the intersection of human language computation and software engineering, and constitutes aninfrastructural system supporting research and development of languageprocessing software.","Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( #AUTHOR_TAG ) and the Alembic Workbench ( Day et al. , 1997 ) ) as well as NLP tools and resources that can be manipulated from the GUI .']"
CC1368,W03-0806,Blueprint for a high performance NLP infrastructure,a distributed architecture for robust automatic speech recognition,"['Kadri Hacioglu', 'Bryan Pellom']",,"In this paper, we attempt to decompose a state-of-the-art speech recognition system into its components and define an infrastructure that allows a flexible, efficient and effective interaction among the components. Motivated by the success of DARPA Communicator program, we select the open source Galaxy architecture as our development test bed. It consists of a hub that allows communication among servers connected to it by message passing and supports the plug-and-play paradigm. In addition to message passing it supports high bandwidth data (binary or audio) transfer between servers via a brokering scheme. For several reasons, we believe that it is the right time to start developing a distributed framework for speech recognition along with data and protocol standards supporting interoperability. We present our work towards that goal using the Colorado University (CU) Sonic recognizer. We divide Sonic into a number of components and structure it around the Hub. We describe the system in some detail and report on its present status with some possibilities for future development. 1","There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .","['The final interface we intend to implement is a collection of web services for NLP.', 'A web service provides a remote procedure that can be called using XML based encodings (XMLRPC or SOAP) of function names, arguments and results transmitted via internet protocols such as HTTP.', 'Systems can automatically discover and communicate with web services that provide the functionality they require by querying databases of standardised descriptions of services with WSDL and UDDI.', 'This standardisation of remote procedures is very exciting from a software engineering viewpoint since it allows systems to be totally distributed.', 'There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .', 'Web services will allow components developed by different researchers in different locations to be composed to build larger systems.']",0,"['There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( #AUTHOR_TAG ) .']"
CC1369,W03-0806,Blueprint for a high performance NLP infrastructure,a maximum entropy partofspeech tagger,['Adwait Ratnaparkhi'],conclusion,,"For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .","['The Generative Programming approach to NLP infrastructure development will allow tools such as sentence boundary detectors, POS taggers, chunkers and named entity recognisers to be rapidly composed from many elemental components.', 'For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']",3,"['For instance , implementing an efficient version of the MXPOST POS tagger ( #AUTHOR_TAG ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .']"
CC1370,W03-0806,Blueprint for a high performance NLP infrastructure,bootstrapping postaggers using unlabelled data,"['Stephen Clark', 'James R Curran', 'Miles Osborne']",experiments,"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.","The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']",4,"['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']"
CC1371,W03-0806,Blueprint for a high performance NLP infrastructure,scaling context space,"['James R Curran', 'Marc Moens']",experiments,"This paper proposes a computationally feasible method for measuring the context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multidimensional vector which is extracted from an English dictionary through principal component analysis. Given a word set C which specifies a context, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, the distance between words in V  becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. 1 Introduction  Semantic distance (or similarity) between words is one of the basic measurements used in many fields of natural language processing, information retrieval, etc. Word distance provides bottom-up information for text understandi..","The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'We have already implemented a P O S tagger, chunker, C C G supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.', 'These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster train- ing times when we move to conjugate gradient methods.']",4,"['The implementation has been inspired by experience in extracting information from very large corpora ( #AUTHOR_TAG ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .', 'These tools currently train in less than 10 minutes on the standard training ma- terials and tag faster than T N T , the fastest existing P O S tagger.', 'These tools use a highly optimised G I S imple- mentation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster train- ing times when we move to conjugate gradient methods.']"
CC1372,W03-0806,Blueprint for a high performance NLP infrastructure,maximum entropy models for natural language ambiguity resolution,['Adwait Ratnaparkhi'],experiments,"This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy.  We discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages:  State-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources.  Knowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or ""knowledge-poor"", but yet succeed in approximating complex linguistic relationships.  Reusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis.  The experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models.","An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by #AUTHOR_TAG that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( Malouf , 2002 ) .']"
CC1373,W03-0806,Blueprint for a high performance NLP infrastructure,a rational design for a weighted finitestate transducer library,"['Mehryar Mohri', 'Fernando C N Pereira', 'Michael Riley']",experiments,,"Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .","['A number of stand-alone tools have also been developed.', 'For example, the suite of LT tools (Mikheev et al., 1999;Grover et al., 2000) perform tokenization, tagging and chunking on XML marked-up text directly.', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['Other tools have been designed around particular techniques , such as finite state machines ( Karttunen et al. , 1997 ; #AUTHOR_TAG ) .']"
CC1374,W03-0806,Blueprint for a high performance NLP infrastructure,scaling context space,"['James R Curran', 'Marc Moens']",introduction,"This paper proposes a computationally feasible method for measuring the context-sensitive semantic distance between words. The distance is computed by adaptive scaling of a semantic space. In the semantic space, each word in the vocabulary V is represented by a multidimensional vector which is extracted from an English dictionary through principal component analysis. Given a word set C which specifies a context, each dimension of the semantic space is scaled up or down according to the distribution of C in the semantic space. In the space thus transformed, the distance between words in V  becomes dependent on the context C. An evaluation through a word prediction task shows that the proposed measurement successfully extracts the context of a text. 1 Introduction  Semantic distance (or similarity) between words is one of the basic measurements used in many fields of natural language processing, information retrieval, etc. Word distance provides bottom-up information for text understandi..","Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['Recent work ( Banko and Brill , 2001 ; #AUTHOR_TAG ) has suggested that some tasks will benefit from using significantly more data .']"
CC1375,W03-0806,Blueprint for a high performance NLP infrastructure,building a large annotated corpus of english the penn treebank computational linguistics,"['Mitchell Marcus', 'Beatrice Santorini', 'Mary Marcinkiewicz']",introduction,,"For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']",0,"['Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( Ide et al. , 2002 ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( #AUTHOR_TAG ) , currently used for training POS taggers .']"
CC1376,W03-0806,Blueprint for a high performance NLP infrastructure,nltk the natural language toolkit,"['Edward Loper', 'Steven Bird']",,,It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .,"['Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language.', 'Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']",2,"['Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools.', 'To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language.', 'Python has a number of advantages over other options, such as Java and Perl.', 'Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation.', 'It has already been used to implement a framework for teaching NLP ( #AUTHOR_TAG ) .']"
CC1377,W03-0806,Blueprint for a high performance NLP infrastructure,mixedinitiative development of language processing systems,"['David Day', 'John Aberdeen', 'Lynette Hirschman', 'Robyn Kozierok', 'Patricia Robinson', 'Marc Vilain']",experiments,"Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort. Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process. These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data. This paper describes a new set of integrated tools, collectively called the Alembic Workbench, that uses a mixed-initiative approach to ""bootstrapping"" the manual tagging process, with the goal of reducing the overhead associated with corpus development. Initial empirical studies using the Alembic Workbench to annotate ""named entities"" demonstrates that this approach can approximately double the production rate. As an added benefit, the combined efforts of machine and user produce domain specific annotation rules that can be used to annotate similar texts automatically through the Alembic-NLP system. The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session.","Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['Many provide graphical user interfaces ( GUI ) for manual annotation ( e.g. General Architecture for Text Engineering ( GATE ) ( Cunningham et al. , 1997 ) and the Alembic Workbench ( #AUTHOR_TAG ) ) as well as NLP tools and resources that can be manipulated from the GUI .']"
CC1378,W03-0806,Blueprint for a high performance NLP infrastructure,developing language processing components with gate,"['Hamish Cunningham', 'Diana Maynard', 'C Ursu K Bontcheva', 'V Tablan', 'M Dimitrov']",experiments,"Fluid leakage through soil in a region thereof is controlled by sequentially passing over the region to dig a plurality of parallel, laterally displaced grooves in the surface. Soil dug from each groove is temporarily stored, and a strip of sheet material is laid over a groove as it is created during each pass, the width of the strip being greater than the width of the groove. Thereafter, the temporarily stored soil is deposited on the strip such that it is covered with soil except along one edge, the other edge of the strip overlying the uncovered edge of an adjacent strip laid down during a previous pass over the region. As a consequence, a first layer of overlapping strips of sheet material covered with soil is installed over the region.","For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .","['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance , GATE currently provides a POS tagger , named entity recogniser and gazetteer and ontology editors ( #AUTHOR_TAG ) .', 'GATE goes beyond earlier systems by using a component-based infrastructure (Cunningham, 2000) which the GUI is built on top of.', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']"
CC1379,W03-0806,Blueprint for a high performance NLP infrastructure,generative programming methods tools and applications,"['Krzysztof Czarnecki', 'Ulrich W Eisenecker']",introduction,"1. What Is This Book About? From Handcrafting to Automated Assembly Lines. Generative Programming. Benefits and Applicability. I. ANALYSIS AND DESIGN METHODS AND TECHNIQUES. 2. Domain Engineering. Why Is This Chapter Worth Reading? What Is Domain Engineering? Domain Analysis. Domain Design and Domain Implementation. Application Engineering. Product-Line Practices. Key Domain Engineering Concepts. Domain. Domain Scope and Scoping. Relationships between Domains. Features and Feature Models. Method Tailoring and Specialization. Survey of Domain Analysis and Domain Engineering Methods. Feature-Oriented Domain Analysis (FODA). Organization Domain Modeling (ODM). Draco. Capture. Domain Analysis and Reuse Environment (DARE). Domain-Specific Software Architecture (DSSA) Approach. Algebraic Approach. Other Approaches. Domain Engineering and Related Approaches. Historical Notes. Summary. 3. Domain Engineering and Object-Oriented Analysis and Design. Why Is This Chapter Worth Reading? OO Technology and Reuse. Solution Space. Problem Space. Relationship between Domain Engineering and Object-Oriented Analysis and Design (OOA/D) Methods. Aspects of Integrating Domain Engineering and OOA/D Methods. Horizontal versus Vertical Methods. Selected Methods. Rational Unified Process. 00ram. Reuse-Driven Software Engineering Business (RSEB). FeatuRSEB. Domain Engineering Method for Reusable Algorithmic Libraries (DEMRAL). 4. Feature Modeling. Why Is This Chapter Worth Reading? Features Revisited. Feature Modeling. Feature Models. Feature Diagrams. Other Infon-Nation Associated with Feature Diagrams in a Feature Model. Assigning Priorities to Variable Features. Availability Sites, Binding Sites, and Binding Modes. Relationship between Feature Diagrams and Other Modeling Notations and Implementation Techniques. Single Inheritance. Multiple Inheritance. Parameterized Inheritance. Static Parameterization. Dynamic Parameterization. Implementing Constraints. Tool Support for Feature Models. Frequently Asked Questions about Feature Diagrams. Feature Modeling Process. How to Find Features. Role of Variability in Modeling. 5. The Process of Generative Programming. Why Is This Chapter Worth Reading? Generative Domain Models. Main Development Steps in Generative Programming. Adapting Domain Engineering for Generative Programming. Domain-Specific Languages. DEMRAL: Example of a Domain Engineering Method for Generative Programming. Outline of DEMRAL. Domain Analysis. Domain Definition. Domain Modeling. Domain Design. Scope Domain Model for Implementation. Identify Packages. Develop Target Architectures and Identify the Implementation Components. Identify User DSLs. Identify Interactions between DSLs. Specify DSLs and Their Translation. Configuration DSLs. Expression DSLs. Domain Implementation. II. IMPLEMENTATION TECHNOLOGIES. 6. Generic Programming. Why Is This Chapter Worth Reading? What Is Generic Programming? Generic versus Generative Programming. Generic Parameters. Parametric versus Subtype Polymorphism. Genericity in Java. Bounded versus Unbounded Polymorphism. A Fresh Look at Polymorphism. Parameterized Components. Parameterized Programming. Types, Interfaces, and Specifications. Adapters. Vertical and Horizontal Parameters. Module Expressions. C++ Standard Template Library. Iterators. Freestanding Functions versus Member Functions. Generic Methodology. Historical Notes. 7. Component-Oriented Template-Based C++ Programming Techniques. Why Is This Chapter Worth Reading? Types of System Configuration. C++ Support for Dynamic Configuration. C++ Support for Static Configuration. Static Typing. Static Binding. Inlining. Templates. Parameterized Inheritance. typedefs. Member Types. Nested Classes. Prohibiting Certain Template Instantiations. Static versus Dynamic Parameterization. Wrappers Based on Parameterized Inheritance. Template Method Based on Parameterized Inheritance. Parameterizing Binding Mode. Consistent Parameterization of Multiple Components. Static Interactions between Components. Components with Influence. Components under Influence. Structured Configurations. Recursive Components. Intelligent Configuration. 8. Aspect-Oriented Decomposition and Composition. Why Is This Chapter Worth Reading? What Is Aspect-Oriented Programming? Aspect-Oriented Decomposition Approaches. Subject-Oriented Programming. Composition Filters. Demeter / Adaptive Programming. Aspect-Oriented Decomposition and Domain Engineering. How Aspects Arise. Composition Mechanisms. Requirements on Composition Mechanisms. Example: Synchronizing a Bounded Buffer. ""Tangled"" Synchronized Stack. Separating Synchronization Using Design Patterns. Separating Synchronization Using SOP. Some Problems with Design Patterns and Some Solutions. Implementing Noninvasive, Dynamic Composition in Smalltalk. Kinds of Crosscutting. How to Express Aspects in Programming Languages. Separating Synchronization Using AspectJ Cool. Implementing Dynamic Cool in Smalltalk. Implementation Technologies for Aspect-Oriented Programming. Technologies for Implementing Aspect-Specific Abstractions. Technologies for Implementing Weaving. AOP and Specialized Language Extensions. AOP and Active Libraries. Final Remarks. 9. Generators. Why Is This Chapter Worth Reading? What Are Generators? Transformational Model of Software Development. Technologies for Building Generators. Compositional versus Transformational Generators. Kinds of Transformations. Compiler Transformations. Source-to-Source Transformations. Transformation Systems. Scheduling Transformations. Existing Transformation Systems and Their Applications. Selected Approaches to Generation. Draco. GenVoca. Approaches Based on Algebraic Specifications. 10. Static Metaprogramming in C++. Why Is This Chapter Worth Reading? What Is Metaprogramming? A Quick Tour of Metaprogramming. Static Metaprogramming. C++ as a Two-Level Language. Functional Flavor of the Static Level. Class Templates as Functions. Integers and Types as Data. Symbolic Names Instead of Variables. Constant Initialization and typedef-Statements Instead of Assignment. Template Recursion Instead of Loops. Conditional Operator and Template Specialization as Conditional Constructs. Template Metaprogramming. Template Metafunctions. Metafinctions as Arguments and Return Values of Other Metafinctions. Representing Metainformation. Member Traits. Traits Classes. Traits Templates. Example: Using Template Metafunctions and Traits Templates to Implement Type Promotions. Compile-Time Lists and Trees as Nested Templates. Compile-Time Control Structures. Explicit Selection Constructs. Template Recursion as a Looping Construct. Explicit Looping Constructs. Code Generation. Simple Code Selection. Composing Templates. Generators Based on Expression Templates. Recursive Code Expansion. Explicit Loops for Generating Code. Example: Using Static Execute Loops to Test Metafunctions. Partial Evaluation in C++. Workarounds for Partial Template Specialization. Problems of Template Metaprogramming. Historical Notes. 11. Intentional Programming. Why Is This Chapter Worth Reading? What Is Intentional Programming? Technology behind IP. System Architecture. Representing Programs in IP: The Source Graph. Source Graph + Methods = Active Source. Working with the IP Programming Environment. Editing. Further Capabilities of the IP Editor. Extending the IP System with New Intentions. Advanced Topics. Questions, Methods, and a Frameworklike Organization. Source-Pattem-Based Polymorphism. Methods as Visitors. Asking Questions Synchronously and Asynchronously. Reduction. The Philosophy behind IP. Why Do We Need Extendible Programming Environments? or What Is the Problem with Fixed Programming Languages? Moving Focus from Fixed Languages to Language Features and the Emergence of an Intention Market. Intentional Programming and Component-Based Development. Frequently Asked Questions. Summary. III. APPLICATION EXAMPLES. 12. List Container. Why Is This Chapter Worth Reading? Overview. Domain Analysis. Domain Design. Implementation Components. Manual Assembly. Specifying Lists. The Generator. Extensions. 13. Bank Account. Why Is This Chapter Worth Reading? The Successful Programming Shop. Design Pattems, Frameworks, and Components. Domain Engineering and Generative Programming. Feature Modeling. Architecture Design. Implementation Components. Configurable Class Hierarchies. Designing a Domain-Specific Language. Bank Account Generator. Testing Generators and Their Products. 14. Generative Matrix Computation Library (GMCL). Why Is This Chapter Worth Reading? Why Matrix Computations? Domain Analysis. Domain Definition. Domain Modeling. Domain Design and Implementation. Matrix Type Generation. Generating Code for Matrix Expressions. Implementing the Matrix Component in IP. APPENDICES. Appendix A: Conceptual Modeling. What Are Concepts? Theories of Concepts. Basic Terminology. The Classical View. The Probabilistic View. The Exemplar View. Summary of the Three Views. Important Issues Concerning Concepts. Stability of Concepts. Concept Core. Informational Contents of Features. Feature Composition and Relationships between Features. Quality of Features. Abstraction and Generalization. Conceptual Modeling, Object-Orientation, and Software Reuse. Appendix B: Instance-Specific Extension Protocol for Smalltalk. Appendix C: Protocol for Attaching Listener Objects in Smalltalk. Appendix D: Glossary of Matrix Computation Terms. Appendix E: Metafunction for Evaluating Dependency Tables. Glossary of Generative Programming Terms. References. Index. 020130977T04062001",Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .,"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']",0,"['Software engineering research on Generative Programming ( #AUTHOR_TAG ) attempts to solve these problems by focusing on the development of configurable elementary components and knowledge to combine these components into complete systems .', 'Our infrastructure for NLP will provide high performance 1 components inspired by Generative Programming principles.']"
CC1380,W03-0806,Blueprint for a high performance NLP infrastructure,the american national corpus more than the web can provide,"['Nancy Ide', 'Randi Reppen', 'Keith Suderman']",introduction,"The American National Corpus (ANC) project is developing a corpus comparable to the British National Corpus (BNC), covering American English. Recent interest in the web as a source of corpus materials has caused some in the language processing community to suggest that the development of a corpus of American English is unnecessary. However, we argue that far from being rendered superfluous by the availability of web materials, the ANC is likely to provide a resource for developing web acquisition techniques to support tasks such as genre and language detection and automatic annotation. This paper presents a comparison of the ANC in terms of both content and format with a test corpus compiled from web data, and a discussion of points of intersection and divergence.","For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .","['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .', 'This will require more efficient learning algorithms and implementations.']",0,"['NLP is experiencing an explosion in the quantity of electronic text available.', 'Some of this new data will be manually annotated.', 'For example , 10 million words of the American National Corpus ( #AUTHOR_TAG ) will have manually corrected POS tags , a tenfold increase over the Penn Treebank ( Marcus et al. , 1993 ) , currently used for training POS taggers .']"
CC1381,W03-0806,Blueprint for a high performance NLP infrastructure,lt ttt  a flexible tokenisation tool,"['Claire Grover', 'Colin Matheson', 'Andrei Mikheev', 'Marc Moens']",experiments,,"For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['For example , the suite of LT tools ( Mikheev et al. , 1999 ; #AUTHOR_TAG ) perform tokenization , tagging and chunking on XML marked-up text directly .']"
CC1382,W03-0806,Blueprint for a high performance NLP infrastructure,modern c design generic programming and design patterns applied c indepth series,['Andrei Alexandrescu'],experiments,"Modern C++ Designis an important book. Fundamentally, it demonstrates 'generic patterns' or 'pattern templates' as a powerful new way of creating extensible designs in C++i??a new way to combine templates and patterns that you may never have dreamt was possible, but is. If your work involves C++ design and coding, you should read this book. Highly recommended. i??Herb SutterWhat's left to say about C++ that hasn't already been said? Plenty, it turns out. i??From the Foreword by John VlissidesIn Modern C++ Design, Andrei Alexandrescu opens new vistas for C++ programmers. Displaying extraordinary creativity and programming virtuosity, Alexandrescu offers a cutting-edge approach to design that unites design patterns, generic programming, and C++, enabling programmers to achieve expressive, flexible, and highly reusable code.This book introduces the concept of generic componentsi??reusable design templates that produce boilerplate code for compiler consumptioni??all within C++. Generic components enable an easier and more seamless transition from design to application code, generate code that better expresses the original design intention, and support the reuse of design structures with minimal recoding.The author describes the specific C++ techniques and features that are used in building generic components and goes on to implement industrial strength generic components for real-world applications. Recurring issues that C++ developers face in their day-to-day activity are discussed in depth and implemented in a generic way. These include: Policy-based design for flexibility Partial template specialization Typelistsi??powerful type manipulation structures Patterns such as Visitor, Singleton, Command, and Factories Multi-method enginesFor each generic component, the book presents the fundamental problems and design options, and finally implements a generic solution.In addition, an accompanying Web site, http://www.awl.com/cseng/titles/0-201-70431-5, makes the code implementations available for the generic components in the book and provides a free, downloadable C++ library, called Loki, created by the author. Loki provides out-of-the-box functionality for virtually any C++ project.Get a value-added service! Try out all the examples from this book at www.codesaw.com. CodeSaw is a free online learning tool that allows you to experiment with live code from your book right in your browser. 0201704315B11102003","To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .","['The infrastructure will be implemented in C/C++.', 'Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']",5,"['The infrastructure will be implemented in C/C++.', 'Templates will be used heavily to provide generality without significantly impacting on efficiency.', 'However, because templates are a static facility we will also provide dynamic versions (using inheritance), which will be slower but accessible from scripting languages and user interfaces.', 'To provide the required configurability in the static version of the code we will use policy templates ( #AUTHOR_TAG ) , and for the dynamic version we will use configuration classes .']"
CC1383,W03-0806,Blueprint for a high performance NLP infrastructure,scaling to very very large corpora for natural language disambiguation,"['Michele Banko', 'Eric Brill']",introduction,"The amount of readily available online  text has reached hundreds of  billions of words and continues to  grow. Yet for most core natural  language tasks, algorithms continue  to be optimized, tested and compared  after training on corpora consisting  of only one million words or less. In  this paper, we evaluate the  performance of different learning  methods on a prototypical natural  language disambiguation task,  confusion set disambiguation, when  trained on orders of magnitude more  labeled data than has previously been  used. We are fortunate that for this  particular application, correctly  labeled training data is free. Since  this will often not be the case, we  examine methods for effectively  exploiting very large corpora when  labeled data comes at a cost","Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .","['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.', 'Other potential applications must process text online or in realtime.', 'For example, Google currently answers 250 million queries per day, thus processing time must be minimised.', 'Clearly, efficient NLP components will need to be developed.', 'At the same time, state-of-the-art performance will be needed for these systems to be of practical use.']",0,"['However, the greatest increase is in the amount of raw text available to be processed, e.g. the English Gigaword Corpus (Linguistic Data Consortium, 2003).', 'Recent work ( #AUTHOR_TAG ; Curran and Moens , 2002 ) has suggested that some tasks will benefit from using significantly more data .', 'Also, many potential applications of NLP will involve processing very large text databases.', 'For instance, biomedical text-mining involves extracting information from the vast body of biological and medical literature; and search engines may eventually apply NLP techniques to the whole web.']"
CC1384,W03-0806,Blueprint for a high performance NLP infrastructure,xml tools and architecture for named entity recognition,"['Andrei Mikheev', 'Claire Grover', 'Marc Moens']",experiments,This paper reports on the development of a Named Entity recognition system developed fully within the xml paradigm,"For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .","['A number of stand-alone tools have also been developed.', 'For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .', 'These tools also store their configuration state, e.g. the transduction rules used in LT CHUNK, in XML configuration files.', 'This gives a greater flexibility but the tradeoff is that these tools can run very slowly.', 'Other tools have been designed around particular techniques, such as finite state machines (Karttunen et al., 1997;Mohri et al., 1998).', 'However, the source code for these tools is not freely available, so they cannot be extended.']",0,"['For example , the suite of LT tools ( #AUTHOR_TAG ; Grover et al. , 2000 ) perform tokenization , tagging and chunking on XML marked-up text directly .']"
CC1385,W03-0806,Blueprint for a high performance NLP infrastructure,a corpusbased appreach to language learning,['Eric Brill'],,,"Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .","['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']",4,"['Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( #AUTHOR_TAG ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .', 'We will base these components on the design of Weka (Witten and Frank, 1999).']"
CC1386,W03-0806,Blueprint for a high performance NLP infrastructure,transformationbased learning in the fast lane,"['Grace Ngai', 'Radu Florian']",experiments,,"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( #AUTHOR_TAG ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( Roche and Schabes , 1997 ) .']"
CC1387,W03-0806,Blueprint for a high performance NLP infrastructure,bootstrapping postaggers using unlabelled data,"['Stephen Clark', 'James R Curran', 'Miles Osborne']",,"This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.","The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .', 'An example of using the Python tagger interface is shown in Figure 1.']",0,"['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( Curran and Clark , 2003 ; #AUTHOR_TAG ) .']"
CC1388,W03-0806,Blueprint for a high performance NLP infrastructure,a gaussian prior for smoothing maximum entropy models,"['Stanley Chen', 'Ronald Rosenfeld']",experiments,"Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.",These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .,"['The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .', 'We expect even faster training times when we move to conjugate gradient methods.']",5,['These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing ( #AUTHOR_TAG ) .']
CC1389,W03-0806,Blueprint for a high performance NLP infrastructure,deterministic partofspeech tagging with finitestate transducers,"['Emmanuel Roche', 'Yves Schabes']",experiments,,"Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['Other attempts to address efficiency include the fast Transformation Based Learning ( TBL ) Toolkit ( Ngai and Florian , 2001 ) which dramatically speeds up training TBL systems , and the translation of TBL rules into finite state machines for very fast tagging ( #AUTHOR_TAG ) .']"
CC1390,W03-0806,Blueprint for a high performance NLP infrastructure,nltk the natural language toolkit,"['Edward Loper', 'Steven Bird']",experiments,,"Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .","['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .', 'Python scripting is extremely simple to learn, read and write, and so using the existing components and designing new components is simple.']",0,"['Finally , the Natural Language Toolkit ( NLTK ) is a package of NLP components implemented in Python ( #AUTHOR_TAG ) .']"
CC1391,W03-0806,Blueprint for a high performance NLP infrastructure,investigating gis and smoothing for maximum entropy taggers,"['James R Curran', 'Stephen Clark']",experiments,"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.","The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .","['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure.', 'These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger.', 'These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999).', 'We expect even faster training times when we move to conjugate gradient methods.']",4,"['The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']"
CC1392,W03-0806,Blueprint for a high performance NLP infrastructure,a comparison of algorithms for maximum entropy parameter estimation,['Robert Malouf'],experiments,"Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices.","An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger (Brants, 2000) has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']",0,"['An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( #AUTHOR_TAG ) .']"
CC1393,W03-0806,Blueprint for a high performance NLP infrastructure,investigating gis and smoothing for maximum entropy taggers,"['James R Curran', 'Stephen Clark']",,"This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar.","The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .","['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .', 'An example of using the Python tagger interface is shown in Figure 1.']",0,"['The basic Python reflection has already been implemented and used for large scale experiments with POS tagging , using pyMPI ( a message passing interface library for Python ) to coordinate experiments across a cluster of over 100 machines ( #AUTHOR_TAG ; Clark et al. , 2003 ) .']"
CC1394,W03-0806,Blueprint for a high performance NLP infrastructure,tnt  a statistical partofspeech tagger,['Thorsten Brants'],experiments,,"The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .","['Efficiency has not been a focus for NLP research in general.', 'However, it will be increasingly important as techniques become more complex and corpus sizes grow.', 'An example of this is the estimation of maximum entropy models, from simple iterative estimation algorithms used by Ratnaparkhi (1998) that converge very slowly, to complex techniques from the optimisation literature that converge much more rapidly (Malouf, 2002).', 'Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian, 2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state machines for very fast tagging (Roche and Schabes, 1997).', 'The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']",0,"['The TNT POS tagger ( #AUTHOR_TAG ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .']"
CC1395,W03-0806,Blueprint for a high performance NLP infrastructure,software architecture for language engineering,['Hamish Cunningham'],experiments,"Every building, and every computer program, has an architecture: structural and organisational principles that underpin its design and construction. The garden shed  once built by one of the authors had an ad hoc architecture, extracted (somewhat painfully) from the imagination during a slow and non-deterministic process that, luckily, resulted in a structure which keeps the rain on the outside and the mower on the inside (at least for the time being). As well as being ad hoc (i.e. not informed by analysis of similar practice or relevant science or engineering) this architecture is implicit: no explicit design was made, and no records or documentation kept of the construction process.  The pyramid in the courtyard of the Louvre, by contrast, was constructed in a process involving explicit design performed by qualified engineers with a wealth of theoretical and practical knowledge of the properties of materials, the relative merits and strengths of different construction techniques, et cetera.  So it is with software: sometimes it is thrown together by  enthusiastic amateurs; sometimes it is architected, built to last, and intended to be 'not something you finish, but something you start' (to paraphrase Brand (1994). A number of researchers argued in the early and middle 1990s that the field of computational infrastructure or architecture for human language computation merited an increase in attention. The reasoning was that the increasingly large-scale and technologically significant nature of language processing science was placing increasing burdens of an engineering nature on research and development workers seeking robust and practical methods (as was the increasingly collaborative nature of research in this field, which puts a large premium on software integration and interoperation). Over the intervening period a number of significant systems and practices have been developed in what we may call Software Architecture for Language Engineering (SALE).  This special issue represented an opportunity for practitioners in this area to report their work in a coordinated setting, and to present a snapshot of the state-ofthe-art in infrastructural work, which may indicate where further development and further take-up of these systems can be of benefit",GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .,"['There are a number of generalised NLP systems in the literature.', 'Many provide graphical user interfaces (GUI) for manual annotation (e.g.', 'General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'For instance, GATE currently provides a POS tagger, named entity recogniser and gazetteer and ontology editors (Cunningham et al., 2002).', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .', 'This allows components to be highly configurable and simplifies the addition of new components to the system.']",0,"['General Architecture for Text Engineering (GATE) (Cunningham et al., 1997) and the Alembic Workbench (Day et al., 1997)) as well as NLP tools and resources that can be manipulated from the GUI.', 'GATE goes beyond earlier systems by using a component-based infrastructure ( #AUTHOR_TAG ) which the GUI is built on top of .']"
CC1396,W04-0910,Paraphrastic grammars,an open source grammar development environment and broadcoverage english grammar using hpsg,"['Ann Copestake', 'Dan Flickinger']",,"The LinGO (Linguistic Grammars Online) project's English Resource Grammar and the LKB grammar development environment are language resources which are freely available for download for any purpose, including commercial use (see http://lingo.stanford.edu). Executable programs and source code are both included. In this paper, we give an outline of the LinGO English grammar and LKB system, and discuss the ways in which they are currently being used. The grammar and processing system can be used independently or combined to give a central component which can be exploited in a variety of ways. Our intention in writing this paper is to encourage more people to use the technology, which supports collaborative development on many levels.","Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( #AUTHOR_TAG ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1397,W04-0910,Paraphrastic grammars,modlisation et traitement informatique de la synonymi linguisticae investigationes,['S Ploux'],,,Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .,"['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993;Lin, 1998).', 'techniques.']",0,['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available ( #AUTHOR_TAG ) .']
CC1398,W04-0910,Paraphrastic grammars,automatic paraphrase acquisition from news articles,"['Y Shinyanma', 'S Sekine', 'K Sudo', 'R Grishman']",introduction,"Paraphrases play an important role in the variety and complexity of natural language documents. However, they add to the difficulty of natural language processing. Here we describe a procedure for ob-taining paraphrases from news articles. Articles derived from dif-ferent newspapers can contain paraphrases if they report the same event on the same day. We exploit this feature by using Named Entity recognition. Our approach is based on the assumption that Named Entities are preserved across paraphrases. We applied our method to articles of two domains and obtained notable examples. Although this is our initial attempt at automatically extracting para-phrases from a corpus, the results are promising. 1","Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['Similarly , ( Barzilay and Lee , 2003 ) and ( #AUTHOR_TAG ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']"
CC1399,W04-0910,Paraphrastic grammars,identifying lexical paraphrases from a single corpus a case study for verbs,"['O Glickman', 'I Dagan']",introduction,,And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .,"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']",0,"['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'And ( #AUTHOR_TAG ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .']"
CC1400,W04-0910,Paraphrastic grammars,un outil multilingue de generation de ltag  application au francais et a l’italien,['M H Candito'],,,"To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .","['Modeling intercategorial synonymic links.', ""A first investigation of Anne Abeillé's TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward."", 'For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeillé FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above.', 'Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising.', 'To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.', 'For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur.', 'But additionnally there will be semantic classes such as, ""binary predicate of semantic type X"" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments), BINARY NPRED (the class of binary predicative nouns), NOVSUPNN1 , the class of support verb constructions taking two nominal arguments.', 'By further associating semantic units (e.g., ""cost"") with the appropriate semantic classes (e.g., ""binary predicate of semantic type X""), we can in this way capture both intra and intercategorial paraphrasing links in a general way.']",3,"['To address this problem , we are currently working on developing a metagrammar in the sense of ( #AUTHOR_TAG ) .', 'This metagrammar allows us to factorise both syntactic and semantic information.', 'Syntactic information is factorised in the usual way.']"
CC1401,W04-0910,Paraphrastic grammars,learning to paraphrase an unsupervised approahc using mutliplesequence alignment,"['R Barzilay', 'L Lee']",introduction,,"Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance, (Lin and Pantel, 2001) acquire two-argument templates (inference rules) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'Similarly , ( #AUTHOR_TAG ) and ( Shinyanma et al. , 2002 ) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source .']"
CC1402,W04-0910,Paraphrastic grammars,framenet theory and practice,"['C Fillmore C Johnson', 'M Petruckand C Baker', 'M Ellsworth', 'J Ruppenhofer']",,,"To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .","['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'In this approach, a word evokes a frame i.e., a simple or a complex event, and each frame is associated with a number of frame elements that is, a number of participants fulfilling a given role in the frame.', 'Finally each frame is associated with a set of target words, the words that evoke that frame.']",5,"['To represent the semantics of predicative units , we use FrameNet inventory of frames and frame elements ( C.#AUTHOR_TAG ) .', 'Johnson et al., 2002).', 'FrameNet is an online lexical resource for English based on the principles of Frame Semantics.', 'Finally each frame is associated with a set of target words, the words that evoke that frame.']"
CC1403,W04-0910,Paraphrastic grammars,discovery of inference rules for question answering natural language engineering,"['Dekang Lin', 'Patrick Pantel']",introduction,,"For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .","['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']",0,"['More recently, work in information extraction (IE) and question answering (QA) has triggered a renewed research interest in paraphrases as IE and QA systems typically need to be able to recognise various verbalisations of the content.', 'Because of the large, open domain corpora these systems deal with, coverage and robustness are key issues and much on the work on paraphrases in that domain is based on automatic learning techniques.', 'For instance , ( #AUTHOR_TAG ) acquire two-argument templates ( inference rules ) from corpora using an extended version of the distributional analysis in which paths in dependency trees that have similar arguments are taken to be close in meaning .', 'Similarly, (Barzilay and Lee, 2003) and (Shinyanma et al., 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.', 'And (Glickman and Dagan, 2003) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts.']"
CC1404,W04-0910,Paraphrastic grammars,automatic retrieval and clustering of similar words,['D Lin'],,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .', 'techniques.']",3,"['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; #AUTHOR_TAG ) .']"
CC1405,W04-0910,Paraphrastic grammars,towards evaluation of nlp systems,"['D Flickinger', 'J Nerbonne', 'I Sag', 'T Wasow']",,,"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'The construction and annotation of the paraphrases reflects the paraphrase typology.', 'In a first phase, we concentrate on simple, non-recursive predicate/argument structure.', 'Given such a structure, the construction and annotation of a test item proceeds as follows.']",1,"['For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( #AUTHOR_TAG ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( Oepen and Flickinger , 1998 ) .']"
CC1406,W04-0910,Paraphrastic grammars,minimal recursion semantics an introduction,"['A Copestake', 'D Flickinger', 'I Sag', 'C Pollard']",,"Minimal recursion semantics (MRS) is a framework for computational semantics that is suitable for parsing and generation and that can be implemented in typed feature structure formalisms. We discuss why, in general, a semantic representation with minimal structure is desirable and illustrate how a descriptively adequate representation with a nonrecursive structure may be achieved. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. We have integrated MRS with a broad-coverage HPSG grammar.","The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; #AUTHOR_TAG ; Copestake et al. , 2001 ) .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"
CC1407,W04-0910,Paraphrastic grammars,a procedure for quantitatively comparing the syntactic coverage of english grammars,"['A Black', 'S Abney', 'D Flickinger', 'C Gdaniec', 'R Grishman', 'P Harrison', 'D Hindel', 'R INgria', 'F Jelinek', 'F Klaavans', 'M Liberman', 'M Marcus', 'S Roukos', 'B Santorini', 'T Strzalkowski']",,"The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes).","While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']",0,"['Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PARSEVAL lines ( #AUTHOR_TAG ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .', 'For english, there is for instance the 15 year old Hewlett- Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987); and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998).']"
CC1408,W04-0910,Paraphrastic grammars,predicate logic unplugged,['J Bos'],,"Die vorliegende Arbeit wurde im Rahmen des Verbundvorhabens Verbmobil vom Bundes-ministerium f ur Bildung, Wissenschaft, Forschung und Technologie (BMBF) unter dem FF orderkennzeichen 01 IV 101 R geff ordert. Die Verantwortung f ur den Inhalt dieser Arbeit liegt bei dem Autor.","The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .","['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['The language chosen for semantic representation is a flat semantics along the line of ( #AUTHOR_TAG ; Copestake et al. , 1999 ; Copestake et al. , 2001 ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']"
CC1409,W04-0910,Paraphrastic grammars,les constructions converses du francais,['G Gross'],,,"In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']",3,"['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular , ( #AUTHOR_TAG ) lists the converses of some 3 500 predicative nouns .']"
CC1410,W04-0910,Paraphrastic grammars,un outil multilingue de generation de ltag  application au francais et a l’italien,['M H Candito'],introduction,,"As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .","['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']",5,"['As we shall briefly discuss in section 4, the grammar is developed with the help of a meta-grammar ( #AUTHOR_TAG ) thus ensuring an additional level of abstraction .', 'The metagrammar is an abstract specification of the linguistic properties (phrase structure, valency, realisation of grammatical functions etc.) encoded in the grammar basic units.', 'This specification is then compiled to automatically produce a specific grammar.']"
CC1411,W04-0910,Paraphrastic grammars,m´ethodes en syntase,['M Gross'],,,"For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .","['For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs.', 'For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']",3,"['For complementing this database and for converse constructions , the LADL tables ( #AUTHOR_TAG ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .']"
CC1412,W04-0910,Paraphrastic grammars,towards systematic grammar profiling test suite technology 10 years after computer speech and language,"['S Oepen', 'D Flickinger']",,,"For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .","['Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar.', 'Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'and its degree of overgeneration (does it generate only the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .', 'Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar.', 'To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section.', 'In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics.', 'The construction and annotation of the paraphrases reflects the paraphrase typology.', 'In a first phase, we concentrate on simple, non-recursive predicate/argument structure.', 'Given such a structure, the construction and annotation of a test item proceeds as follows.']",0,"['Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?)', 'While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation.', 'Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested.', 'For english , there is for instance the 15 year old HewlettPackard test suite , a simple text file listing test sentences and grouping them according to linguistics phenomena ( Flickinger et al. , 1987 ) ; and more recently , the much more sophisticated TSNLP ( Test Suite for Natural Language Processing ) which includes some 9500 test items for English , French and German , each of them being annotated with syntactic and application related information ( #AUTHOR_TAG ) .']"
CC1413,W04-0910,Paraphrastic grammars,distributional clustering of english words,"['F Pereira', 'N Tishby', 'L Lee']",,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.","For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .","['Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997).', 'Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries.', 'For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .', 'techniques.']",3,"['For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( #AUTHOR_TAG ; Lin , 1998 ) .']"
CC1414,W04-0910,Paraphrastic grammars,semantics and syntax in lexical functional grammar,['M Dalrymple'],,,"Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; Copestake et al. , 2001 ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( #AUTHOR_TAG ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1415,W04-0910,Paraphrastic grammars,semantic construction in ftag,"['C Gardent', 'L Kallmeyer']",,,Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .,"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.', 'The association between tree nodes and unification variables encodes the syntax/semantics interface -it specifies which node in the tree provides the value for which variable in the final semantic representation.']",0,"['Semantic construction proceeds from the derived tree ( #AUTHOR_TAG ) rather than -- as is more common in TAG -- from the derivation tree .', 'This is done by associating each elementary tree with a semantic representation and by decorating relevant tree nodes with unification variables and constants occuring in associated semantic representation.']"
CC1416,W04-0910,Paraphrastic grammars,an algebra for semantic construction in constraintbased grammars,"['A Copestake', 'A Lascarides', 'D Flickinger']",,,"Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .","['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']",0,"['""Semantic grammars"" already exist which describe not only the syntax but also the semantics of natural language.', 'Thus for instance , ( Copestake and Flickinger , 2000 ; #AUTHOR_TAG ) describes a Head Driven Phrase Structure Grammar ( HPSG ) which supports the parallel construction of a phrase structure ( or derived ) tree and of a semantic representation and ( Dalrymple , 1999 ) show how to equip Lexical Functional grammar ( LFG ) with a glue semantics .']"
CC1417,W04-0910,Paraphrastic grammars,an algebra for semantic construction in constraintbased grammars,"['A Copestake', 'A Lascarides', 'D Flickinger']",,,"The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .","['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .', 'However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships.', 'Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . .', ', x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.']",1,"['The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; #AUTHOR_TAG ) .']"
CC1418,W04-0910,Paraphrastic grammars,alternations and verb semantic classes for french analysis and class formation chapter 5,['P Saint-Dizier'],,"In this paper, we show how alternations (called here contexts) can be defined for French, what their semantic properties are and how verb semantic classes can be constructed from syntactic criteria following (Levin 93). We then analyze the global quality of the results in terms of overlap with classes formed from the same verb-senses using WordNetlike classification criteria. Finally, we propose a method which combines these two approaches to form verb semantic classes better suited for natural language processing.","For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .","['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .', 'For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions.', 'In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns.']",0,"['For shuffling paraphrases , french alternations are partially described in ( #AUTHOR_TAG ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs .']"
CC1419,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,modern information retrieval,"['R B Yates', 'B R Neto']",,"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships","It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .","['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']",4,"['It compares favorably to other stemming or root extraction algorithms ( #AUTHOR_TAG ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']"
CC1420,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,towards an arabic information retrieval system,['Y Houmame'],,"Arabic, the mother tongue of over 300 million people around the world, is known as one of the most difficult languages in Automatic Natural Language processing (NLP) in general and information retrieval in particular. Hence, Arabic cannot trust any web information retrieval system as reliable and relevant as Google search engine. In this context, we dared to focus all our researches to implement an Arabic web-based information retrieval system entitled ARABIRS (ARABic Information Retrieval System). Therefore, to launch such a process so long and hard, we will start with Indexing as one of the crucial steps of the system","It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .","['The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form.', 'This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root.', 'In this work, we use the Arabic root extraction technique in (El Kourdi, 2004).', 'It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .', 'In the remainder of this paper, we will use the term ""root"" and ""term"" interchangeably to refer to canonical forms obtained through this root extraction process.']",4,"['It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and #AUTHOR_TAG ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .']"
CC1421,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,automatic indexing based on bayesian inference networksquot,"['K Tzeras', 'S Hartman']",experiments,"In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.","Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( #AUTHOR_TAG ) , minimum description length principal ( Lang , 1995 ) , and the X2 statistic .']"
CC1422,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],related work,,"This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .","['Concerning Arabic, one automatic categorizer has been reported to have been put under operational use to classify Arabic documents; it is referred to as ""Sakhr\'s categorizer"" (Sakhr, 2004).', 'Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer.', ""Sakhr's marketing literature claims that this categorizer is based on Arabic morphology and some research that has been carried out on natural language processing."", 'The present work evaluates the performance on Arabic documents of the Naïve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .', 'A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories).', 'The present work expands the work in (Yahyaoui, 2001) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.']",2,"['Concerning Arabic, one automatic categorizer has been reported to have been put under operational use to classify Arabic documents; it is referred to as ""Sakhr\'s categorizer"" (Sakhr, 2004).', 'Unfortunately, there is no technical documentation or specification concerning this Arabic categorizer.', 'The present work evaluates the performance on Arabic documents of the Naive Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997).', 'The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets.', 'This work is a continuation of that initiated in ( #AUTHOR_TAG ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .']"
CC1423,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],conclusion,,"To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .","['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .', 'In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments.', 'The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively.', 'Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (Yahyaoui, 2001).', 'This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm.', 'Future work will be directed at experimenting with other root extraction algorithms.', ""Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al.,200), where EM has increased the classification accuracy by 30% for classifying English documents.""]",1,"['To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( #AUTHOR_TAG ) .']"
CC1424,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,a reexamination of text categorization methods”,"['Y Yang', 'X Liu']",related work,,A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .,"['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .', 'More recently, (Sebastiani, 2002) has performed a good survey of document categorization; recent works can also be found in (Joachims, 2002), (Crammer and Singer, 2003), and (Lewis et al., 2004).']",0,['A good study comparing document categorization algorithms can be found in ( #AUTHOR_TAG ) .']
CC1425,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,a computational morphology system for arabic”,"['R Al-Shalabi', 'M Evens']",,"This paper describes a new algorithm for morphological analysis of Arabic words, which has been tested on a corpus of 242 abstracts from the Saudi Arabian National Computer Conference . It runs an order of magnitude faster than other algorithms in the literature.","This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .","['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'The infix form (or stem) needs further to be processed in order to obtain the root.', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).', 'As an example, two close roots (i.e., roots made of the same letters), but semantically different, can yield the same infix form thus creating ambiguity.']",0,"['In Arabic, however, the use of stems will not yield satisfactory categorization.', 'This is mainly due to the fact that Arabic is a non-concatenative language ( #AUTHOR_TAG ) , and that the stem/infix obtained by suppression of infix and prefix add-ons is not the same for words derived from the same origin called the root .', 'This processing is not straightforward: it necessitates expert knowledge in Arabic language word morphology (Al-Shalabi and Evens, 1998).']"
CC1426,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,a comparative study on feature selection in text categorization,"['Y Yang', 'J P Pedersen']",experiments,This paper is a comparative study of feature selection methods in statistical learning of text categorization The focus is on aggres sive dimensionality reduction Five meth ods were evaluated including term selection based on document frequency DF informa tion gain IG mutual information MI a test CHI and term strength TS We found IG and CHI most e ective in our ex periments Using IG thresholding with a k nearest neighbor classi er on the Reuters cor pus removal of up to removal of unique terms actually yielded an improved classi cation accuracy measured by average preci sion DF thresholding performed similarly Indeed we found strong correlations between the DF IG and CHI values of a term This suggests that DF thresholding the simplest method with the lowest cost in computation can be reliably used instead of IG or CHI when the computation of these measures are too expensive TS compares favorably with the other methods with up to vocabulary reduction but is not competitive at higher vo cabulary reduction levels In contrast MI had relatively poor performance due to its bias towards favoring rare terms and its sen sitivity to probability estimation errors,"( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', '( #AUTHOR_TAG ) has found strong correlations between DF , IG and the X2 statistic for a term .', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"
CC1427,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,machine learning in automated text categorization”,['F Sebastiani'],related work,"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.Comment: Accepted for publication on ACM Computing Survey","More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']",0,"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( #AUTHOR_TAG ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']"
CC1428,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,an evaluation of statistical approaches to text categorization”,['Y Yang'],introduction,"This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.","Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .","['With the explosive growth of text documents on the web, relevant information retrieval has become a crucial task to satisfy the needs of different end users.', 'To this end, automatic text categorization has emerged as a way to cope with such a problem.', 'Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization.', 'It consists of assigning and labeling documents using a set of predefined categories based on document contents.', 'As such, one of the primary objectives of automatic text categorization has been the enhancement and the support of information retrieval tasks to tackle problems, such as information filtering and routing, clustering of related documents, and the classification of documents into pre-specified subject themes.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .', 'Such applications have included electronic email filtering, newsgroups classification, and survey data grouping.', 'Barq for instance uses automatic categorization to provide similar documents feature (Rachidi et al., 2003).', 'In this paper, NB which is a statistical machine learning algorithm is used to learn to classify non-vocalized 1 Arabic web text documents.']",0,"['To this end, automatic text categorization has emerged as a way to cope with such a problem.', 'Automatic text (or document) categorization attempts to replace and save human effort required in performing manual categorization.', 'Automatic text categorization has been used in search engines , digital library systems , and document management systems ( #AUTHOR_TAG ) .']"
CC1429,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,toward an arabic web page classifierquot master project,['M Yahyaoui'],experiments,,"In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",1,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the kh 2 statistic.', 'In this paper , we use TF-IDF ( a kind of augmented DF ) as a feature selection criterion , in order to ensure results are comparable with those in ( #AUTHOR_TAG ) .', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"
CC1430,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,on the specification of term values in automatic indexingquot,"['G Salton', 'C S Yang']",experiments,,"TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .","['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .', 'Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .']",4,"['While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents.', 'The importance of each term is assumed to be inversely proportional to the number of documents that contain that term.', 'TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. ( #AUTHOR_TAG ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .']"
CC1431,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,newsweeder learning to filter netnewsquot,['K Lang'],experiments,,"Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .","['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization ; they include document frequency ( DF ) , information gain ( IG ) ( Tzeras and Hartman , 1993 ) , minimum description length principal ( #AUTHOR_TAG ) , and the X2 statistic .', 'On the other hand, (Rogati and Yang, 2002) reports the kh 2 to produce best performance.', 'TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999).', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']"
CC1432,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,comparison of two learning algorithms for text categorizationquot,"['D Lewis', 'M Ringnette']",related work,,"include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .","['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']",0,"['Many machine learning algorithms have been applied for many years to text categorization.', 'include decision tree learning and Bayesian learning , nearest neighbor learning , and artificial neural networks , early such works may be found in ( #AUTHOR_TAG ) , ( Creecy and Masand , 1992 ) and ( Wiene and Pedersen , 1995 ) , respectively .']"
CC1433,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,modern information retrieval,"['R B Yates', 'B R Neto']",experiments,"Digital technologies can help scholars to navigate the vast quantities of musical data and source materials now available to them, but an imaginative leap is needed in order to conceptualize the kinds of musicological research questions we might ask of electronic corpora. In particular, our data-rich digital world offers enormous potential for the exploration of musical transmission and relatedness.  In this article, we explore the 16th- and 17th-century instrumental battaglia (battle piece), a genre with a very distinctive collective identity arising from the use of numerous shared ingredients (including melodic, motivic, textural, harmonic and rhythmic features). However, a battaglia is not defined by the presence of a core set of essential features, and exact concordance between these pieces is often remarkably low. This kind of musical 'family resemblance' (formulated after Wittgenstein) poses a serious challenge to both traditional musicological apparatus (for example, finding aids such as thematic catalogues) and Music Information Retrieval (which has often privileged melodic similarity at the expense of other kinds of musical relatedness).  This case study provides a stimulus for rethinking the complex nature of musical similarity. In doing this, we outline a set of requirements for digital tools that could support the discovery, exploration and representation of these kinds of relationships",TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993), minimum description length principal (Lang, 1995), and the χ 2 statistic.', '(Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term.', 'On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance.', 'In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001).', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .', 'Specifically, it is used as a metric for measuring the importance of a word in a document within a collection, so as to improve the recall and the precision of the retrieved documents.']",0,"['Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words.', 'TF-IDF ( term frequency-inverse document frequency ) is one of the widely used feature selection techniques in information retrieval ( #AUTHOR_TAG ) .']"
CC1434,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,learning to classify text using svm,['T Joachims'],related work,,"More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']",0,"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( #AUTHOR_TAG ) , ( Crammer and Singer , 2003 ) , and ( Lewis et al. , 2004 ) .']"
CC1435,W04-1610,Automatic Arabic document categorization based on the Naïve Bayes algorithm,using clustering to boost text classificationquot,"['Y C Fang', 'S Parthasarathy', 'F Schwartz']",related work,"In recent years we have seen a tremendous growth in the number of text document collections available on the Internet. Automatic text categorization, the process of assigning unseen documents to user-defined categories, is an important task that can help in the organization and querying of such collections. In this article we consider the problem of classifying online papers from a specific journal in the geological sciences, over a set of expert defined categories. We evaluate two general strategies and several variants thereof. The first strategy is based on Naive Bayes, a popular text classification algorithm. The second strategy is based on Principle Direction Divisive Partitioning, an unsupervised document clustering algorithm. While the performance of both approaches is quite good, some of the new variants that we propose including one, which involves a combination of these two approaches yield even better results.","For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .","['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']",0,"['The bulk of the text categorization work has been devoted to cope with automatic categorization of English and Latin character documents.', 'For example , ( #AUTHOR_TAG ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .']"
CC1436,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,wordsketch extraction and display of significant collocations for lexicography,"['Adam Kilgarriff', 'David Tugwell']",related work,,"Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']",1,"['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ) , on linguisitic acquisition ( by the use of Part-of-Speech filters hand-crafted by a linguist ) ( Oueslati , 1999 ) or , more frequently , on a combination of the two ( Smadja , 1993 ; #AUTHOR_TAG , for example ) .']"
CC1437,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,two methods for extracting quotspecificquot singleword terms from specialized corpora,"['Chantal Lemay', ""Marie-Claude L'Homme"", 'Patrick Drouin']",,,"The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .","['To construct this test set, we have focused our attention on ten domain-specific terms: commande (command), configuration, fichier (file), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']",5,"['The terms have been identified as the most specific to our corpus by a program developed by Drouin (2003) and called TermoStat.', 'The ten most specific nouns have been produced by comparing our corpus of computing to the French corpus Le Monde , composed of newspaper articles ( #AUTHOR_TAG ) .']"
CC1438,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,symbolic word clustering for mediumsized corpora,"['Benoit Habert', 'Ellie Naulleau', 'Adeline Nazarenko']",related work,"When trying to identify essential concepts and relationships in a medium-size corpus, it is not always possible to rely on statistical methods, as the frequencies are too low. We present an alternative method, symbolic, based on the simplification of parse trees. We discuss the results on nominal phrases of two technical corpora, analyzed by two different robust parsers used for terminology updating in an industrial company. We compare our results with Hindle's scores of similarity.","This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .","['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']",0,"['A number of applications have relied on distributional analysis (Harris, 1971) in order to build classes of semantically related terms.', 'This approach , which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness ( #AUTHOR_TAG , for example ) , does not specify the relationship itself .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not di\x1berentiated.']"
CC1439,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,conceptual structuring through term variation,['Beatrice Daille'],related work,"Term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks. In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French. Conceptual relationships are deduced from specific types of term variations, morphological and syntagmatic, and are expressed through lexical functions. The linguistic precision of the conceptual structuring through morphological variations is of 95%.","More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']",0,"['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : #AUTHOR_TAG uses derivational morphology ; Grabar and Zweigenbaum ( 2002 ) use , as a starting point , a number of identical characters .']"
CC1440,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,termextraction using nontechnical corpora as a point of leverage,['Patrick Drouin'],,,The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .,"['To construct this test set, we have focused our attention on ten domain-speci\x1cc terms: commande (command), con\x1cguration, \x1cchier (\x1cle), Internet, logiciel (software), option, ordinateur (computer ), serveur (server ), syst�me (system), utilisateur (user ).', 'The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .', 'The ten most speci\x1cc nouns have been produced by comparing our corpus of computing to the French corpus Le Monde, composed of newspaper articles (Lemay et al., 2004).', 'Note that to prevent any bias in the results, none of these terms were used as positive examples during the pattern inference step.', '(They were removed from the example set.)']",5,['The terms have been identified as the most specific to our corpus by a program developed by #AUTHOR_TAG and called TER1vloSTAT .']
CC1441,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,lexicallybased terminology structuring some inherent limits,"['Natalia Grabar', 'Pierre Zweigenbaum']",related work,"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.","More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .","['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']",0,"['More recent work on terminology structuring has focussed on formal similarity to develop hypotheses on the semantic relationships between terms : Daille ( 2003 ) uses derivational morphology ; #AUTHOR_TAG use , as a starting point , a number of identical characters .']"
CC1442,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,aide a lacquisition de connaissances a partir de corpus,['Rochdi Oueslati'],related work,"Le probleme d'identification des termes presente un interet particulier pour les applications du taln. En effet, la conception d'outils d'identification de termes et de relations entre termes est d'une aide considerable aux terminologues et aux cogniticiens qui veulent analyser un domaine nouveau. Les terminologues s'interessent surtout a l'etude des termes particulierement dans les domaines de specialite ou les termes designent des objets du domaine de facon la moins ambigue possible. Pour construire une terminologie on part souvent de textes et on applique un ensemble de methodes qui facilitent l'identification des termes. Les methodes classiques utilisent souvent des grammaires et des dictionnaires afin d'acquerir des concepts du domaine d'etude. L'approche que nous presentons dans cette these utilise une approche distributionnelle basee sur les travaux de z. Harris et utilise des algorithmes originaux pour la synthese automatique de contextes entre termes afin d'identifier des relations semantiques propres au domaine. Les resultats obtenus sont d'abord filtres puis valides par un linguiste avant d'etre structures sous forme hierarchique. Ils sont ensuite exploites afin d'acquerir d'autres connaissances en utilisant un processus iteratif et incremental base sur l'inference. L'utilisation d'un langage d'expression de contraintes entre termes du domaines permet de reperer un nombre fini de schemas morphosyntaxiques qui expriment des relations generiques notamment des definitions et des proprietes d'objets. Les resultats obtenus peuvent interesser d'autres travaux comme ceux lies a la construction de bases de connaissances terminologiques ou a la construction d'ontologies partielles propres au domaine.","Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\x1b and Tugwell, 2001, for example).","['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\x1b and Tugwell, 2001, for example).', 'It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs.', 'The original acquisition methodology we present in the next section will allow us to overcome this limitation.']",1,"['On the other hand, other work has been carried out in order to acquire collocations.', 'Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990), on linguisitic acquisition (by the use of Part-of-Speech filters hand-crafted by a linguist) ( #AUTHOR_TAG ) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri\\x1b and Tugwell, 2001, for example).', 'On the other hand, other work has been carried out in order to acquire collocations.']"
CC1443,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,inductive logic programming theory and methods,"['Stephen Muggleton', 'Luc De-Raedt']",method,"AbstractInductive Logic Programming (ILP) is a new discipline which investigates the inductive construction of first-order clausal theories from examples and background knowledge. We survey the most important theories and methods of this new field. First, various problem specifications of ILP are formalized in semantic settings for ILP, yielding a ""model-theory"" for ILP. Second, a generic ILP algorithm is presented. Third, the inference rules and corresponding operators used in ILP are presented, resulting in a ""proof-theory"" for ILP. Fourth, since inductive inference does not produce statements which are assured to follow from what is given, inductive inferences require an alternative form of justification. This can take the form of either probabilistic support or logical constraints on the hypothesis language. Information compression techniques used within ILP are presented within a unifying Bayesian approach to confirmation and corroboration of hypotheses. Also, different ways to constrain the hypothesis language or specify the declarative bias are presented. Fifth, some advanced topics in ILP are addressed. These include aspects of computational learning theory as applied to ILP, and the issue of predicate invention. Finally, we survey some applications and implementations of ILP. ILP applications fall under two different categories: first, scientific discovery and knowledge acquisition, and second, programming assistants","ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.","['ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\x88\x92 ) of the elements one wants to acquire and their context.', 'The contextual patterns produced can then be applied to the corpus in order to retrieve new elements.', 'The acquisition process can be summarized in 3 steps:']",0,"['ASARES is based on a Machine Learning technique , Inductive Logic Programming ( ILP ) ( #AUTHOR_TAG ) , which infers general morpho-syntactic patterns from a set of examples ( this set is noted E + hereafter ) and counter-examples ( E â\\x88\\x92 ) of the elements one wants to acquire and their context.']"
CC1444,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,lexicallybased terminology structuring some inherent limits,"['Natalia Grabar', 'Pierre Zweigenbaum']",introduction,"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.","However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .","['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']",1,"['However , most strategies are based on `` internal  or `` external methods  ( #AUTHOR_TAG ) , i.e. methods that rely on the form of terms or on the information gathered from contexts .', '(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identification process.)', 'The work reported here infers specific semantic relationships based on sets of examples and counterexamples.']"
CC1445,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,selection de termes dans un dictionnaire dinformatique  comparaison de corpus et criteres lexicosemantiques,"[""Marie-Claude L'Homme""]",introduction,"Resume Le present article propose une methode de selection des termes devant faire partie d'un dictionnaire specialise et, plus precisement, un dictionnaire fondamental d'informatique. La methode repose principalement sur un ensemble de criteres lexico-semantiques appliques a un corpus specialise. Elle tient egalement compte de la frequence et de la repartition des unites dans ce corpus. Dans ce travail, nous avons voulu savoir jusqu'a quel point des techniques de comparaison de corpus permettaient de ramener des termes coincidant avec la liste obtenue par l'application des criteres. L'examen de la liste generee automatiquement montre qu'un peu plus de 50 % des unites classees comme etant specifiques par la metrique sont egalement retenues par le terminographe. Les resultats revelent que la technique revet un interet dans la mesure ou elle permet d'aligner des choix sur des donnees extraites de corpus. Toutefois, la selection automatique recele un certain nombre d'imperfections qui doivent etre corrigees par une analyse terminographique.",The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .,"['In this paper, the method is applied to a French corpus on computing to and noun-verb combinations in which verbs convey a meaning of realization.', 'The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']",4,['The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( #AUTHOR_TAG ) .']
CC1446,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,acquisition of qualia elements from corpora — evaluation of a symbolic learning method,"['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']",method,"This paper presents and evaluates a system extracting from a corpus noun-verb pairs whose components are related by a special kind of link: the qualia roles as defined in the Generative Lexicon. This system is based on a symbolic learning method that automatically learns, from noun-verb pairs that are or are not related by a qualia link, rules characterizing positive examples from negative ones in terms of their surrounding part-of-speech or semantic contexts. The qualia noun-verb pair extraction is thus performed by applying the learnt rules on a part-of-speech or semantically tagged text. Stress is put on the quality of the learning when compared with traditional statistical or syntactical-based approaches. The linguistic relevance of the rules is also evaluated through a comparison with manually acquired qualia patterns.","In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .","['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']",4,"['In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( #AUTHOR_TAG ) .']"
CC1447,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,the generative lexicon,['James Pustejovsky'],method,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']",0,"['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( #AUTHOR_TAG ) and called qualia relations ( Bouillon et al. , 2001 ) .', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']"
CC1448,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,query expansion using lexicalsemantic relations,['Ellen M Voorhees'],introduction,"Applications such as office automation, news filtering, help facilities in complex systems, and the like require the ability to retrieve documents from full-text databases where vocabulary problems can be particularly severe. Experiments performed on small collections with single-domain thesauri suggest that expanding query vectors with words that are lexically related to the original query words can ameliorate some of the problems of mismatched vocabularies. This paper examines the utility of lexical query expansion in the large, diverse TREC collection. Concepts are represented by WordNet synonym sets and are expanded by following the typed links included in WordNet. Experimental results show this query expansion technique makes little difference in retrieval effectiveness if the original queries are relatively complete descriptions of the information being sought even when the concepts to be expanded are selected by hand. Less well developed queries can be significantly improved by expansion of hand-chosen concepts. However, an automatic procedure that can approximate the set of hand picked synonym sets has yet to be devised, and expanding by the synonym sets that are automatically generated can degrade retrieval performance.","Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .","['Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval.', 'Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']",1,"['Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by #AUTHOR_TAG with WoRDNET relations ) .']"
CC1449,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,using partofspeech and semantic tagging for the corpusbased learning of qualia structure elements,"['Pierrette Bouillon', 'Vincent Claveau', 'Cecile Fabre', 'Pascale Sebillot']",method,This paper describes the im plementation and results of a machine learning method de veloped within the inductive logic programming ILP frame work Muggleton and De Raedt to automatically extract from a corpus tagged with parts of speech POS and semantic classes noun verb pairs whose components are bound by one of the relations de ned in the qualia structure in the Genera tive Lexicon Pustejovsky We demonstrate that the seman tic tagging of the corpus improves the quality of the learning both on a theoretical and an empiri cal point of view We also show that a set of the rules learnt by our ILP method have a linguistic signi cance regarding the detec tion of the clues that distinguish in terms of POS and seman tic surrounding context noun verb pairs that are linked by one qualia role from others that are not semantically related,"ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .","['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .', 'Here, we propose to use asares in a quite similar way to retrieve our valid N-V pairs.', 'However, the N-V combinations sought are more specific than those that were identified in these previous experiments.']",0,"['ASARES has been previously applied to the acquisition of word pairs sharing semantic relations defined in the Generative Lexicon framework ( Pustejovsky , 1995 ) and called qualia relations ( #AUTHOR_TAG ) .']"
CC1450,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,a comparative evaluation of collocation extraction techniques,['Darren Pearce'],method,"This paper describes an experiment that attempts to compare a range of existing collocation extraction techniques as well as the implementation of a new technique based on tests for lexical substitutability. After a description of the experiment details, the techniques are discussed with particular emphasis on any adaptations that are required in order to evaluate it in the way proposed. This is followed by a discussion on the relative strengths and weaknesses of the techniques with reference to the results obtained. Since there is no general agreement on the exact nature of collocation, evaluating techniques with reference to any single standard is somewhat controversial. Departing from this point, part of the concluding discussion includes initial proposals for a common framework for evaluation of collocation extraction techniques.","Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :","['The main benefits of this acquisition technique lie in the inferred patterns.', 'Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']",0,"['Indeed , contrary to the more classical statistical methods ( Mutual Information , Loglike ... , see below ) used for collocation acquisition ( see ( #AUTHOR_TAG ) for a review ) , these patterns allow :']"
CC1451,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,learning semantic lexicons from a partofspeech and semantically tagged corpus using inductive logic programming,"['Vincent Claveau', 'Pascale Sebillot', 'Cecile Fabre', 'Pierrette Bouillon']",method,"This paper describes an inductive logic programming learning method designed to acquire from a corpus specific Noun-Verb (N-V) pairs---relevant in information retrieval applications to perform index expansion---in order to build up semantic lexicons based on Pustejovsky's generative lexicon (GL) principles (Pustejovsky, 1995). In one of the components of this lexical model, called the qualia structure, words are described in terms of semantic roles. For example, the telic role indicates the purpose or function of an item (cut for knife), the agentive role its creation mode (build for house), etc. The qualia structure of a noun is mainly made up of verbal associations, encoding relational information. The learning method enables us to automatically extract, from a morpho-syntactically and semantically tagged corpus, N-V pairs whose elements are linked by one of the semantic relations defined in the qualia structure in GL. It also infers rules explaining what in the surrounding context distinguishes such pairs from others also found in sentences of the corpus but which are not relevant. Stress is put here on the learning efficiency that is required to be able to deal with all the available contextual information, and to produce linguistically meaningful rules.",ASARES is presented in detail in ( #AUTHOR_TAG ) .,"['The method used for the acquisition of N-V pairs relies mainly on asares, a pattern inference tool.', 'ASARES is presented in detail in ( #AUTHOR_TAG ) .', 'We simply give a short account of its basic principles herein.']",5,['ASARES is presented in detail in ( #AUTHOR_TAG ) .']
CC1452,W04-1805,Discovering Specific Semantic Relationships between Nouns and Verbs in a Specialized French Corpus,structures mathematiques du langage,['Zellig Harris'],related work,,A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .,"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'This approach, which uses words that appear in the context of terms to formulate hypotheses on their semantic relatedness (Habert et al., 1996, for example), does not specify the relationship itself.', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']",0,"['A number of applications have relied on distributional analysis ( #AUTHOR_TAG ) in order to build classes of semantically related terms .', 'Hence, synonyms, co-hyponyms, hyperonyms, etc. are not differentiated.']"
CC1453,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,trec2001 crosslingual retrieval at bbn,"['J Xu', 'A Fraser', 'R Weischedel']",introduction,,"It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .","['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']",0,"['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( #AUTHOR_TAG ; Xu et al. , 2002 ) .']"
CC1454,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,improving machine learning approaches to coreference resolution,"['V Ng', 'C Cardie']",introduction,"Human speakers generally have no difficulty in determining which noun phrases in a text or dialogue refer to the same real-world entity. This task of identifying co-referring noun phrases --- noun phrase coreference resolution --- can present a serious challenge to a natural language processing system, however. Indeed, it is one of the critical problems that currently limits the performance of many practical natural language processing tasks.    State-of-the-art coreference resolution systems operate by relying on a set of hand-crafted heuristics that requires a lot of time and linguistic expertise to develop. Recently, machine learning techniques have been used to circumvent both of these problems by automating the acquisition of coreference resolution heuristics, yielding coreference systems that offer performance comparable to their heuristic-based counterparts. In this dissertation, we present a machine learning-based solution to noun phrase coreference that extends eariler work in the area and outperforms the best existing learning-based coreference engine on a suite of standard coreference data sets. Performance gains accrue from more effective use of the available training data via a set of linguistic and extra-linguistic extensions to the standard machine learning framework for coreference resolution","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; #AUTHOR_TAG ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1455,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,proceedings of ace evaluation and pi meeting,['NIST'],introduction,,"Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",5,"[""The EDR has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the fo-cus of several recent investigations (Bikel et al., 1997;Miller et al., 1998;Borthwick, 1999;Mikheev et al., 1999;Soon et al., 2001;Ng and Cardie, 2002;Florian et al., 2004), and have been at the center of evaluations such as: MUC-6, MUC-7, and the CoNLL'02 and CoNLL'03 shared tasks."", 'Instead , we will adopt the nomenclature of the Automatic Content Extraction program ( #AUTHOR_TAG ) : we will call the instances of textual references to objects/abstractions mentions , which can be either named ( e.g. John Mayor ) , nominal ( the president ) or pronominal ( she , it ) .']"
CC1456,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a stochastic finitestate wordsegmentation algorithm for chinese,"['R Sproat', 'C Shih', 'W Gale', 'N Chang']",,,"In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .","['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .', 'For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.', 'Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.', 'The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model.']",1,"['In addition to the model based upon a dictionary of stems and words , we also experimented with models based upon character n-grams , similar to those used for Chinese segmentation ( #AUTHOR_TAG ) .']"
CC1457,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],conclusion,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .","['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']",5,"['These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the #AUTHOR_TAG Arabic data .', 'The experiments are performed on a clearly specified partition of the data, so comparisons against the presented work can be correctly and accurately made in the future.', 'In addition, we also report results on the official test data.']"
CC1458,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,the unicode standard,"['Joan Aliprand', 'Julie Allen', 'Joe Becker', 'Mark Davis', 'Michael Everson']",,"The Unicode Standard is a global character set for worldwide computing covering the major modern scripts of the world as well as classical forms of Greek, Sanskrit, and Pali. The history and implications of Unicode Standard are discussed. The principles underpinning the design of the Unicode Standard are described with reference to those principles that also are present in USMARC and UNIMARC. Unicode give the potential to support every script. Expanding the character set would have consequences for transcription. Faithfulness of transcription has implications for retrieval. The addition of more characters to support more exact cataloging affects the economic cost of cataloging. The need for characters should be related not to the production of a surrogate for the physical item that has been cataloged, but to facilitating retrieval.","Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .","['(2) .', 'We define a token and introduce whitespace boundaries between every span of one or more alphabetic or numeric characters.', 'Each punctuation symbol is considered a separate token.', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']",5,"['(2) .', 'Character classes , such as punctuation , are defined according to the Unicode Standard ( #AUTHOR_TAG ) .']"
CC1459,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,empirical studies in strategies for arabic information retrieval,"['J Xu', 'A Fraser', 'R Weischedel']",introduction,,"It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .","['We separate the EDR task into two parts: a mention detection step, which identifies and classifies all the mentions in a text -and a coreference resolution step, which combinines the detected mentions into groups that refer to the same object.', 'In its entirety, the EDR task is arguably harder than traditional named entity recognition, because of the additional complexity involved in extracting non-named mentions (nominal and pronominal) and the requirement of grouping mentions into entities.', 'This is particularly true for Arabic where nominals and pronouns are also attached to the word they modify.', 'In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'In addition to the different forms of the Arabic word that result from the derivational and inflectional process, most prepositions, conjunctions, pronouns, and possessive forms are attached to the Arabic surface word.', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']",0,"['In fact, most Arabic words are morphologically derived from a list of base forms or stems, to which prefixes and suffixes can be attached to form Arabic surface forms (blank-delimited words).', 'It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( Xu et al. , 2001 ; #AUTHOR_TAG ) .']"
CC1460,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .,['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .'],1,['The coreference system system is similar to the Bell tree algorithm as described by ( #AUTHOR_TAG ) .']
CC1461,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,arabic verbs and essentials of grammar,"['J Wightwick', 'M Gaafar']",,"Your one-stop guide to mastering the basics of Arabic Can one book have all you need to communicate confidently in a new language? Yes, and that book is Arabic Verbs & Essentials of Grammar. It offers a solid foundation of major verbal and grammatical concepts of the language, from pronouns to idioms and expressions and from irregular verbs to expressions of time. Each unit is devoted to one topic, so you can find what you need right away and get focused instruction immediately. Concise yet thorough, the explanations are supported by numerous examples to help you master the different concepts. And for those tricky verbs, Arabic Verbs & Essentials of Grammar includes a Verb Index of the most common verbs, cross-referenced with the abundant verb tables appearing throughout the book. This book will give you: An excellent introduction to the basics of Arabic if you are a beginner or a quick, thorough reference if you already have experience in the language Contemporary usage of verbs, adjectives, pronouns, prepositions, conjunctions, and other grammar essentials Examples that reflect contemporary usage and real-life situations","Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .","['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']",0,"['Arabic has two kinds of plurals : broken plurals and sound plurals ( #AUTHOR_TAG ; Chen and Gey , 2002 ) .']"
CC1462,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",introduction,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; #AUTHOR_TAG ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']"
CC1463,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.","The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .","['We formulate the mention detection problem as a classification problem, which takes as input segmented Arabic text.', 'We assign to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions.', 'We use a maximum entropy Markov model (MEMM) classifier.', 'The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .', 'One big advantage of this approach is that it can combine arbitrary and diverse types of information in making a classification decision.']",0,"['The principle of maximum entropy states that when one searches among probability distributions that model the observed data ( evidence ) , the preferred one is the one that maximizes the entropy ( a measure of the uncertainty of the model ) ( #AUTHOR_TAG ) .']"
CC1464,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .","['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']",5,"['the mention sub-type , which is a sub-category of the mention type ( #AUTHOR_TAG ) ( e.g. OrgGovernmental , FacilityPath , etc. ) .']"
CC1465,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to named entity recognition,['A Borthwick'],introduction,"This thesis describes a novel statistical named-entity (i.e. ""proper name"") recognition system known as ""MENE"" (Maximum Entropy Named Entity). Named entity (N.E.) recognition is a form of information extraction in which we seek to classify every word in a document as being a person-name, organization, location, date, time, monetary value, percentage, or ""none of the above"". The task has particular significance for Internet search engines, machine translation, the automatic indexing of documents, and as a foundation for work on more complex information extraction tasks.  Two of the most significant problems facing the constructor of a named entity system are the questions of portability and system performance. A practical N.E. system will need to be ported frequently to new bodies of text and even to new languages. The challenge is to build a system which can be ported with minimal expense (in particular minimal programming by a computational linguist) while maintaining a high degree of accuracy in the new domains or languages.  MENE attempts to address these issues through the use of maximum entropy probabilistic modeling. It utilizes a very flexible object-based architecture which allows it to make use of a broad range of knowledge sources in making its tagging decisions. In the DARPA-sponsored MUC-7 named entity evaluation, the system displayed an accuracy rate which was well-above the median, demonstrating that it can achieve the performance goal. In addition, we demonstrate that the system can be used as a post-processing tool to enhance the output of a hand-coded named entity recognizer through experiments in which MENE improved on the performance of N.E. systems from three different sites. Furthermore, when all three external recognizers are combined under MENE, we are able to achieve very strong results which, in some cases, appear to be competitive with human performance.  Finally, we demonstrate the trans-lingual portability of the system. We ported the system to two Japanese-language named entity tasks, one of which involved a new named entity category, ""artifact"". Our results on these tasks were competitive with the best systems built by native Japanese speakers despite the fact that the author speaks no Japanese.","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; #AUTHOR_TAG ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.']"
CC1466,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,language model based arabic word segmentation,"['Y-S Lee', 'K Papineni', 'S Roukos', 'O Emam', 'H Hassan']",,"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix * (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest",#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .,"['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .', 'A trigram language model was used to score and select among hypothesized segmentations determined by a set of prefix and suffix expansion rules.']",5,['#AUTHOR_TAG demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation .']
CC1467,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",introduction,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .,"['Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in .', 'Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']",5,['Both systems are built around from the maximum-entropy technique ( #AUTHOR_TAG ) .']
CC1468,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",introduction,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.","Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classi cation problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granulariity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the pre xes and su_xes are quite frequent, directly processing unsegmented words results in signi cant data sparseness.']",1,"['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( #AUTHOR_TAG ) and the coreference resolution system is similar to the one described in ( Luo et al. , 2004 ) .']"
CC1469,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a statistical model for multilingual entity detection and tracking,"['R Florian', 'H Hassan', 'A Ittycheriah', 'H Jing', 'N Kambhatla', 'X Luo', 'N Nicolov', 'S Roukos']",,"Abstract : Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.",The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .,"['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .', 'We denote these features as backward token tri-grams and forward token tri-grams for the previous and next context of t i respectively.', 'For a token t i , the backward token n-gram feature will contains the previous n − 1 tokens in the history (t i−n+1 , . . .', 't i−1 ) and the forward token n-gram feature will contains the next n − 1 tokens (t i+1 , . . .', 't i+n−1 ).']",0,['The context of a current token ti is clearly one of the most important features in predicting whether ti is a mention or not ( #AUTHOR_TAG ) .']
CC1470,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .,"['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .', 'For Arabic, since words are morphologically derived from a list of roots (stems), we expected that a feature based on the right and left stems would lead to improvement in system accuracy.']",5,['Features using the word context ( left and right tokens ) have been shown to be very helpful in coreference resolution ( #AUTHOR_TAG ) .']
CC1471,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a machine learning approach to coreference resolution of noun phrases,"['W M Soon', 'H T Ng', 'C Y Lim']",introduction,"this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data set","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; #AUTHOR_TAG ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1472,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,language model based arabic word segmentation,"['Y-S Lee', 'K Papineni', 'S Roukos', 'O Emam', 'H Hassan']",,"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix * (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97 % exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest","As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .","['However, an analysis of the errors indicated that the character based model is more effective at segmenting words that do not appear in the training data.', 'We seeked to exploit this ability to generalize to improve the dictionary based model.', 'As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.', 'From this, we create a vocabulary of stems and affixes by requiring that tokens appear more than twice in the supervised training data or more than ten times in the unsupervised, segmented corpus.']",5,"['As in ( #AUTHOR_TAG ) , we used unsupervised training data which is automatically segmented to discover previously unseen stems .', 'In our case, the character n-gram model is used to segment a portion of the Arabic Gigaword corpus.']"
CC1473,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,named entity recognition without gazetteers,"['A Mikheev', 'M Moens', 'C Grover']",introduction,"It is often claimed that Named Entity  recognition systems need extensive  gazetteers|lists of names of people, organisations,  locations, and other named  entities. Indeed, the compilation of such  gazetteers is sometimes mentioned as a  bottleneck in the design of Named Entity  recognition systems.  We report on a Named Entity recognition  system which combines rule-based  grammars with statistical (maximum entropy)  models. We report on the system  &apos;s performance with gazetteers of different  types and dierent sizes, using test  material from the muc{7 competition.  We show that, for the text type and task  of this competition, it is sucient to use  relatively small gazetteers of well-known  names, rather than large gazetteers of  low-frequency names. We conclude with  observations about the domain independence  of the competition and of our experiments.  1 Introduction  Named Entity recognition involves processing a text and identifying certain occurrences of words or expressions ..","The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; #AUTHOR_TAG ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1474,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,automatic content extraction,['ACE'],experiments,"This paper presents an integrated approach to automatically provide an overview of content on Thai websites based on tag cloud. This approach is intended to address the information overload issue by presenting the overview to users in order that they could assess whether the information meets their needs. The approach has incorporated Web content extraction, Thai word segmentation, and information presentation to generate a tag cloud in Thai language as an overview of the key content in the webpage. From the experimental study, the generated Thai Tag clouds are able to provide an overview of the tags which frequently appear in the title and body of the content. Moreover, the first few lines in the tag cloud offer an improved readability","We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .","['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']",5,"['The system is trained on the Arabic ACE 2003 and part of the 2004 data.', 'We introduce here a clearly defined and replicable split of the #AUTHOR_TAG data , so that future investigations can accurately and correctly compare against the results presented here .']"
CC1475,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,an empirical study of smoothing techinques for language modeling,"['S F Chen', 'J Goodman']",,"We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t...","The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .","['In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines.', 'The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations.', 'The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries.', 'The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .', 'Differing from (Lee et al., 2003), we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty.', 'Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation.']",5,"['The final machine is a trigram language model , specifically a Kneser-Ney ( #AUTHOR_TAG ) based backoff language model .']"
CC1476,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,bbn description of the sift system as used for muc7,"['S Miller', 'M Crystal', 'H Fox', 'L Ramshaw', 'R Schwarz', 'R Stone', 'R Weischedel']",introduction,,"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; #AUTHOR_TAG ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1477,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,nymble a highperformance learning namefinder,"['D M Bikel', 'S Miller', 'R Schwartz', 'R Weischedel']",introduction,,"The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .","['In this paper we focus on the Entity Detection and Recognition task (EDR) for Arabic as described in ACE 2004 framework (ACE, 2004).', ""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks ."", 'Usually, in computational linguistics literature, a named entity is an instance of a location, a person, or an organization, and the NER task consists of identifying each of these occurrences.', 'Instead, we will adopt the nomenclature of the Automatic Content Extraction program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g.', 'John Mayor), nominal (the president) or pronominal (she, it).', 'An entity is the aggregate of all the mentions (of any level) which refer to one conceptual entity.', 'For instance, in the sentence President John Smith said he has no comments there are two mentions (named and pronomial) but only one entity, formed by the set {John Smith, he}.']",0,"[""The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( #AUTHOR_TAG ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .""]"
CC1478,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",experiments,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .","['In this section, we present the coreference results on the devtest defined earlier.', 'First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.', 'We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'We report results with two metrics: ECM-F and ACE- Value.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']",5,"['In this section, we present the coreference results on the devtest defined earlier.', 'First, to see the effect of stem matching features, we compare two coreference systems: one with the stem features, the other with- out.', 'We test the two systems on both ""true"" and system mentions of the devtest set.', 'True mentions mean that input to the coreference system are mentions marked by human, while system mentions are output from the mention detection system.', 'ECM-F is an entity-constrained mention Fmeasure ( cfXXX ( #AUTHOR_TAG ) for how ECM-F is computed ) , and ACE-Value is the official ACE evaluation metric .', 'The result is shown in Table 4: the baseline numbers without stem features are listed under \\Base,"" and the results of the coreference system with stem features are listed under \\Base+Stem.""']"
CC1479,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,building an arabic stemmer for information retrieval,"['Aitao Chen', 'Fredic Gey']",,"In TREC 2002 the Berkeley group participated only in the English-Arabic cross-language retrieval (CLIR) track. One Arabic monolingual run and three English-Arabic cross-language runs were submitted. Our approach to the crosslanguage retrieval was to translate the English topics into Arabic using online English-Arabic machine translation systems. The four official runs are named as BKYMON, BKYCL1, BKYCL2, and BKYCL3. The BKYMON is the Arabic monolingual run, and the other three runs are English-to-Arabic cross-language runs. This paper reports on the construction of an Arabic stoplist and two Arabic stemmers, and the experiments on Arabic monolingual retrieval, English-to-Arabic cross-language retrieval.","Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .","['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .', 'The formation of broken plurals is common, more complex and often irregular.', 'As an example, the plural form of the noun (man) is (men), which is formed by inserting the infix .', 'The plural form of the noun (book) is (books), which is formed by deleting the infix .', 'The plural form and the singular form may also be completely different (e.g. for woman, but for women).', 'The sound plurals are formed by adding plural suffixes to singular nouns (e.g., meaning researcher): the plural suffix is for feminine nouns in grammatical cases (e.g.,']",0,"['Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; #AUTHOR_TAG ) .']"
CC1480,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a mentionsynchronous coreference resolution algorithm based on the bell tree,"['Xiaoqiang Luo', 'Abe Ittycheriah', 'Hongyan Jing', 'Nanda Kambhatla', 'Salim Roukos']",introduction,This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.,"Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .","['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .', 'Both systems are built around from the maximum-entropy technique (Berger et al., 1996).', 'We formulate the mention detection task as a sequence classification problem.', 'While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language.', 'The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes.', 'We begin with a segmentation of the written text before starting the classification.', 'This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens).', 'The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions).', 'Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.']",1,"['Both tasks are performed with a statistical framework : the mention detection system is similar to the one presented in ( Florian et al. , 2004 ) and the coreference resolution system is similar to the one described in ( #AUTHOR_TAG ) .']"
CC1481,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,proceedings of ace evaluation and pi meeting,['NIST'],experiments,,"As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .","['We want to investigate the usefulness of stem n- gram features in the mention detection system.', ""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'Detecting the mention boundaries (set of consecutive tokens) and their main type is one of the important steps of our mention detection system.', 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'Hence, to build our mention detection system we spent a lot of effort in improving the rst step: detecting the mention boundary and their main type.', 'In this paper, we report the results in terms of precision, recall, and F-measure3.']",5,"[""As stated before , the experiments are run in the ACE '04 framework ( #AUTHOR_TAG ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) ."", 'The score that the ACE community uses (ACE value) attributes a higher importance (outlined by its weight) to the main type compared to other sub-tasks, such as the mention level and the class.', 'In this paper, we report the results in terms of precision, recall, and F-measure3.']"
CC1482,W05-0709,The impact of morphological stemming on Arabic mention detection and coreference resolution,a maximum entropy approach to natural language processing,"['A Berger', 'S Della Pietra', 'V Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.","where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .","['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']",5,"['where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( #AUTHOR_TAG ) .']"
CC1483,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .,"['We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair.', 'Test subjects were invited via email to participate in the experiment.', 'Thus, they were not supervised during the experiment.', '#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .', 'Their results differed particularly in cases of antonymy or distributionally related pairs.', 'We created a manual with a detailed introduction to SR stressing the crucial points.', 'The manual was presented to the subjects before the experiment and could be re-accessed at any time.', 'During the experiment, one concept pair at a time was presented to the test subjects in random ordering.', 'Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair.', ""Figure 2 shows the system's GUI.""]",4,['#AUTHOR_TAG observed that some annotators were not familiar with the exact definition of semantic relatedness .']
CC1484,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing similarity,"['Ludovic Lebart', 'Martin Rajman']",introduction,"In this paper we study similarity measures for moving curves which can, for example, model changing coastlines or retreating glacier termini. Points on a moving curve have two parameters, namely the position along the curve as well as time. We therefore focus on similarity measures for surfaces, specifically the Fr 'echet distance between surfaces. While the Fr 'echet distance between surfaces is not even known to be computable, we show for variants arising in the context of moving curves that they are polynomial-time solvable or NP-complete depending on the restrictions imposed on how the moving curves are matched. We achieve the polynomial-time solutions by a novel approach for computing a surface in the so-called free-space diagram based on max-flow min-cut duality","words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.","['Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'It is defined on different kinds of textual units, e.g.', 'documents, parts of a document (e.g.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']",0,"['Linguistic distance plays an important role in many applications like information retrieval, word sense disambiguation, text summarization or spelling correction.', 'words and their surrounding context), words or concepts ( #AUTHOR_TAG ). 2 Linguistic distance between words is inverse to their semantic similarity or relatedness.']"
CC1485,W06-1104,Automatically creating datasets for measures of semantic relatedness,identifying semantic relations and functional properties of human verb associations,"['Sabine Schulte im Walde', 'Alissa Melinger']",related work,"This paper uses human verb associations as the basis for an investigation of verb properties, focusing on semantic verb relations and prominent nominal features. First, the lexical semantic taxonymy GermaNet is checked on the types of classic semantic relations in our data; verb-verb pairs not covered by GermaNet can help to detect missing links in the taxonomy, and provide a useful basis for defining non-classical relations. Second, a statistical grammar is used for determining the conceptual roles of the noun responses. We present prominent syntax-semantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions.","In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.","['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im #AUTHOR_TAG ) . Table 1: Comparison of previous experiments.']"
CC1486,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",,,"According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .","['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']",0,"['According to #AUTHOR_TAG , there are three prevalent approaches for evaluating SR measures : mathematical analysis , applicationspecific evaluation and comparison with human judgments .']"
CC1487,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",experiments,,#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .,"['The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts.', 'To create more highly related concept pairs, more sophisticated weighting schemes or selection on the basis of lexical chain- ing could be used.', 'However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'This empty band is not observed here.', 'However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8).', 'The plot clearly shows an empty horizontal band with no judgments.', 'The connection between averaged judgments and standard deviation is plotted in Figure 5.']",1,"['The distribution of averaged human judgments on the whole test set (see Figure 3) is almost balanced with a slight underrepresentation of highly related concepts.', '#AUTHOR_TAG pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .', 'The connection between averaged judgments and standard deviation is plotted in Figure 5.']"
CC1488,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'This setup is also scalable to a higher number of word pairs ( 350 ) as was shown in #AUTHOR_TAG .', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"
CC1489,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],related work,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.",This experiment was again replicated by #AUTHOR_TAG with 10 subjects .,"['In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']",0,"['In the seminal work by Rubenstein and Goodenough (1965), similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards.', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'This experiment was again replicated by #AUTHOR_TAG with 10 subjects .', 'Table 1']"
CC1490,W06-1104,Automatically creating datasets for measures of semantic relatedness,project ‘semantic information retrieval’,['SIR Project'],experiments,"The aim of the SCHEMA Network of Excellence is to bring together a critical mass of universities, research centers, industrial partners and end users, in order to design a reference system for content-based semantic scene analysis, interpretation and understanding. Relevant research areas include: content-based multimedia analysis and automatic annotation of semantic multimedia content, combined textual and multimedia information retrieval, semantic -web, MPEG-7 and MPEG-21 standards, user interfaces and human factors. In this paper, recent advances in content-based analysis, indexing and retrieval of digital media within the SCHEMA Network are presented. These advances will be integrated in the SCHEMA module-based, expandable reference system","In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .","['We extracted word pairs from three different domain-specific corpora (see Table 2).', 'This is motivated by the aim to enable research in information retrieval incorporating SR measures.', ""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]",4,"[""In particular , the `` Semantic Information Retrieval '' project ( SIR #AUTHOR_TAG ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .""]"
CC1491,W06-1104,Automatically creating datasets for measures of semantic relatedness,cooccurrence retrieval a flexible framework for lexical distributional similarity,"['Julie Weeds', 'David Weir']",,"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.","dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( #AUTHOR_TAG ) .']"
CC1492,W06-1104,Automatically creating datasets for measures of semantic relatedness,lexikalischsemantische wortnetze chapter computerlinguistik und sprachtechnologie,['Claudia Kunze'],experiments,,"Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .","['We implemented a set of filters for word pairs.', 'One group of filters removed unwanted word pairs.', 'Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist.', 'Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']",5,"['We implemented a set of filters for word pairs.', 'One group of filters removed unwanted word pairs.', 'Word pairs are filtered if they contain at least one word that a) has less than three letters b) contains only uppercase letters (mostly acronyms) or c) can be found in a stoplist.', 'Another filter enforced a specified fraction of combinations of nouns (N), verbs (V) and adjectives (A) to be present in the result set.', 'We used the following parameters:  were noun-noun pairs, 15% noun-verb pairs and so on.', 'Word pairs containing polysemous words are expanded to concept pairs using GermaNet ( #AUTHOR_TAG ) , the German equivalent to WordNet , as a sense inventory for each word .', 'It is the most complete resource of this type for German.']"
CC1493,W06-1104,Automatically creating datasets for measures of semantic relatedness,an informationtheoretic definition of similarity in,['Dekang Lin'],,,"Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4","['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4', 'However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain application.']",0,"['Mathematical analysis can assess a measure with respect to some formal properties , e.g. whether a measure is a metric ( #AUTHOR_TAG ) .4']"
CC1494,W06-1104,Automatically creating datasets for measures of semantic relatedness,contextual correlates of synonymy,"['Herbert Rubenstein', 'John B Goodenough']",experiments,"Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning.",#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .,"['In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647.', '#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .', 'The values may again not be compared directly.', 'Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.']",1,['#AUTHOR_TAG reported an intra-subject correlation of r = .85 for 15 subjects judging the similarity of a subset ( 36 ) of the original 65 word pairs .']
CC1495,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],related work,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.","We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .","['In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006).', 'We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']",5,"['We used the revised experimental setup ( #AUTHOR_TAG ) , based on discrete relatedness scores and presentation of word pairs in isolation , that is scalable to the higher number of pairs .', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']"
CC1496,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,"In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .","['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.', 'Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure.']",1,"['In our experiment , we annotated a high number of pairs similar in size to the test sets by Finkelstein ( 2002 ) and #AUTHOR_TAG .', 'We used the revised experimental setup (Gurevych, 2005), based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs.', 'We annotated semantic relatedness instead of similarity and included also non noun-noun pairs.']"
CC1497,W06-1104,Automatically creating datasets for measures of semantic relatedness,wordnet an electronic lexical database chapter combining local context and wordnet similarity for word sense identification,"['Claudia Leacock', 'Martin Chodorow']",,,"Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; #AUTHOR_TAG ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']"
CC1498,W06-1104,Automatically creating datasets for measures of semantic relatedness,evaluating wordnetbased measures of semantic distance,"['Alexander Budanitsky', 'Graeme Hirst']",introduction,,Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .,"['Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (Gurevych, 2005). 3', 'Dissimilar words can be semantically related, e.g.', 'via functional relationships (night -dark) or when they are antonyms (high -low).', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']",0,"['Semantic similarity is typically defined via the lexical relations of synonymy (automobile -car) and hypernymy (vehicle -car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist be-tween two words (Gurevych, 2005). 3', 'Dissimilar words can be semantically related, e.g.', 'Many NLP applications require knowledge about semantic relatedness rather than just similarity ( #AUTHOR_TAG ) .']"
CC1499,W06-1104,Automatically creating datasets for measures of semantic relatedness,contextual correlates of synonymy,"['Herbert Rubenstein', 'John B Goodenough']",related work,"Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning.","In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .","['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .', 'Test subjects were instructed to order the cards according to the ""similarity of meaning"" and then assign a continuous similarity value (0.0 -4.0) to each card.', 'Miller and Charles (1991) replicated the experiment with 38 test subjects judging on a subset of 30 pairs taken from the original 65 pairs.', 'This experiment was again replicated by Resnik (1995) with 10 subjects.', 'Table 1']",0,"['In the seminal work by #AUTHOR_TAG , similarity judgments were obtained from 51 test subjects on 65 noun pairs written on paper cards .']"
CC1500,W06-1104,Automatically creating datasets for measures of semantic relatedness,using measures of semantic relatedness for word sense disambiguation,"['Siddharth Patwardhan', 'Satanjeev Banerjee', 'Ted Pedersen']",,This paper generalizes the Adapted Lesk Algorithm of Banerjee and Pedersen (2002) to a method of word sense disambiguation based on semantic relatedness. This is possible since Lesk&apos;s original algorithm (1986) is based on gloss overlaps which can be viewed as a measure of semantic relatedness. We evaluate a variety of measures of semantic relatedness when applied to word sense disambiguation by carrying out experiments using the English lexical sample data of Senseval-2. We find that the gloss overlaps of Adapted Lesk and the semantic distance measure of Jiang and Conrath (1997) result in the highest accuracy,"The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.","['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']",0,"['The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( #AUTHOR_TAG ) or malapropism detection ( Budanitsky and Hirst , 2006 ) . Lebart and Rajman (2000) argue for application-specific evaluation of similarity measures, because measures are always used for some task.']"
CC1501,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing similarity,"['Ludovic Lebart', 'Martin Rajman']",,"In this paper we study similarity measures for moving curves which can, for example, model changing coastlines or retreating glacier termini. Points on a moving curve have two parameters, namely the position along the curve as well as time. We therefore focus on similarity measures for surfaces, specifically the Fr 'echet distance between surfaces. While the Fr 'echet distance between surfaces is not even known to be computable, we show for variants arising in the context of moving curves that they are polynomial-time solvable or NP-complete depending on the restrictions imposed on how the moving curves are matched. We achieve the polynomial-time solutions by a novel approach for computing a surface in the so-called free-space diagram based on max-flow min-cut duality","#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .","['The latter question is tackled by applicationspecific evaluation, where a measure is tested within the framework of a certain application, e.g.', 'word sense disambiguation (Patwardhan et al., 2003) or malapropism detection (Budanitsky and Hirst, 2006).', '#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .', 'But they also note that evaluating a measure as part of a usually complex application only indirectly assesses its quality.', 'A certain measure may work well in one application, but not in another.', 'Application-based evaluation can only state the fact, but give little explanation about the reasons.']",0,"['#AUTHOR_TAG argue for application-specific evaluation of similarity measures , because measures are always used for some task .']"
CC1502,W06-1104,Automatically creating datasets for measures of semantic relatedness,automatic text processing the transformation analysis and retrieval of information by computer,['Gerard Salton'],experiments,,The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .,"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]",5,"['The three preprocessing steps (tokenization, POS-tagging, lemmatization) are performed using TreeTagger (Schmid, 1995).', ""The resulting list of POS-tagged lemmas is weighted using the SMART ` ltc ' 8 tf.idf-weighting scheme ( #AUTHOR_TAG ) .""]"
CC1503,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,#AUTHOR_TAG reported a correlation of r = .69 .,"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG reported a correlation of r = .69 .', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"
CC1504,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],experiments,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.","As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .","['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']",5,"['GermaNet contains only a few conceptual glosses.', 'As they are required to enable test subjects to distinguish between senses , we use artificial glosses composed from synonyms and hypernyms as a surrogate , e.g. for brother : `` brother , male sibling  vs. `` brother , comrade , friend  ( #AUTHOR_TAG ) .', 'We removed words which had more than three senses.']"
CC1505,W06-1104,Automatically creating datasets for measures of semantic relatedness,nonclassical lexical semantic relations,"['Jane Morris', 'Graeme Hirst']",related work,,#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .,"['Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of.', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', '#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']",0,['#AUTHOR_TAG pointed out that many relations between words in a text are non-classical ( i.e. other than typical taxonomic relations like synonymy or hypernymy ) and therefore not covered by semantic similarity .']
CC1506,W06-1104,Automatically creating datasets for measures of semantic relatedness,placing search in context the concept revisited,"['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']",related work,"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.","#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .","['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', 'Gurevych (2005) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German.', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', '#AUTHOR_TAG annotated a larger set of word pairs ( 353 ) , too .', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']"
CC1507,W06-1104,Automatically creating datasets for measures of semantic relatedness,probabilistic partofspeech tagging using decision trees,['Helmut Schmid'],experiments,"In this paper, a new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data. In this tagging method, transition probabilities are estimated using a decision tree. Based on this method, a part-of-speech tagger (called TreeTagger) has been implemented which achieves 96.36 % accuracy on Penn-Treebank data which is better than that of a trigram tagger (96.06 %) on the same data.","The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .","['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .', ""The resulting list of POS-tagged lemmas is weighted using the SMART 'ltc' 8 tf.idf-weighting scheme (Salton, 1989).""]",5,"['The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( #AUTHOR_TAG ) .']"
CC1508,W06-1104,Automatically creating datasets for measures of semantic relatedness,verb semantics and lexical selection,"['Zhibiao Wu', 'Martha Palmer']",,"This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.Comment: 6 pages, Figures and bib files are in part","dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['dictionary-based ( Lesk , 1986 ) , ontology-based ( #AUTHOR_TAG ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .']"
CC1509,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']"
CC1510,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Michael Lesk'],,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.', 'dictionary-based ( #AUTHOR_TAG ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( #AUTHOR_TAG ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .', 'Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g.']"
CC1511,W06-1104,Automatically creating datasets for measures of semantic relatedness,automatic generation of a coarse grained wordnet,"['Rada Mihalcea', 'Dan Moldovan']",experiments,,"If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6","['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.', 'To limit their impact on the experiment, a threshold for the maximal number of senses can be defined.', 'Words with a number of senses above the threshold are removed from the list.']",0,"['If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( #AUTHOR_TAG ) . 6', 'Pairs containing such words are not suitable for evaluation.']"
CC1512,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],related work,,"Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .","['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'Previous studies only included general terms as opposed to domain-specific vocabularies and therefore failed to produce datasets that can be used to evaluate the ability of a measure to cope with domain-specific or technical terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']",0,"['We assume that due to lexical-semantic cohesion, texts contain a sufficient number of words related by means of different lexical and semantic relations.', 'Resulting from our corpus-based approach, test sets will also contain domain-specific terms.', 'This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car).', 'Furthermore , manually selected word pairs are often biased towards highly related pairs ( #AUTHOR_TAG ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .', 'Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations.', 'Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e.', 'other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity.']"
CC1513,W06-1104,Automatically creating datasets for measures of semantic relatedness,placing search in context the concept revisited,"['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman']",experiments,"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.",#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .,"['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', 'The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness.', '#AUTHOR_TAG did not report inter-subject correlation for their larger dataset .', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']"
CC1514,W06-1104,Automatically creating datasets for measures of semantic relatedness,using information content to evaluate semantic similarity,['Philip Resnik'],experiments,"Evaluating Semantic similarity has a widely application areas range from Psychology, Linguistics, Cognitive Science to Artificial Intelligence. This paper proposes the merely use of HowNet to evaluate Information Content (IC) as the semantic similarity of two terms or word senses. While the conventional ways of measuring the IC of word senses must depend on both an ontology like WordNet and a large corpus, experiments of this paper prove that the semantic similarity measured in this method is easy to calculate and more approach to human judgments.","#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .","['The summarized inter-subject correlation between 21 subjects was r=.478 (cf.  is statistically significant at p < .05.', 'This correlation coefficient is an upper bound of performance for automatic SR measures applied on the same dataset.', 'Resnik (1995) reported a correlation of r=.9026. 10', '#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset.', 'Gurevych (2006) reported a correlation of r=.69.', 'Test subjects were trained students of computational linguistics, and word pairs were selected analytically.']",1,"['#AUTHOR_TAG reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .']"
CC1515,W06-1104,Automatically creating datasets for measures of semantic relatedness,using the structure of a conceptual network in computing semantic relatedness,['Iryna Gurevych'],related work,"Abstract. We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences.",#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'However, the original experimental setup is not scalable as ordering several hundred paper cards is a cumbersome task.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'This elaborate process is not feasible for a larger dataset or if domain-specific test sets should be compiled quickly.', 'Therefore, we automatically create word pairs using a corpus-based approach.']",0,"['A comprehensive evaluation of SR measures requires a higher number of word pairs.', 'Furthermore, semantic relatedness is an intuitive concept and being forced to assign fine-grained continuous values is felt to overstrain the test subjects.', '#AUTHOR_TAG replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .', 'She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation.', 'This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006).', 'Finkelstein et al. (2002) annotated a larger set of word pairs (353), too.', 'They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup.', 'In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005) Table 1: Comparison of previous experiments.', 'R/G=Rubenstein and Goodenough, M/C=Miller and Charles, Res=Resnik, Fin=Finkelstein, Gur=Gurevych, Z/G=Zesch and Gurevych similarity from ""not similar"" to ""synonymous"".', 'Therefore, we automatically create word pairs using a corpus-based approach.']"
CC1516,W06-1104,Automatically creating datasets for measures of semantic relatedness,computing semantic relatedness across parts of speech,['Iryna Gurevych'],experiments,,"Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .","['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Judgments for such relations strongly depend on experience and cultural background of the test subjects.', 'While most people may agree 10 Note that Resnik used the averaged correlation coefficient.', 'We computed the summarized correlation coefficient using a Fisher Z-value transformation.', 'that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group.', 'Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']",1,"['Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task.', 'Therefore , inter-subject correlation is lower than the results obtained by #AUTHOR_TAG .']"
CC1517,W06-1104,Automatically creating datasets for measures of semantic relatedness,semantic similarity based on corpus statistics and lexical taxonomy,"['Jay J Jiang', 'David W Conrath']",,"This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.","Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .","['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .', 'The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora.']",0,"['Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( Wu and Palmer , 1994 ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; #AUTHOR_TAG ) or distributional ( Weeds and Weir , 2005 ) .']"
CC1518,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],method,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .","['• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']",3,"['* Site based corpus annotation -in which the user can specify a web site to annotate * Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate * Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger).', 'Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by #AUTHOR_TAGb ) and shingling techniques described by Chakrabarti ( 2002 ) .']"
CC1519,W06-1705,Annotated web as corpus,linguistic search engine,['A Kilgarriff'],related work,"Users of search engines often have specific questions which they hope or believe a particular resource can answer. The problem, from the computer system's perspective, is cognitive understanding of the contents in the source and finding the desired answer. Most of the search engines, with Google on the top, able to retrieve most likely relevant information based on a query. But not capable of providing answer to a question due to lack of deduction capability. In order to find a specific answer to a question, the engine needs to understand the information content and able to do deductive reasoning. Conventional information representation models used in the search engines rely on an extensive use of keywords and their frequencies in storing and retrieving information and other characteristic data on specific body of information. It is believed that we need new approaches for the development of future search engines which will be more effective. Semantic model is an alternative to conventional approach. We have proposed logical-linguistic model where logic and linguistic formalism are used in providing mechanism for computer to understand the contents of the source and deduce answers to questions. The capability of deduction is much depended on the knowledge representation framework used. The approach applies semantic analysis in transforming and normalising information from natural language texts into a declarative knowledge based representation of first order predicate logic. Retrieval of relevant information can then be performed through plausible logical implication and answer to query is carried out using a theorem proving technique. This paper elaborates on the model and how it is used in search engine and question answering system as one unified model","Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]",0,"[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( #AUTHOR_TAG ; Resnik and Elkiss , 2003 ) .""]"
CC1520,W06-1705,Annotated web as corpus,introduction to the special issue on the web as corpus,"['A Kilgarriff', 'G Grefenstette']",introduction,"The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored.","In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( Kennedy , 1998 : 56 ) unless the web is used as a corpus ( #AUTHOR_TAG ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).']"
CC1521,W06-1705,Annotated web as corpus,supporting text mining for escience the challenges for gridenabled natural language processing,"['J Carroll', 'R Evans', 'E Klein']",related work,"Over the last few years, language technology has moved rapidly from 'applied research' to 'engineering', and from small-scale to large-scale engineering. Applications such as advanced text mining systems are feasible, but very resource-intensive, while research seeking to address the underlying language processing questions faces very real practical and methodological limitations. The e-Science vision, and the creation of the e-Science Grid, promises the level of integrated large-scale technological support required to sustain this important and successful new technology area. In this paper, we discuss the foundations for the deployment of text mining and other language technology on the Grid - the protocols and tools required to build distributed large-scale language technology systems, meeting the needs of users, application builders and researchers.","In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .","['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms, P2P is a technology that takes advantage of the resources and services available at the edge of the Internet (Shirky, 2001).', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']",0,"['In the areas of Natural Language Processing ( NLP ) and computational linguistics , proposals have been made for using the computational Grid for data-intensive NLP and text-mining for eScience ( #AUTHOR_TAG ; Hughes et al , 2004 ) .']"
CC1522,W06-1705,Annotated web as corpus,the linguists search engine getting started guide,"['P Resnik', 'A Elkiss']",related work,,"Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]",0,"[""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( Fletcher , 2004a ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; #AUTHOR_TAG ) .""]"
CC1523,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],related work,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists , corpus linguists and lexicographers have been proposed : WebCorp ( Kehoe and Renouf , 2002 ) , KWiCFinder ( #AUTHOR_TAGa ) and the Linguist 's Search Engine ( Kilgarriff , 2003 ; Resnik and Elkiss , 2003 ) .""]"
CC1524,W06-1705,Annotated web as corpus,concordancing the web with kwicfinder third north american,['W H Fletcher'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; #AUTHOR_TAG , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]"
CC1525,W06-1705,Annotated web as corpus,web googles missing pages mystery solved httpaixtalblogspotcom200502webgooglesmissingpagesmysteryhtml accessed,['J Veronis'],related work,,Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .,"['A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies.', 'Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .', 'Tools based on static corpora do not suffer from this problem, e.g.', 'BNCweb 7 , developed at the University of Zurich, and View 8 (Variation in English Words and Phrases, developed at Brigham Young University) are both based on the British National Corpus.', 'Both BNCweb and View enable access to annotated corpora and facilitate searching on part-ofspeech tags.', 'In addition, PIE 9 (Phrases in English), developed at USNA, which performs searches on n-grams (based on words, parts-ofspeech and characters), is currently restricted to the British National Corpus as well, although other static corpora are being added to its database.', 'In contrast, little progress has been made toward annotating sizable sample corpora from the web.']",0,['Word frequency counts in internet search engines are inconsistent and unreliable ( #AUTHOR_TAG ) .']
CC1526,W06-1705,Annotated web as corpus,google as a corpus tool in,['T Robb'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two ap- proaches e.g. automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS4, Amal- gam5, Connexor6) are available, for example, in the application of part-of-speech tags to corpora.', 'Existing tagging systems are �small scale� and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing sin- gle-server systems.', 'This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', 'Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist�s Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).']",0,"['This corpus annotation bot- tleneck becomes even more problematic for vo- luminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; #AUTHOR_TAG ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1527,W06-1705,Annotated web as corpus,automatic profiling of learner texts,"['S Granger', 'P Rayson']",introduction,"In this chapter Crystal's (1991) notion of 'profiling', i.e. the identification of the most salient features in a particular person (clinical linguistics) or register (stylistics), is applied to the field of interlanguage studies. Starting from the assumption that every interlanguage is characterized by a 'unique matrix of frequencies of various linguistic forms' (Krzeszowski 1990: 212), we have submitted two similar-sized corpora of native and non-native writing to a lexical frequency software program to uncover some of the distinguishing features of learner writing. The non-native speaker corpus is taken from the International Corpus of Learner English (ICLE) database. It consists of argumentative essay writing by advanced French-speaking learners of English. The control corpus of similar writing is taken from the Louvain Corpus of Native English Essays (LOCNESS) database. Though limited to one specific type of interlanguage, the approach presented here is applicable to any learner variety and demonstrates a potential of automatic profiling for revealing the stylistic characteristics of EFL texts. In the present study, the learner data is shown to display many of the stylistic features of spoken, rather than written, English.","In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( Mair , 2005 ; #AUTHOR_TAG ) .']"
CC1528,W06-1705,Annotated web as corpus,an introduction to corpus linguistics,['G Kennedy'],introduction,"On first looking into a corpus, teachers as well as students may well be blinded by the sheer scale of the resource and by the possibilities for research that it offers. As the author of this book puts it, ""The research topics in a machine-readable cor pus are potentially as various and wide ranging as are the facts about a language and the use of that language"" (274). The value of this book is that it provides practical examples of the range of research possibilities that a corpus offers, as well as indi cating how corpus-based research projects may be undertaken. It is informed throughout by the view that corpus linguistics is not a separate branch of linguistics, but rather ""descriptive linguistics aided by new technology"" (268). There is some theoretical discussion of the place of corpus linguistics in the wider field, but in general, the author's approach is to let the results speak for themselves. The author's declared aim is to whet the appetites of teachers and students, and in this he clearly succeeds. In the ""Introduction,"" the author suggests that some readers might usefully begin with chapter 3, ""Corpus-Based Descriptions of English."" This is the central part of the book and by far the most valuable in terms of whetting the appetite. It consists of a very comprehensive and wide-ranging review of previous corpus-based research, divided into the following sections: lexical description, grammatical studies cen tered on morphemes or words, grammatical studies centered on the sentence, pragmatics and spoken discourse, and studies of variation. The first section of the chapter investigates how computerized corpora are increasingly being used in lexi cography and continues with a review of collocational studies based on the LOB corpus, as well as work by Sinclair and Renouf on collocational frameworks. Under ""word-centered"" grammatical studies, previous work on modals, voice, aspect, the subjunctive, as well as prepositions and conjunctions are very comprehensively ex emplified and described. The list continues. The latter sections of this chapter re view work by Kuc*era and Francis on sentence length, Altenberg on verb complementation, Mair on nonfinite complementation, Meyer on apposition,","In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['In corpus linguistics building such megacorpora is beyond the scope of individual researchers , and they are not easily accessible ( #AUTHOR_TAG : 56 ) unless the web is used as a corpus ( Kilgarriff and Grefenstette , 2003 ) .']"
CC1529,W06-1705,Annotated web as corpus,distributed video encoding over a peertopeer network,"['D Hughes', 'J Walkerdine']",method,"How does the work advance the state-of-the-art?: Current video encoding technologies tend to focus on single machine solutions, while little or no work on distributed video encoding systems has been undertaken. Current work on distributed computation over peer-to-peer networks primarily focuses upon systems with heavily centralised control [1] [2]. The Distributed Video Encoder is a novel example of fully decentralized ad-hoc distributed computation.","This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .","['The second stage of our work will involve im- plementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plugins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .', 'It is our intention to implement our distributed corpus annotation framework as a plug- in.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS11).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']",0,"['This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( #AUTHOR_TAG ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( Walkerdine and Rayson , 2004 ) .']"
CC1530,W06-1705,Annotated web as corpus,listening to napster in peertopeer harnessing the power of disruptive technologies,['C Shirky'],related work,,"In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .","['In the areas of Natural Language Processing (NLP) and computational linguistics, proposals have been made for using the computational Grid for data-intensive NLP and text-mining for e-Science (Carroll et al., 2005;Hughes et al, 2004).', 'While such an approach promises much in terms of emerging infrastructure, we wish to exploit existing computing infrastructure that is more accessible to linguists via a P2P approach.', 'In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .', 'Better known for file-sharing and Instant Messenger applications, P2P has increasingly been applied in distributed computational systems.', 'Examples include SETI@home (looking for radio evidence of extraterrestrial life), ClimatePrediction.net (studying climate change), Predictor@home (investigating protein-related diseases) and Einstein@home (searching for gravitational signals).']",0,"['In simple terms , P2P is a technology that takes advantage of the resources and services available at the edge of the Internet ( #AUTHOR_TAG ) .']"
CC1531,W06-1705,Annotated web as corpus,p2p4dl digital library over peertopeer,"['J Walkerdine', 'P Rayson']",method,,"This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .","['The second stage of our work will involve implementing the framework within a P2P environment.', ""We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API)."", 'We have designed this environment so that specific application functionality can be captured within plug-ins that can then integrate with the environment and utilise its functionality.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .', 'It is our intention to implement our distributed corpus annotation framework as a plugin.', 'This will involve implementing new functionality and integrating this with our existing annotation tools (such as CLAWS 11 ).', 'The development environment is also flexible enough to utilise the BOINC platform, and such support will be built into it.']",0,"['The second stage of our work will involve implementing the framework within a P2P environment.', 'This system has been successfully tested with the development of plug-ins supporting instant messaging , distributed video encoding ( Hughes and Walkerdine , 2005 ) , distributed virtual worlds ( Hughes et al. , 2005 ) and digital library management ( #AUTHOR_TAG ) .']"
CC1532,W06-1705,Annotated web as corpus,the biggest corpus of allquot,['M Rundell'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; #AUTHOR_TAG ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1533,W06-1705,Annotated web as corpus,introduction to the special issue on the web as corpus,"['A Kilgarriff', 'G Grefenstette']",related work,"The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists' playground. This special issue of Computational Linguistics explores ways in which this dream is being explored.","The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( Kilgarriff , 2001 ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( #AUTHOR_TAG ) .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1534,W06-1705,Annotated web as corpus,bootcat bootstrapping corpora and terms from the web in,"['M Baroni', 'S Bernardini']",related work,"This paper introduces the BootCaT toolkit, a suite of perl programs implementing an iterative procedure to bootstrap specialized corpora and terms from the web. The procedure requires only a small set of seed terms as input. The seeds are used to build a corpus via automated Google queries, and more terms are extracted from this corpus. In turn, these new terms are used as seeds to build a larger corpus via automated queries, and so forth. The corpus and the unigram terms are then used to extract multi-word terms. We conducted an evaluation of the tools by applying them to the construction of English and Italian corpora and term lists from the domain of psychiatry. The results illustrate the potential usefulness of the tools.",#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', '#AUTHOR_TAG built a corpus by iteratively searching Google for a small set of seed terms .']"
CC1535,W06-1705,Annotated web as corpus,creating specialized and general corpora using automated search engine queries web as corpus workshop,"['M Baroni', 'S Sharoff']",introduction,,"Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .","['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']",0,"['A key aspect of our case study research will be to investigate extending corpus collection to new document types.', 'Most web-derived corpora have exploited raw text or HTML pages , so efforts have focussed on boilerplate removal and cleanup of these formats with tools like Hyppia-BTE , Tidy and Parcels3 ( #AUTHOR_TAG ) .']"
CC1536,W06-1705,Annotated web as corpus,using the web to overcome data sparseness,"['F Keller', 'M Lapata', 'O Ourioupina']",introduction,"This paper shows that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the web by querying a search engine. We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus. We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments.","Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005;Granger and Rayson, 1998).', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['Increasingly , corpus researchers are tapping the Web to overcome the sparse data problem ( #AUTHOR_TAG ) .']"
CC1537,W06-1705,Annotated web as corpus,finding syntactic structure in unparsed corpora the gsearch corpus query system computers and the humanities,"['S Corley', 'M Corley', 'F Keller', 'M Crocker', 'S Trewin']",related work,,The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .,"['""Real-time"" linguistic analysis of web data at the syntactic level has been piloted by the Linguist\'s Search Engine (LSE).', 'Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org)', 'or build their own collections from AltaVista search engine results.', 'The second method pushes the new collection onto a queue for the LSE annotator to analyse.', 'A new collection does not become available for analysis until the LSE completes the annotation process, which may entail significant delay with multiple users of the LSE server.', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.', 'In contrast, the Sketch Engine system to assist lexicographers to construct dictionary entries requires large pre-annotated corpora.', ""A word sketch is an automatic one-page corpus-derived summary of a word's grammatical and collocational behaviour."", 'Word Sketches were first used to prepare the Macmillan English Dictionary for Advanced Learners (2002, edited by Michael Rundell).', 'They have also served as the starting point for high-accuracy Word Sense Disambiguation.', 'More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite).']",0,"['Using this tool, linguists can either perform syntactic searches via parse trees on a pre-analysed web collection of around three million sentences from the Internet Archive (www.archive.org)', 'The Gsearch system ( #AUTHOR_TAG ) also selects sentences by syntactic criteria from large on-line text collections .', 'Gsearch annotates corpora with a fast chart parser to obviate the need for corpora with pre-existing syntactic mark-up.']"
CC1538,W06-1705,Annotated web as corpus,introduction to the special issue on evaluating word sense disambiguation systems,"['P Edmonds', 'A Kilgarriff']",introduction,"Has system performance on Word Sense Disambiguation (WSD) reached a limit? Automatic systems don't perform nearly as well as humans on the task, and from the results of the SENSEVAL exercises, recent improvements in system performance appear negligible or even negative. Still, systems do perform much better than the baselines, so something is being done right. System evaluation is crucial to explain these results and to show the way forward. Indeed, the success of any project in WSD is tied to the evaluation methodology used, and especially to the formalization of the task that the systems perform. The evaluation of WSD has turned out to be as difficult as designing the systems in the first place.","Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .","['Linguistic annotation of corpora contributes crucially to the study of language at several levels: morphology, syntax, semantics, and discourse.', 'Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']",0,"['Its significance is reflected both in the growing interest in annotation software for word sense tagging ( #AUTHOR_TAG ) and in the long-standing use of part-of-speech taggers , parsers and morphological analysers for data from English and many other languages .']"
CC1539,W06-1705,Annotated web as corpus,web as corpus,['A Kilgarriff'],related work,,"The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .","['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g. automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS4, Amal- gam5, Connexor6) are available, for example, in the application of part-of-speech tags to corpora.', 'Existing tagging systems are �small scale� and typically impose some limitation to prevent over- load (e.g. restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Studies have used several different methods to mine web data.', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', 'Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist�s Search En- gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).']",0,"['This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times ( #AUTHOR_TAG ; Robb , 2003 ; Rundell , 2000 ; Fletcher , 2001 , 2004b ) and received a special issue of the journal Computational Linguistics ( Kilgarriff and Grefenstette , 2003 ) .', 'Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler.', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.']"
CC1540,W06-1705,Annotated web as corpus,word sense disambiguation by web mining for word cooccurrence probabilities,['P Turney'],related work,"This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word co-occurrence probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler.",#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .,"['The vast majority of previous work on corpus annotation has utilised either manual coding or automated software tagging systems, or else a semi-automatic combination of the two approaches e.g.', 'automated tagging followed by manual correction.', 'In most cases a stand-alone system or client-server approach has been taken by annotation software using batch processing techniques to tag corpora.', 'Only a handful of web-based or email services (CLAWS 4 , Amalgam 5 , Connexor 6 ) are available, for example, in the application of part-of-speech tags to corpora.', ""Existing tagging systems are 'small scale' and typically impose some limitation to prevent overload (e.g."", 'restricted access or document size).', 'Larger systems to support multiple document tagging processes would require resources that cannot be realistically provided by existing single-server systems.', 'This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web.', 'The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001;Robb, 2003;Rundell, 2000;Fletcher, 2001Fletcher, , 2004b and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003).', 'Studies have used several different methods to mine web data.', '#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .', 'Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms.', ""Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist's Search Engine (Kilgarriff, 2003;Resnik and Elkiss, 2003).""]",0,['#AUTHOR_TAG extracts word co-occurrence probabilities from unlabelled text collected from a web crawler .']
CC1541,W06-1705,Annotated web as corpus,facilitating the compilation and dissemination of adhoc web corpora,['W H Fletcher'],,"Since the World Wide Web gained prominence in the mid-1990s it has tantalized language investigators and ins ructors as a virtually unlimited source of machine-readable texts for compiling corpora and developing teaching materials. The broad range of languages and content domains found online also offers translators enormous promise both for translation by-example and as a comprehensive supplement to published reference works This paper surveys the impediments which s ill prevent the Web from realizing its full potential as a linguistic resource and discusses tools to overcome the remaining hurdles. Identifying online documents which are both relevant and reliable presents a major challenge. As a partial solution the author's Web concordancer KWiCFinder au omates the process of seeking and retrieving webpages Enhancements which permit more focused queries than existing search engines and provide search results in an interactive explora ory environmen are described in detail. Despite the efficiency of automated downloading and excerpting, selecting Web documents still entails significant time and effort. To multiply the benefits of a search, an online forum for sharing annotated search reports and linguistically interesting texts with other users is outlined. Furthermore, the orien ation of commercial sea ch engines toward the general public makes them less beneficial for linguistic research. The author sketches plans for a specialized Search Engine for Applied Linguis s and a selective Web Corpus Archive which build on his experience with KWiCFinder. He compares his available and proposed solutions to existing resou ces, and su veys ways to exploi them in language teaching. Together these proposed services will enable language learners and professionals to tap into the Web effectively and efficiently for instruction research and translation. t","Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .","['We will explore techniques to make the resulting annotated web corpus data available in static form to enable replication and verification of corpus studies based on such data.', 'The initial solution will be to store the resulting reference 11 http://www.comp.lancs.ac.uk/ucrel/claws/ corpus in the Sketch Engine.', 'We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Current practise elsewhere includes the distribution of URL lists, but given the dynamic nature of the web, this is not sufficiently robust.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .', 'Other requirements for reference corpora such as retrieval and storage of metadata for web pages are beyond the scope of what we propose here.']",0,"['We will also investigate whether the distributed environment underlying our approach offers a solution to the problem of reproducibility in web-based corpus studies based in general.', 'Other solutions such as complete caching of the corpora are not typically adopted due to legal concerns over copyright and redistribution of web data , issues considered at length by #AUTHOR_TAGa ) .']"
CC1542,W06-1705,Annotated web as corpus,the corpusbased study of language change in progress the extra value of tagged corpora,['C Mair'],introduction,,"In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .","['In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003).', 'Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002).', 'This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006.', 'In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .', 'As the size of a corpus increases, a near linear increase in computing power is required to annotate the text.', 'Although processing power is steadily growing, it has already become impractical for a single computer to annotate a mega-corpus.']",0,"['In addition , the advantages of using linguistically annotated data over raw data are well documented ( #AUTHOR_TAG ; Granger and Rayson , 1998 ) .']"
CC1543,W06-1705,Annotated web as corpus,blueprint for a high performance nlp infrastructure,['J R Curran'],related work,"Natural Language Processing (NLP) system de-velopers face a number of new challenges. In-terest is increasing for real-world systems that use NLP tools and techniques. The quantity of text now available for training and processing is increasing dramatically. Also, the range of languages and tasks being researched contin-ues to grow rapidly. Thus it is an ideal time to consider the development of new experimen-tal frameworks. We describe the requirements, initial design and exploratory implementation of a high performance NLP infrastructure.",#AUTHOR_TAG,['#AUTHOR_TAG'],0,['#AUTHOR_TAG']
CC1544,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free culture how big media uses technology and the law to lock down culture and control creativity,['Lawrence Lessig'],,"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0.",We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .,"['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .', 'Narrative writings or essays are creative works and they generally treat ownership as authorship, even for the most enthusiastic fellows of free culture (Stallman, 2001).']",5,['We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( #AUTHOR_TAG ) .']
CC1545,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins,['George P Landow'],related work,,Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .,"['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .', 'Storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""However, in our opinion Storyspace is a product of its time and in fact it isn't a web application."", 'Although it is possible to label links, it lacks a lot of features we need.', 'Moreover, no hypertext writing tool available is released under an open source licence.', 'We hope that Novelle will bridge this gap -we will choose the exact licence when our first public release is ready.']",0,"['Apart from wikis, blogs, and cognitive mapping, we were also inspired by the experiences of early hypertext writing tools, in particular Intermedia and Storyspace.', 'In fact, they were used expecially in academic writing with some success.', 'Intermedia is no more developed and nobody of us had the opportunity to try it ( #AUTHOR_TAG ) .', 'Storyspace is currently distributed by Eastgate ( 2005), and we have used it for a time.', ""However, in our opinion Storyspace is a product of its time and in fact it isn't a web application."", 'Moreover, no hypertext writing tool available is released under an open source licence.']"
CC1546,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,hypertext 20 the convergence of contemporary critical theory and technology the johns hopkins,['George P Landow'],introduction,,"Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) .","[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.', 'In hypertexts transitions from one lexia to another are not necessarily sequential, but navigational.', 'The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992):']",5,"[""Following the example of #AUTHOR_TAG , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) ."", 'Consequently, a hypertext is a set of lexias.', 'The main problems of hypertexts, acknowledged since the beginning, have been traced as follows (Nelson, 1992):']"
CC1547,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,understanding comics,['Scott McCloud'],introduction,"During the spring semester of 2010, as part of my graduate program in English Education, I took a class titled American Comic Book. I took what I learned there, turned around, and immediately applied it to my own teaching. I teach 7th grade language arts and developed a unit on understanding and creating comics, pulling from what I was learning in the class at the University of Iowa, and utilized some other resources including Great Source u27s Daybook of Critical Reading and Writing, and ideas from other books on using graphic novels as a teaching tool. The unit was taught during April and May of this year. I have collected my lesson plans, examples of student work, and much, much more on a website, http://sites.google.com/site/7thgradecomicsunit/","For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) .","['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) ."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']",0,"[""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", ""For example , a ` web page ' is more similar to an infinite canvas than a written page ( #AUTHOR_TAG ) ."", 'This situation could make new problems rise up: Who owns the text?']"
CC1548,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free software free society selected essays of,['Richard M Stallman'],,"Please contact the GNU Press for information regarding bulk purchases for classroom or user group use, reselling, or any other questions or comments. Permission is granted to make and distribute verbatim copies of this book provided the copyright notice and this permission notice are preserved on all copies. Permission is granted to copy and distribute translations of this book into another language, from the original English, with respect to the conditions on distribution of modified versions above, provided that it has been approved by the Free Software Foundation. The waning days of the 20th century seemed like an Orwellian nightmare: laws preventing publication of scientific research on software; laws preventing sharing software; an overabundance of software patents preventing development; and end-user license agreements that strip the user of all freedoms--including ownership, privacy, sharing, and understanding how their software works. This collection of essays and speeches by Richard M. Stallman addresses many of these issues. Above all, Stallman discusses the philosophy underlying the free software movement. This movement combats the oppression of federal laws and evil end-user license agreements in hopes of spreading the idea of software freedom. With the force of hundreds of thousands of developers working to create GNU software and the GNU/Linux operating system, free software has secured a spot on the servers that control the Internet, and--as it moves into the desktop computer market--is a threat to Microsoft and other proprietary software companies. These essays cater to a wide audience; you do not need a computer science background to understand the philosophy and ideas herein. However, there is a "" Note on Software, "" to help the less technically inclined reader become familiar with some common computer science jargon and concepts, as well as footnotes throughout. Many of these essays have been updated and revised from their originally published version. Each essay carries permission to redistribute verbatim copies. The ordering of the essays is fairly arbitrary, in that there is no required order to read the essays in, for they were written independently of each other over a period of 18 years. The first section, "" The GNU Project and Free Software, "" is intended to familiarize you with the history and philosophy of free software and the GNU project. Furthermore, it provides a road map for developers, educators, and business people to pragmatically incorporate free software into society, business, and life. The second section, "" Copyright, ...","Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .","['We believe that ownership has an important role and we do not want to force our users to take a non-attributive copyright licence to their work.', 'We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve (Lessig, 2004).', 'Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']",0,"['Narrative writings or essays are creative works and they generally treat ownership as authorship , even for the most enthusiastic fellows of free culture ( #AUTHOR_TAG ) .']"
CC1549,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,wikipedia from wikipedia the free encyclopedia,['Wikipedia'],,"administrator, born in Campinas, Brazil. He holds a doctoral degree in physiology from the University of Sao Paulo. His main contributions as a researcher and publisher concern the following areas[1]: neuroethology; medical informatics; health sciences; internet and web applications in medicine, biology and health; ; artificial neural networks; distance education and e-learning; telemedicine; biological and health impacts of non-ionizing radiation and history of neuroscience. He is currently the president of the Edumed Institute, a non-profit R&amp;D institution and director of Edulogica Educacao &amp; Tecnologia, a consultant and commercial enterprise specializing in distance education. artificial intelligenc Research and education Sabbatini began his scientific career in neurophysiology in 1966, while he was a medical student at the Medical School of the University of Sao Paulo at Ribeirao Preto [2]. He began to work in basic biomedical research under the supervision of Prof. Miguel Rolando Covian, an Argentine neurophysiologist, who encouraged him to found, after he graduated in 1968, the first research laboratory of neuroethology in Latin America [3] , at the Department of Physiology. Also there, he started in 1970 one of the first Brazilian and Latin American groups of research, development and education on the computer applications in biomedicine. Sabbatini got a doctorate in behavioral neuroscience in 1977 and immediately thereafter went to spend two and a half years doing postdoctoral work i","On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .","['The emphasis on narrativity takes into account the use of blogs as public diaries on the web, that is still the main current interpretation of this literary genre, or metagenre (McNeill, 2005).', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']",0,"['Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'On the other side , wikis started as collective works where each entry is not owned by a single author e.g. #AUTHOR_TAG .', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']"
CC1550,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free culture how big media uses technology and the law to lock down culture and control creativity,['Lawrence Lessig'],,"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0.",Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .,"['If a user lets others edit some lexias, he has the right to retain or refuse the attribution when other users have edited it.', 'In the first instance, the edited version simply moves ahead the document history.', 'In the second one, the last user, who has edited the lexia, may claim the attribution for himself.', 'The lexia will be marked as a derivative work from the original one, and a new document history timeline will start (see Figure 2).', 'Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .', 'If nobody claims the document for himself, it will fall in the public domain.', 'The set of lexias in the public domain will form a special document, owned by a special user, called Public Domain.', ""If the author refuses the permission to create derivative works, i.e. to edit his own lexias, users still have the right to comment the author's work."", 'So as to come to terms with this idea, we need a concept invented by Nelson (1992), i.e. transclusion.', ""Rather than copy-and-paste contents from a lexia, a user may recall a quotation of the author's lexia and write a comment in the surroundings."", ""In doing so, the link list of the author's lexia will be updated with a special citation link marker, called quotation link (see later for details)."", ""Usually, the quotation will be 'frozen', as in the moment where it was transcluded (see Figure 3)."", 'Consequently the transclusion resembles a copiedand-pasted text chunk, but the link to the original document will always be consistent, i.e. neither it expires nor it returns an error.', 'Otherwise the user who has transcluded the quotation may choose to keep updated the links to the original document.', 'This choice has to be made when the transclusion is done.', 'If so, the transcluded quotation will update automatically, following the history timeline of the original document.', 'For example, if the original document changes topic from stars to pentagons, the quotation transcluded will change topic too (see Figure 4).']",0,['Authors may choose this right with the No-Deriv option of the Creative Commons licences ( #AUTHOR_TAG ) .']
CC1551,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,the wiki way  quick collaboration on the web,"['Ward Cunningham', 'Bo Leuf']",related work,"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001","While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .","['The main source of Novelle are wikis and blogs.', 'While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .', 'So we have tested and compared the most used tools available for blogging: Bloggers, WordPress, MovableType and LiveJournal.']",0,"['While wikis have spread from a detailed design ( #AUTHOR_TAG ) , unfortunately blogs have not been designed under a model .']"
CC1552,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,free culture how big media uses technology and the law to lock down culture and control creativity,['Lawrence Lessig'],,"espanolEl presente texto publicado en la Zona REMIX de la revista Communiars se corresponde con la introduccion del afamado libro Cultura Libre, del profesor Lawrence Lessig, presidente de la organizacion Creative Commons, dedicada a promover el acceso e intercambio culturales. El texto > (en espanol Cultura Libre. Como los grandes medios usan la tecnologia y la ley para bloquear la cultura y controlar la creatividad) es un libro publicado en 2004 y centrado en presentar otra manera de organizar la cultura y el conocimiento, abriendo las restricciones del obsoleto paradigma del copyright, y apoyandose en el modelo copyleft promovido desde el software libre. La introduccion que aqui se presenta traduce el espiritu abierto de un texto clave para la comprension y evolucion de la actualidad cultural. La version que se publica procede la version PDF de Free Culture, licenciada bajo Creative Commons en su variante BY-NC 1.0. EnglishThe present text published in the REMIX Zone of the Communiars Journal corresponds to the introduction of the famous book Free Culture, by Professor Lawrence Lessig, president of the Creative Commons organization, dedicated to promoting cultural access and exchange. The text Free Culture. How big media uses technology and the law to lock down culture and control creativity is a book published in 2004 and focused on presenting another way of organizing culture and knowledge, opening the restrictions of the obsolete paradigm of copyright, and relying on the copyleft model promoted by free software. The introduction presented here translates the open spirit of a key text for the understanding and evolution of current cultural reality. The version that is published is part of PDF version of Free Culture, licensed under Creative Commons in its variant BY-NC 1.0.","We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG ) .","['With our typology of links, we aim to solve the framing problem as defined in Section 1.2.', 'We want to model views as dynamic objects -the creation of context will be still arbitrary, but changes are very easily.', 'We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG ) .']",5,"['We would also provide a user facility for choosing the right licence for every lexia , following the model of Creative Commons licences ( #AUTHOR_TAG ) .']"
CC1553,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,genre under construction the diary on the internet,['Laurie McNeill'],related work,"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary","Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .","['Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .', ""The only way to retrieve information is through a search engine or a calendar, i.e. the date of the 'post' -a lexia in the jargon of bloggers.""]",0,"['Generally speaking , we find that the personal public diary metaphor behind blogs ( #AUTHOR_TAG ) may bring to an unsatisfactory representation of the context .']"
CC1554,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,genre under construction the diary on the internet,['Laurie McNeill'],,"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary","The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG ) .","['The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG ) .', 'Furthermore we noticed that blogs and wikis are currently subjected to osmosis, because they have in common the underlying core technology.', 'So blogs are a literary metagenre which started as authored personal diaries or journals.', ""Now they try to collect themselves in so-called 'blogspheres'."", 'On the other side, wikis started as collective works where each entry is not owned by a single author -e.g.', 'Wikipedia (2005).', 'Now personal wiki tools are arising for brainstorming and mind mapping.', 'See Section 4 for further aspects.']",0,"['The emphasis on narrativity takes into account the use of blogs as public diaries on the web , that is still the main current interpretation of this literary genre , or metagenre ( #AUTHOR_TAG ) .', 'So blogs are a literary metagenre which started as authored personal diaries or journals.']"
CC1555,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,the printing revolution in early modern europe,['Elizabeth L Eisenstein'],introduction,"What difference did printing make? Although the importance of the advent of printing for the Western world has long been recognized, it was Elizabeth Eisenstein in her monumental, two-volume work, The Printing Press as an Agent of Change, who provided the first full-scale treatment of the subject. This illustrated and abridged edition provides a stimulating survey of the communications revolution of the fifteenth century. After summarizing the initial changes, and introducing the establishment of printing shops, it considers how printing effected three major cultural movements: the Renaissance, the Reformation, and the rise of modern science. First Edition Hb (1984) 0-521-25858-8 First Edition Pb (1984) 0-521-27735-3","For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) .","['Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Terms as �chapter�, �page� or �foot- note� simply become meaningless in the new texts, or they highly change their meaning.', 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) ."", 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', 'For example, a �web page� is more similar to an infinite canvas than a written page (McCloud, 2001).', 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an �opera aperta� (open work), as Eco would define it (1962).', 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap - the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text? Which role is suitable for authors? We have to analyse them before presenting the architecture of Novelle.']",0,"['Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', 'Terms as chapter, page or foot- note simply become meaningless in the new texts, or they highly change their meaning.', 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example , when books should n't be copied by hand any longer , authors took the advantage and start writing original books and evaluation -- i.e. literary criticism -- unlike in the previous times ( #AUTHOR_TAG ) .""]"
CC1556,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,writing space the computer hypertext and the history of writing erlbaum associates,['Jay David Bolter'],introduction,Getting the books writing space the computer hypertext and the history of writing now is not type of challenging means. You could not solitary going bearing in mind books deposit or library or borrowing from your friends to gain access to them. This is an totally easy means to specifically acquire lead by on-line. This online declaration writing space the computer hypertext and the history of writing can be one of the options to accompany you gone having new time.,1.1 Hypertext as a New Writing Space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.,"['1.1 Hypertext as a New Writing Space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis (McNeill, 2005) emphasize annotation, comment, and strong editing.', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']",0,['1.1 Hypertext as a New Writing Space #AUTHOR_TAG was the first scholar who stressed the impact of the digital revolution to the medium of writing.']
CC1557,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,wikipedia from wikipedia the free encyclopedia,['Wikipedia'],,"administrator, born in Campinas, Brazil. He holds a doctoral degree in physiology from the University of Sao Paulo. His main contributions as a researcher and publisher concern the following areas[1]: neuroethology; medical informatics; health sciences; internet and web applications in medicine, biology and health; ; artificial neural networks; distance education and e-learning; telemedicine; biological and health impacts of non-ionizing radiation and history of neuroscience. He is currently the president of the Edumed Institute, a non-profit R&amp;D institution and director of Edulogica Educacao &amp; Tecnologia, a consultant and commercial enterprise specializing in distance education. artificial intelligenc Research and education Sabbatini began his scientific career in neurophysiology in 1966, while he was a medical student at the Medical School of the University of Sao Paulo at Ribeirao Preto [2]. He began to work in basic biomedical research under the supervision of Prof. Miguel Rolando Covian, an Argentine neurophysiologist, who encouraged him to found, after he graduated in 1968, the first research laboratory of neuroethology in Latin America [3] , at the Department of Physiology. Also there, he started in 1970 one of the first Brazilian and Latin American groups of research, development and education on the computer applications in biomedicine. Sabbatini got a doctorate in behavioral neuroscience in 1977 and immediately thereafter went to spend two and a half years doing postdoctoral work i","In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG ) .","['AJAX is not a technology in itself but a term that refers to the use of a group of technologies together, in particular Javascript and XML.', 'In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG ) .']",0,"['AJAX is not a technology in itself but a term that refers to the use of a group of technologies together, in particular Javascript and XML.', 'In other words AJAX is a web development technique for creating interactive web applications using a combination of XHTML and CSS , Document Object Model ( or DOM ) , the XMLHTTPRequest object ( #AUTHOR_TAG ) .']"
CC1558,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,learning creating and using knowledge concept maps as facilitative tools in schools and corporations lawrence erlbaum associates,['Joseph Donald Novak'],related work,,"Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG ) .","[""Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG ) .']",0,"[""Concept mapping has been used at least in education for over thirty years, in particular at the Cornell University, where Piaget's ideas gave the roots to the assimilation theory by David Ausubel."", 'Very briefly, concept maps show the relationships between concepts labelling both nodes and arcs.', 'Every arc always has a definite direction , i.e. arcs are arrows ( #AUTHOR_TAG ) .']"
CC1559,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,ruby on rails web developement that doesn’t hurt url httpwwwrubyonrailsorg retrieved the 03rd of january,['Ruby on Rails'],,,The Ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes .,['The Ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes .'],5,['The Ruby on #AUTHOR_TAG framework permits us to quickly develop web applications without rewriting common functions and classes .']
CC1560,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,the wiki way  quick collaboration on the web,"['Ward Cunningham', 'Bo Leuf']",introduction,"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001","Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .","['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'History snapshots of the timeline may be considered as permanent views, i.e. views with a timestamp.', 'Consequently, except in the case of sandboxes, every change in the document cannot be erased.', 'This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.']",0,"['In wikis every document keeps track of its own history: creating a document means to start a history, editing a document to move ahead, restoring to move back onto the history timeline, destroying a document to stop the history itself.', 'Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( #AUTHOR_TAG ) .', 'Figure 1 shows the model.', 'This model will have a strong impact on the role of links and on the underpinning structure of Novelle itself.']"
CC1561,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,genre under construction the diary on the internet,['Laurie McNeill'],introduction,"The emergence of the diary as a digital form has generated the kinds of introduction and explanation that typically accumulate around emerging genres, even though online diarists in many ways strive to reproduce the stereotypical print diary. However, as diarists and readers explore the nature of blogs, both in diary entries and comments pages, a tension is apparent between users ' accounts or explanations of the genre and their actual practices, and this tension provides a rich site for studying the evolution of the diary genre. Readers ' and writers ' comments illustrate the blogging community's ideas about genre as a concept and how these ideas transfer to the ""new "" world of online media. In this paper, I look at the diary's transition from page to screen, and consider how readers and writers build on and diverge from print culture practices in establishing expectations and ""rules "" for Weblogs. Examining how diarists and their communities establish and police the digital diary, and how generic knowledge is circulated and codified, helps understand the particular social actions the diary can perform only on the Internet. In contemporary Western culture the diary is among the most familiar genres, an enduring everyday form that has evolved and adapted across time yet has remained recognizable in each instance. As a genre that straddles and confuses ideals of the literary and non-literary","Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .","['1.1 Hypertext as a New Writing Space Bolter (1991) was the first scholar who stressed the impact of the digital revolution to the medium of writing.', ""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', ""For example, when books shouldn't be copied by hand any longer, authors took the advantage and start writing original books and evaluation -i.e."", 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', ""For example, a 'web page' is more similar to an infinite canvas than a written page (McCloud, 2001)."", 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', ""From a positive point of view these new forms of writing may realize the postmodernist and decostructionist dreams of an 'opera aperta' (open work), as Eco would define it (1962)."", 'From a more pessimistic one, an author may feel to have lost power in this openness.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'They give more power to readers, eventually filling the gap -the so-called active readers become authors as well.', 'This situation could make new problems rise up: Who owns the text?', 'Which role is suitable for authors?', 'We have to analyse them before presenting the architecture of Novelle.']",0,"[""Terms as 'chapter', 'page' or 'footnote' simply become meaningless in the new texts, or they highly change their meaning."", 'When Gutenberg invented the printing press and Aldo Manuzio invented the book as we know it, new forms of writings arose.', 'literary criticism -unlike in the previous times (Eisenstein, 1983).', 'Nowadays the use of computers for writing has drammatically changed, expecially after their interconnection via the internet, since at least the foundation of the web (Berners-Lee, 1999).', 'Moreover, what seems to be lost is the relations, like the texture underpinning the text itself.', 'Henceforth the collaborative traits of blogs and wikis ( #AUTHOR_TAG ) emphasize annotation , comment , and strong editing .', 'This situation could make new problems rise up: Who owns the text?']"
CC1562,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,the wiki way  quick collaboration on the web,"['Ward Cunningham', 'Bo Leuf']",,"Foreword. Preface. Why This Book? Why You Want to Read This. Book Structure. The Authors. Contributors and Colleagues. Errata and Omissions. Contacting Us. Read the Book, Use the Wiki! I. FROM CONCEPTS TO USING WIKI. 1. Introduction to Discussion and Collaboration Servers. In this Chapter. Collaboration and Discussion Tools. Collaboration Models. Who Uses Collaborative Discussion Servers? Whatever For? Features of a Web-Based Collaboration. On the Horizon: WebDAV. Comparing Wiki to Other Collaboration Tools. 2. What's a ""Wiki""? The Wiki Concept. The Essence of Wiki. The User Experience. Usefulness Criteria. Wiki Basics. Wiki Clones. Wiki Implementations by Language. Other Wiki Offerings. Non-Wiki Servers. Wiki Application. Pros and Cons of a Wiki-Style Server. Why Consider Setting Up a Wiki? Other Issues. 3. Installing Wiki. QuickiWiki--Instant Serve. Installing Perl. Installing QuickiWiki. Multiple Instances. Wiki and Webserver. Wiki on IIS or PWS. The Apache Webserver. Installing Apache. Reconfiguring Apache. Testing Webserver Wiki. Wrapper Scripts. General Security Issues. Security and Database Integrity. Server Vulnerabilities. Addressing wiki Vulnerabilities. Configuring Your Browser Client. Fonts, Size and Layout. 4. Using Wiki. In this Chapter. Quicki Quick-Start. A Virtual Notebook. Making Wiki Notes, A Walkthrough. Wiki as PIM. A Working Example. The Content Model. Internal and External Hyperlink Models. Browsing Pages. Editing Pages. The Browser Editing Model. Building Wiki Content. Editing and Markup Conventions. 5. Structuring Wiki Content. In this Chapter. Wiki Structure. Structure Types. Only a Click Away. How Hard to Try. When to Impose Structure. When Not to Impose Structure. What is the Purpose of the Wiki? Structure Patterns. When to Spin Off New Wiki Servers. II. UNDERSTANDING THE HACKS. 6. Customizing Your Wiki. In this Chapter. Hacking Your Wiki Source. Copyright and Open Source License Policy. Why Customize? What to Customize. 7. Wiki Components Examined. In this Chapter. Dissecting QuickiWiki. QuickiWiki Component Model. Core QuickiWiki Modules. Sever Component. Optional Extended Components. Analyzing Page Content. Managing User Access. 8. Alternatives and Extensions. Parsing the Requests. ClusterWiki Component Model. The Library Module. Special Features. Spell Checking. Uploading Files. A Standard Wiki? 9. Wiki Administration and Tools. In this Chapter. Events History. Tracking Page Edits. Usage Statistics. Abuse Management. Access Management. Permission Models. Adding Authentication and Authorization. Administering the Database. Page Conversions. Page Management. Backup Issues. Server Resources and Wiki Loading. Avoiding User Waits. Implementing Wiki Constraints. Debugging a Wiki. Programming Resources. Backups. Low-Tech Debugging. Higher-Level Debugging. III. IMAGINE THE POSSIBILITIES. 10. Insights and Other Voices. In this Chapter. Wiki Culture. Wiki as Open Community. Writing Style Contention. Why Wiki Works. The Open-Edit Issue. When Wiki Doesn't Work. Public Wiki Issues. Wiki Style Guidelines. Notifying About Update. Design and Portability. Wiki Trade-Offs. Portability. The Future of Wiki. 11. Wiki Goes Edu. In this Chapter. CoWeb at Georgia Tech. Introduction to CoWeb. CoWeb Usage. Supported CoWeb User Roles. CoWeb Open Authoring Projects. Overall Conclusions. 12. Wiki at Work. In this Chapter. Case Studies. WikiWikiWeb. New York Times Digital. TWiki at TakeFive. TWiki at Motorola. Kehei Wiki Case Studies. A Rotary Wiki. Wiki Workplace Essentials. Why a Workplace Wiki? Planning the Wiki. Selection Stage. Implementation Stage. Day-to-Day Operations. Appendix A: Syntax Comparisons. Hyperlink Anchors. Markup Conventions. Escaped Blocks. HTML Tag Inclusion. Other Syntax Extensions Seen. Appendix B: Wiki Resources. Book Resources. Internet Resources. Appendix C: List of Tips. Index. 020171499XTO5232001","The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .","['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'In fact a document is still a set of lexias, but every document is only the set of historical versions of the document itself.', 'Generally, people avoid commenting, preferring to edit each document.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']",0,"['On the contrary, in wikis no lexia is authored and there is no hierarchy between lexias.', 'The paradigm is ""write many , read many"" ( #AUTHOR_TAG ) .']"
CC1563,W06-2807,Writing with Dyslexia: The Education and Early Work of Wendy Wasserstein,ajax a new approach to web applications url httpwwwadaptivepathcompublicationsessays archives000385php retrieved the 22nd of december,['Jesse James Garrett'],,,AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .,"['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']",0,"['We used the Asyncronous Javascript And XML (or AJAX) paradigm to create the graphical user interface.', 'AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( #AUTHOR_TAG ) .']"
CC1564,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,floresta sint´actica” a treebank for portuguese,"['S Afonso', 'E Bick', 'R Haber', 'D Santos']",experiments,,"This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .","['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']",1,"['This is noticeable for German ( Brants et al. , 2002 ) and Portuguese ( #AUTHOR_TAG ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']"
CC1565,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,pseudoprojective dependency parsing,"['J Nivre', 'J Nilsson']",introduction,,â¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG ) .,"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• History-based feature models for predicting the next parser action (Black et al., 1992).', '• Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'â\x80¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG ) .']",5,"['* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '* Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', 'â\x80¢ Graph transformations for recovering nonprojective structures ( #AUTHOR_TAG ) .']"
CC1566,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,the tiger treebank,"['S Brants', 'S Dipper', 'S Hansen', 'W Lezius', 'G Smith']",experiments,"Proceedings of the 16th Nordic Conference   of Computational Linguistics NODALIDA-2007.  Editors: Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit.  University of Tartu, Tartu, 2007.  ISBN 978-9985-4-0513-0 (online)  ISBN 978-9985-4-0514-7 (CD-ROM)  pp. 81-88","This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DËzeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .","['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002), although this cannot be explained by a high proportion of non-projective structures.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']",1,"['A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots.', 'This is noticeable for German ( #AUTHOR_TAG ) and Portuguese ( Afonso et al. , 2002 ) , which still have high overall accuracy thanks to very high attachment scores , but much more conspicuous for Czech ( B Â¨ ohmov Â´ a et al. , 2003 ) , Dutch ( van der Beek et al. , 2002 ) and Slovene ( DË\x87zeroski et al. , 2006 ) , where root precision drops more drastically to about 69 % , 71 % and 41 % , respectively , and root recall is also affected negatively .', 'On the other hand, all three languages behave like high-accuracy languages with respect to attachment score.', 'One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.']"
CC1567,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,memorybased dependency parsing,"['J Nivre', 'J Hall', 'J Nilsson']",method,,The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG .,['The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG .'],5,['The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by #AUTHOR_TAG .']
CC1568,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,mamba meets tiger reconstructing a swedish treebank from antiquity,"['J Nilsson', 'J Hall', 'J Nivre']",experiments,,"Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #AUTHOR_TAG ) .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #AUTHOR_TAG ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']",0,"['Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( #AUTHOR_TAG ) .']"
CC1569,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,design and implementation of the bulgarian hpsgbased treebank,"['K Simov', 'P Osenova', 'A Simov', 'M Kouylekov']",experiments,,"Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']",0,"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( #AUTHOR_TAG ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']"
CC1570,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,sinica treebank design criteria representational issues and implementation,"['K Chen', 'C Luo', 'M Chang', 'F Chen', 'C Chen', 'C Huang', 'Z Gao']",experiments,"The disclosed apparatus measures the mass rate of flow of gas through an orifice of a flow nozzle under critical flow conditions and comprises a reservoir having an inlet connected to a source of high pressure through a first valve and an outlet connected to a flow nozzle adapted to conduct pressurized gas from said reservoir under critical flow conditions to a region of low pressure downstream of the flow nozzle. The critical flow conditions are established by a second valve mounted downstream of the nozzle orifice. Density and pressure measuring devices are coupled to the reservoir and provide signals representative of the density and pressure, respectively, of the gas flowing through the flow nozzle. The density and pressure signals are applied to a device which calculates the square root of the product of density and pressure to provide a signal proportional to the mass rate of gas flow.","Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']",0,"['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( #AUTHOR_TAG ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']"
CC1571,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,towards historybased grammars using richer models for probabilistic parsing,"['E Black', 'F Jelinek', 'J D Lafferty', 'D M Magerman', 'R L Mercer', 'S Roukos']",introduction,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.",â¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG ) .,"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'â\x80¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG ) .', '• Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002).', '• Graph transformations for recovering nonprojective structures .']",5,['â\x80¢ History-based feature models for predicting the next parser action ( #AUTHOR_TAG ) .']
CC1572,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,maltparser a datadriven parsergenerator for dependency parsing,"['J Nivre', 'J Hall', 'J Nilsson']",introduction,,"All experiments have been performed using MaltParser ( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1","['All experiments have been performed using MaltParser ( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1']",5,"['All experiments have been performed using MaltParser ( #AUTHOR_TAG ) , version 0.4 , which is made available together with the suite of programs used for preand post-processing .1']"
CC1573,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,libsvm a library for support vector machines software available at httpwwwcsientuedutw cjlinlibsvm,"['C-C Chang', 'C-J Lin']",method,,"More specifically , we use LIBSVM ( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .","['We use support vector machines to predict the next parser action from a feature vector representing the history.', 'More specifically , we use LIBSVM ( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .', 'Symbolic features are converted to numerical features using the standard technique of binarization, and we split values of the FEATS field into its atomic components. 4', 'or some languages, we divide the training data into smaller sets, based on some feature s (normally the CPOS or POS of the next input token), which may reduce training times without a significant loss in accuracy (Yamada and Matsumoto, 2003).', 'To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.']",5,"['More specifically , we use LIBSVM ( #AUTHOR_TAG ) with a quadratic kernel K ( xZ , xj ) = ( - yxT xj + r ) 2 and the built-in one-versus-all strategy for multi-class classification .']"
CC1574,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,headdriven statistical models for natural language parsing,['M Collins'],experiments,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG ) .,['6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG ) .'],1,['6The analysis is reminiscent of the treatment of coordination in the Collins parser ( #AUTHOR_TAG ) .']
CC1575,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,pseudoprojective dependency parsing,"['J Nivre', 'J Nilsson']",method,,"Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG .","['Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG .']",0,"['Although the parser only derives projective graphs , the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudoprojective approach of #AUTHOR_TAG .']"
CC1576,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,statistical dependency analysis with support vector machines,"['H Yamada', 'Y Matsumoto']",method,"In this paper, we propose a method for analyzing word-word dependencies using deterministic bottom-up manner using Support Vector machines. We experimented with dependency trees converted from Penn treebank data, and achieved over 90% accuracy of word-word dependency. Though the result is little worse than the most up-to-date phrase structure based parsers, it looks satisfactorily accurate considering that our parser uses no information from phrase structures.","For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ) .","['For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ) .', 'To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t.']",0,"['For some languages , we divide the training data into smaller sets , based on some feature s ( normally the CPOS or POS of the next input token ) , which may reduce training times without a significant loss in accuracy ( #AUTHOR_TAG ) .']"
CC1577,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,practical annotation scheme for an hpsg treebank of bulgarian in,"['K Simov', 'P Osenova']",experiments,"The paper presents an HPSG-based annotation scheme for constructing a Bulgarian treebank: BulTreeBank. It differs from other grammar-based annotation schemes in having a hybrid status with respect to the partial parsing component and the full parsing module. As the parsing complexity is handled preferably by the pre-processing step, the task of the HPSG module is maximally facilitated and simplified.","Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.']",0,"['before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian ( Simov et al. , 2005 ; #AUTHOR_TAG ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .']"
CC1578,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,stylebook for the japanese treebank in verbmobil verbmobilreport 240 seminar f¨ur sprachwissenschaft,"['Y Kawata', 'J Bartels']",experiments,,"Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .","['An overall error analysis is beyond the scope of this paper, but we will offer a few general observations 5 Detailed specifications of the feature models and learning algorithm parameters can be found on the MaltParser web page.', 'before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'If we start by considering languages with a labeled attachment score of 85% or higher, they are characterized by high precision and recall for root nodes, typically 95/90, and by a graceful degradation of attachment score as arcs grow longer, typically 95-90-85, for arcs of length 1, 2 and 3-6.', 'Typical examples are Bulgarian (Simov et al., 2005;Simov and Osenova, 2003), Chinese (Chen et al., 2003), Danish (Kromann, 2003), and Swedish .', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']",1,"['before we turn to Swedish and Turkish, focusing on recall and precision of root nodes, as a reflection of global syntactic structure, and on attachment score as a function of arc length.', 'Japanese ( #AUTHOR_TAG ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .']"
CC1579,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,japanese dependency analysis using cascaded chunking,"['T Kudo', 'Y Matsumoto']",introduction,"In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.",Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .,"['• A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', '• History-based feature models for predicting the next parser action (Black et al., 1992).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .', '• Graph transformations for recovering nonprojective structures .']",5,"['* A deterministic algorithm for building labeled projective dependency graphs (Nivre, 2006).', 'Support vector machines for mapping histories to parser actions ( #AUTHOR_TAG ) .']"
CC1580,W06-2933,Labeled pseudo-projective dependency parsing with support vector machines,the annotation process in the turkish treebank,"['N B Atalay', 'K Oflazer', 'B Say']",experiments,"We present a progress report of the Turkish Treebank concentrating on various aspects of its design and implementation. In addition to a review of the corpus compilation process and the design  of the annotation scheme, we describe the details of various pre-processing stages and the computer-assisted annotation process","By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .","['The results for Arabic (Hajič et al., 2004;Smrž et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).', 'By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']",1,"['By contrast , Turkish ( Oflazer et al. , 2003 ; #AUTHOR_TAG ) exhibits high root accuracy but consistently low attachment scores ( about 88 % for length 1 and 68 % for length 2 ) .', 'It is noteworthy that Arabic and Turkish, being ""typological outliers"", show patterns that are different both from each other and from most of the other languages.']"
CC1581,W06-3309,Generative content models for structural analysis of medical abstracts,categorization of sentence types in medical abstracts,"['Larry McKnight', 'Padmini Srinivasan']",introduction,"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques.",#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .,"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', '#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,['#AUTHOR_TAG have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques .']
CC1582,W06-3309,Generative content models for structural analysis of medical abstracts,semiautomatic indexing of full text biomedical articles,"['Clifford W Gay', 'Mehmet Kayaalp', 'Alan R Aronson']",introduction,"The main application of U.S. National Library of Medicine's Medical Text Indexer (MTI) is to provide indexing recommendations to the Library's indexing staff. The current input to MTI consists of the titles and abstracts of articles to be indexed. This study reports on an extension of MTI to the full text of articles appearing in online medical journals that are indexed for Medline. Using a collection of 17 journal issues containing 500 articles, we report on the effectiveness of the contribution of terms by the whole article and also by each section. We obtain the best results using a model consisting of the sections Results, Results and Discussion, and Conclusions together with the article's title and abstract, the captions of tables and figures, and sections that have no titles. The resulting model provides indexing significantly better (7.4%) than what is currently achieved using only titles and abstracts.","For example , #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example , #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example , #AUTHOR_TAG experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.']"
CC1583,W06-3309,Generative content models for structural analysis of medical abstracts,modern applied statistics with splus,"['William N Venables', 'Brian D Ripley']",method,,"#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .","['In an attempt to further boost performance, we employed Linear Discriminant Analysis (LDA) to find a linear projection of the four-dimensional vec- tors that maximizes the separation of the Gaussians (corresponding to the HMM states).', '#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .', 'Each sentence/vector is then mul- tiplied by this matrix, and new HMM models are re-computed from the projected data.']",5,"['#AUTHOR_TAG describe an efficient algorithm ( of linear complexity in the number of training sentences ) for computing the LDA transform matrix , which entails computing the withinand between-covariance matrices of the classes , and using Singular Value Decomposition ( SVD ) to compute the eigenvectors of the new space .']"
CC1584,W06-3309,Generative content models for structural analysis of medical abstracts,knowledge extraction for clinical question answering preliminary results,"['Dina Demner-Fushman', 'Jimmy Lin']",conclusion,"The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing of text in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalize major aspects of evidence-based medicine in the form of knowledge extractors that serve as the fundamental building blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specific knowledge can be effectively leveraged to extract PICO frame elements from MEDLINE abstracts. Clinical in-formation systems in support of physicians ' decision-making process have the potential to improve the qual-ity of patient care in real-world settings","Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .","['Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.', 'The true utility of content models is to structure abstracts that have no structure to begin with.', 'Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.', 'Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.']",3,"['Such a component would serve as the first stage of a clinical question answering system ( #AUTHOR_TAG ) or summarization system ( McKeown et al. , 2003 ) .']"
CC1585,W06-3309,Generative content models for structural analysis of medical abstracts,on discriminative vs generative classifiers a comparison of logistic regression and naive bayes,"['Andrew Y Ng', 'Michael Jordan']",introduction,"Comparison of generative and discriminative classifiers is an ever-lasting topic. As an important contribution to this topic, based on their theoretical and empirical comparisons between the naive Bayes classifier and linear logistic regression, Ng and Jordan (NIPS 841-848, 2001) claimed that there exist two distinct regimes of performance between the generative and discriminative classifiers with regard to the training-set size. In this paper, our empirical and simulation studies, as a complement of their work, however, suggest that the existence of the two distinct regimes may not be so reliable. In addition, for real world datasets, so far there is no theoretically correct, general criterion for choosing between the discriminative and the generative approaches to classification of an observation x into a class y; the choice depends on the relative confidence we have in the correctness of the specification of either p(y vertical bar x) or p(x, y) for the data. This can be to some extent a demonstration of why Efron (J Am Stat Assoc 70(352):892-898, 1975) and O'Neill (J Am Stat Assoc 75(369):154-160, 1980) prefer normal-based linear discriminant analysis (LDA) when no model mis-specification occurs but other empirical studies may prefer linear logistic regression instead. Furthermore, we suggest that pairing of either LDA assuming a common diagonal covariance matrix (LDA-A) or the naive Bayes classifier and linear logistic regression may not be perfect, and hence it may not be reliable for any claim that was derived from the comparison between LDA-A or the naive Bayes classifier and linear logistic regression to be generalised to all generative and discriminative classifiers","Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG ) .","['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', 'Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'Since HMMs are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'In fact, we demonstrate that HMMs are competitive with SVMs, with the added advantage of lower computational complexity.', 'In addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'In the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs.']",0,"['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( Joachims , 1998 ; #AUTHOR_TAG ) .', 'In the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs.']"
CC1586,W06-3309,Generative content models for structural analysis of medical abstracts,catching the drift probabilistic content models with applications to generation and summarization,"['Regina Barzilay', 'Lillian Lee']",method,"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.","Following Ruch et al. ( 2003 ) and #AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .","['Following Ruch et al. ( 2003 ) and #AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .', 'The four states in our HMMs correspond to the information that characterizes each section (""introduction"", ""methods"", ""results"", and ""conclusions"") and state transitions capture the discourse flow from section to section.']",5,"['Following Ruch et al. ( 2003 ) and #AUTHOR_TAG , we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts .']"
CC1587,W06-3309,Generative content models for structural analysis of medical abstracts,what’s yours and what’s mine determining intellectual attribution in scientific text,"['Simone Teufel', 'Marc Moens']",introduction,"We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves.We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text.","The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( #AUTHOR_TAG ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'For a variety of reasons, medicine is an interesting domain of research.', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"
CC1588,W06-3309,Generative content models for structural analysis of medical abstracts,the htk book,"['Steve Young', 'Gunnar Evermann', 'Thomas Hain', 'Dan Kershaw', 'Gareth Moore', 'Julian Odell', 'Dave Ollason', 'Dan Povey', 'Valtcho Valtchev', 'Phil Woodland']",method,,"Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .","['We then built a four-state Hidden Markov Model that outputs these four-dimensional vectors.', 'The transition probability matrix of the HMM was initialized with uniform probabilities over a fully connected graph.', 'The output probabilities were modeled as four-dimensional Gaussians mixtures with diagonal covariance matrices.', 'Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .', 'For testing, we performed a Viterbi (maximum likelihood) estimation of the label of each test sentence/vector (also using the HTK toolkit).']",5,"['We then built a four-state Hidden Markov Model that outputs these four-dimensional vectors.', 'The output probabilities were modeled as four-dimensional Gaussians mixtures with diagonal covariance matrices.', 'Using the section labels , the HMM was trained using the HTK toolkit ( #AUTHOR_TAG ) , which efficiently performs the forward-backward algorithm and BaumWelch estimation .']"
CC1589,W06-3309,Generative content models for structural analysis of medical abstracts,genre analysis english inacademic and research settings,['John M Swales'],introduction,"In recent years the concept of 'register' has been increasingly replaced by emphasis on the analysis of genre, which relates work in sociolinguistics, text linguistics and discourse analysis to the study of specialist areas of language. This book is a clear, authoritative guide to this complex area. He provides a survey of approaches to varieties of language, and considers these in relation to communication and task-based language learning. Swales outlines an approach to the analysis of genre, and then proceeds to consider examples of different genres and how they can be made accessible through genre analysis. This is important reading for all those working in teaching English for academic purposes and also of interest to those working in post-secondary writing and composition due to relevant issues in writing across the curriculum.","As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\x98asan , 2001 ) .', 'The ability to explicitly identify these sections in un-structured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'Demner-Fushman et al. (2005) found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction""  , ""methods""  , ""results""  , and ""conclusions""  ( SalangerMeyer , 1990 ; #AUTHOR_TAG ; OrË\\x98asan , 2001 ) .']"
CC1590,W06-3309,Generative content models for structural analysis of medical abstracts,catching the drift probabilistic content models with applications to generation and summarization,"['Regina Barzilay', 'Lillian Lee']",conclusion,"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.","An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG ) .","['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG ) .', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second, using continuous distributions allows us to leverage a variety of tools (e.g., LDA) that have been shown to be successful in other fields, such as speech recognition (Evermann et al., 2004).']",1,"['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors ( log probabilities of observing entire sentences based on our language models ) , as opposed to sequences of terms , as done in ( #AUTHOR_TAG ) .']"
CC1591,W06-3309,Generative content models for structural analysis of medical abstracts,text categorization with support vector machines learning with many relevant features,['Thorsten Joachims'],introduction,"Abstract. This paper explores the use of Support Vector Machines (SVMs) for learning text classi ers from examples. It analyzes the particular properties of learning with text data and identi es why SVMs are appropriate for this task. Empirical results support the theoretical ndings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly overavariety of di erent learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.","Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .","['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .', 'However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing.', 'Under certain conditions, generative approaches with linear complexity are preferable, even if their performance is lower than that which can be achieved through discriminative training.', 'Since HMMs are very wellsuited to modeling sequences, our discourse modeling task lends itself naturally to this particular generative approach.', 'In fact, we demonstrate that HMMs are competitive with SVMs, with the added advantage of lower computational complexity.', 'In addition, generative models can be directly applied to tackle certain classes of problems, such as sentence ordering, in ways that discriminative approaches cannot readily.', 'In the context of machine learning, we see our work as contributing to the ongoing debate between generative and discriminative approacheswe provide a case study in an interesting domain that begins to explore some of these tradeoffs.']",0,"['Discriminative approaches ( especially SVMs ) have been shown to be very effective for many supervised classification tasks ; see , for example , ( #AUTHOR_TAG ; Ng and Jordan , 2001 ) .']"
CC1592,W06-3309,Generative content models for structural analysis of medical abstracts,effective mapping of biomedical text to the umls metathesaurus the metamap program,['Alan R Aronson'],introduction,"The UMLS (r) Metathesaurus(r), the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classi-fied by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map bio-medical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomed-ical literature at the library","Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( #AUTHOR_TAG ) for concept identification and SemRep ( Rindflesch and Fiszman , 2003 ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']"
CC1593,W06-3309,Generative content models for structural analysis of medical abstracts,an unsupervised approach to recognizing discourse relations,"['Daniel Marcu', 'Abdessamad Echihabi']",related work,"We present an unsupervised approach to recognizing discourse relations of CON-TRAST, EXPLANATION-EVIDENCE, CON-DITION and ELABORATION that hold be-tween arbitrary spans of texts. We show that discourse relation classifiers trained on examples that are automatically ex-tracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not ex-plicitly marked by cue phrases.","Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ) .","['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ) .', 'Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts.']",1,"['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( McKeown , 1985 ; #AUTHOR_TAG ) .']"
CC1594,W06-3309,Generative content models for structural analysis of medical abstracts,answering physicians’ clinical questions obstacles and potential solutions,"['John W Ely', 'Jerome A Osheroff', 'M Lee Chambliss', 'Mark H Ebell', 'Marcy E Rosenbaum']",introduction,"To identify the most frequent obstacles preventing physicians from answering their patient-care questions and the most requested improvements to clinical information resources.Qualitative analysis of questions asked by 48 randomly selected generalist physicians during ambulatory care.Frequency of reported obstacles to answering patient-care questions and recommendations from physicians for improving clinical information resources.The physicians asked 1,062 questions but pursued answers to only 585 (55%). The most commonly reported obstacle to the pursuit of an answer was the physician's doubt that an answer existed (52 questions, 11%). Among pursued questions, the most common obstacle was the failure of the selected resource to provide an answer (153 questions, 26%). During audiotaped interviews, physicians made 80 recommendations for improving clinical information resources. For example, they requested comprehensive resources that answer questions likely to occur in practice with emphasis on treatment and bottom-line advice. They asked for help in locating information quickly by using lists, tables, bolded subheadings, and algorithms and by avoiding lengthy, uninterrupted prose.Physicians do not seek answers to many of their questions, often suspecting a lack of usable information. When they do seek answers, they often cannot find the information they need. Clinical resource developers could use the recommendations made by practicing physicians to provide resources that are more useful for answering clinical questions.","The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; Gorman et al. , 1994 ; #AUTHOR_TAG ) .']"
CC1595,W06-3309,Generative content models for structural analysis of medical abstracts,what’s yours and what’s mine determining intellectual attribution in scientific text,"['Simone Teufel', 'Marc Moens']",related work,"We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves.We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text.","Our task is closer to the work of #AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts .","['Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985;Marcu and Echihabi, 2002).', 'Our task is closer to the work of #AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts .']",1,"['Our task is closer to the work of #AUTHOR_TAG , who looked at the problem of intellectual attribution in scientific texts .']"
CC1596,W06-3309,Generative content models for structural analysis of medical abstracts,leveraging a common representation for personalized search and summarization in a medical digital library,"['Kathleen McKeown', 'Noemie Elhadad', 'Vasileios Hatzivassiloglou']",conclusion,"Despite the large amount of online medical literature, it can be difficult for clinicians to find relevant information at the point of patient care. In this paper, we present techniques to personalize the results of search, making use of the online patient record as a sophisticated, pre-existing user model. Our work in PERSIVAL, a medical digital library, includes methods for re-ranking the results of search to prioritize those that better match the patient record. It also generates summaries of the re-ranked results which highlight information that is relevant to the patient under the physician's care. We focus on the use of a common representation for the articles returned by search and the patient record which facilitates both the re-ranking and the summarization tasks. This common approach to both tasks has a strong positive effect on the ability to personalize information","Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ) .","['Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process.', 'In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled.', 'The true utility of content models is to structure abstracts that have no structure to begin with.', 'Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance.', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ) .', 'We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured.']",3,"['The true utility of content models is to structure abstracts that have no structure to begin with.', 'Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( #AUTHOR_TAG ) .']"
CC1597,W06-3309,Generative content models for structural analysis of medical abstracts,categorization of sentence types in medical abstracts,"['Larry McKnight', 'Padmini Srinivasan']",introduction,"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques.",Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.,['Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.'],1,['Table (b) again reproduces the results from #AUTHOR_TAG (2003) for a comparable task on a different subset of 206 unstructured abstracts.']
CC1598,W06-3309,Generative content models for structural analysis of medical abstracts,zone analysis in biology articles as a basis for information extraction,"['Yoko Mizuta', 'Anna Korhonen', 'Tony Mullen', 'Nigel Collier']",introduction,"In the field of biomedicine, an overwhelming amount of experimental data has become available as a result of the high throughput of research in this domain. The amount of results reported has now grown beyond the limits of what can be managed by manual means. This makes it increasingly difficult for the researchers in this area to keep up with the latest developments. Information extraction (IE) in the biological domain aims to provide an effective automatic means to dynamically manage the information contained in archived journal articles and abstract collections and thus help researchers in their work. However, while considerable advances have been made in certain areas of IE, pinpointing and organizing factual information (such as experimental results) remains a challenge. In this paper we propose tackling this task by incorporating into IE information about rhetorical zones, i.e. classification of spans of text in terms of argumentation and intellectual attribution. As the first step towards this goal, we introduce a scheme for annotating biological texts for rhetorical zones and provide a qualitative and quantitative analysis of the data annotated according to this scheme. We also discuss our preliminary research on automatic zone analysis, and its incorporation into our IE framework.","The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #AUTHOR_TAG ) , and question answering .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #AUTHOR_TAG ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( #AUTHOR_TAG ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.']"
CC1599,W06-3309,Generative content models for structural analysis of medical abstracts,information needs in office practice are they being met,"['David G Covell', 'Gwen C Uman', 'Phil R Manning']",introduction,"We studied the self-reported information needs of 47 physicians during a half day of typical office practice. The physicians raised 269 questions about patient management. Questions related to all medical specialties and were highly specific to the individual patient's problem. Subspecialists most frequently asked questions related to other subspecialties. Only 30% of physicians' information needs were met during the patient visit, usually by another physician or other health professional. Reasons print sources were not used included the age of textbooks in the office, poor organization of journal articles, inadequate indexing of books and drug information sources, lack of knowledge of an appropriate source, and the time required to find the desired information. Better methods are needed to provide answers to questions that arise in office practice.","The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['The need for information systems to support physicians at the point of care has been well studied ( #AUTHOR_TAG ; Gorman et al. , 1994 ; Ely et al. , 2005 ) .']"
CC1600,W06-3309,Generative content models for structural analysis of medical abstracts,using argumentation to retrieve articles with similar citations an inquiry into improving related articles search in the medline digital library,"['Imad Tbahriti', 'Christine Chichester', 'Fr´ed´erique Lisacek', 'Patrick Ruch']",introduction,"The aim of this study is to investigate the relationships between citations and the scientific argumentation found abstracts. We design a related article search task and observe how the argumentation can affect the search results. We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain. In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves. Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS and CONCLUSION. A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories. The categories are used to generate four different argumentative indexes. A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms. To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark; it means that two articles are considered as ""related"" if they share a significant set of co-citations. Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low. A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles.","The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #AUTHOR_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #AUTHOR_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( Teufel and Moens , 2000 ) , information retrieval ( #AUTHOR_TAG ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.']"
CC1601,W06-3309,Generative content models for structural analysis of medical abstracts,categorization of sentence types in medical abstracts,"['Larry McKnight', 'Padmini Srinivasan']",experiments,"This study evaluated the use of machine learning techniques in the classification of sentence type. 7253 structured abstracts and 204 unstructured abstracts of Randomized Controlled Trials from MedLINE were parsed into sentences and each sentence was labeled as one of four types (Introduction, Method, Result, or Conclusion). Support Vector Machine (SVM) and Linear Classifier models were generated and evaluated on cross-validated data. Treating sentences as a simple ""bag of words"", the SVM model had an average ROC area of 0.92. Adding a feature of relative sentence location improved performance markedly for some models and overall increasing the average ROC to 0.95. Linear classifier performance was significantly worse than the SVM in all datasets. Using the SVM model trained on structured abstracts to predict unstructured abstracts yielded performance similar to that of models trained with unstructured abstracts in 3 of the 4 types. We conclude that classification of sentence type seems feasible within the domain of RCT's. Identification of sentence types may be helpful for providing context to end users or other text summarization techniques.","The table also presents the closest comparable experimental results reported by #AUTHOR_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .","['The results of our second set of experiments (with RCTs only) are shown in Tables 2(a) and 2(b). Table 2(a) reports the multi-way classification error rate; once again, applying the Markov assumption to model discourse transitions improves performance, and using LDA further reduces error rate.', 'Table 2(b) reports accuracy, precision, recall, and F- measure for four separate binary classifiers (HMM with LDA) specifically trained for each of the sections (one per row in the table).', 'The table also presents the closest comparable experimental results reported by #AUTHOR_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .', 'This collection has significantly more training examples than our corpus of 27,075 abstracts, which could be a source of performance differences.', 'Furthermore, details regarding their procedure for mapping structured abstract headings to one of the four general labels was not discussed in their paper.', 'Nevertheless, our HMM- based approach is at least competitive with SVMs, perhaps better in some cases.']",1,"['The table also presents the closest comparable experimental results reported by #AUTHOR_TAG .1 McKnight and Srinivasan ( henceforth , M&S ) created a test collection consisting of 37,151 RCTs from approximately 12 million MEDLINE abstracts dated between 1976 and 2001 .']"
CC1602,W06-3309,Generative content models for structural analysis of medical abstracts,text generation using discourse strategies and focus constraints to generate natural language text,['Kathleen R McKeown'],related work,Preface Introduction 2. Discourse structure 3. Focusing in discourse 4. TEXT system implementation 5. Discourse history 6. Related generation research 7. Summary and conclusions Appendices Bibliography Index.,"Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .","['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .', 'Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts.']",1,"['Although this study falls under the general topic of discourse modeling , our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements ( #AUTHOR_TAG ; Marcu and Echihabi , 2002 ) .']"
CC1603,W06-3309,Generative content models for structural analysis of medical abstracts,catching the drift probabilistic content models with applications to generation and summarization,"['Regina Barzilay', 'Lillian Lee']",introduction,"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.","Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) .', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( #AUTHOR_TAG ) .', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']"
CC1604,W06-3309,Generative content models for structural analysis of medical abstracts,discoursal movements in medical english abstracts and their linguistic exponents a genre analysis study,['Franc¸oise Salanger-Meyer'],introduction,,"This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orasan, 2001).', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts , which has been confirmed to follow the four-section pattern discussed above ( #AUTHOR_TAG ) .']"
CC1605,W06-3309,Generative content models for structural analysis of medical abstracts,can primary care physicians’ questions be answered using the medical journal literature,"['Paul N Gorman', 'Joan S Ash', 'Leslie W Wykoff']",introduction,"Medical librarians and informatics professionals believe the medical journal literature can be useful in clinical practice, but evidence suggests that practicing physicians do not share this belief. The authors designed a study to determine whether a random sample of ""native"" questions asked by primary care practitioners could be answered using the journal literature. Participants included forty-nine active, nonacademic primary care physicians providing ambulatory care in rural and nonrural Oregon, and seven medical librarians. The study was conducted in three stages: (1) office interviews with physicians to record clinical questions; (2) online searches to locate answers to selected questions; and (3) clinician feedback regarding the relevance and usefulness of the information retrieved. Of 295 questions recorded during forty-nine interviews, 60 questions were selected at random for searches. The average total time spent searching for and selecting articles for each question was forty-three minutes. The average cost per question searched was $27.37. Clinician feedback was received for 48 of 56 questions (four physicians could not be located, so their questions were not used in tabulating the results). For 28 questions (56%), clinicians judged the material relevant; for 22 questions (46%) the information provided a ""clear answer"" to their question. They expected the information would have had an impact on their patient in nineteen (40%) cases, and an impact on themselves or their practice in twenty-four (51%) cases. If the results can be generalized, and if the time and cost of performing searches can be reduced, increased use of the journal literature could significantly improve the extent to which primary care physicians' information needs are met.","The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore, the availability of rich ontological resources, in the form of the Unified Medical Language System (UMLS) (Lindberg et al., 1993), and the availability of software that leverages this knowledge-MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction-provide a foundation for studying the role of semantics in various tasks.', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied ( Covell et al. , 1985 ; #AUTHOR_TAG ; Ely et al. , 2005 ) .', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments.""]"
CC1606,W06-3309,Generative content models for structural analysis of medical abstracts,the interaction of domain knowledge and linguistic structure in natural language processing interpreting hypernymic propositions in biomedical text,"['Thomas C Rindflesch', 'Marcelo Fiszman']",introduction,"Interpretation of semantic propositions in free-text documents such as MEDLINE citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. In this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. In order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the Unified Medical Language System (UMLS). After introducing the syntactic processing on which our system depends, we focus on the UMLS knowledge that supports interpretation of hypernymic propositions. We first use semantic groups from the Semantic Network to ensure that the two concepts involved are compatible; hierarchical information in the Metathesaurus then determines which concept is more general and which more specific. A preliminary evaluation of a sample based on the semantic group Chemicals and Drugs provides 83% precision. An error analysis was conducted and potential solutions to the problems encountered are presented. The research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. Additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. The approach has the potential to support a range of applications, including information retrieval and ontology engineering.","Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .","['Certain types of text follow a predictable structure, the knowledge of which would be useful in many natural language processing applications.', 'As an example, scientific abstracts across many different fields generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"" (Salanger-Meyer, 1990;Swales, 1990;Orȃsan, 2001).', 'The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization (Teufel and Moens, 2000), information retrieval (Tbahriti et al., 2005), information extraction (Mizuta et al., 2005), and question answering.', 'Although there is a trend towards analysis of full article texts, we believe that abstracts still provide a tremendous amount of information, and much value can still be extracted from them.', 'For example, Gay et al. (2005) experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4% improvement in F-score.', 'found a correlation between the quality and strength of clinical conclusions in the full article texts and abstracts.', 'This paper presents experiments with generative content models for analyzing the discourse structure of medical abstracts, which has been confirmed to follow the four-section pattern discussed above (Salanger-Meyer, 1990).', 'For a variety of reasons, medicine is an interesting domain of research.', 'The need for information systems to support physicians at the point of care has been well studied (Covell et al., 1985;Gorman et al., 1994;Ely et al., 2005).', 'Retrieval techniques can have a large impact on how physicians access and leverage clinical evidence.', ""Information that satisfies physicians' needs can be found in the MEDLINE database maintained by the U.S. National Library of Medicine (NLM), which also serves as a readily available corpus of abstracts for our experiments."", 'Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .', 'McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques.', 'Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf.', '(Barzilay and Lee, 2004).', 'Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well.']",0,"['Furthermore , the availability of rich ontological resources , in the form of the Unified Medical Language System ( UMLS ) ( Lindberg et al. , 1993 ) , and the availability of software that leverages this knowledge -- MetaMap ( Aronson , 2001 ) for concept identification and SemRep ( #AUTHOR_TAG ) for relation extraction -- provide a foundation for studying the role of semantics in various tasks .']"
CC1607,W06-3309,Generative content models for structural analysis of medical abstracts,catching the drift probabilistic content models with applications to generation and summarization,"['Regina Barzilay', 'Lillian Lee']",related work,"We consider the problem of modeling the content structure of texts within a specic domain, in terms of the topics the texts address and the order in which these topics appear. We rst present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.","Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .","['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .', 'However, our study differs in several important respects.', 'Barzilay and Lee employed an unsupervised approach to building topic sequence models for the newswire text genre using clustering techniques.', 'In contrast, because the discourse structure of medical abstracts is welldefined and training data is relatively easy to obtain, we were able to apply a supervised approach.', 'Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task.', 'Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.']",1,"['Although not the first to employ a generative approach to directly model content , the seminal work of #AUTHOR_TAG is a noteworthy point of reference and comparison .']"
CC1608,W06-3309,Generative content models for structural analysis of medical abstracts,cuhtk conversational telephone speech transcription system,"['Gunnar Evermann', 'H Y Chan', 'Mark J F Gales', 'Thomas Hain', 'Xunying Liu', 'David Mrva', 'Lan Wang', 'Phil Woodland']",conclusion,,"Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .","['An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).', 'This technique provides two important advantages.', 'First, Gaussian modeling adds an extra degree of freedom during training, by capturing second-order statistics.', 'This is not possible when modeling word sequences, where only the probability of a sentence is actually used in the HMM training.', 'Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']",0,"['Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( #AUTHOR_TAG ) .']"
CC1609,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,junior science book of,['Nancy Larrick'],experiments,"This study aimed to describe the validity of SETS-oriented science book for junior high school students in environmental pollutions materials. In this research, researcher uses a quantitative descriptive method and uses validation sheet in the form of questionnaire as data-collection techniques. The questionnaire contains the criteria of material feasibility, language, and presentation. The result of validation gave the material feasibility score 3.9, for the language is 3.6, and the presentation criteria scored 3.9. So, it can be concluded that a science book with SETS-oriented is very well to be used as a learning facility according to the assessment of the validator","We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .","['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']",5,"['We work with a semi-technical text on meteorological phenomena ( #AUTHOR_TAG ) , meant for primary school students .', 'The text gradually introduces concepts related to precipitation, and explains them.', 'Its nature makes it appropriate for the semantic analysis task in an incremental approach.', 'The system will mimic the way in which a human reader accumulates knowledge and uses what was written before to process ideas introduced later in the text.']"
CC1610,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,semantic interpretation of nominalizations,"['Richard D Hull', 'Fernando Gomez']",related work,"A computational approach to the semantic interpretation of nominalizations is described. Interpretation of nominalizations involves three tasks: deciding whether the nominalization is being used in a verbal or non-verbal sense; disambiguating the nominalized verb when a verbal sense is used; and determining the fillers of the thematic roles of the verbal concept or predicate of the nominalization. A verbal sense can be recognized by the presence of modifiers that represent the arguments of the verbal concept. It is these same modifiers which provide the semantic clues to disambiguate the nominalized verb. In the absence of explicit modifiers, heuristics are used to discriminate between verbal and nonverbal senses. A correspondence between verbs and their nominalizations is exploited so that only a small amount of additional knowledge is needed to handle the nominal form. These methods are tested in the domain of encyclopedic texts and the results are shown.","Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .']"
CC1611,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,pattern matching for case analysis a computational definition of closeness,"['Sylvain Delisle', 'Terry Copeck', 'Stan Szpakowicz', 'Ken Barker']",,Proposes a conceptually and technically neat method to identify known semantic patterns close to a novel pattern. This occurs in the context of a system to acquire knowledge incrementally from systematically processed expository technical text. This semi-automatic system requires the user to respond to specific multiple-choice questions about the current sentence. The questions are prepared from linguistic elements previously encountered in the text similar to elements in the new sentence. We present a metric to characterize the similarity between semantic case patterns. The computation is based on syntactic indicators of semantic relations and is defined in terms of symbolic pattern matching.>,"This idea was inspired by #AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .","['To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by #AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", 'In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005).']",4,"['To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P .', ""This idea was inspired by #AUTHOR_TAG , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) ."", 'In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004;Carreras and Marquez, 2005).']"
CC1612,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,classbased construction of a verb lexicon in,"['Karin Kipper', 'Hoa Trang Dang', 'Martha Palmer']",related work,,"Most approaches rely on VerbNet ( #AUTHOR_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .","['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( #AUTHOR_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .']",0,"['Most approaches rely on VerbNet ( #AUTHOR_TAG ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; Shi and Mihalcea , 2005 ) .']"
CC1613,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,systematic construction of a versatile case system,"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']",introduction,"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text.","The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAGa ) .","['We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser.', 'Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers.', 'For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun).', 'It then tries to find among the previously processed instances another main element with a matching syntactic graph.', 'If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph.', 'We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases.', 'The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAGa ) .']",4,"['The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( #AUTHOR_TAGa ) .']"
CC1614,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,classifying the semantic relations in nouncompounds via a domain specific hierarchy,"['Barbara Rosario', 'Marti Hearst']",related work,"We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.","In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( #AUTHOR_TAG ) or the system ( Gomez , 1998 ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1615,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,automatic labeling of semantic roles,"['Daniel Gildea', 'Daniel Jurafsky']",related work,"We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.","Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; #AUTHOR_TAG ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1616,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,building concept reprezentations from reusable components,"['Peter Clark', 'Bruce Porter']",related work,"Our goal is to build knowledge-based systems capable of answering a wide variety of questions, including questions that are unanticipated when the knowledge base is built. For systems to achieve this level of competence and generality, they require the ability to dynamically construct new concept representations, and to do so in response to the questions arLd tasks posed to them. Our approach to meeting this requirement is to build knowledge bases of generalized, representational components, and to develop methods for automatically composing components on demand. This work extends the normal inheritance approach used in frame-based systems, and imports ideas from several different areas of AI, in particular compositional modeling, terminological reasoning, and ontological engineering. The contribution of this work is a novel integration of these methods that improves the efficiency of building knowledge bases and the robustness of using them.","It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG ) .","['In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts.', 'It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG ) .', ""The system's interface facilitates the expert's task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary.""]",0,"['It helps them build complex knowledge bases by combining components : events , entities and modifiers ( #AUTHOR_TAG ) .']"
CC1617,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,a representation of complex events and processes for the acquisition of knowledge from text,['Fernando Gomez'],related work,,"In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods , lexical resources are specifically tailored to meet the requirements of the domain ( Rosario and Hearst , 2001 ) or the system ( #AUTHOR_TAG ) .', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1618,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,the descriptive technique of panini,['Vidya Niwas Misra'],introduction,,He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG ) .,"['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 .', 'He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG ) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005).']",0,"['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 .', 'He was a grammarian who analysed Sanskrit ( #AUTHOR_TAG ) .', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesnie`re, 1959; Gruber, 1965; Fill- more, 1968).', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005).']"
CC1619,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,semantic role labelling using different syntactic views,"['Sameer Pradhan', 'Wayne Ward', 'Kadri Hacioglu', 'James H Martin', 'Daniel Jurafsky']",related work,"Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements.","Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .","['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .']",0,"['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; #AUTHOR_TAG ; Shi and Mihalcea , 2005 ) .']"
CC1620,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,remarks on nominalizations,['Noam Chomsky'],introduction,This article deals with French nouns derived from non-stative verbs which nevertheless systematically exhibit a stative interpretation in some of their uses e.g. emprisonement 'action of putting sdy in jail' vs. 'state of being jailed,"This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 ) .","['In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning).', 'Anecdotal support for this stance comes from the fact that the grammatical notion of case is the basis for semantic relations (Misra, 1966;Gruber, 1965;Fillmore, 1968).', 'Tesnière (1959), who proposes a grouping of verb arguments into actants and circumstances, gives a set of rules to connect specific types of actants -for example, agent or instrument -to such grammatical elements as subject, direct object, indirect object.', 'This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 ) .']",0,"['This idea was expanded to include nouns and their modifiers through verb nominalizations ( #AUTHOR_TAG ; Quirk et al. , 1985 ) .']"
CC1621,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,putting pieces together combining framenet verbnet and wordnet for robust semantic parsing,"['Lei Shi', 'Rada Mihalcea']",related work,"This paper describes our work in integrating three different lexical resources: FrameNet, VerbNet, and WordNet, into a unified, richer knowledge-base, to the end of enabling more robust semantic parsing. The construction of each of these lexical resources has required many years of laborious human effort, and they all have their strengths and shortcomings. By linking them together, we build an improved resource in which (1) the coverage of FrameNet is extended, (2) the VerbNet lexicon is augmented with frame semantics, and (3) selectional restrictions are implemented using WordNet semantic classes. The synergistic exploitation of various lexical resources is crucial for many complex language processing applications, and we prove it once again effective in building a robust semantic parser.","Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG ) .","['In current work on semantic relation analysis, the focus is on semantic roles -relations between verbs and their arguments.', 'Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG ) .']",0,"['Most approaches rely on VerbNet ( Kipper et al. , 2000 ) and FrameNet ( Baker et al. , 1998 ) to provide associations between verbs and semantic roles , that are then mapped onto the current instance , as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez , 2004 ; Carreras and Marquez , 2005 ) and also ( Gildea and Jurafsky , 2002 ; Pradhan et al. , 2005 ; #AUTHOR_TAG ) .']"
CC1622,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,realistic parsing practical solutions of difficult problems,"['Sylvain Delisle', 'Stan Szpakowicz']",experiments,"This paper describes work on the linguistic analysis of texts within a project devoted to knowledge acquisition from text. We focus on syntactic processing and present some key elements of the project's parser that allow it to deal successfully with technical texts. The parser is fully implemented and tested on a variety of real texts; improvements and enhancements are in progress. Because our knowledge acquisition method assumes no a priori model of the domain of the source text, the parser relies as much as possible on lexical and syntactic clues. That is why it strives for full syntactic analysis rather than some form of text skimming. We present a practical approach to four acknowledged difficult problems which to date have no generally acceptable answers: phrase attachment; time constraints for problematic input (how to avoid long and unproductive computation); parsing conjoined structures (how to preserve broad coverage without losing control of the parsing process); and the treatment of fragmentary input or fragments that are a by-product of a fallback parsing strategy. We review recent related work and conclude by listing several future work items.","The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG ) .","['The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG ) .', 'The parser, written in Prolog, implements a classic constituency English grammar from Quirk et al. (1985).', 'Pairs of syntactic units connected by grammatical relations are extracted from the parse trees.', 'A dependency parser would produce a similar output, but DIPETT also provides verb subcategorization information (such as, for example, subject-verb-object or subject-verb-objectindirect object), which we use to select the (best) matching syntactic structures.']",5,"['The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( #AUTHOR_TAG ) .']"
CC1623,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,the descent of hierarchy and selection in relational semantics,"['Barbara Rosario', 'Marti Hearst', 'Charles Fillmore']",related work,"In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on two-word noun compounds from the biomedical domain, obtaining classification accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning relations to noun compounds.","Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( Fillmore and Atkins , 1998 ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; #AUTHOR_TAG ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1624,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,studies in lexical relations,['Jeffrey Gruber'],introduction,,"The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .","['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 .', 'He was a grammarian who analysed Sanskrit (Misra, 1966).', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).']",0,"['The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; #AUTHOR_TAG ; Fillmore , 1968 ) .']"
CC1625,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,the case for case,['Charles Fillmore'],introduction,"This article puts forward an economic case for Scotland staying in the union. There have been many debates regarding the economic consequences of independence. It has been argued that Scotland would be better off. Independence, however, is an uncertain business; a new state might gain new freedoms but would lose present sources of stability, and some questions about independence are simply unanswerable in advance. It is nevertheless possible to draw some conclusions about its possible economic effects","The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .","['When analysing texts, it is essential to see how elements of meaning are interconnected.', 'This is an old idea.', 'The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5 th century B.C. and the work of Panini 1 .', 'He was a grammarian who analysed Sanskrit (Misra, 1966).', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .', 'Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic role labelling (Baker et al., 1998;Kipper et al., 2000;Carreras and Marquez, 2004;Carreras and Marquez, 2005;Atserias et al., 2001;Shi and Mihalcea, 2005).']",0,"['This is an old idea.', 'The idea resurfaced forcefully at several points in the more recent history of linguistic research ( Tesni`ere , 1959 ; Gruber , 1965 ; #AUTHOR_TAG ) .']"
CC1626,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,systematic construction of a versatile case system,"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']",,"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text.",This design idea was adopted from TANKA ( #AUTHOR_TAGb ) .,"['Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA ( #AUTHOR_TAGb ) .', 'The only manually encoded knowledge is a dictionary of markers (subordinators, coordinators, prepositions).', 'This resource does not affect the syntacticsemantic graph-matching heuristic.']",5,"['Our system begins operation with a minimum of manually encoded knowledge, and accumulates information as it processes the text.', 'This design idea was adopted from TANKA ( #AUTHOR_TAGb ) .', 'This resource does not affect the syntacticsemantic graph-matching heuristic.']"
CC1627,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,framenet and lexicographic relevance,"['Charles Fillmore', 'Beryl T Atkins']",related work,,"Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .","['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998).', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units ( clauses in ( #AUTHOR_TAG ; Gildea and Jurafsky , 2002 ; Hull and Gomez , 1996 ) ; noun phrases in ( Hull and Gomez , 1996 ; Rosario et al. , 2002 ) ) .', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1628,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,the berkeley framenet project,"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']",related work,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work",Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ) .,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']",0,"['Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( #AUTHOR_TAG ) .', 'In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998).', 'Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998;Gildea and Jurafsky, 2002;Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996;Rosario et al., 2002)).', 'Lists of semantic relations are designed to capture salient domain information.']"
CC1629,W06-3813,Matching syntactic-semantic graphs for semantic relation assignment,systematic construction of a versatile case system,"['Ken Barker', 'Terry Copeck', 'Sylvain Delisle', 'Stan Szpakowicz']",experiments,"Case systems abound in natural language processing. Almost any attempt to recognize and uniformly represent relationships within a clause - a unit at the centre of any linguistic system that goes beyond word level statistics - must be based on semantic roles drawn from a small, closed set. The set of roles describing relationships between a verb and its arguments within a clause is a case system. What is required of such a case system? How does a natural language practitioner build a system that is complete and detailed yet practical and natural? This paper chronicles the construction of a case system from its origin in English marker words to its successful application in the analysis of English text.",The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAGa ) .,"['The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAGa ) .', 'Three lists of relations for three syntactic levels -inter-clause, intra-clause (case) and nounmodifier relations -were next combined based on syntactic and semantic phenomena.', 'The resulting list is the one used in the experiments we present in this paper.', 'The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier).', 'There is no consensus in the literature on a list of semantic relations that would work in all situations.', 'This is, no doubt, because a general list of relations such as the one we use would not be appropriate for the semantic analysis of texts in a specific domain, such as for example medical texts.', 'All the relations in the list we use were necessary, and sufficient, for the analysis of the input text.']",5,"['The list of semantic relations with which we work is based on extensive literature study ( #AUTHOR_TAGa ) .', 'The resulting list is the one used in the experiments we present in this paper.', 'The relations are grouped by general similarity into 6 relation classes (H denotes the head of a base NP, M denotes the modifier).', 'There is no consensus in the literature on a list of semantic relations that would work in all situations.', 'All the relations in the list we use were necessary, and sufficient, for the analysis of the input text.']"
CC1630,W10-1710,Anaphora Models and Reordering for Phrase-Based SMT,chunkbased verb reordering in vso sentences for arabicenglish statistical machine translation,"['Arianna Bisazza', 'Marcello Federico']",conclusion,"In Arabic-to-English phrase-based statistical machine translation, a large number of syntactic disfluencies are due to wrong long-range reordering of the verb in VSO sentences, where the verb is anticipated with respect to the English word order. In this paper, we propose a chunk-based reordering technique to automatically detect and displace clause-initial verbs in the Arabic side of a word-aligned parallel corpus. This method is applied to preprocess the training data, and to collect statistics about verb movements. From this analysis, specific verb reordering lattices are then built on the test sentences before decoding them. The application of our reordering methods on the training and test sets results in consistent BLEU score improvements on the NIST-MT 2009 Arabic-English benchmark.","Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .","['Word reordering between German and English is a complex problem.', 'Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .', 'It turned out that there is a larger variety of long reordering patterns in this case.', 'Nevertheless, some experiments performed after the official evaluation showed promising results.', 'We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice.', 'Applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences.', 'Finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade-off between decoderdriven short-range and lattice-driven long-range reordering.']",4,"['Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( #AUTHOR_TAG ) , we tried to adapt the same approach to the German-English language pair .']"
CC1631,W10-3814,New Parameterizations and Features for PSCFG-Based Machine Translation,a discriminative latent variable model for statistical machine translation,"['Phil Blunsom', 'Trevor Cohn', 'Miles Osborne']",conclusion,"Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics","Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .","['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']",3,"['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of #AUTHOR_TAG and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']"
CC1632,W10-3814,New Parameterizations and Features for PSCFG-Based Machine Translation,probabilistic inference for machine translation,"['Phil Blunsom', 'Miles Osborne']",conclusion,"Recent advances in statistical machine translation (SMT) have used dynamic programming  (DP) based beam search methods for approximate inference within probabilistic  translation models. Despite their success, these methods compromise the probabilistic  interpretation of the underlying model thus limiting the application of probabilistically  defined decision rules during training and decoding.  As an alternative, in this thesis, we propose a novel Monte Carlo sampling approach  for theoretically sound approximate probabilistic inference within these models. The  distribution we are interested in is the conditional distribution of a log-linear translation  model; however, often, there is no tractable way of computing the normalisation term  of the model. Instead, a Gibbs sampling approach for phrase-based machine translation  models is developed which obviates the need of computing this term yet produces  samples from the required distribution.  We establish that the sampler effectively explores the distribution defined by a  phrase-based models by showing that it converges in a reasonable amount of time to  the desired distribution, irrespective of initialisation. Empirical evidence is provided to  confirm that the sampler can provide accurate estimates of expectations of functions of  interest. The mix of high probability and low probability derivations obtained through  sampling is shown to provide a more accurate estimate of expectations than merely  using the n-most highly probable derivations.  Subsequently, we show that the sampler provides a tractable solution for finding the  maximum probability translation in the model. We also present a unified approach to  approximating two additional intractable problems: minimum risk training and minimum  Bayes risk decoding. Key to our approach is the use of the sampler which  allows us to explore the entire probability distribution and maintain a strict probabilistic  formulation through the translation pipeline. For these tasks, sampling allies  the simplicity of n-best list approaches with the extended view of the distribution that  lattice-based approaches benefit from, while avoiding the biases associated with beam  search. Our approach is theoretically well-motivated and can give better and more  stable results than current state of the art methods","Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .","['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']",3,"['Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations.', 'Future research should apply the work of Blunsom et al. ( 2008 ) and #AUTHOR_TAG , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .']"
CC1633,W11-1402,Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing,automatically predicting peerreview helpfulness,"['Wenting Xiong', 'Diane Litman']",introduction,"Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a first step towards enhancing existing peer-review systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review uni-grams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction.","In our prior work ( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .","['In our prior work ( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g.', 'author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model.', 'In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2).']",2,"['In our prior work ( #AUTHOR_TAG ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .', 'While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g.']"
CC1634,W11-1402,Understanding Differences in Perceived Peer-Review Helpfulness using Natural Language Processing,automatically predicting peerreview helpfulness,"['Wenting Xiong', 'Diane Litman']",experiments,"Identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. As a first step towards enhancing existing peer-review systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. In addition, we investigate the utility of incorporating additional specialized features tailored to peer review. Our preliminary results show that the structural features, review uni-grams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction.",( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG ) . ),"['10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/).', 'provide complementary perspectives.', 'While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features.', 'To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG ) . )']",2,"['To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below.', 'Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings.', '( Details of how the average-expert model performs can be found in our prior work ( #AUTHOR_TAG ) . )']"
CC1635,W11-1410,"Review of Leacock, Chodorow, Gamon & Tetreault (2014): Automated Grammatical Error Detection for Language Learners",building a korean web corpus for analyzing learner language,"['Markus Dickinson', 'Ross Israel', 'Sun-Hee Lee']",,"Post-positional particles are a significant source of errors for learners of Korean. Following methodology that has proven effective in handling English preposition errors, we are beginning the process of building a machine learner for particle error detection in L2 Korean writing. As a first step, however, we must acquire data, and thus we present a methodology for constructing large-scale corpora of Korean from the Web, exploring the feasibility of building corpora appropriate for a given topic and grammatical construction.","We follow our previous work ( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .","['In selecting features for Korean, we have to ac- count for relatively free word order (Chung et al., 2010).', 'We follow our previous work ( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .', 'Each word is broken down into: stem, affixes, stem POS, and affixes POS.', 'We also have features for the preceding and following noun and verb, thereby approximating relevant selectional properties.', 'Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4).']",2,"['In selecting features for Korean, we have to ac- count for relatively free word order (Chung et al., 2010).', 'We follow our previous work ( #AUTHOR_TAG ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .']"
CC1636,W11-2020,Novel Approaches to Pattern-based Interaction Quality Modeling,on nomatchs noinputs and bargeins do nonacoustic features support anger detection,"['Alexander Schmitt', 'Tobias Heinroth', 'Jackson Liscombe']",,"Most studies on speech-based emotion recognition are based on prosodic and acoustic features, only employing artificial acted corpora where the results cannot be generalized to telephone-based speech applications. In contrast, we present an approach based on utterances from 1,911 calls from a deployed telephone-based speech application, taking advantage of additional dialogue features, NLU features and ASR features that are incorporated into the emotion recognition process. Depending on the task, non-acoustic features add 2.3% in classification accuracy compared to using only acoustic features.","The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .","['The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .', '(Schmitt et al., 2009).', 'From all 4,832 user turns, 68.5% were non-angry, 14.3% slightly angry, 5.0% very angry and 12.2% contained garbage, i.e. nonspeech events.', 'In total, the number of interaction parameters servings as input variables for the model amounts to 52.']",2,"['The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( #AUTHOR_TAG ) .']"
CC1637,W14-1609,Lexicon Infused Phrase Embeddings for Named Entity Resolution,design challenges and misconceptions in named entity recognition,"['Lev Ratinov', 'Dan Roth']",introduction,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .","['One simple language model that discovers useful embeddings is known as Brown clustering (Brown et al., 1992).', 'A Brown clustering is a class-based bigram model in which (1) the probability of a document is the product of the probabilities of its bigrams, (2) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and (3) each word type has non-zero probability only on a single class.', 'Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus.', 'The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged.', 'This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .', 'There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993).']",4,"['This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( #AUTHOR_TAG ; Koo et al. , 2008 ; Miller et al. , 2004 ) .']"
CC1638,W14-1609,Lexicon Infused Phrase Embeddings for Named Entity Resolution,design challenges and misconceptions in named entity recognition,"['Lev Ratinov', 'Dan Roth']",introduction,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","Following #AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .","['Following #AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .', 'Since, as seen in section 2.1, Brown clusters are hierarchical, we use features corresponding to prefixes of the path from the root to the leaf for each word type.']",5,"['Following #AUTHOR_TAG , we also compare the performance of our system with a system using features based on the Brown clusters of the word types in a document .']"
CC1639,W14-1609,Lexicon Infused Phrase Embeddings for Named Entity Resolution,design challenges and misconceptions in named entity recognition,"['Lev Ratinov', 'Dan Roth']",introduction,"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.",It is inspired by the system described in #AUTHOR_TAG .,"['In this section we describe in detail the baseline NER system we use.', 'It is inspired by the system described in #AUTHOR_TAG .', 'Because NER annotations are commonly not nested (for example, in the text ""the US Army"", ""US Army"" is treated as a single entity, instead of the location ""US"" and the organization ""US Army"") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity.', 'Following Ratinov and Roth (2009) we use the BILOU encoding, where each token can either BEGIN an entity, be INSIDE an entity, be the LAST token in an entity, be OUTSIDE an entity, or be the single UNIQUE token in an entity.']",4,['It is inspired by the system described in #AUTHOR_TAG .']
CC1640,W14-1619,Probabilistic Modeling of Joint-context in Distributional Similarity,learning syntactic categories using paradigmatic representations of word context,"['Mehmet Ali Yatbaz', 'Enis Sert', 'Deniz Yuret']",introduction,We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus.,"This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .","['We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired.', 'It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993;Hanks, 2013).', 'This provides grounds to expect that such model has the potential to excel for verbs.', 'To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme.', 'This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .', 'However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme.']",4,"['This choice is inspired by recent work on learning syntactic categories ( #AUTHOR_TAG ) , which successfully utilized such language models to represent word window contexts of target words .']"
CC1641,W14-1815,Natural Language Generation with Vocabulary Constraints,learning to freestyle hip hop challengeresponse induction via transduction rule segmentation,"['Dekai Wu', 'Karteek Addanki', 'Markus Saers', 'Meriem Beloucif']",related work,"We present a novel model, Freestyle, that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics, by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations. In this attack on the woefully under-explored natural language genre of music lyrics, we exploit a strictly unsupervised transduction grammar induction approach. Our task is particularly ambitious in that no use of any a priori linguistic or phonetic information is allowed, even though the domain of hip hop lyrics is particularly noisy and unstructured. We evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction, and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses, measured on fluency and rhyming criteria as judged by human evaluators. To highlight some of the inherent challenges in adapting other algorithms to this novel task, we also compare the quality of the responses generated by our model to those generated by an out-ofthe-box phrase based SMT system. We tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module, which is also acquired via unsupervised learning and report improved quality of the generated responses. Finally, we report results with Maghrebi French hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages.","Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .","['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']",1,"['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( #AUTHOR_TAG ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .']"
CC1642,W14-1815,Natural Language Generation with Vocabulary Constraints,generating diagnostic multiple choice comprehension cloze questions,"['Jack Mostow', 'Hyeju Jang']",related work,"This paper describes and evaluates DQGen, which automatically generates multiple choice cloze questions to test a child's comprehension while reading a given text. Unlike previous methods, it generates different types of distracters designed to diagnose different types of comprehension failure, and tests comprehension not only of an individual sentence but of the context that precedes it. We evaluate the quality of the overall questions and the individual distracters, according to 8 human judges blind to the correct answers and intended distracter types. The results, errors, and judges' comments reveal limitations and suggest how to address some of them.","Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .","['Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .', 'Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.']",4,"['Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and #AUTHOR_TAG , which deal with automatic generation of classic fill in the blank questions .']"
CC1643,W14-1815,Natural Language Generation with Vocabulary Constraints,automatic analysis of rhythmic poetry with applications to generation and translation,"['Erica Greene', 'Tugba Bodrumlu', 'Kevin Knight']",related work,"We employ statistical methods to analyze, generate, and translate rhythmic poetry. We first apply unsupervised learning to reveal word-stress patterns in a corpus of raw poetry. We then use these word-stress patterns, in addition to rhyme and discourse models, to generate English love poetry. Finally, we translate Italian poetry into English, choosing target realizations that conform to desired rhythmic patterns.","Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .","['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']",1,"['Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( #AUTHOR_TAG ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .']"
CC1644,W14-1815,Natural Language Generation with Vocabulary Constraints,automatic generation of tamil lyrics for melodies,"['Ananth Ramakrishnan A', 'Sankar Kuppan', 'Sobha Lalitha Devi']",related work,"This paper presents our on-going work to automatically generate lyrics for a given melody, for phonetic languages such as Tamil. We approach the task of identifying the required syllable pattern for the lyric as a sequence labeling problem and hence use the popular CRF++ toolkit for learning. A corpus comprising of 10 melodies was used to train the system to understand the syllable patterns. The trained model is then used to guess the syllabic pattern for a new melody to produce an optimal sequence of syllables. This sequence is presented to the Sentence Generation module which uses the Dijkstra's shortest path algorithm to come up with a meaningful phrase matching the syllabic pattern.","Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .","['The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images.', 'Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .', 'In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.']",1,"['Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan #AUTHOR_TAG ) , where specified meter or rhyme schemes are enforced .']"
CC1645,W14-3902,Code Mixing: A Challenge for Language Identification in the Language of Social Media,dcusymantec at the wmt 2013 quality estimation shared task,"['Raphael Rubino', 'Joachim Wagner', 'Jennifer Foster', 'Johann Roturier', 'Rasoul Samad Zadeh Kaljahi', 'Fred Hollowood']",experiments,,"3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .","['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', '3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']",2,"['Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '3. Length of words (L): Instead of using the raw length value as a feature , we follow our previous work ( #AUTHOR_TAG ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .']"
CC1646,W14-3902,Code Mixing: A Challenge for Language Identification in the Language of Social Media,dcu aspectbased polarity classification for semeval task 4,"['Joachim Wagner', 'Piyush Arora', 'Santiago Cortes', 'Utsab Barman', 'Dasha Bogdanova', 'Jennifer Foster', 'Lamia Tounsi']",experiments,"We describe the work carried out by DCU on the Aspect Based Sentiment Analysis task at SemEval 2014. Our team submitted one constrained run for the restaurant domain and one for the laptop domain for sub-task B (aspect term polarity prediction), ranking highest out of 36 systems on the restaurant test set and joint highest out of 32 systems on the laptop test set.","raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .","['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', '2. Presence in Dictionaries (D): We use presence in a dictionary as a features for all available dictionaries in previous experiments.', 'raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', 'We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features.', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']",2,"['1. Char-n-grams (G): We start with a character n-gram-based approach (Cavnar and Trenkle, 1994), which is most common and followed by many language identification researchers.', 'Following the work of King and Abney (2013), we select character n-grams (n=1 to 5) and the word as the features in our experiments.', 'raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; #AUTHOR_TAG ) and create multiple features for length using a decision tree ( J48 ) .', '4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.']"
CC1647,W98-1124,The Theory and Practice of Discourse Parsing and Summarization,from discourse structures to text summaries,['Daniel Marcu'],,We describe experiments that show that the concepts of rhetorical analysts and nucleanty can be used effectively for deternumng the most nnportant umts m a text We show how these concepts can be xmplemented and we discuss results that we obtained with a chscourse-based summanzatmn program,"The only disambiguation metric that we used in our previous work ( #AUTHOR_TAGb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right .","['The shape-based metric.', ""The only disambiguation metric that we used in our previous work ( #AUTHOR_TAGb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', 'I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches.', 'According to the shape-based metric, we consider that a discourse tree A is ""better"" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness).']",2,"[""The only disambiguation metric that we used in our previous work ( #AUTHOR_TAGb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right ."", 'The explanation for this metric is that text processing is, essentially, a left-to-right process.', 'In many genres, people write texts so that the most important ideas go first, both at the paragraph and at the text levels.', 'I The more text writers add, the more they elaborate on the text that went before: as a consequence, incremental discourse building consists mostly of expansion of the tight branches.', 'According to the shape-based metric, we consider that a discourse tree A is ""better"" than another discourse tree B if A is more skewed to the right than B (see Marcu (1997c) for a mathematical formulation of the notion of skewedness).']"
CCT1,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,a program for aligning sentences in bilingual corpora,"['William A Gale', 'Kenneth W Church']",method,"Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.","A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .","['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']",0,"['Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; #AUTHOR_TAG ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.']"
CCT2,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,aligning a parallel englishchinese corpus statistically with lexical criteria,['Dekai Wu'],introduction,We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.,"Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #AUTHOR_TAG ) .","['However, a major obstacle to this approach is the lack of parallel corpora for model training.', 'Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #AUTHOR_TAG ) .', 'In this paper, we will describe a method which automatically searches for parallel texts on the Web.', ""We will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in CLIR.""]",0,"['Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( #AUTHOR_TAG ) .']"
CCT3,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,aligning sentences in bilingual corpora using lexical information,['S F Chen'],method,"In this paper, we describe a fast algorithm for aligning sentences with their translations in a bilingual corpus. Existing efficient algorithms ignore word identities and only consider sentence length (Brown et al., 1991b; Gale and Church, 1991). Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment. We find the alignment that maximizes the probability of generating the corpus with this translation model. We have achieved an error rate of approximately 0.4% on Canadian Hansard data, which is a significant improvement over previous results. The algorithm is language independent.","A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .","['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']",0,"['Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; #AUTHOR_TAG ) .', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.']"
CCT4,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,aligning a parallel englishchinese corpus statistically with lexical criteria,['Dekai Wu'],method,We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues.,"For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #AUTHOR_TAG ) .","['Beside HTML markups, other criteria may also be incorporated.', 'For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #AUTHOR_TAG ) .', 'We hope to implement such correspondences in our future research.']",3,"['For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( #AUTHOR_TAG ) .']"
CCT5,A00-1004,Automatic construction of parallel English-Chinese corpus for cross-language information retrieval,aligning sentences in parallel corpora,"['P F Brown', 'J C Lai', 'R L Mercer']",method,"In this paper we describe a statistical tech-nique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our da.ta, the only information about the sentences that we use for calculating alignments i the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment com-putation is fast and therefore practical for appli-cation to very large collections of text. We have used this technique to align several million sen-tences in the English-French Hans~trd corpora nd have achieved an accuracy in excess of 99 % in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without he benefit of anchor points the correlation between the lengths of aligned sentences i strong enough that we should expect o achieve an accuracy of between 96 % and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried","A number of alignment techniques have been proposed , varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .","['The parallel Web pages we collected from various sites are not all of the same quality.', 'Some are highly parallel and easy to align while others can be very noisy.', 'Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'The method we adopted is that of Simard et al. (1992).', 'Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.', 'Cognates are identical sequences of characters in corresponding words in two languages.', 'They are commonly found in English and French.', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.', 'Because the HTML structures of parallel pages are normally similar, the markup was found to be helpful for alignment.']",0,"['Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.', 'A number of alignment techniques have been proposed , varying from statistical methods ( #AUTHOR_TAG ; Gale and Church , 1991 ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .', 'In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the HTML markup in both texts are taken as cognates.']"
CCT6,A00-1008,Plan-based dialogue management in a physics tutor,spelling correction using context,"['M A Elmi', 'M W Evens']",introduction,"In computing, spell checking is the process of detecting and sometimes providing spelling suggestions for incorrectly spelled words in a text. Basically, a spell checker is a computer program that uses a dictionary of words to perform spell checking. The bigger the dictionary is, the higher is the error detection rate. The fact that spell checkers are based on regular dictionaries, they suffer from data sparseness problem as they cannot capture large vocabulary of words including proper names, domain-specific terms, technical jargons, special acronyms, and terminologies. As a result, they exhibit low error detection rate and often fail to catch major errors in the text. This paper proposes a new context-sensitive spelling correction method for detecting and correcting non-word and real-word errors in digital text documents. The approach hinges around data statistics from Google Web 1T 5-gram data set which consists of a big volume of n-gram word sequences, extracted from the World Wide Web. Fundamentally, the proposed method comprises an error detector that detects misspellings, a candidate spellings generator based on a character 2-gram model that generates correction suggestions, and an error corrector that performs contextual error correction. Experiments conducted on a set of text documents from different domains and containing misspellings, showed an outstanding spelling error correction rate and a drastic reduction of both non-word and real-word errors. In a further study, the proposed algorithm is to be parallelized so as to lower the computational cost of the error detection and correction processes.Comment: LACSC - Lebanese Association for Computational Sciences -   http://www.lacsc.or",Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG .,"['The first system we have implemented with APE is a prototype Atlas-Andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.', 'Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.', ""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG .""]",5,"[""Robust natural language understanding in Atlas-Andes is provided by RosÃ© 's CARMEL system ( RosÃ© 2000 ) ; it uses the spelling correction algorithm devised by #AUTHOR_TAG .""]"
CCT7,A00-1014,MIMIC,using dialogue representations for concepttospeech generation,"['Christine H Nakatani', 'Jennifer Chu-Carroll']",,"We present an implemented concept-to-speech (CTS) system that offers original proposals for certain couplings of dialogue computation with prosodic computation. Specifically, the semantic interpretation, task modeling and dialogue strategy modules in a working spoken dialogue system are used to generate prosodic features to better convey the meaning of system replies. The new CTS system embodies and extends theoretical work on intonational meaning in a more general, robust and rigorous way than earlier approaches, by reflecting compositional aspects of both dialogue and intonation interepretation in an original computational framework for prosodic generation.",2 See ( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .,"[""2 See ( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .""]",0,"[""2 See ( #AUTHOR_TAG ) for how MIMIC 's dialoguelevel knowledge is used to override default prosodic assignments for concept-to-speech generation .""]"
CCT8,A00-1014,MIMIC,the commandtalk spoken dialogue system,"['Amanda Stent', 'John Dowding', 'Jean Mark Gawron', 'Elizabeth Owen Bratt', 'Robert Moore']",,"CommandTalk (Moore et al., 1997) is a spokenlanguage interface to the ModSAF battlefield simulator that allows simulation operators to generate and execute military exercises by creating forces and control measures, assigning missions to forces, and controlling the display (Ceranowicz, 1994). CommandTalk consists of independent, cooperating agents interacting through SRI's Open Agent Architecture (OAA) (Martin et al., 1998). This architecture allows components to be developed independently, and then flexibly and dynamically combined to support distributed computation. Most of the agents that compose CommandTalk have been described elsewhere !for more detail, see (Moore et al., 1997)). This paper describes extensions to CommandTalk to support spoken dialogue. While we make no theoretical claims about the nature and structure of dialogue, we are influenced by the theoretical work of (Grosz and Sidner, 1986) and will use terminology from that tradition when appropriate. We also follow (Chu-Carroll and Brown, 1997) in distinguishing task initiative and dialogue initiative. Section 2 demonstrates the dialogue capabilities of CommandTalk by way of an extended example. Section 3 describes how language in CommandTalk is modeled for understanding and generation. Section 4 describes the architecture of the dialogue manager in detail. Section 5 compares CommandTalk with other spo-","The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; #AUTHOR_TAG ) ) .","['The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; #AUTHOR_TAG ) ) .', 'To instantiate an attribute, MIMIC adopts the lnfoSeek dialogue act to solicit the missing information.', 'In contrast, when MIMIC has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute (GiveOptions).', 'Given an invalid query, MIMIC notifies the user of the failed query and provides an openended prompt when it only has dialogue initiative.', ""When MIMIC has both initiatives, however, in addition to No-tifyFailure, it suggests an alternative close to the user's original query and provides a limited prompt."", 'Finally, when MIMIC has neither initiative, it simply adopts No-tifyFailure, allowing the user to determine the next discourse goal.']",1,"['The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems ( e.g. , ( Bennacef et at. , 1996 ; #AUTHOR_TAG ) ) .']"
CCT9,A00-1014,MIMIC,mixed initiative in dialogue an investigation into discourse segmentation,"['Marilyn Walker', 'Steve Whittaker']",,"Conversation between two people is usually of mixed-initiative, with control over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles.Comment: 8 pages, late","Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .","['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step ( 4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1.']",0,"['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( Whittaker and Stenton , 1988 ; #AUTHOR_TAG ; Chu-Carroll and Brown , 1998 ) .', 'Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step ( 4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1.']"
CCT10,A00-1014,MIMIC,cues and control in expertclient dialogues,"['Steve Whittaker', 'Phil Stenton']",,,"Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .","['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .', 'Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution.', 'Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step ( 4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998), which we leave for future work.', 'rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1.']",0,"['Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction ( #AUTHOR_TAG ; Walker and Whittaker , 1990 ; Chu-Carroll and Brown , 1998 ) .']"
CCT11,A00-1014,MIMIC,evaluating automatic dialogue strategy adaptation for a spoken dialogue system,"['Jennifer Chu-Carroll', 'Jill S Nickerson']",experiments,"In this paper, we describe an empirical evaluation of an adaptive mixed initiative spoken dialogue system. We conducted two sets of experiments to evaluate the mixed initiative and automatic adaptation aspects of the system, and analyzed the resulting dialogues along three dimensions: performance factors, discourse features, and initiative distribution. Our results show that 1) both the mixed initiative and automatic adaptation aspects led to better system performance in terms of user satisfaction and dialogue efficiency, and 2) the system's adaptation behavior better matched user expectations, more efficiently resolved dialogue anomalies, and resulted in higher overall dialogue quality.",A companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ) .,"[""We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ) .', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', 'User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated.', 'In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.']",2,"[""We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail ( #AUTHOR_TAG ) .', 'Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme (Walker et al., 1997), were automatically logged, derived, or manually annotated.', 'In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.']"
CCT12,A00-1014,MIMIC,evaluating response strategies in a webbased spoken dialogue agent,"['Diane J Litman', 'Shimei Pan', 'Marilyn A Walker']",,,"5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ) , which we leave for future work .","['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ) , which we leave for future work .']",3,"['5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( #AUTHOR_TAG ) , which we leave for future work .']"
CCT13,A00-1014,MIMIC,paradise a framework for evaluating spoken dialogue agents,"['Marilyn A Walker', 'Diane J Litman', 'Candance A Kamm', 'Alicia Abella']",experiments,"This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity.Comment: 10 pages, uses aclap, psfig, lingmacros, time","Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .","[""We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities."", 'We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems.', 'In this section we summarize these experiments and their results.', 'A companion paper describes the evaluation process and results in further detail (Chu-Carroll and Nickerson, 2000).', 'Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information.', 'User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system.', 'Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .', 'In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.']",5,"['Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( #AUTHOR_TAG ) , were automatically logged , derived , or manually annotated .']"
CCT14,A00-1015,Javox,natural language access to software applications,"['Paul Schmidt', 'Sibylle Rieder', 'Axel Theofilidis', 'Marius Groenendijk', 'Peter Phelan', 'Henrik Schulz', 'Thierry Declerck', 'Andrew Brenenkamp']",,"This paper reports on the ESPRIT project MELISSA (Methods and Tools for Natural-Language Interfacing with Standard Software Applications)1. MELISSA aims at developing the technology and tools enabling end users to interface with computer applications, using natural-language (NL), and to obtain a precompetitive product validated in selected enduser applications. This paper gives an overview of the approach to solving (NL) interfacing problem and outlines some of the methods and software components developed in the project.",The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #AUTHOR_TAG ) .,"['Previous systems to assist in the development of spoken-langnage systems (SLSs) have focused on building stand-alone, customized applications, such as (Sutton et al., 1996) and (Pargellis et al., 1999).', 'The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #AUTHOR_TAG ) .', 'It is intended to both speed the development of SLSs and to localize the speech-specific code within the application.', 'JAVOX allows developers to add speech interfaces to applications at the end of the development process; SLSs no longer need to be built from the ground up.']",1,"['The goal of the JAVOX toolkit is to speech-enable traditional desktop applications -- this is similar to the goals of the MELISSA project ( #AUTHOR_TAG ) .', 'JAVOX allows developers to add speech interfaces to applications at the end of the development process; SLSs no longer need to be built from the ground up.']"
CCT15,A00-1019,Unit completion for a computer-aided translation typing system,a corpusbased approach to automatic compound extraction,"['Keh-Yih Su', 'Ming-Wen Wu', 'Jing-Shin Chang']",method,,"It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ; Lin , 99 ) .","['One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Ganssier, 1995;Kupiec, 1993;hua Chen and Chen, 94;Fung, 1995;Evans and Zhai, 1996).', 'It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ; Lin , 99 ) .', 'Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995;Furuse and Iida, 96).', 'In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags.', ""An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as JrNp in the following) to the 39,093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35,939 of them (92%)."", '4: Completion results of several translation models, spared: theoretical proportion of characters saved; ok: number of target units accepted by the user; good: number of target units that matched the expected whether they were proposed or not; nu: number of sentences for which no target unit was found by the translation model; u: number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed.']",0,"['It is also possible to focus on non-compositional compounds , a key point in bilingual applications ( #AUTHOR_TAG ; Melamed , 1997 ; Lin , 99 ) .', 'Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995;Furuse and Iida, 96).', 'In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags.']"
CCT16,A00-1019,Unit completion for a computer-aided translation typing system,retrieving collocations by cooccurrences and word order constraints,"['Sayori Shimohata', 'Toshiyuki Sugio', 'Junji Nagata']",method,,"This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; #AUTHOR_TAG ; Russell , 1998 ) .","['Finding relevant units in a text has been explored in many areas of natural language processing.', 'Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.', 'For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus.', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; #AUTHOR_TAG ; Russell , 1998 ) .']",0,"['Finding relevant units in a text has been explored in many areas of natural language processing.', 'Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus.', 'This method allows the efficient retrieval of arbitrary length n-grams ( Nagao and Mori , 94 ; Haruno et al. , 96 ; Ikehaxa et al. , 96 ; #AUTHOR_TAG ; Russell , 1998 ) .']"
CCT17,A00-1019,Unit completion for a computer-aided translation typing system,an algorithm for finding noun phrase correspondences in bilingual corpora,['Julian Kupiec'],method,"The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus. The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages. Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers. The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated. Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings.","One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) .","['One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997;Lin, 99).', 'Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995;Furuse and Iida, 96).', 'In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags.', ""An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as JrNp in the following) to the 39,093 english sequences still surviving after previous filters ~'1 and ~'2 removes 35,939 of them (92%)."", '4: Completion results of several translation models, spared: theoretical proportion of characters saved; ok: number of target units accepted by the user; good: number of target units that matched the expected whether they were proposed or not; nu: number of sentences for which no target unit was found by the translation model; u: number of sentences for which at least one helpful unit has been found by the model, but not necessarily proposed.']",5,"['One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints ( Gaussier , 1995 ; #AUTHOR_TAG ; hua Chen and Chen , 94 ; Fung , 1995 ; Evans and Zhai , 1996 ) .', 'It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994;Melamed, 1997;Lin, 99).']"
CCT18,A00-1016,A compact architecture for dialogue management based on scripts and meta-outputs,commandtalk a spokenlanguage interface for battlefield simulations,"['R Moore', 'J Dowding', 'H Bratt', 'J Gawron']",experiments,,"The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .","['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .', 'The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)).', 'Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000).', 'Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon.', 'The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized.', 'Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager.', 'We refer to these as linguistic level representations.']",5,"['The speech and language processing architecture is based on that of the SRI CommandTalk system ( #AUTHOR_TAG ; Stent et a. , 1999 ) .']"
CCT19,A00-1016,A compact architecture for dialogue management based on scripts and meta-outputs,commandtalk a spokenlanguage interface for battlefield simulations,"['R Moore', 'J Dowding', 'H Bratt', 'J Gawron']",introduction,,"CornmandTalk ( #AUTHOR_TAG ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .","['The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem.', 'More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'A number of other systems have addressed part of the task.', 'CornmandTalk ( #AUTHOR_TAG ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', ""Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous."", 'Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995;Pyre et al., 1995).', 'In most of this and other related work the treatment is some variant of the following.', 'If there is a speech interface, the input speech signal is converted into text.', ""Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.""]",0,"['The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system.', 'More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998;Perzanowski et al., 1999).', 'CornmandTalk ( #AUTHOR_TAG ) , Circuit Fix-It Shop ( Smith , 1997 ) and TRAINS-96 ( Traum and Allen , 1994 ; Traum and Andersen , 1999 ) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents .', ""Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous."", 'If there is a speech interface, the input speech signal is converted into text.', ""Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.""]"
CCT20,A00-1018,An automatic reviser,tagging english text with a probabilistic model,['B Merialdo'],,"In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined:using text that has been tagged by hand and computing relative frequency counts,using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.Experminents show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.",We do this with a first-order HMM part-ofspeech tagger ( Merialdo #AUTHOR_TAG ) .,"['1.', 'An aligner.', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length (Gale and Church [7]) and a notion of cognateness (Simard [161).', '2. Transducers.', 'In order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'This is done with transducers (Kaplan and Kay, [10]).', '3. Part-of-speech tagger.', 'Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates).', '~ These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo #AUTHOR_TAG ) .', 'I In the rest of the paper, we will use deceptive cognate very Iosely often to refer to normative usage of word in general.']",5,"['3. Part-of-speech tagger.', 'We do this with a first-order HMM part-ofspeech tagger ( Merialdo #AUTHOR_TAG ) .']"
CCT21,A00-1018,An automatic reviser,a program for aligning sentences in bilingual corpora,"['W Gale', 'K Church']",,"Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.",This alignment is done on the basis of both length ( Gale and Church #AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .,"['1.', 'An aligner.', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length ( Gale and Church #AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .', '2. Transducers.', 'In order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', 'This is done with transducers (Kaplan and Kay, [10]).', '3. Part-of-speech tagger.', 'Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates).', '~ These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.', 'We do this with a first-order HMM part-ofspeech tagger (Merialdo [13]).', 'I In the rest of the paper, we will use deceptive cognate very Iosely often to refer to normative usage of word in general.']",5,"['An aligner.', 'After identification of word and sentence boundaries the text is processed into a bi-text by an alignment program.', 'This alignment is done on the basis of both length ( Gale and Church #AUTHOR_TAG ) and a notion of cognateness ( Simard [ 16 ] ) .', 'In order to compare numerical expressions, which often diverge in format between given pairs of languages, normalisation toward a common format is required.', '3. Part-of-speech tagger.', 'Misleading similarities in graphical form can sometime induce translation mistakes (deceptive cognates).', '~ These forbidden pairs normally involve only one of several possible parts of speech, hence the need to disambiguate them.']"
CCT22,A00-1021,Ranking suspected answers to natural language questions using predictive annotation,disambiguation of proper names in text,"['Nina Wacholder', 'Yael Ravin', 'Misook Choi']",experiments,"Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents. We analyze the types of ambiguity --- structural and semantic --- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T. J. Watson Research Center.","â¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .","['â\x80¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', 'We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching.', 'Thus the text ""for 5 centuries"" matches the DURATIONS pattern ""for :CARDINAL _timeperiod"", where :CAR-DINAL is the label for cardinal numbers, and _timeperiod marks a time expression.']",5,"['â\x80¢ Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; #AUTHOR_TAG ) , which performs lemmatization , and discovers proper names and technical terms .', 'We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching.']"
CCT23,A00-1022,Message classification in the call center,an information extraction core system for real world german text processing,"['Giinter Neumann', 'Rolf Backofen', 'Judith Baur', 'Markus Becker', 'Christian Braun']",experiments,"This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner.Comment: 9 pages; in Proc. of 5th ANLP, 199",100000 word stems of German ( #AUTHOR_TAG ) .,"['MorphAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words.', 'We are using a lexicon of approx.', '100000 word stems of German ( #AUTHOR_TAG ) .']",5,['100000 word stems of German ( #AUTHOR_TAG ) .']
CCT24,A00-1022,Message classification in the call center,an information extraction core system for real world german text processing,"['Giinter Neumann', 'Rolf Backofen', 'Judith Baur', 'Markus Becker', 'Christian Braun']",,"This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner.Comment: 9 pages; in Proc. of 5th ANLP, 199","Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #AUTHOR_TAG ) .","['Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #AUTHOR_TAG ) .', 'The fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient STP components and 4Almost all tools we examined build a single multicategorizer except for SVM-Light, which builds multiple binary classifiers.', 'generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines includes a text tokenizer, a lexical processor and a chunk parser.', 'The chunk parser itself is subdivided into three components.', 'In the first step, phrasal fragments like general nominal expressions and verb groups are recognized.', 'Next, the dependency-based structure of the fragments of each sentence is computed using a set of specific sentence patterns.', 'Third, the grammatical functions are determined for each dependency-based structure on the basis of a large subcategorization lexicon.', 'The present application benefits from the high modularity of the usage of the components.', 'Thus, it is possible to run only a subset of the components and to tailor their output.', 'The experiments described in Section 4 make use of this feature.']",5,"['Linguistic preprocessing of text documents is carried out by re-using smes , an information extraction core system for real-world German text processing ( #AUTHOR_TAG ) .']"
CCT25,A00-1025,Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering system,errordriven pruning of treebank grammars for base noun phrase identification,"['C Cardie', 'D Pierce']",,"Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a ""treebank"" corpus; then the grammar is improved by selecting rules with high ""benefit"" scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.","Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #AUTHOR_TAG .","['The NP-based QA System.', 'Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal (Marcus et al., 1993).', 'In the experiments below, the NP filter follows the application of the document retrieval and text summarization components.', 'Pronoun answer hypotheses are discarded, and the NPs are assembled into 50-byte chunks.']",5,"['Our implementation of the NP-based QA system uses the Empire noun phrase finder , which is described in detail in #AUTHOR_TAG .', 'Empire identifies base NPs --non-recursive noun phrases --using a very simple algorithm that matches part-of-speech tag sequences based on a learned noun phrase grammar.', 'The approach is able to achieve 94% precision and recall for base NPs derived from the Penn Treebank Wall Street Journal (Marcus et al., 1993).', 'Pronoun answer hypotheses are discarded, and the NPs are assembled into 50-byte chunks.']"
CCT26,A00-1025,Examining the role of statistical and linguistic knowledge sources in a general-knowledge question-answering system,the tipster summac text summarization evaluation,"['I Mani', 'T Firmin', 'D House', 'G Klein', 'B Sundheim', 'L Hirschman']",,"All too frequently, a software cost estimate is required in the early stages of the life-cycle when requirements and design specifications are immature. To produce a cost estimate under these conditions requires extensive use of expert judgment and addressing significant estimatio","Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :","['We next hypothesize that query-dependent text summarization algorithms will improve the performance of the QA system by focusing the system on the most relevant portions of the retrieved documents.', 'The goal for query-dependent summarization algorithms is to provide a short summary of a document with respect to a specific query.', 'Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :']",1,"['Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( #AUTHOR_TAG ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :']"
CCT27,A00-1031,TnT,a maximum entropy model for partofspeech tagging,['Adwait Ratnaparkhi'],introduction,,The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #AUTHOR_TAG ) .,"['The aim of this paper is to give a detailed account of the techniques used in TnT.', 'Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #AUTHOR_TAG ) .', 'For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).']",1,"['Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).', 'The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( #AUTHOR_TAG ) .', 'For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).']"
CCT28,A00-1031,TnT,a maximum entropy model for partofspeech tagging,['Adwait Ratnaparkhi'],conclusion,,"According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .","['We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature.', 'For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers.', 'In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor.', 'The rather large amount of freedom was not handled in detail in previous publications: handling of start-and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.', 'Note that the decisions we made yield good results for both the German and the English Corpus.', 'They do so for several other corpora as well.', 'The architecture remains applicable to a large variety of languages.', 'According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .', 'It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both.']",1,"['According to current tagger comparisons ( van Halteren et al. , 1998 ; Zavrel and Daelemans , 1999 ) , and according to a comparsion of the results presented here with those in ( #AUTHOR_TAG ) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here .']"
CCT29,A00-2004,TV program segmentation using text-visual analysis,text segmentation based on similarity between words,['Hideki Kozima'],,"This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis.","This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .","['Four similarity measures were examined.', 'The cosine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results.', 'Our spread activation based semantic measure (R98( .....,)) improved a.ccura(:y.', ""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]",1,"[""This confirms that although Kozima 's approach ( #AUTHOR_TAG ) is computationally expensive , it does produce more precise segmentation .""]"
CCT30,A00-2004,TV program segmentation using text-visual analysis,text segmentation based on similarity between words,['Hideki Kozima'],,"This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis.","R98 ( , , , , â ) uses a variant of Kozima 's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity .","[""R98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity is a function of word co- occurrence statistics in the given document.', 'Words that belong to the same sentence are considered to be related.', 'Given the co-occurrence frequencies f(wi, wj), the transition probability matrix t is computed by equation 10.', 'Equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix, x = 5 was used in the experiment.']",2,"[""R98 ( , , , , â\x80\x9e ) uses a variant of Kozima 's semantic similarity measure ( #AUTHOR_TAG ) to compute block similarity ."", 'Word similarity is a function of word co- occurrence statistics in the given document.', 'Equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix, x = 5 was used in the experiment.']"
CCT31,A00-2022,Ambiguity Packing in Constraint-based Parsing Practical Results,a bag of useful techniques for efficient and robust parsing,"['B Kiefer', 'H-U Krieger', 'J Carroll', 'R Malouf']",conclusion,This paper describes new and improved techniques which help a unification-based parser to process input efficiently and robustly. We show that combining these methods leads to a speed-up in parsing time of more than an order of magnitude. The methods are correct in the sense that none of them rule out legal rule applications,"Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .","['In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments.', 'Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']",1,"['Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of #AUTHOR_TAG , who report large speed-ups from the elimination of disjunction processing during unification .', 'Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.']"
CCT32,A00-2036,Splittability of Bilexical Context-Free Grammars is Undecidable,a linear observed time statistical parser based on maximum entropy models,['A Ratnaparkhi'],conclusion,"This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall.","We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ) .","['In this paper we have provided an original mathematical argument in favour of this thesis.', 'Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ) .', 'We leave this for future work.']",3,"['Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( #AUTHOR_TAG ) and ( Chelba and Jelinek , 1998 ) .']"
CCT33,A00-2036,Splittability of Bilexical Context-Free Grammars is Undecidable,exploiting syntactic structure for language modeling,"['C Chelba', 'F Jelinek']",conclusion,"It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called ""syntactic distances"", where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.Comment: ACL2","We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ) .","['In this paper we have provided an original mathematical argument in favour of this thesis.', 'Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ) .', 'We leave this for future work.']",3,"['Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).', 'We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( #AUTHOR_TAG ) .']"
CCT34,D08-1001,Revealing the structure of medical dictations with conditional random fields,a critique and improvement of an evaluation metric for text segmentation,"['Lev Pevzner', 'Marti Hearst']",experiments,"The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution. We propose a simple modification to the Pk metric that remedies these problems. This new metriccalled Window Diffmoves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.","Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #AUTHOR_TAG .","['Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #AUTHOR_TAG .', 'WindowDiff returns 0 in case of a perfect segmentation; 1 is the worst possible score.', 'However, it only takes into account segment boundaries and disregards segment types.', 'In section 5.2, we mentioned that loopy BP is not guaranteed to converge in a finite number of iterations.', 'Since we optimize pseudolikelihood for parameter estimation, we are not affected by this limitation in the training phase.', 'However, we use loopy BP with a TRP schedule during testing, so we must expect to encounter non-convergence for some examples.', 'Theoretical results on this topic are discussed by Heskes (2004).', 'We give here an empirical observation of convergence behaviour of loopy BP in our setting; the maximum number of iterations of the TRP schedule was restricted to 1,000.', 'Table 4 shows the percentage of examples converging within this limit and the average number of iterations required by the converging examples, broken down by the different corpora.', 'From these results, we conclude that there is a connection between the quality of the annotation and the convergence behaviour of loopy BP.', ""In practice, even though loopy BP didn't converge for some examples, the solutions after 1,000 iterations where satisfactory.""]",5,"['Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by #AUTHOR_TAG .']"
CCT35,D08-1002,"It's a contradiction---no, it's not",natural logic for textual inference,"['B MacCartney', 'C D Manning']",experiments,"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains.","Although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .","['Meronyms: For some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'For example, in the set born in(Mozart,•) there is no contradiction between the y values ""Salzburg"" and ""Austria"", but ""Salzburg"" conflicts with ""Vienna"".', 'Although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .', 'We therefore simply assigned contradictions between meronyms a probability close to zero.', 'We used the Tipster Gazetteer 4 and WordNet to identify meronyms, both of which have high precision but low coverage.']",4,"['Meronyms: For some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. ""part of"" relation.', 'Although this is only true in cases where y occurs in an upward monotone context ( #AUTHOR_TAG ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .', 'We therefore simply assigned contradictions between meronyms a probability close to zero.', 'We used the Tipster Gazetteer 4 and WordNet to identify meronyms, both of which have high precision but low coverage.']"
CCT36,D08-1004,Modeling annotators,headdriven statistical models for natural language parsing,['Michael Collins'],,"This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.",We parse each sentence with the Collins parser ( #AUTHOR_TAG ) .,"['To train our model, we use L-BFGS to locally maximize the log of the objective function (1): 15 These are the function words with count ≥ 40 in a random sample of 100 documents, and which were associated with the O-I tag transition at more than twice the average rate.', 'We do not use any other lexical φ-features that reference x, for fear that they would enable the learner to explain the rationales without changing θ as desired (see the end of section 5.3). 14', 'We parse each sentence with the Collins parser ( #AUTHOR_TAG ) .', 'Then the document has one big parse tree, whose root is DOC, with each sentence being a child of DOC.']",5,['We parse each sentence with the Collins parser ( #AUTHOR_TAG ) .']
CCT37,D08-1004,Modeling annotators,a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts,"['B Pang', 'L Lee']",experiments,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.","It is based on the dataset of #AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .","['In Zaidan et al. (2007), we introduced the �Movie Review Polarity Dataset Enriched with Annotator Rationales.�8', 'It is based on the dataset of #AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .', 'All our experiments use F9 as their final blind test set.']",2,"['In Zaidan et al. (2007), we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.8', 'It is based on the dataset of #AUTHOR_TAG ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .']"
CCT38,D08-1004,Modeling annotators,a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts,"['B Pang', 'L Lee']",,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.",We collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .,"['A human annotator can provide hints to a machine learner by highlighting contextual ""rationales"" for each of his or her annotations (Zaidan et al., 2007).', 'How can one exploit this side information to better learn the desired parameters θ?', 'We present a generative model of how a given annotator, knowing the true θ, stochastically chooses rationales.', 'Thus, observing the rationales helps us infer the true θ.', 'We collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .', 'Our new generative approach exploits the rationales more effectively than our previous ""masking SVM"" approach.', 'It is also more principled, and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks.']",5,['We collect substring rationales for a sentiment classification task ( #AUTHOR_TAG ) and use them to obtain significant accuracy improvements for each annotator .']
CCT39,D08-1004,Modeling annotators,a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts,"['B Pang', 'L Lee']",method,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.","We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .","['where f (•) extracts a feature vector from a classified document, θ are the corresponding weights of those features, and Z θ (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count ≥ 4 in the full 2000-document corpus.', 'Define f h (x, y) to be y if v h appears at least once in x, and 0 otherwise.', 'Thus θ ∈ R 17744 , and positive weights in θ favor class label y = +1 and equally discourage y = −1, while negative weights do the opposite.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.', 'Future work should consider more complex features and how they are signaled by rationales, as discussed in section 3.2.']",5,"['where f (*) extracts a feature vector from a classified document, th are the corresponding weights of those features, and Z th (x) def = y u(x, y) is a normalizer.', 'We use the same set of binary features as in previous work on this dataset ( Pang et al. , 2002 ; #AUTHOR_TAG ; Zaidan et al. , 2007 ) .', 'Specifically, let V = {v 1 , ..., v 17744 } be the set of word types with count >= 4 in the full 2000-document corpus.', 'This standard unigram feature set is linguistically impoverished, but serves as a good starting point for studying rationales.']"
CCT40,D08-1007,Discriminative learning of selectional preference from unlabeled text,automatic retrieval and clustering of similar words,['Dekang Lin'],experiments,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"We gather similar words using #AUTHOR_TAGa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .","['We first evaluate D S P on disambiguating positives from pseudo-negatives, comparing to recently-proposed systems that also require no manually- compiled resources like WordNet.', 'We convert Da- gan et al. (1999)�s similarity-smoothed probability to MI by replacing the empirical Pr n|v in Equa- tion (2) with the smoothed PrSIM from Equation (1).', 'We also test an MI model inspired by Erk (2007): MISIM n;v =log X Simn_;n Prv;n_ n__SIMS n', ""We gather similar words using #AUTHOR_TAGa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .""]",5,"[""We gather similar words using #AUTHOR_TAGa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .""]"
CCT41,D08-1007,Discriminative learning of selectional preference from unlabeled text,offline strategies for online question answering answering questions before they are asked,"['Michael Fleischman', 'Eduard Hovy', 'Abdessamad Echihabi']",method,,"We also made use of the person-name/instance pairs automatically extracted by #AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.","['We also made use of the person-name/instance pairs automatically extracted by #AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.', 'We have features for all concepts and therefore learn their association with each verb.']",5,"['We also made use of the person-name/instance pairs automatically extracted by #AUTHOR_TAG .2This data provides counts for pairs such as ""Edwin Moses , hurdler"" and ""William Farley , industrialist.']"
CCT42,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",experiments,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .","['Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al., 2004).', ""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) ."", 'pronoun is the direct object of a verb predicate, v.', ""A pronoun's antecedent must obey v's selectional preferences."", 'If we have a better model of SP, we should be able to better select pronoun antecedents.']",1,"[""We study the cases where a 9Recall that even the #AUTHOR_TAG system , built on the world 's largest corpus , achieves only 34 % recall ( Table 1 ) ( with only 48 % of positives and 27 % of all pairs previously observed , but see Footnote 5 ) .""]"
CCT43,D08-1007,Discriminative learning of selectional preference from unlabeled text,automatic retrieval and clustering of similar words,['Dekang Lin'],experiments,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"#AUTHOR_TAGa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat .","['It is interesting to inspect the feature weights returned by our system.', 'In particular, the weights on the verb co-occurrence features (Section 3.3.1)', 'provide a high-quality, argument-specific similarityranking of other verb contexts.', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAGa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", 'Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP cooc on the verb join, 3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a Other features are also weighted intuitively.', 'Note that case is a strong indicator for some arguments, for example the weight on being lower-case is high for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations.']",0,"['It is interesting to inspect the feature weights returned by our system.', 'In particular, the weights on the verb co-occurrence features (Section 3.3.1)', 'provide a high-quality, argument-specific similarityranking of other verb contexts.', 'The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).', ""#AUTHOR_TAGa ) 's similar word list for eat misses these but includes sleep ( ranked 6 ) and sit ( ranked 14 ) , because these have similar subjects to eat ."", 'Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSP cooc on the verb join, 3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a Other features are also weighted intuitively.']"
CCT44,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",experiments,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","Also , the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .","['5 Available from the LDC as LDC2006T13.', 'This collection was generated from approximately 1 trillion tokens of online text.', 'Unfortunately, tokens appearing less than 200 times have been mapped to the UNK symbol, and only N-grams appearing more than 40 times are included.', 'Unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'The similarity-smoothed examples will be undefined if SIMS(w) is empty.', 'Also , the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .', 'As a reasonable default for these cases, we assign them a negative decision.']",5,"['5 Available from the LDC as LDC2006T13.', 'This collection was generated from approximately 1 trillion tokens of online text.', 'Unfortunately, tokens appearing less than 200 times have been mapped to the UNK symbol, and only N-grams appearing more than 40 times are included.', 'Unlike results from search engines, however, experiments with this corpus are replicable.', 'not be able to provide a score for each example.', 'Also , the #AUTHOR_TAG approach will be undefined if the pair is unobserved on the web .']"
CCT45,D08-1007,Discriminative learning of selectional preference from unlabeled text,inducing a semantically annotated lexicon via embased clustering,"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']",method,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.","Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .","['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pairwise Disambiguation.', 'Unlike this task, we classify each predicate-argument pair independently as plausible/implausible.', 'We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1']",1,"['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .']"
CCT46,D08-1007,Discriminative learning of selectional preference from unlabeled text,cooccurrence retrieval a flexible framework for lexical distributional similarity,"['Julie Weeds', 'David Weir']",method,"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.",The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .,"['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation.', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']",1,"['The advantage of tuning similarity to the application of interest has been shown previously by #AUTHOR_TAG .', 'Our approach, on the other hand, discriminatively sets millions of individual similarity values.', 'Like Weeds and Weir (2005), our similarity values are asymmetric.']"
CCT47,D08-1007,Discriminative learning of selectional preference from unlabeled text,isp learning inferential selectional preferences,"['Patrick Pantel', 'Rahul Bhagat', 'Bonaventura Coppola', 'Timothy Chklovski', 'Eduard Hovy']",method,"Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness.",MI was also recently used for inference-rule SPs by #AUTHOR_TAG .,"['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a For a fixed verb, MI is proportional to Keller and Lapata (2003)'s conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n)."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by #AUTHOR_TAG .']",0,['MI was also recently used for inference-rule SPs by #AUTHOR_TAG .']
CCT48,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",method,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .","['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed.', 'The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones.', 'We refer to this as Pairwise Disambiguation.', 'Unlike this task, we classify each predicate-argument pair independently as plausible/implausible.', 'We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1']",1,"['Our training examples are similar to the data created for pseudodisambiguation , the usual evaluation task for SP models ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'We refer to this as Pairwise Disambiguation.', 'We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1']"
CCT49,D08-1007,Discriminative learning of selectional preference from unlabeled text,inducing a semantically annotated lexicon via embased clustering,"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']",related work,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.","Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( #AUTHOR_TAG ) .","['Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996).', 'For example, we might have a class Mexican Food and learn that the entire class is suitable for eating.', 'Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( #AUTHOR_TAG ) .', 'Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models.']",0,"['Usually , the classes are from WordNet ( Miller et al. , 1990 ) , although they can also be inferred from clustering ( #AUTHOR_TAG ) .']"
CCT50,D08-1007,Discriminative learning of selectional preference from unlabeled text,inducing a semantically annotated lexicon via embased clustering,"['Mats Rooth', 'Stefan Riezler', 'Detlef Prescher', 'Glenn Carroll', 'Franz Beil']",experiments,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.","Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .","['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)).', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']",1,"['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; Keller and Lapata , 2003 ; #AUTHOR_TAG ) .', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']"
CCT51,D08-1007,Discriminative learning of selectional preference from unlabeled text,isp learning inferential selectional preferences,"['Patrick Pantel', 'Rahul Bhagat', 'Bonaventura Coppola', 'Timothy Chklovski', 'Eduard Hovy']",related work,"Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness.","Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ) .","['Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ) .', 'Inferences such as ""[X wins Y] ⇒ [X plays Y]"" are only valid for certain argu-ments X and Y.', 'We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments.']",0,"['Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules ( #AUTHOR_TAG ; Roberto et al. , 2007 ) .']"
CCT52,D08-1007,Discriminative learning of selectional preference from unlabeled text,automatic retrieval and clustering of similar words,['Dekang Lin'],related work,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #AUTHOR_TAGa ) 's information-theoretic metric work best .,"['where Sim(v ′ , v) returns a real-valued similarity between two verbs v ′ and v (normalized over all pair similarities in the sum).', 'In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs.', 'One key issue is how to define the set of similar words, SIMS(w).', ""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #AUTHOR_TAGa ) 's information-theoretic metric work best ."", 'Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet.']",0,"[""Erk ( 2007 ) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and #AUTHOR_TAGa ) 's information-theoretic metric work best .""]"
CCT53,D08-1007,Discriminative learning of selectional preference from unlabeled text,automatic retrieval and clustering of similar words,['Dekang Lin'],experiments,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,"We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( #AUTHOR_TAGb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .","['We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( #AUTHOR_TAGb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .', 'Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved.', 'Passive subjects (the car was bought) were converted to objects (bought car).', 'We set the MI-threshold, τ , to be 0, and the negative-to-positive ratio, K, to be 2.']",5,"['We parsed the 3 GB AQUAINT corpus ( Voorhees , 2002 ) using Minipar ( #AUTHOR_TAGb ) , and collected verb-object and verb-subject frequencies , building an empirical MI model from this data .']"
CCT54,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",experiments,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .","['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness.', 'We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)).', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']",1,"['Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times ( Erk , 2007 ; #AUTHOR_TAG ; Rooth et al. , 1999 ) .', 'We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise.', 'Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature.', 'When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.']"
CCT55,D08-1007,Discriminative learning of selectional preference from unlabeled text,word association norms mutual information and lexicography,"['Kenneth Ward Church', 'Patrick Hanks']",method,"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words",We measure this association using pointwise Mutual Information ( MI ) ( #AUTHOR_TAG ) .,"['To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text.', 'To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data.', 'We measure this association using pointwise Mutual Information ( MI ) ( #AUTHOR_TAG ) .', 'The MI between a verb predicate, v, and its object argument, n, is:']",5,['We measure this association using pointwise Mutual Information ( MI ) ( #AUTHOR_TAG ) .']
CCT56,D08-1007,Discriminative learning of selectional preference from unlabeled text,using the web to obtain frequencies for unseen bigrams,"['Frank Keller', 'Mirella Lapata']",method,"This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudo disambiguation task.","Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .","['. That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n).', 'For example, a feature for a verb-object pair might be, ""the verb is eat and the object is lower-case.""', 'In this representation, features for one predicate will be completely independent from those for every other predicate.', ""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) ."", 'Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.', 'MI was also recently used for inference-rule SPs by Pantel et al. (2007).']",4,"[""Thus rather than a single training procedure , we can actually partition the examples by predicate , and train a 1For a fixed verb , MI is proportional to #AUTHOR_TAG 's conditional probability scores for pseudodisambiguation of ( v , n , n â\x80² ) triples : Pr ( v | n ) = Pr ( v , n ) / Pr ( n ) , which was shown to be a better measure of association than co-occurrence frequency f ( v , n ) .""]"
CCT57,D08-1009,Scaling textual inference to the web,natural logic for textual inference,"['B MacCartney', 'C D Manning']",introduction,"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains.","HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG ) .","['HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG ) .']",1,"['HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( #AUTHOR_TAG ) .']"
CCT58,D08-1009,Scaling textual inference to the web,natural logic for textual inference,"['B MacCartney', 'C D Manning']",related work,"This paper presents the first use of a computational model of natural logic---a system of logical inference which operates over natural language---for textual inference. Most current approaches to the PASCAL RTE textual inference task achieve robustness by sacrificing semantic precision; while broadly effective, they are easily confounded by ubiquitous inferences involving monotonicity. At the other extreme, systems which rely on first-order logic and theorem proving are precise, but excessively brittle. This work aims at a middle way. Our system finds a low-cost edit sequence which transforms the premise into the hypothesis; learns to classify entailment relations across atomic edits; and composes atomic entailments into a top-level entailment judgment. We provide the first reported results for any system on the FraCaS test suite. We also evaluate on RTE3 data, and show that hybridizing an existing RTE system with our natural logic system yields significant performance gains.","While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms .","['Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005).', ""While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms ."", 'For instance, (Braz et al., 2005) represents T , H , and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer linear program derived from that representation.']",1,"['Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005).', ""While many approaches have addressed this problem , our work is most closely related to that of ( Raina et al. , 2005 ; #AUTHOR_TAG ; Tatu and Moldovan , 2006 ; Braz et al. , 2005 ) , which convert the inputs into logical forms and then attempt to ` prove ' H from T plus a set of axioms .""]"
CCT59,D08-1036,A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers,a fully bayesian approach to unsupervised partofspeech tagging,"['Sharon Goldwater', 'Tom Griffiths']",,,The studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used .,"['The studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.', 'We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set).']",1,"['The studies presented by #AUTHOR_TAG and Johnson ( 2007 ) differed in the number of states that they used .', 'evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.', 'We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set).']"
CCT60,D08-1036,A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers,a fully bayesian approach to unsupervised partofspeech tagging,"['Sharon Goldwater', 'Tom Griffiths']",conclusion,,"On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #AUTHOR_TAG .","['As might be expected, our evaluation measures disagree somewhat, but the following broad tendancies seem clear.', 'On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #AUTHOR_TAG .', 'This is perhaps not too surprising, as the Bayesian prior plays a comparatively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets.']",1,"['On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in #AUTHOR_TAG .', 'This is perhaps not too surprising, as the Bayesian prior plays a comparatively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets.']"
CCT61,D08-1039,Triplet lexicon models for statistical machine translation,word triggers and the em algorithm,"['Christoph Tillmann', 'Hermann Ney']",method,"In this paper, we study the use of so-called word trigger pairs to improve an existing language model, which is typically a trigram model in combination with a cache component. A word trigger pair is defined as a long-distance word pair. We present two methods to select the most significant single word trigger pairs. The selected trigger pairs are used in a combined model where the interpolation parameters and trigger interaction parameters are trained by the EM algorithm.","The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .","['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']",1,"['The resulting training procedure is analogous to the one presented in ( Brown et al. , 1993 ) and ( #AUTHOR_TAG ) .']"
CCT62,D08-1042,A dependency-based word subsequence kernel,kernel methods for relation extraction,"['D Zelenko', 'C Aone', 'A Richardella']",related work,"We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.","Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ) .","['A few kernels based on dependency trees have also been proposed.', 'Zelenko et al. (2003) proposed a tree kernel over shallow parse tree representations of sentences.', 'This tree kernel was slightly generalized by Culotta and Sorensen (2004) to compute similarity between two dependency trees.', 'In addition to the words, this kernel also incorporates word classes into the kernel.', 'The kernel is based on counting matching subsequences of children of matching nodes.', 'But as was also noted in (Bunescu and Mooney, 2005a), this kernel is opaque i.e. it is not obvious what the implicit features are and the authors do not describe it either.', 'In contrast, our dependency-based word subsequence kernel, which also computes similarity between two dependency trees, is very transparent with the implicit features being simply the dependency paths.', 'Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ) .', 'Bunescu and Mooney (2005a) give a shortest path dependency kernel for relation extraction.', 'Their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'This kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'Their kernel also uses word classes in addition to the words themselves.']",3,"['A few kernels based on dependency trees have also been proposed.', 'This tree kernel was slightly generalized by Culotta and Sorensen (2004) to compute similarity between two dependency trees.', 'In addition to the words, this kernel also incorporates word classes into the kernel.', 'Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( #AUTHOR_TAG ) .', 'Bunescu and Mooney (2005a) give a shortest path dependency kernel for relation extraction.', 'Their kernel, however, does not find similarity between two sentences but between the shortest dependency paths connecting the two entities of interests in the sentences.', 'This kernel uses general dependency graphs but if the graph is a tree then the shortest path is the only path between the entities.', 'Their kernel also uses word classes in addition to the words themselves.']"
CCT63,D08-1066,Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective,a discriminative latent variable model for statistical machine translation,"['P Blunsom', 'T Cohn', 'M Osborne']",conclusion,"Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. (c) 2008 Association for Computational Linguistics","Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .","['There are various strands of future research.', 'Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.', 'Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper.']",3,"['Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior.', 'Secondly , as ( #AUTHOR_TAG ) show , marginalizing out the different segmentations during decoding leads to improved performance .', 'We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect.', 'Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation.']"
CCT64,D08-1066,Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective,a family of additive online algorithms for category ranking”,"['K Crammer', 'Y Singer']",related work,"We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis of the family of algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm.","More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .","['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']",0,"['A good study comparing document categorization algorithms can be found in (Yang and Liu, 1999).', 'More recently , ( Sebastiani , 2002 ) has performed a good survey of document categorization ; recent works can also be found in ( Joachims , 2002 ) , ( #AUTHOR_TAG ) , and ( Lewis et al. , 2004 ) .']"
CCT65,D08-1066,Phrase Translation Probabilities with ITG Priors and Smoothing as Learning Objective,an iterativelytrained segmentationfree phrase translation model for statistical machine translation,['Quirk'],conclusion,"Attempts to estimate phrase translation probablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by Koehn, et al. (2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.'s model. Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time.",Based on this advise ( Moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .,"['The most similar efforts to ours, mainly (DeNero et al., 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'Based on this advise ( Moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .', 'In this work we also start out from a generative model with latent segmentation variables.', 'However, we find out that concentrating the learning effort on smoothing is crucial for good performance.', 'For this, we devise ITG-based priors over segmentations and employ a penalized version of Deleted Estimation working with EM at its core.', 'The fact that our results (at least) match the heuristic estimates on a reasonably sized data set (947k parallel sentence pairs) is rather encouraging.']",1,"['The most similar efforts to ours, mainly (DeNero et al., 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.', 'Based on this advise ( Moore and #AUTHOR_TAG ) exclude the latent segmentation variables and opt for a heuristic training procedure .']"
CCT66,D08-1113,Latent-variable modeling of string transductions with finite-state methods,learning structured models for phone recognition,"['Slav Petrov', 'Adam Pauls', 'Dan Klein']",conclusion,"We present a maximally streamlined approach to learning HMM-based acoustic models for automatic speech recognition. In our approach, an initial monophone HMM is iteratively refined using a split-merge EM procedure which makes no assumptions about subphone structure or context-dependent structure, and which uses only a single Gaussian per HMM state. Despite the much simplified training process, our acoustic model achieves state-of-the-art results on phone classification (where it outperforms almost all other methods) and competitive performance on phone recognition (where it outperforms standard CD triphone / subphone / GMM approaches). We also present an analysis of what is and is not learned by our system.",Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .,"['In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al., 2007).', 'Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .', 'Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'Finally, rather than define a fixed set of feature templates as in Fig. 2, we would like to refine empirically useful features during training, resulting in language-specific backoff patterns and adaptively sized n-gram windows.', 'Many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods.']",0,['Latent variables we wish to consider are an increased number of word classes ; more flexible regions -- see #AUTHOR_TAG on learning a state transition diagram for acoustic regions in phone recognition -- and phonological features and syllable boundaries .']
CCT67,D08-1113,Latent-variable modeling of string transductions with finite-state methods,applying manytomany alignments and hidden markov models to lettertophoneme conversion,"['Sittichai Jiampojamarn', 'Grzegorz Kondrak', 'Tarek Sherif']",conclusion,,"We would like to use features that look at wide context on the input side , which is inexpensive ( #AUTHOR_TAG ) .","['In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive ( #AUTHOR_TAG ) .', 'Latent variables we wish to consider are an increased number of word classes; more flexible regions-see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition-and phonological features and syllable boundaries.', 'Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).', 'Finally, rather than define a fixed set of feature templates as in Fig. 2, we would like to refine empirically useful features during training, resulting in language-specific backoff patterns and adaptively sized n-gram windows.', 'Many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods.']",3,"['In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks.', 'We would like to use features that look at wide context on the input side , which is inexpensive ( #AUTHOR_TAG ) .', 'Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997).']"
CCT68,D09-1003,Semi-supervised semantic role labeling using the latent words language model,automatic retrieval and clustering of similar words,['Dekang Lin'],conclusion,Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.,#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .,"['We have presented the Latent Words Language Model and showed how it learns, from unlabeled texts, latent words that capture the meaning of a certain word, depending on the context.', 'We then experimented with different methods to incorporate the latent words for Semantic Role Labeling, and tested different methods on the PropBank dataset.', 'Our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'On the full training set the best method performed 2.33% better than the fully supervised model, which is a 10.91% error reduction.', 'Using only 5% of the training data the best semi-supervised model still achieved 60.29%, compared to 40.49% by the supervised model, which is an error reduction of 33.27%.', 'These results demonstrate that the latent words learned by the LWLM help for this complex information extraction task.', 'Furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'We would like to perform experiments on employing this model in other information extraction tasks, such as Word Sense Disambiguation or Named Entity Recognition.', 'The current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .']",0,"['We have presented the Latent Words Language Model and showed how it learns, from unlabeled texts, latent words that capture the meaning of a certain word, depending on the context.', 'We then experimented with different methods to incorporate the latent words for Semantic Role Labeling, and tested different methods on the PropBank dataset.', 'Our best performing method showed a significant improvement over the supervised model and over methods previously proposed in the literature.', 'On the full training set the best method performed 2.33% better than the fully supervised model, which is a 10.91% error reduction.', 'Using only 5% of the training data the best semi-supervised model still achieved 60.29%, compared to 40.49% by the supervised model, which is an error reduction of 33.27%.', 'These results demonstrate that the latent words learned by the LWLM help for this complex information extraction task.', 'Furthermore we have shown that the latent words are simple to incorporate in an existing classifier by adding additional features.', 'We would like to perform experiments on employing this model in other information extraction tasks, such as Word Sense Disambiguation or Named Entity Recognition.', 'The current model uses the context in a very straightforward way, i.e. the two words left and right of the current word, but in the future we would like to explore more advanced methods to improve the similarity estimates.', '#AUTHOR_TAG for example discusses a method where a syntactic parse of the text is performed and the context of a word is modeled using dependency triples .']"
CCT69,D09-1053,Model adaptation via model interpolation and boosting for web search ranking,language model adaptation with map estimation and the perceptron algorithm,"['M Bacchiani', 'B Roark', 'M Saraclar']",conclusion,"In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm. Used in isolation, we show that MAP estimation outperforms the latter approach, for reasons which argue for combining the two approaches. When combined, the resulting system provides a 0.7 percent absolute reduction in word error rate over MAP estimation alone. In addition, we demonstrate that, in a multi-pass recognition scenario, it is better to use the perceptron algorithm on early pass word lattices, since the improved error rate improves acoustic model adaptation.","In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .","['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']",0,"['In this paper , we extend two classes of model adaptation methods ( i.e. , model interpolation and error-driven learning ) , which have been well studied in statistical language modeling for speech and natural language applications ( e.g. , #AUTHOR_TAG ; Bellegarda , 2004 ; Gao et al. , 2006 ) , to ranking models for Web search applications .']"
CCT70,D09-1144,Automatic acquisition of theargument-predicaterelations from a frame-annotated corpus,a probabilistic account of logical metonymy,"['Mirella Lapata', 'Alex Lascarides']",conclusion,"In this article we investigate logical metonymy, that is, constructions in which the argument of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the book means enjoy reading the book, and easy problem means a problem that is easy to solve). The systematic variation in the interpretation of such constructions suggests a rich and complex theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy typically fail to describe exhaustively all the possible interpretations, or they don&apos;t rank those interpretations in terms of their likelihood. In view of this, we acquire the meanings of metonymic verbs and adjectives from a large corpus and propose a probabilistic model that provides a ranking on the set of possible interpretations. We identify the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model&apos;s ranking of meanings correlates reliably with human intuitions","As already mentioned in the literature , see for example ( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .","['As already mentioned in the literature , see for example ( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to paraphrasing or query expansion, see for example (Voorhees, 1994).', ""Suppose that a search engine or a question answering system receives the query schnelle Bombe 'quick bomb'."", 'Probably, in this case the user is interested in finding information about bombs that explode quickly rather then about bombs in general.', ""Knowledge about predicates associated with the noun Bombe 'bomb' could be used for predicting a set of probable implicit predicates."", 'For generation of the semantically and syntactically correct paraphrases it is sometimes not enough to guess the most probable argument-predicate pairs.', 'Information about types of an argument-predicate relation could be helpful, i.e. which semantic and syntactic position does the argument fill in the argument structure of the predicate.', ""For example, compare eine Bombe explodiert schnell 'a bomb explodes quickly' for schnelle Bombe with ein Buch schnell lesen/schreiben 'to read/write a book quickly' for schnelles Buch 'quick book'."", 'In the first case the argument Bombe fills the subject position, while in the second case Buch fills the object position.', 'Since FrameNet contains information about syntactic realization patterns for frame elements, representation of argument-predicate relations in terms of frames directly supports generation of semantically and syntactically correct paraphrases.']",0,"['As already mentioned in the literature , see for example ( #AUTHOR_TAG ) , knowledge about implicit predicates could be potentially useful for a variety of NLP tasks such as language generation , information extraction , question answering or machine translation .', 'Many applications of semantic relations in NLP are connected to paraphrasing or query expansion, see for example (Voorhees, 1994).']"
CCT71,D10-1002,Self-training PCFG grammars with latent annotations across languages,kbest combination of syntactic parsers,"['Hui Zhang', 'Min Zhang', 'Chew Lim Tan', 'Haizhou Li']",conclusion,,Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ) .,"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG.', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ) .', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']",1,"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG.', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set ( #AUTHOR_TAG ) .', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']"
CCT72,D10-1002,Self-training PCFG grammars with latent annotations across languages,coarsetofine nbest parsing and maxent discriminative reranking,"['Eugene Charniak', 'Mark Johnson']",conclusion,,"Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .","['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']",1,"['Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( #AUTHOR_TAG ) and products of latent variable grammars ( Petrov , 2010 ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).']"
CCT73,D10-1002,Self-training PCFG grammars with latent annotations across languages,products of random latent variable grammars,['Slav Petrov'],conclusion,"We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-of-the-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.","Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .","['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data.', 'Two primary factors appear to be determining the efficacy of our self-training approach.', 'First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars.', 'Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .', 'Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009).', 'In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models.', 'One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models.', 'It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more.', 'A simple but computationally expensive way to do this would be to parse the data with an SM7 product model.']",1,"['We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data.', 'Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( #AUTHOR_TAG ) , despite being a single generative PCFG .']"
CCT74,D10-1038,Topic Segmentation and Labeling in Asynchronous Conversations,you talking to me a corpus and algorithm for conversation disentanglement,"['Micha Elsner', 'Eugene Charniak']",conclusion,"When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. This is, to our knowledge, the first such corpus for internet chat. We propose a graph-theoretic model for disentanglement, using discourse-based features which have not been previously applied to this task. The model's predicted disentanglements are highly correlated with manual annotations.",In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #AUTHOR_TAG ) .,"[""Another possibly critical feature is the 'mention of names'."", ""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", ""In addition to these, 'Subject of the email', 'topic-shift cue words' can also be beneficial for a model."", 'As a next step for this research, we will investigate how to exploit these features in our methods.', 'We are also interested in the near future to transfer our approach to other similar domains by hierarchical Bayesian multi-task learning and other domain adaptation methods.', 'We plan to work on both synchronous (e.g., chats, meetings) and asynchronous (e.g., blogs) domains.']",0,"[""In multi-party discussion people usually mention each other 's name for the purpose of disentanglement ( #AUTHOR_TAG ) ."", ""In our corpus we found 175 instances where a participant mentions other participant's name."", 'We plan to work on both synchronous (e.g., chats, meetings) and asynchronous (e.g., blogs) domains.']"
CCT75,D10-1056,Two Decades of Unsupervised POS induction: How far have we come?,classbased ngram models of natural language,"['Peter F Brown', 'Vincent J Della Pietra', 'Peter V Desouza', 'Jennifer C Lai', 'Robert L Mercer']",conclusion,,"We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .","['Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes.', 'We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .', 'These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set.']",0,"['We found that the oldest system ( #AUTHOR_TAG ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .']"
CCT76,D10-1123,Discovery of Semantic Relations between Web Services,scaling textual inference to the web,"['S Schoenmackers', 'O Etzioni', 'D Weld']",conclusion,"Most Web-based Q/A systems work by finding pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, we introduce the Holmes system, which utilizes textual inference (TI) over tuples extracted from text.    Whereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, Holmes utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, Holmes doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, Holmes's runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus---they are ""approximately"" functional in a well-defined sense.","Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .","['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'We release this list for further use by the research community. 2', 'Future Work: Functionality is one of the several properties a relation can possess.', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .', 'These properties are very useful in increasing our understanding about these Open IE relation strings.', 'We believe that the general principles developed in this work, for example, connecting the Open IE knowledge with an existing knowledge resource, will come in very handy in identifying these other properties.']",0,"['We run our techniques on a large set of relations to output a first repository of typed functional relations.', 'We release this list for further use by the research community. 2', 'Others include selectional preferences , transitivity ( #AUTHOR_TAG ) , mutual exclusion , symmetry , etc. .']"
CCT77,D12-1027,Source Language Adaptation Approaches for Resource-Poor Machine Translation,improved statistical machine translation for resourcepoor languages using related resourcerich languages,"['Preslav Nakov', 'Hwee Tou Ng']",method,"We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian-English (using Malay) and Spanish-English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.","Finally , we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .","['Sophisticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian-English bi-text.', 'The second table is built from the simple concatenation.', 'The two tables are then merged as follows: all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'Each phrase pair retains its original scores, which are further augmented with 1-3 additional feature scores indicating its origin: the first/second/third feature is 1 if the pair came from the first/second/both table(s), and 0 otherwise.', 'We experiment using all three, the first two, or the first feature only; we also try setting the features to 0.5 instead of 0. This makes the following six combinations (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use the one that achieves the highest BLEU score on the development set.']",5,"['Sophisticated phrase table combination.', 'Finally , we experiment with a method for combining phrase tables proposed in ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) .', 'The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian-English bi-text.', 'The two tables are then merged as follows: all phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.', 'Each phrase pair retains its original scores, which are further augmented with 1-3 additional feature scores indicating its origin: the first/second/third feature is 1 if the pair came from the first/second/both table(s), and 0 otherwise.']"
CCT78,D12-1027,Source Language Adaptation Approaches for Resource-Poor Machine Translation,improved statistical machine translation for resourcepoor languages using related resourcerich languages,"['Preslav Nakov', 'Hwee Tou Ng']",related work,"We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. The evaluation for Indonesian-English (using Malay) and Spanish-English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data.","For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .","['A third relevant line of research is on reusing bitexts between related languages without or with very little adaptation, which works well for very closely related languages.', 'For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']",1,"['For example , our previous work ( #AUTHOR_TAG ; Nakov and Ng , 2012 ) experimented with various techniques for combining a small bi-text for a resource-poor language ( Indonesian or Spanish , pretending that Spanish is resource-poor ) with a much larger bi-text for a related resource-rich language ( Malay or Portuguese ) ; the target language of all bi-texts was English .', 'However, our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese-Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay-Indonesian, which use unified spelling.', 'Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches.']"
CCT79,D12-1084,Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder,paraphrase fragment extraction from monolingual comparable corpora,"['Rui Wang', 'Chris Callison-Burch']",experiments,"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.","We found the same number using our previous approach ( #AUTHOR_TAG ) , which is roughly equivalent to our core module .","['We mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', ""However, we do include a measure we call productivity to indicate the algorithm's completeness."", 'It is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'Tab. 2 shows the evaluation results.', 'We reach our best precision by using the VP-fragment heuristics, which is still more productive than the LCS method.', 'The grammatical filter gives us a higher precision compared to the purely alignment-based approaches.', 'Enhancing the system with coreference resolution raises the score even further.', 'We cannot directly compare this performance to other systems, as all other approaches have different data sources.', 'However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One state-of-theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67.', 'We found the same number using our previous approach ( #AUTHOR_TAG ) , which is roughly equivalent to our core module .', 'Our approach outperforms both by 17% with similar estimated productivity.']",2,"['We mainly compute precision for this task, as the recall of paraphrase fragments is difficult to define.', 'It is defined as the ratio between the number of resulting fragment pairs and the number of sentence pairs used as input.', 'Tab. 2 shows the evaluation results.', 'We reach our best precision by using the VP-fragment heuristics, which is still more productive than the LCS method.', 'The grammatical filter gives us a higher precision compared to the purely alignment-based approaches.', 'Enhancing the system with coreference resolution raises the score even further.', 'However, precision is usually manually evaluated, so the figures are at least indicative for a comparison with previous work: One state-of-theart system introduced by Zhao et al. (2008) extracts paraphrase fragments from bilingual parallel corpora and reaches a precision of 0.67.', 'We found the same number using our previous approach ( #AUTHOR_TAG ) , which is roughly equivalent to our core module .']"
CCT80,D12-1084,Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder,learning script knowledge with web experiments,"['Michaela Regneri', 'Alexander Koller', 'Manfred Pinkal']",related work,"We describe a novel approach to unsupervised learning of the events that make up a script, along with constraints on their temporal ordering. We collect natural-language descriptions of script-specific event sequences from volunteers over the Internet. Then we compute a graph representation of the script's temporal structure using a multiple sequence alignment algorithm. The evaluation of our system shows that we outperform two informed baselines.",We take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ) .,"['We take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ) .', 'In this earlier work, we focused on event structures and their possible realizations in natural language.', 'The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style.', 'We aligned them with a hand-crafted similarity measure that was specifically designed for this text type.', 'In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and is thus applicable to standard texts.']",2,"['We take some core ideas from our previous work on mining script information ( #AUTHOR_TAG ) .', 'The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style.', 'In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task.', 'The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and is thus applicable to standard texts.']"
CCT81,D12-1084,Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder,paraphrase fragment extraction from monolingual comparable corpora,"['Rui Wang', 'Chris Callison-Burch']",related work,"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.",Our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .,"['From an applicational point of view, sentential paraphrases are difficult to use in other NLP tasks.', 'At the phrasal level, interchangeable patterns (Shinyama et al., 2002;Shinyama and Sekine, 2003) or inference rules (Lin and Pantel, 2001) are extracted.', 'In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words.', 'They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009).', 'The research on general paraphrase fragment extraction at the sub-sentential level is mainly based on phrase pair extraction techniques from the MT literature.', 'Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability.', 'Quirk et al. (2007) extract fragments using a generative model of noisy translations.', 'Our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .', 'Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments.', 'Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008).']",2,['Our own work ( #AUTHOR_TAG ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .']
CCT82,D12-1084,Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder,paraphrase fragment extraction from monolingual comparable corpora,"['Rui Wang', 'Chris Callison-Burch']",,"We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different arti-cles about the same topics or events. The pro-cedure consists of document pair extraction, sentence pair extraction, and fragment pair ex-traction. At each stage, we evaluate the in-termediate results manually, and tune the later stages accordingly. With this minimally su-pervised approach, we achieve 62 % of accu-racy on the paraphrase fragment pairs we col-lected and 67 % extracted from the MSR cor-pus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.","Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .","['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .', 'We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.', '5.3).', 'Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.']",2,"['Provided with the candidate fragment elements , we previously ( #AUTHOR_TAG ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .']"
CCT83,D14-1076,Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees,document summarization via guided sentence compression,"['Chen Li', 'Fei Liu', 'Fuliang Weng', 'Yang Liu']",experiments,"Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the 'sentence compression + sentence selection ' pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. To create an annotated corpus, the human anno-tators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model us-ing a set of word-, syntax-, and document-level features. During summarization, we use multiple compressed sentences in the inte-ger linear programming framework to select salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sen-tence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art.","Similar to ( #AUTHOR_TAGa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .","['Similar to ( #AUTHOR_TAGa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .']",1,"['Similar to ( #AUTHOR_TAGa ) , our summarization system is , which consists of three key components : an initial sentence pre-selection module to select some important sentence candidates ; the above compression model to generate n-best compressions for each sentence ; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences .']"
CCT84,D15-1148,Detecting Content-Heavy Sentences: A Cross-Language Case Study,assessing the discourse factors that influence the quality of machine translation,"['Junyi Jessy Li', 'Marine Carpuat', 'Ani Nenkova']",related work,"We present a study of aspects of discourse structure -- specifically discourse devices used to organize information in a sen-tence -- that significantly impact the qual-ity of machine translation. Our analysis is based on manual evaluations of trans-lations of news from Chinese and Ara-bic to English. We find that there is a particularly strong mismatch in the no-tion of what constitutes a sentence in Chi-nese and English, which occurs often and is associated with significant degradation in translation quality. Also related to lower translation quality is the need to em-ploy multiple explicit discourse connec-tives (because, but, etc.), as well as the presence of ambiguous discourse connec-tives in the English translation. Further-more, the mismatches between discourse expressions across languages significantly impact translation quality.",Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .,"['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .', 'Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences.', 'This prior work was carried over a dataset containing a single reference translation for each Chinese sentence.', 'In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence.', 'We define heavy sentences based on agreement of translator choices and reader preferences.']",1,['Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( #AUTHOR_TAG ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .']
CCT85,E03-1004,Czech-English Dependency-based Machine Translation,automatic procedures in tectogrammatical tagging,['Alena Bohmova'],experiments,"This paper describes a specific part of the Prague Dependency Treebank annotation, the step from the surface dependency structure towards the underlying representation of the sentence. The first section explains the theoretical basis of the project. In Section 2 all the procedure of conversion to the tectogrammatical structure is summarized and Section 3 presents in detail the present stage of the automated part of the conversion procedure.",These automatic transformations are based on linguistic rules ( #AUTHOR_TAG ) .,"['During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules ( #AUTHOR_TAG ) .', 'Subsequently, tectogrammatical functors are assigned by the C4.5 classifier (2abokrtsk9 et al., 2002).']",5,"['During the tectogrammatical parsing of Czech, the analytical tree structure is converted into the tectogrammatical one.', 'These automatic transformations are based on linguistic rules ( #AUTHOR_TAG ) .', 'Subsequently, tectogrammatical functors are assigned by the C4.5 classifier (2abokrtsk9 et al., 2002).']"
CCT86,E03-1004,Czech-English Dependency-based Machine Translation,a maximumentropyinspired parser,['Eugene Charniak'],experiments,,"We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( #AUTHOR_TAG ) .","['The analytical parsing of Czech runs in two steps: the statistical dependency parser, which creates the structure of a dependency tree, and a classifier assigning analytical functors.', 'We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( #AUTHOR_TAG ) .', 'In the second step, we used a module for automatic analytical functor assignment (2abokrtskyT et al., 2002).']",5,"['We carried out two parallel experiments with two parsers available for Czech , parser I ( Hajie et al. , 1998 ) and parser II ( #AUTHOR_TAG ) .']"
CCT87,E03-1004,Czech-English Dependency-based Machine Translation,improved statistical alignment models,"['F J Och', 'H Ney']",,"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen","To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.","['To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', 'As a result, the entry/translation pairs seen in the parallel corpus of WSJ become more probable.', 'For entry/translation pairs not seen in the parallel text, the probability distribution among translations is uniform.', 'The translation is ""GIZA++ se- lected"" if its probability is higher than a threshold, which is in our case set to 0.10.']",5,"['To make the dictionary more sensitive to a specific domain, which is in our case the domain of financial news, we created a probabilistic CzechEnglish dictionary by running GIZA + + training ( translation models 1-4 , see #AUTHOR_TAG ) on the training part of the English-Czech WSJ parallel corpus extended by the parallel corpus of entry/translation pairs from the manual dictionary.', 'For entry/translation pairs not seen in the parallel text, the probability distribution among translations is uniform.']"
CCT88,E03-1004,Czech-English Dependency-based Machine Translation,bleu a method for automatic evaluation of machine translation,"['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'WeiJing Zhu']",introduction,"Human evaluations of machine translation are extensive but expensive. Human eval-uations can take months to finish and in-volve human labor that can not be reused. We propose a method of automatic ma-chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu-ation, and that has little marginal cost per run. We present this method as an auto-mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.",For the evaluation of the results we use the BLEU score ( #AUTHOR_TAG ) .,"['First, we summarize resources available for the experiments (Section 2).', 'Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input.', 'In Section 4 we describe the process of the filtering of dictionaries used in the transfer procedure (for its characterization, see Section 5).', 'The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7.', 'For the evaluation of the results we use the BLEU score ( #AUTHOR_TAG ) .', 'Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (Al-Onaizan et al., 1999;Och and Ney, 2000;Germann et al., 2001), trained on the same parallel corpus.']",5,['For the evaluation of the results we use the BLEU score ( #AUTHOR_TAG ) .']
CCT89,E03-1004,Czech-English Dependency-based Machine Translation,improved statistical alignment models,"['F J Och', 'H Ney']",introduction,"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignmen","We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus .","['First, we summarize resources available for the experiments (Section 2).', 'Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input.', 'In Section 4 we describe the process of the filtering of dictionaries used in the transfer procedure (for its characterization, see Section 5).', 'The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7.', 'For the evaluation of the results we use the BLEU score (Papineni et al., 2001).', 'Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus .']",1,"['First, we summarize resources available for the experiments (Section 2).', 'Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input.', 'The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7.', 'For the evaluation of the results we use the BLEU score (Papineni et al., 2001).', 'Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations.', 'We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; #AUTHOR_TAG ; Germann et al. , 2001 ) , trained on the same parallel corpus .']"
CCT90,E03-1004,Czech-English Dependency-based Machine Translation,bleu a method for automatic evaluation of machine translation,"['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'WeiJing Zhu']",experiments,"Human evaluations of machine translation are extensive but expensive. Human eval-uations can take months to finish and in-volve human labor that can not be reused. We propose a method of automatic ma-chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu-ation, and that has little marginal cost per run. We present this method as an auto-mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.","We evaluated our translations with IBM 's BLEU evaluation metric ( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) .","[""We evaluated our translations with IBM 's BLEU evaluation metric ( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) ."", 'We used four reference retranslations of 490 sentences selected from the WSJ sections 22, 23, and 24, which were themselves used as the fifth reference.', 'The evaluation method used is to hold out each reference in turn and evaluate it against the remaining four, averaging the five BLEU scores.']",5,"[""We evaluated our translations with IBM 's BLEU evaluation metric ( #AUTHOR_TAG ) , using the same evaluation method and reference retranslations that were used for evaluation at HLT Workshop 2002 at CLSP ( Haji 6 et al. , 2002 ) .""]"
CCT91,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,the mathematics of statistical machine translation parameter estimation,"['P F Brown', 'S A Della Pietra', 'V J Della Pietra', 'R L Mercer']",experiments,"We describe a series o,f ive statistical models o,f the translation process and give algorithms,for estimating the parameters o,f these models given a set o,f pairs o,f sentences that are translations o,f one another. We define a concept o,f word-by-word alignment between such pairs o,f sentences. For any given pair of such sentences each o,f our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable o,f these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair o,f sentences. We have a great deal o,f data in French and English from the proceedings o,f the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we,feel that because our algorithms have minimal inguistic content hey would work well on other pairs o,f languages. We also,feel, again because of the minimal inguistic content o,f our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. 1",We performed translation experiments with an implementation of the IBM-4 translation model ( #AUTHOR_TAG ) .,"['We compared the two statistical lexica obtained from the baseline system and from the maximum entropy training on the transformed corpus.', 'For the baseline lexicon, we observed an average of 5.82 Catalan translation candidates per English word and 6.16 Spanish translation candidates.', 'These numbers are significantly reduced in the lexicon which was trained on the transformed corpus using maximum entropy: there, we have an average of 4.20 for Catalan and 4.46 for Spanish.', 'Especially for (nominative) English pronouns (which have many verbs as translation candidates in the baseline lexicon), the number of translation candidates was substantially scaled down by a factor around 4. This shows that our method was successful in producing a more focused lexicon probability distribution.', 'We performed translation experiments with an implementation of the IBM-4 translation model ( #AUTHOR_TAG ) .', 'A description of the system can be found in (Tillmann and Ney, 2002).', 'Table 5 presents an assessment of translation quality for both the language pairs English-Catalan and English-Spanish.', 'We see that there is a significant decrease in error rate for the translation into Catalan.', 'This change is consistent across both error rates, the WER and 100-BLEU.', 'For translations from English into Spanish, the improvement is less substantial.', 'A reason for this might be that the Spanish vocabulary contains more entries and the ratio between fullforms and baseforms is higher: 1.57 for Spanish versus 1.53 for Catalan4 .', 'This makes it more difficult for the system to choose the correct inflection when generating a Spanish sentence.', 'We assume that the extension of our approach to other word classes than verbs will yield a quality gain for translations into Spanish.']",5,['We performed translation experiments with an implementation of the IBM-4 translation model ( #AUTHOR_TAG ) .']
CCT92,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,a maximum entropy approach to natural language processing,"['A L Berger', 'S A Della Pietra', 'V J Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",The maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .,"['The maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .', 'This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy.', 'The distribution is required to satisfy constraints, which represent facts known from the data.', 'These constraints are expressed on the basis of feature functions hu,(s,t),']",5,['The maximum entropy approach ( #AUTHOR_TAG ) presents a powerful framework for the combination of several knowledge sources .']
CCT93,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,improved alignment models for statistical machine translation,"['F J Och', 'C Tillmann', 'H Ney']",,"In this paper, we describe improved alignment models for statistical machine translation. The statistical translation approach uses two types of information: a translation model and a language model. The language model used is a bigram or general m-gram model. The translation model is decomposed into a lexical and an alignment model. We describe two different approaches for statistical translation and present experimental results. The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. We present results using the Verbmobil task (German-English, 6000word vocabulary) which is a limited-domain spoken-language task. The experimental tests were performed on both the text transcription and the speech recognizer output. 1 S t a t i s t i c a l M a c h i n e T r a n s l a t i o n The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string f / = fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)}","For descriptions of SMT systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .","['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']",0,"['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( Germann et al. , 2001 ; #AUTHOR_TAG ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']"
CCT94,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,a maximum entropy approach to natural language processing,"['A L Berger', 'S A Della Pietra', 'V J Della Pietra']",,"The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.","For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 ) .","['where A = {Am } is the set of model parameters with one weight A, for each feature function hm .', 'For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 ) .']",0,"['For an introduction to maximum entropy modeling and training procedures , the reader is referred to the corresponding literature , for instance ( #AUTHOR_TAG ) or ( Ratnaparkhi , 1997 ) .']"
CCT95,E03-1007,Using POS information for statistical machine translation into morphologically rich languages,fast decoding and optimal decoding for machine translation,"['U Germann', 'M Jahr', 'K Knight', 'D Marcu', 'K Yamada']",,"A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.","For descriptions of SMT systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .","['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']",0,"['The input string can be preprocessed before being passed to the search algorithm.', 'If necessary, the inverse of these transformations will be applied to the generated output string.', 'In the work presented here, we restrict ourselves to transforming only one language of the two: the source, which has the less inflected morphology.', 'For descriptions of SMT systems see for example ( #AUTHOR_TAG ; Och et al. , 1999 ; Tillmann and Ney , 2002 ; Vogel et al. , 2000 ; Wang and Waibel , 1997 ) .']"
CCT96,E09-1100,Character-level dependencies in Chinese,exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation,"['Hai Zhao', 'Chunyu Kit']",method,This paper presents a novel approach to improve Chinese word seg- mentation (CWS) that attempts to utilize unlabeled data such as training and test data without annotation for further enhancement of the state-of-the-art perfor- mance of supervised learning. The lexical information plays the role of infor- mation transformation from unlabeled text to supervised learning model. Four types of unsupervised segmentation criteria are used for word candidate extrac- tion and the corresponding word likelihood computation. The information output by unsupervised segmentation criteria as features therefore is integrated into su- pervised learning model to strengthen the learning for the matching subsequence. The effectiveness of the proposed method is verified in data sets from the latest in- ternational CWS evaluation. Our experimental results show that character-based conditional random fields framework can effectively make use of such informa- tion from unlabeled data for performance enhancement on top of the best existing results.,"Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAGc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .","['Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAGc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .', 'Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus.']",1,"['Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; #AUTHOR_TAGc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .']"
CCT97,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,dynamic compilation of weighted contextfree grammars,"['Mehryar Mohri', 'Fernando C N Pereira']",,"Weighted context-free grammars are a convenient formalism for representing grammatical constructions and their likelihoods in a variety of language-processing applications. In particular, speech understanding applications require appropriate grammars both to constrain speech recognition and to help extract the meaning of utterances. In many of those applications, the actual languages described are regular, but context-free representations are much more concise and easier to create. We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages. The resulting automata can then be combined with other speech recognition components. Our method allows the recognizer to dynamically activate or deactivate grammar rules and substitute a new regular language for some terminal symbols, depending on previously recognized inputs, all without recompilation. We also report experimental results showing the practicality of the approach.",1 The representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .,"['1 The representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', 'However, in this paper we use our representation as an intermediate result in approximating an unrestricted context-free grammar, with the final objective of obtaining a single minimal deterministic automaton.', ""For this purpose, Mohri and Pereira's representation offers little advantage.""]",1,"['1 The representation in #AUTHOR_TAG is even more compact than ours for grammars that are not self-embedding .', 'However, in this paper we use our representation as an intermediate result in approximating an unrestricted context-free grammar, with the final objective of obtaining a single minimal deterministic automaton.']"
CCT98,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,finitestate approximation of constraintbased grammars using leftcorner grammar transforms,['Mark Johnson'],,,"This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG .","['By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG .', 'Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method.']",0,"['By restricting the height of the stack of a pushdown automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results.', 'This idea was proposed by Krauwer and des Tombe ( 1981 ) , Langendoen and Langsam ( 1987 ) , and Pulman ( 1986 ) , and was rediscovered by Black ( 1989 ) and recently by #AUTHOR_TAG .']"
CCT99,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,approximating contextfree grammars with a finitestate calculus,['Edmund Grimley-Evans'],,,"We rephrase the method of #AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .","['We rephrase the method of #AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .', 'Then an additional mechanism is introduced that ensures for each rule A --~ X1 • .. Xm separately that the list of visits to the states qo,.. • • qm satisfies some reasonable criteria: a visit to qi, with 0 < i < m, should be followed by one to qi+l or q0.', 'The latter option amounts to a nested incarnation of the rule.', 'There is a complementary condition for what should precede a visit to qi, with 0 < i < m.']",5,"['We rephrase the method of #AUTHOR_TAG as follows : First , we construct the approximating finite automaton according to the unparameterized RTN method above .']"
CCT100,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,precise ngram probabilities from stochastic contextfree grammars,"['Andreas Stolcke', 'Jonathan Segal']",,"We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. The procedure is fully implemented and has proved viable and useful in practice.","This method can be generalized , inspired by #AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .","['This method can be generalized , inspired by #AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .', 'By ignoring the probabilities, each N = 1, 2, 3 .... gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al ... an such that • for each substring v = ai+l ... ai+N (0 < i < n --N) we have A --+* wvy, for some w and y,']",0,"['This method can be generalized , inspired by #AUTHOR_TAG , who derive N-gram probabilities from stochastic context-free grammars .']"
CCT101,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,contextfree parsing through regular approximation,['Mark-Jan Nederhof'],,"We show that context-free parsing can be realised by a 2-phase process, relying on an approximated context-free grammar. In the first phase a finite transducer performs parsing according to the approximation. In the second phase, the approximated parses are refined according to the original grammar.",See #AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata .,['See #AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata .'],0,['See #AUTHOR_TAG for a variant of this approximation that constructs finite transducers rather than finite automata .']
CCT102,J00-1003,Practical Experiments with Regular Approximation of Context-Free Languages,contextfree parsing through regular approximation,['Mark-Jan Nederhof'],,"We show that context-free parsing can be realised by a 2-phase process, relying on an approximated context-free grammar. In the first phase a finite transducer performs parsing according to the approximation. In the second phase, the approximated parses are refined according to the original grammar.","A very similar formulation , for another grammar transformation , is given in #AUTHOR_TAG .","['Figure 10 (b) is no longer possible, since no nonterminal in the transformed grammar would contain 1 in its superscript.Because of the demonstrated increase of the counter f, this transformation is guaranteed to remove self-embedding from the grammar.', 'However, it is not as selective as the transformation we saw before, in the sense that it may also block subderivations that are not of the form A --** ~Afl.', 'Consider for example the subderivation from Figure10, but replacing the lower occurrence of S by any other nonterminal C that is mutually recursive with S, A, and B. Such a subderivation S ---** b c C d a would also be blocked by choosing d = 0.', 'In general, increasing d allows more of such derivations that are not of the form A ~"" o~Afl but also allows more derivations that are of that form.The reason for considering this transformation rather than any other that eliminates self-embedding is purely pragmatic: of the many variants we have tried that yield nontrivial subset approximations, this transformation has the lowest complexity in terms of the sizes of intermediate structures and of the resulting finite automata.In the actual implementation, we have integrated the grammar transformation and the construction of the finite automaton, which avoids reanalysis of the grammar to determine the partition of mutually recursive nonterminals after transformation.', 'This integration makes use, for example, of the fact that for fixed Ni and fixed f, the set of nonterminals of the form A,f, with A c Ni, is (potentially) mutually right-recursive.A set of such nonterminals can therefore be treated as the corresponding case from Figure2, assuming the value right.The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'A very similar formulation , for another grammar transformation , is given in #AUTHOR_TAG .']",1,"['This integration makes use, for example, of the fact that for fixed Ni and fixed f, the set of nonterminals of the form A,f, with A c Ni, is (potentially) mutually right-recursive.A set of such nonterminals can therefore be treated as the corresponding case from Figure2, assuming the value right.The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here.', 'A very similar formulation , for another grammar transformation , is given in #AUTHOR_TAG .']"
CCT103,J00-2003,A Multistrategy Approach to Improving Pronunciation by Analogy,algorithms for graphemephoneme translation for english and french applications for database searches and speech synthesis,"['Michel Divay', 'Anthony J Vitale']",introduction,"Letter-to-sound rules, also known as grapheme-to-phoneme rules, are important computational tools and have been used for a variety of purposes including word or name lookups for database searches and speech synthesis.These rules are especially useful when integrated into database searches on names and addresses, since they can complement orthographic search algorithms that make use of permutation, deletion, and insertion by allowing for a comparison with the phonetic equivalent. In databases, phonetics can help retrieve a word or a proper name without the user needing to know the correct spelling. A phonetic index is built with the vocabulary of the application. This could be an entire dictionary, or a list of proper names. The searched word is then converted into phonetics and retrieved with its information, if the word is in the phonetic index. This phonetic lookup can be used to retrieve a misspelled word in a dictionary or a database, or in a text editor to suggest corrections.Such rules are also necessary to formalize grapheme-phoneme correspondences in speech synthesis architecture. In text-to-speech systems, these rules are typically used to create phonemes from computer text. These phonemic symbols, in turn, are used to feed lower-level phonetic modules (such as timing, intonation, vowel formant trajectories, etc.) which, in turn, feed a vocal tract model and finally output a waveform and, via a digital-analogue converter, synthesized speech. Such rules are a necessary and integral part of a text-to-speech system since a database lookup (dictionary search) is not sufficient to handle derived forms, new words, nonce forms, proper nouns, low-frequency technical jargon, and the like; such forms typically are not included in the database. And while the use of a dictionary is more important now that denser and faster memory is available to smaller systems, letter-to-sound still plays a crucial and central role in speech synthesis technology.Grapheme-to-phoneme technology is also useful in speech recognition, as a way of generating pronunciations for new words that may be available in grapheme form, or for naive users to add new words more easily. In that case, the system must generate the multiple variations of the word.While there are different problems in languages that use non-alphabetic writing systems (syllabaries, as in Japanese, or logographic systems, as in Chinese) (DeFrancis 1984), all alphabetic systems have a structured set of correspondences. These range from the trivial in languages like Spanish or Swahili, to extremely complex in languages such as English and French. This paper will outline some of the previous attempts to construct such rule sets and will describe new and successful approaches to the construction of letter-to-sound rules for English and French.","Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and #AUTHOR_TAG .","['which states that the letter substring B with left context A and right context C receives the pronunciation (i.e., phoneme substring) D. Such rules can also be straightforwardly cast in the IF... THEN form commonly featured in high-level programming languages and employed in expert, knowledge-based systems technology.', 'They also constitute a formal model of universal computation (Post 1943).', 'Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and #AUTHOR_TAG .']",0,"['which states that the letter substring B with left context A and right context C receives the pronunciation (i.e., phoneme substring) D. Such rules can also be straightforwardly cast in the IF... THEN form commonly featured in high-level programming languages and employed in expert, knowledge-based systems technology.', 'Conventionally, these rules are specified by an expert linguist, conversant with the sound and spelling systems of the language of concern.', 'Typical letter-to-sound rule sets are those described by Ainsworth ( 1973 ) , McIlroy ( 1973 ) , Elovitz et al. ( 1976 ) , Hurmicutt ( 1976 ) , and #AUTHOR_TAG .']"
CCT104,J00-2003,A Multistrategy Approach to Improving Pronunciation by Analogy,algorithms for graphemephoneme translation for english and french applications for database searches and speech synthesis,"['Michel Divay', 'Anthony J Vitale']",introduction,"Letter-to-sound rules, also known as grapheme-to-phoneme rules, are important computational tools and have been used for a variety of purposes including word or name lookups for database searches and speech synthesis.These rules are especially useful when integrated into database searches on names and addresses, since they can complement orthographic search algorithms that make use of permutation, deletion, and insertion by allowing for a comparison with the phonetic equivalent. In databases, phonetics can help retrieve a word or a proper name without the user needing to know the correct spelling. A phonetic index is built with the vocabulary of the application. This could be an entire dictionary, or a list of proper names. The searched word is then converted into phonetics and retrieved with its information, if the word is in the phonetic index. This phonetic lookup can be used to retrieve a misspelled word in a dictionary or a database, or in a text editor to suggest corrections.Such rules are also necessary to formalize grapheme-phoneme correspondences in speech synthesis architecture. In text-to-speech systems, these rules are typically used to create phonemes from computer text. These phonemic symbols, in turn, are used to feed lower-level phonetic modules (such as timing, intonation, vowel formant trajectories, etc.) which, in turn, feed a vocal tract model and finally output a waveform and, via a digital-analogue converter, synthesized speech. Such rules are a necessary and integral part of a text-to-speech system since a database lookup (dictionary search) is not sufficient to handle derived forms, new words, nonce forms, proper nouns, low-frequency technical jargon, and the like; such forms typically are not included in the database. And while the use of a dictionary is more important now that denser and faster memory is available to smaller systems, letter-to-sound still plays a crucial and central role in speech synthesis technology.Grapheme-to-phoneme technology is also useful in speech recognition, as a way of generating pronunciations for new words that may be available in grapheme form, or for naive users to add new words more easily. In that case, the system must generate the multiple variations of the word.While there are different problems in languages that use non-alphabetic writing systems (syllabaries, as in Japanese, or logographic systems, as in Chinese) (DeFrancis 1984), all alphabetic systems have a structured set of correspondences. These range from the trivial in languages like Spanish or Swahili, to extremely complex in languages such as English and French. This paper will outline some of the previous attempts to construct such rule sets and will describe new and successful approaches to the construction of letter-to-sound rules for English and French.","For instance , #AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) .","['It is also conceivable that data-driven techniques can actually outperform traditional rules.', 'However, this possibility is not usually given much credence.', ""For instance , #AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', 'Dutoit (1997) takes this further, stating ""such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores"" (p.', '115, note 14).']",0,"['It is also conceivable that data-driven techniques can actually outperform traditional rules.', ""For instance , #AUTHOR_TAG recently wrote : `` To our knowledge , learning algorithms , although promising , have not ( yet ) reached the level of rule sets developed by humans '' ( p. 520 ) ."", '520).', 'Dutoit (1997) takes this further, stating ""such training-based strategies are often assumed to exhibit much more intelligence than they do in practice, as revealed by their poor transcription scores"" (p.']"
CCT105,J00-2003,A Multistrategy Approach to Improving Pronunciation by Analogy,using an online dictionary to find rhyming words and pronunciations for unknown words,"['Roy J Byrd', 'Martin S Chodorow']",introduction,"Humans know a great deal about relationships among words. This paper discusses relationships among word pronunciations. We describe a computer system which models human judgement of rhyme by assigning specific roles to the location of primary stress, the similarity of phonetic segments, and other factors. By using the model as an experimental tool, we expect to improve our understanding of rhyme. A related computer model will attempt to generate pronunciations for unknown words by analogy with those for known words. The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words. As in the case of rhyme, the computer model will be an important tool for improving our understanding of these processes. Both models serve as the basis for functions in the WordSmith automated dictionary system.","See also the work of #AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .","['Pronunciation by analogy (PbA) is a data-driven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by Glushko (1979) and Kay and Marcel (1981).', 'It was first proposed for TTS applications over a decade ago by Dedina andNusbaum (1986, 1991).', 'See also the work of #AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .', 'As detailed by Damper (1995) and Damper and Eastmond (1997), PbA shares many similarities with the artificial intelligence paradigms variously called case-based, memory-based, or instance-based reasoning as applied to letter-to-phoneme conversion (Stanfill and Waltz 1986;Lehnert 1987;Stanfill 1987Stanfill , 1988Golding 1991;Golding and Rosenbloom 1991;van den Bosch and Daelemans 1993).']",0,"['Pronunciation by analogy (PbA) is a data-driven technique for the automatic phonemization of text, originally proposed as a model of reading, e.g., by Glushko (1979) and Kay and Marcel (1981).', 'See also the work of #AUTHOR_TAG , which considers computer-based pronunciation by analogy but does not mention the possible application to text-to-speech synthesis .']"
CCT106,J00-2003,A Multistrategy Approach to Improving Pronunciation by Analogy,classifier combination for improved lexical disambiguation,"['Eric Brill', 'Jun Wu']",experiments,"One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees..","Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .","['Clearly, the above characterization is very wide ranging.', 'Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .', 'According to Abbott (1999, 290), ""While the reasons [that] combining models works so well are not rigorously understood, there is ample evidence that improvements over single models are typical....', 'A strong case can be made for combining models across algorithm families as a means of providing uncorrelated output estimates.""', 'Our purpose in this paper is to study and exploit such fusion by model (or strategy) combination as a way of achieving performance gains in PbA.']",0,"['Consequently , fusion has been applied to a wide variety of pattern recognition and decision theoretic problems -- using a plethora of theories , techniques , and tools -- including some applications in computational linguistics ( e.g. , #AUTHOR_TAG ; van Halteren , Zavrel , and Daelemans 1998 ) and speech technology ( e.g. , Bowles and Damper 1989 ; Romary and Pierre11989 ) .']"
CCT107,J00-2004,Models of Translational Equivalence among Words,automatic evaluation and uniform filter cascades for inducing nbest translation lexicons,['I Dan Melamed'],,"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora.","Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; #AUTHOR_TAG 1995; Wu and Xia 1994).","['Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; #AUTHOR_TAG 1995; Wu and Xia 1994).', 'Most of these algorithms can be summarized as follows:']",0,"['Many researchers have proposed greedy algorithms for estimating nonprobabilistic word-to-word translation models, also known as translation lexicons (e.g., Catizone, Russell, and Warwick 1989; Gale and Church 1991; Fung 1995; Kumano and Hirakawa 1994; #AUTHOR_TAG 1995; Wu and Xia 1994).']"
CCT108,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",related work,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio",Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAGb ) .,"['Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAGb ) .', 'These models involve conditional probabilities, but they can be compared to symmetric models if the latter are normalized by the appropriate marginal distribution.', 'I shall review these models using the notation in Table 1.']",0,['Most probabilistic translation model reestimation algorithms published to date are variations on the theme proposed by #AUTHOR_TAGb ) .']
CCT109,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",method,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio","Unlike the models proposed by #AUTHOR_TAGb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .","['The probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by #AUTHOR_TAGb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .', ""Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions."", 'A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8.']",1,"['The probability distribution trans (.1, ~) is a word-to-word translation model.', 'Unlike the models proposed by #AUTHOR_TAGb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .']"
CCT110,J00-2004,Models of Translational Equivalence among Words,a perspective on word sense disambiguation methods and their evaluation,"['Philip Resnik', 'David Yarowsky']",,"In this position paper, we make several observations about the state of the art in automatic word sense disambiguation. Motivated by these observations, we offer several specific proposals to the community regarding improved evaluation criteria, common training and testing resources, and the definition of sense inventories.","If each word 's translation is treated as a sense tag ( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !","['in bitext space is another kind of collocation.', ""If each word 's translation is treated as a sense tag ( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.', 'This hypothesis implies that if u and v are possible mutual translations, and a token u co-occurs with a token v in the bitext, then with very high probability the pair (u, v) was generated from the same concept and should be linked.', 'To test this hypothesis, I ran one iteration of Method A on 300,000 aligned sentence pairs from the Canadian Hansards bitext.', 'I then plotted the links (u,v) ratio ~ for several values of cooc (u, v) in Figure 2. The curves show that the ratio links (u,v) cooc (u,v) tends to be either very high or very low.', 'This bimodality is not an artifact of the competitive linking process, because in the first iteration, linking decisions are based only on the initial similarity metric.']",5,"[""If each word 's translation is treated as a sense tag ( #AUTHOR_TAG ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !"", 'Method B exploits this property under the hypothesis that ""one sense per collocation"" holds for translational collocations.']"
CCT111,J00-2004,Models of Translational Equivalence among Words,automatic evaluation and uniform filter cascades for inducing nbest translation lexicons,['I Dan Melamed'],,"This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora.","In informal experiments described elsewhere ( #AUTHOR_TAG ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .","['In informal experiments described elsewhere ( #AUTHOR_TAG ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows:']",2,"['In informal experiments described elsewhere ( #AUTHOR_TAG ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .']"
CCT112,J00-2004,Models of Translational Equivalence among Words,reading more into foreign languages,"['John Nerbonne', 'Lauri Karttunen', 'Elena Paskaleva', 'Gabor Proszeky', 'Tiit Roosmaa']",method,"GLOSSER is designed to support reading and learning to read in a foreign language. There are four language pairs currently supported by GLOSSER: English Bulgarian, English-Estonian, English Hungarian and French-Dutch. The program is operational on UNIX and Windows '95 platforms, and has undergone a pilot user-study. A demonstration (in UNIX) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis (ICALL), including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples.","â¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â¢ multilingual document filtering ( e.g. , Oard 1997 ) , â¢ computer-assisted language learning ( e.g. , #AUTHOR_TAG ) , â¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,","['â\x80¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , #AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']",0,"['â\x80¢ cross-language information retrieval ( e.g. , McCarley 1999 ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , #AUTHOR_TAG ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']"
CCT113,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio","In this situation , #AUTHOR_TAGb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''","[""In this situation , #AUTHOR_TAGb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''"", 'The single most probable assignment Ama~ is the maximum a posteriori (MAP) assignment:']",4,"[""In this situation , #AUTHOR_TAGb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''""]"
CCT114,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio","Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #AUTHOR_TAG ) .","['In Method B, the estimation of the auxiliary parameters A + and A-depends only on the overall distribution of co-occurrence counts and link frequencies.', 'All word pairs that co-occur the same number of times and are linked the same number of times are assigned the same score.', 'More accurate models can be induced by taking into account various features of the linked tokens.', 'For example, frequent words are translated less consistently than rare words (Catizone, Russell, and Warwick 1989).', 'To account for these differences, we can estimate separate values of A + and A-for different ranges of cooc (u, v).', 'Similarly, the auxiliary parameters can be conditioned on the linked parts of speech.', 'A kind of word order correlation bias can be effected by conditioning the auxiliary parameters on the relative positions of linked word tokens in their respective texts.', 'Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #AUTHOR_TAG ) .', 'Brown et al. 1993).', 'When the auxiliary parameters are conditioned on different link classes, their optimization is carried out separately for each class: B (links (u, v)[cooc(u, v), A +) scorec (u, vlZ = class(u, v)) = log B(links(u, v)[cooc(u, v), A z)"" (37) Section 6.1.1 describes the link classes used in the experiments below.']",5,"['Just as easily , we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX #AUTHOR_TAG ) .']"
CCT115,J00-2004,Models of Translational Equivalence among Words,but dictionaries are data too,"['Peter F Brown', 'Stephen A Della Pietra', 'Vincent J Della Pietra', 'Meredith J Goldsmith', 'Jan Hajic', 'Robert L Mercer', 'Surya Mohanty']",,"Although empiricist approaches tomachine translation depend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic model from this combined ata and we show how these param-eters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between ratio","Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( #AUTHOR_TAGb ) .","[""Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( #AUTHOR_TAGb ) ."", 'Objective and more accurate tests can be carried out using a ""gold standard.""', 'I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English.', 'This bitext was selected to facilitate widespread use and standardization (see Melamed [1998c] for details).', 'The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool.', 'The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.', 'The annotation was replicated five times by seven different annotators.']",1,"[""Until now , translation models have been evaluated either subjectively ( e.g. White and O'Connell 1993 ) or using relative metrics , such as perplexity with respect to other models ( #AUTHOR_TAGb ) ."", 'I hired bilingual annotators to link roughly 16,000 corresponding words between on-line versions of the Bible in French and English.', 'The entire Bible bitext comprised 29,614 verse pairs, of which 250 verse pairs were hand-linked using a specially developed annotation tool.', 'The annotation style guide (Melamed 1998b) was based on the intuitions of the annotators, so it was not biased towards any particular translation model.', 'The annotation was replicated five times by seven different annotators.']"
CCT116,J00-2004,Models of Translational Equivalence among Words,accurate methods for the statistics of surprise and coincidence,['Ted Dunning'],,"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.","In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by #AUTHOR_TAG slightly outperforms 02 .","['In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by #AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows:']",0,"['In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by #AUTHOR_TAG slightly outperforms 02 .', 'Let the cells of the contingency table be named as follows:']"
CCT117,J00-2004,Models of Translational Equivalence among Words,building parallel ltag for french and italian,['Marie-Helene Candito'],method,"In this paper we view Lexicalized Tree Adjoining Grammars as the compilation of a more abstract and modular layer of linguistic description: the metagrammar (MG). MG provides a hierarchical representation of lexico-syntactic descriptions and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions. This makes it possible for a tool to compile an instance of MG into an LTAG, automatically performing the relevant combinations of linguistic phenomena. We then describe the instantiation of an MG for Italian and French. The work for French was performed starting with an existing LTAG, which has been augmented as a result. The work for Italian was performed by systematic contrast with the French MG. The automatic compilation gives two parallel LTAG, compatible for multilingual NLP applications.","There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .","['The above equation holds regardless of how we represent concepts.', 'There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .', 'Of course, for a representation to be used, a method must exist for estimating its distribution in data.']",0,"['There are many plausible representations , such as pairs of trees from synchronous tree adjoining grammars ( Abeille et al. 1990 ; Shieber 1994 ; #AUTHOR_TAG ) , lexical conceptual structures ( Dorr 1992 ) and WordNet synsets ( Fellbaum 1998 ; Vossen 1998 ) .']"
CCT118,J00-2004,Models of Translational Equivalence among Words,should we translate the documents or the queries in crosslanguage information retrieval,['J Scott McCarley'],method,"Previous comparisons of document and query translation suffered difficulty due to differing quality of machine translation in these two opposite directions. We avoid this difficulty by training identical statistical translation models for both translation directions using the same training data. We investigate information retrieval between English and French, incorporating both translations directions into both document translation and query translation-based information retrieval, as well as into hybrid systems. We find that hybrids of document and query translation-based systems out-perform query translation systems, even human-quality query translation systems.","â¢ cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , â¢ multilingual document filtering ( e.g. , Oard 1997 ) , â¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,","['â\x80¢ cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']",0,"['â\x80¢ cross-language information retrieval ( e.g. , #AUTHOR_TAG ) , â\x80¢ multilingual document filtering ( e.g. , Oard 1997 ) , â\x80¢ computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , â\x80¢ certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , â\x80¢ concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,']"
CCT119,J00-2005,Pipelines and Size Constraints,the linguistic basis of text generation,['Laurence Danlos'],introduction,"This study presents an original and penetrating analysis of the complex problems surrounding the automatic generation of natural language text. Laurence Danlos provides a valuable critical review of current research in this important and increasingly active field, and goes on to describe a new theoretical model that is thoroughly grounded in linguistic principles.The model emphasizes the semantic, syntactic and lexical constraints that must be dealt with when establishing a relationship between meaning and form, and it is consideration of such linguistic constraints that determines Danlos' generation algorithm. The book concludes with a description of a generation system based on this algorithm which produces texts in several domains and also a system for the synthesis of spoken messages from semantic representations.The book is a significant addition to the literature on text generation, and will be of particular interest to all computational linguists and AI researchers who have wrestled with the problem of vocabulary selection.",Many other such cases are described in Danlos 's book ( #AUTHOR_TAG ) .,"[""Many other such cases are described in Danlos 's book ( #AUTHOR_TAG ) ."", 'The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules.', 'This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module.']",0,"[""Many other such cases are described in Danlos 's book ( #AUTHOR_TAG ) .""]"
CCT120,J00-2005,Pipelines and Size Constraints,describing complex charts in natural language a caption generation system,"['Vibhu Mittal', 'Johanna Moore', 'Guiseppe Carenini', 'Steven Roth']",introduction,"Graphical presentations can be used to communicate information in relational data sets succinctly and effectively. However, novel graphical presentations that represent many attributes and relationships are often difficult to understand completely until explained. Automatically generated graphical presentations must therefore either be limited to generating simple, conventionalized graphical presentations, or risk incomprehensibility. A possible solution to this problem would be to extend automatic graphical presentation systems to generate explanatory captions in natural language, to enable users to understand the information expressed in the graphic. This paper presents a system to do so. It uses a text planner to determine the content and structure of the captions based on: (1) a representation of the structure of the graphical presentation and its mapping to the data it depicts, (2) a framework for identifying the perceptual complexity of graphical elements, and (3) the structure of the data expressed in the graphic. The output of the planner is further processed regarding issues such as ordering, aggregation, centering, generating referring expressions, and lexical choice. We discuss the architecture of our system and its strengths and limitations. Our implementation is currently limited to 2-D charts and maps, but, except for lexical information, it is completely domain independent. We illustrate our discussion with figures and generated captions about housing sales in Pittsburgh.","This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #AUTHOR_TAG ) .","['Despite these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998).', 'This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #AUTHOR_TAG ) .']",0,"['This may be because pipelines have many engineering advantages , and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems ( #AUTHOR_TAG ) .']"
CCT121,J00-2005,Pipelines and Size Constraints,has a consensus nl generation architecture appeared and is it psycholinguistically plausible,['Ehud Reiter'],introduction,"I survey some recent applications-oriented  NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations  these modules perform, and the way the modules interact with each other. I also  compare this &apos;consensus architecture&apos; among  applied NLG systems with psycholinguistic  knowledge about how humans speak, and argue  that at least some aspects of the consensns  architecture seem to be in agreement  with what is known about human language production, despite the fact that psycholinguistic  plausibility was not in general a goal  of the developers of the surveyed systems","Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ) .","['Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ) .', 'This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mittal et al. 1998).']",0,"['Despite these arguments , most applied NLG systems use a pipelined architecture ; indeed , a pipeline was used in every one of the systems surveyed by #AUTHOR_TAG and Paiva ( 1998 ) .']"
CCT122,J00-2009,Book Reviews: Lexical Semantics and Knowledge Representation in Multilingual Text Generation,upper modeling organizing knowledge for natural language processing,['John A Bateman'],,"Abstract : A general, reusable computational resource has been developed within the Penman text generation project for organizing domain knowledge appropriately for linguistic realization. This resource, called the upper model, provides a domain- and task-independent classification system' that supports sophisticated natural language processing while significantly simplifying the interface between domain-specific knowledge and general linguistic resources. This paper presents the results of our experiences in designing and using the upper model in a variety of applications over the past 5 years. In particular, we present our conclusions concerning the appropriate organization of an upper model, its domain- independence, and the types of interrelationships that need to be supported between upper model and grammar and semantics.","The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #AUTHOR_TAG ) .","['At first I found Chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms.', 'However, at a second, more-careful, reading, everything falls into place.', ""The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993)."", 'The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #AUTHOR_TAG ) .', 'Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system.']",0,"['The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG , the Upper Model ( #AUTHOR_TAG ) .']"
CCT123,J00-2013,"Extended Finite State Models of Language András Kornai (editor) (BBN Technologies) Cambridge University Press (Studies in natural language processing), 1999, xii+278 pp and CD-ROM; hardbound, ISBN 0-521-63198-X, $59.95",regular models of phonological rule systems,"['Ronald M Kaplan', 'Martin Kay']",,This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology. It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism. This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.,"Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #AUTHOR_TAG . ''","[""Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #AUTHOR_TAG . ''"", ""These works inspired Koskenniemi's two-level system, and the Xerox rule compiler (Dalrymple et al. 1987)."", 'Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), ""The Japanese lexical transducer based on stem-suffix style forms"" and Kim and Jang (Chapter 7), ""Acquiring rules for reducing morphological ambiguity from POS tagged corpus in Korean.""', 'The latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part-of-speech tagged corpora.']",0,"[""Shortly after the publication of The Sound Pattern of English ( Chomsky and Halle 1968 ) , Kornai points out , `` Johnson ( 1970 ) demonstrated that the context-sensitive machinery of SPE ... [ could ] be replaced by a much simpler one , based on finite-state transducers ( FSTs ) ; the same conclusion was reached independently by Kaplan and Kay , whose work remained an underground classic until it was finally published in #AUTHOR_TAG . ''"", 'Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), ""The Japanese lexical transducer based on stem-suffix style forms"" and Kim and Jang (Chapter 7), ""Acquiring rules for reducing morphological ambiguity from POS tagged corpus in Korean.""', 'The latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part-of-speech tagged corpora.']"
CCT124,J00-2014,Surface Opacity of Metrical Structure in Optimality Theory,efficient generation in primitive optimality theory,['Jason Eisner'],introduction,"This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, ""factored automata,"" where regular languages are represented compactly via formal intersections ki=1Ai of FSAs.","OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .","['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']",0,"['OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of ""simple constraints"" ( e.g. , #AUTHOR_TAGb ) is of course an empirical question .']"
CCT125,J00-2014,Surface Opacity of Metrical Structure in Optimality Theory,comparing a linguistic and a stochastic tagger,"['Christer Samuelsson', 'Atro Voutilainen']",,"Concerning different approaches to automatic PoS tagging: EngCG-2, a constraint-based morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set. The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed.Comment: 8 pages, LaTeX, 2 postscript figures. E-ACL'9","#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .","['Second, weights are an annoyance when writing grammars by hand.', 'In some cases rankings may work well enough.', '#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .', ""The tension between preserving the original author's text (faithfulness to the underlying form) and making it readable in various ways (well-formedness) is right up OT's alley."", 'The same applies to document layout: I have often wished I could write OT-style TeX macros~ Third, even in statistical corpus-based NLP, estimating a full Gibbs distribution is not always feasible.', 'Even if strict ranking is not quite accurate, sparse data or the complexity of parameter estimation may make it easier to learn a good OT grammar than a good arbitrary Gibbs model.', ""A well-known example is Yarowsky's (1996) work on word sense disambiguation using decision lists (a kind of OT grammar)."", 'Although decision lists are not very powerful because of their simple output space, they have the characteristic OT property that each generalization partially masks lower-ranked generalizations.']",0,"['#AUTHOR_TAG report excellent part-of-speech tagging results using a handcrafted approach that is close to OT .3 More speculatively , imagine an OT grammar for stylistic revision of parsed sentences .']"
CCT126,J00-2014,Surface Opacity of Metrical Structure in Optimality Theory,efficient generation in primitive optimality theory,['Jason Eisner'],,"This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, ""factored automata,"" where regular languages are represented compactly via formal intersections ki=1Ai of FSAs.",But typical OT grammars offer much richer finite-state models of left context ( #AUTHOR_TAGa ) than provided by the traditional HMM finite-state topologies .,"['For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-speech tagging.', ""Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from ~', based on criteria of well-formedness (transition probabilities) and faithfulness to the input (emission probabilities)."", 'But typical OT grammars offer much richer finite-state models of left context ( #AUTHOR_TAGa ) than provided by the traditional HMM finite-state topologies .', 'Now, among methods that use a Gibbs distribution to choose among linguistic forms, OT generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'When is this appropriate?', 'It seems to me that there are three possible uses.']",0,"['For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-speech tagging.', ""Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from ~', based on criteria of well-formedness (transition probabilities) and faithfulness to the input (emission probabilities)."", 'But typical OT grammars offer much richer finite-state models of left context ( #AUTHOR_TAGa ) than provided by the traditional HMM finite-state topologies .', 'Now, among methods that use a Gibbs distribution to choose among linguistic forms, OT generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs.', 'When is this appropriate?', 'It seems to me that there are three possible uses.']"
CCT127,J00-3001,Extracting the Lowest-Frequency Words: Pitfalls and Possibilities,word association norms mutual information and lexicography,"['Kenneth W Church', 'Patrick Hanks']",introduction,"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words","#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .","['Our original question concerned the extent to which recall and precision are influenced by the size of the window.', 'It turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest-frequency words on recall, precision, and the number of words extracted as potentially relevant terms.', 'It is common practice in information retrieval to discard the lowest-frequency words a priori as nonsignificant (Rijsbergen 1979).', ""In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993)."", '#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .']",0,"['#AUTHOR_TAG use mutual information to identify collocations , a method they claim is reasonably effective for words with a frequency of not less than five .']"
CCT128,J00-3001,Extracting the Lowest-Frequency Words: Pitfalls and Possibilities,word association norms mutual information and lexicography,"['Kenneth W Church', 'Patrick Hanks']",conclusion,"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words","While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :","[""Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161."", 'The extraction results for both tests as measured by F are 0.31 and 0.33, respectively.', 'This procedure allows us to extract 64/139 = 46.0% of the lowfrequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 64/139 = 46.0%', 'and 79/170 = 46.7%,', ""respectively, using Fisher's exact test."", ""Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G 2 and Fisher's exact test for these words."", ""For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests)."", ""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]",0,"[""While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( #AUTHOR_TAG ) :""]"
CCT129,J00-3001,Extracting the Lowest-Frequency Words: Pitfalls and Possibilities,word association norms mutual information and lexicography,"['Kenneth W Church', 'Patrick Hanks']",,"The term word association is used in a very particular sense in the p!ycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word &quot;nurse&quot; if it follows a highly associated word such as &quot;doctor.&quot;) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic rehtions of the doctor/nurse type (content word/content word) to lexico-syntactlc co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, makin it possible to estimate norms for tens of thousands of words","Given the definition of Mutual Information ( #AUTOR_TAG 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4.","['Given the definition of Mutual Information ( #AUTOR_TAG 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4.', 'P(x) is the relative frequency of the target word, P(y) is the relative frequency of the seed term, and P(x, y) is the frequency of the target word in the window.', 'In terms of the contingency table, we have: nu n++ I(x, y) = log2 n1+ s where S n++ n++ —u, we find that is the frequency of the seed.', 'Substituting nn = nl+ - n12, we find that /11+ -- F/12 I(x,y) = log 2 n++ nl+ S \' //++ //++ 1 = log 2 n++ //1+ S \' n++(nl+ -- nu) "" n++ = log2(n++) - log2(S) - log2(nl+) + log2(nl+ - n12).']",5,"['Given the definition of Mutual Information ( #AUTOR_TAG 1990), P(x, y) I(x, y) = log2 p(x)p(y), we consider the distribution of a window word according to the contingency table (a) in Table 4.']"
CCT130,J00-3006,Language Communication,chinese numbernames tree adjoining languages and mild contextsensitivity,['Daniel Radzinsky'],,,"For example , #AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .","['Second, the complexity of a grammar class is measured by the worst case: a grammar class has a complexity x if there exists some grammar in this class such that there exists an infinite series of long-enough sentences that parse in time x by this grammar.', 'However, what matters in engineering practice is the average case for a specific grammar.', 'Specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time-consuming behavior of the algorithm never happens for this grammar.', 'Average, since it can happen that the grammar does admit hard-toparse sentences that are not used (or at least not frequently used) in the real corpus.', 'For example , #AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .', 'Do such arguments--no doubt important for mathematical linguistics--have any direct consequences for an engineering linguistics?', 'Even if a Chinese grammar includes a non-context-flee rule for parsing such numerals, how frequently will it be activated?', 'Does it imply impossibility of processing real Chinese texts in reasonable time?', 'Clearly, the average time for a specific grammar cannot be calculated in such a mathematically elegant way as the worst-case complexity of a grammar class; for the time being, the only practical way to compare the complexity of natural language processing formalisms is the hard one--building real-world systems and comparing their efficiency and coverage.']",0,"['For example , #AUTHOR_TAG proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao , for the number 5000000000000000005000000000000005000000000005000000005000 , are not context-free , which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time .', 'Even if a Chinese grammar includes a non-context-flee rule for parsing such numerals, how frequently will it be activated?']"
CCT131,J00-4001,Automatic Text Categorization in Terms of Genre and Author,robust text processing in automated information retrieval,['Tornek Strzalkowski'],method,"This paper outlines a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval. The backbone of our system is a traditional retrieval engine which builds inverted index files from pre-processed docu- ments, and then searches and ranks the documents in response to user queries. Natural language processing is used to (1) preprocess the documents in order to extract contents-carrying terms, (2) discover interterm dependencies and build a conceptual hierarchy specific to the database domain, and (3) process user&apos;s natural language requests into effective search queries. The basic assumption of this design is that term-based representation of contents is in principle sufficient to build an effective if not optimal search query out of any users request. This has been confirmed by an experiment that compared effectiveness of expert-user prepared queries with those derived automatically from an initial narrative information request. In this paper we show that largescale natural language processing (hundreds of millions of words and more) is not only required for a better retrieval, but it is also doable, given appropriate resources. We report on selected preliminary restfits of experiments with 500 MByte database of Wall Street Journal articles, as well as some earlier restfits with a smaller document collection","For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .","['The particular analysis-level style markers can be calculated only when this specific computational tool is utilized.', 'However, the SCBD is a general-purpose tool and was not designed for providing stylistic information exclusively.', 'Thus, any NLP tool (e.g., part-of-speech taggers, parsers, etc.) can provide similar measures.', 'The appropriate analysis-level style markers have to be defined according to the methodology used by the tool in order to analyze the text.', 'For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .', 'This parser produces trees to represent the structure of the sentences that compose the text.', 'However, it is set to ""skip"" or surrender attempts to parse clauses after reaching a time-out threshold.', 'When the parser skips, it notes that in the parse tree.', 'The measures proposed by Karlgren (1999) as indicators of clausal complexity are the average parse tree depth and the number of parser skips per sentence, which in essence are analysis-level style markers.']",0,"['For example , some similar measures have been used in stylistic experiments in information retrieval on the basis of a robust parser built for information retrieval purposes ( #AUTHOR_TAG ) .']"
CCT132,J00-4001,Automatic Text Categorization in Terms of Genre and Author,using registerdiversified corpora for general language studies,['Douglas Biber'],,"The present study summarizes corpus-based research on linguistic characteristics from several different structural levels, in English as well as other languages, showing that register variation is inherent in natural language. It further argues that, due to the importance and systematicity of the linguistic differences among registers, diversified corpora representing a broad range of register variation are required as the basis for general language studies.First, the extent of cross-register differences are illustrated from consideration of individual grammatical and lexical features; these register differences are also important for probabilistic part-of-speech taggers and syntactic parsers, because the probabilities associated with grammatically ambiguous forms are often markedly different across registers. Then, corpus-based multidimensional analyses of English are summarized, showing that linguistic features from several structural levels function together as underlying dimensions of variation, with each dimension defining a different set of linguistic relations among registers. Finally, the paper discusses how such analyses, based on register-diversified corpora, can be used to address two current issues in computational linguistics: the automatic classification of texts into register categories and cross-linguistic comparisons of register variation.",Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .,"['where Cx is the covariance matrix of x.', 'Using this classification method we can also derive the probability that a case belongs to a particular group (i.e., posterior probabilities), which is roughly proportional to the Mahalanobis distance from that group centroid.', 'Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']",0,['Discriminant analysis has been employed by researchers in automatic text genre detection ( #AUTHOR_TAGb ; Karlgren and Cutting 1994 ) since it offers a simple and robust solution despite the fact that it presupposes normal distributions of the discriminating variables .']
CCT133,J06-2003,Building and Using a Lexical Knowledge Base of Near-Synonym Differences,unsupervised word sense disambiguation rivaling supervised methods,['David Yarowsky'],,"This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%",The algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation .,"['The algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation .', 'He classified the senses of a word on the basis of other words that the given word co-occurs with.', 'Collins and Singer (1999)']",4,['The algorithm we implemented is inspired by the work of #AUTHOR_TAG on word sense disambiguation .']
CCT134,J06-3002,The Notion of Argument in Prepositional Phrase Attachment,corpus based pp attachment ambiguity resolution with a semantic dictionary,"['Jiri Stetina', 'Makoto Nagao']",method,"This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PPattachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.","The changes made were inspired by those described in #AUTHOR_TAG , page 75 ) .","['The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in #AUTHOR_TAG , page 75 ) .', 'To lemmatize the words we used �morpha,� a lemmatizer developed by John A. Carroll and freely available at the address: http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html.', 'Upon simple observation, it showed a better performance than the frequently used Porter Stemmer for this task.']",4,"['The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files.', 'Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet.', 'The changes made were inspired by those described in #AUTHOR_TAG , page 75 ) .', 'To lemmatize the words we used morpha, a lemmatizer developed by John A. Carroll and freely available at the address: http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html.', 'Upon simple observation, it showed a better performance than the frequently used Porter Stemmer for this task.']"
CCT135,J11-1005,Syntactic Processing Using the Generalized Perceptron and Beam Search,joint word segmentation and pos tagging using a single perceptron,"['Yue Zhang', 'Stephen Clark']",introduction,"For Chinese POS tagging, word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing POS information, segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard. Recent research has explored the integration of segmentation and POS tagging, by decoding under restricted versions of the full combined search space. In this paper, we propose a joint segmentation and POS tagging model that does not impose any hard constraints on the interaction between word and POS information. Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach.","For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAGa ) , while being more than an order of magnitude faster .","['In Section 2 we describe our general framework of the generic beam-search algorithm and the generalized perceptron.', 'Then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, presented in our single coherent framework.', 'We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework.', 'For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy.', 'For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAGa ) , while being more than an order of magnitude faster .']",1,"['For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( #AUTHOR_TAGa ) , while being more than an order of magnitude faster .']"
CCT136,J12-4003,Semantic Role Labeling of Implicit Arguments for Nominal Predicates,beyond nombank a study of implicit arguments for nominal predicates,"['Matthew Gerber', 'Joyce Chai']",conclusion,"Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.","Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .","['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .', 'In each test, the discriminative model was retrained and reevaluated without a particular group of features.', 'We summarize the findings of this study in this section.']",2,"['Previously ( #AUTHOR_TAG ) , we assessed the importance of various implicit argument feature groups by conducting feature ablation tests .']"
CCT137,J12-4003,Semantic Role Labeling of Implicit Arguments for Nominal Predicates,beyond nombank a study of implicit arguments for nominal predicates,"['Matthew Gerber', 'Joyce Chai']",experiments,"Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.","This evaluation set-up is an improvement versus the one we previously reported ( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing .","['In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'This evaluation set-up is an improvement versus the one we previously reported ( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing .']",2,"['In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances.', 'Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold.', 'This evaluation set-up is an improvement versus the one we previously reported ( #AUTHOR_TAG ) , in which fixed partitions were used for training , development , and testing .']"
CCT138,J15-3005,Discriminative Syntax-Based Word Ordering for Text Generation,ccgbank a corpus of ccg derivations and dependency structures extracted from the penn treebank,"['Julia Hockenmaier', 'Mark Steedman']",introduction,"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word-word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train wide-coverage statistical parsers that obtain state-of-the-art rates of dependency recovery. In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks.",CCGBank ( #AUTHOR_TAG ) is used to train the model .,"['We now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder.', 'CCGBank ( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.', 'All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses.', 'From the generation perspective this assumption is too strong, because sentences can have multiple orderings (with multiple derivations) that are both gram- matical and fluent.', 'Nevertheless, it is the most feasible choice given the training data available.']",5,"['CCGBank ( #AUTHOR_TAG ) is used to train the model .', 'For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses.']"
CCT139,J15-3005,Discriminative Syntax-Based Word Ordering for Text Generation,syntactic processing using the generalized perceptron and beam search,"['Yue Zhang', 'Stephen Clark']",introduction,"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model, trained by the generalized perceptron together with a generic beam-search decoder. We apply the framework to word segmentation, joint segmentation and POS-tagging, dependency parsing, and phrase-structure parsing. Both components of the framework are conceptually and computationally very simple. The beam-search decoder only requires the syntactic processing task to be broken into a sequence of decisions, such that, at each stage in the process, the decoder is able to consider the top-n candidates and generate all possibilities for the next stage. Once the decoder has been defined, it is applied to the training data, using trivial updates according to the generalized perceptron to induce a model. This simple framework performs surprisingly well, giving accuracy results competitive with the state-of-the-art on all the tasks we consider. The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives, including log-linear and large-margin training algorithms and dynamic-programming for decoding. Moreover, the framework offers the freedom to define arbitrary features which can make alternative training and decoding algorithms prohibitively slow. We discuss how the general framework is applied to each of the problems studied in this article, making comparisons with alternative learning and decoding algorithms. We also show how the comparability of candidates considered by the beam is an important factor in the performance. We argue that the conceptual and computational simplicity of the framework, together with its language-independent nature, make it a competitive choice for a range of syntactic processing tasks and one that should be considered for comparison by developers of alternative approaches.","In our previous papers ( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .","['In our formulation of the word ordering problem, a hypothesis is a phrase or sentence together with its CCG derivation.', 'Hypotheses are constructed bottom-up: starting from single words, smaller phrases are combined into larger ones according to CCG rules.', 'To allow the combination of hypotheses, we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted hypotheses.', 'When a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses.', 'The data structure for accepted hypotheses is similar to that used for best-first parsing (Caraballo and Charniak 1998), and we adopt the term chart for this structure.', 'However, note there are important differences to the parsing problem.', 'First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'In our previous papers ( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .', 'However, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used.', 'We will also investigate the possibility of applying dynamic-programming-style pruning to the chart.']",1,"['Hypotheses are constructed bottom-up: starting from single words, smaller phrases are combined into larger ones according to CCG rules.', 'The data structure for accepted hypotheses is similar to that used for best-first parsing (Caraballo and Charniak 1998), and we adopt the term chart for this structure.', 'However, note there are important differences to the parsing problem.', 'First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling.', 'Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem.', 'In our previous papers ( #AUTHOR_TAG ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .']"
CCT140,K15-1001,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,minimum error rate training in statistical machine translation,['Franz Josef Och'],experiments,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature func-tion weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature func-tion its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and repre-senting the exact error surface of all trans-lations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experi-ments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT.",This was done by MERT optimization ( #AUTHOR_TAG ) towards post-edits under the TER target metric .,"['this experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'We select feedback of varying grade by directly inspecting the optimal w * , thus this feedback is idealized.', 'However, the experiment also has a realistic background since we show that α-informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized TER, and that learning from weak and strong feedback leads to convergence in TER on test data.', 'For this experiment, the post-edit data from the LIG corpus were randomly split into 3 subsets: PE-train (6,881 sentences), PE-dev, and PE-test (2,000 sentences each).', 'PE-train was used for our online learning experiments.', ""PE-test was held out for testing the algorithms' progress on unseen data."", 'PE-dev was used to obtain w * to define the utility model.', 'This was done by MERT optimization ( #AUTHOR_TAG ) towards post-edits under the TER target metric .', 'Note that the goal of our experi-  ments is not to improve SMT performance over any algorithm that has access to full information to compute w * .', 'Rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'The feedback data in this experiment were generated by searching the n-best list for translations that are α-informative at α ∈ {0.1, 0.5, 1.0} (with possible non-zero slack).', 'This is achieved by scanning the n-best list output for every input x t and returning the firstȳ t = y t that satisfies Equation (2). 5 This setting can be thought of as an idealized scenario where a user picks translations from the n-best list that are considered improvements under the optimal w * .']",5,"['this experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback.', 'We select feedback of varying grade by directly inspecting the optimal w * , thus this feedback is idealized.', 'However, the experiment also has a realistic background since we show that a-informative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized TER, and that learning from weak and strong feedback leads to convergence in TER on test data.', 'For this experiment, the post-edit data from the LIG corpus were randomly split into 3 subsets: PE-train (6,881 sentences), PE-dev, and PE-test (2,000 sentences each).', 'PE-train was used for our online learning experiments.', ""PE-test was held out for testing the algorithms' progress on unseen data."", 'PE-dev was used to obtain w * to define the utility model.', 'This was done by MERT optimization ( #AUTHOR_TAG ) towards post-edits under the TER target metric .', 'Note that the goal of our experi-  ments is not to improve SMT performance over any algorithm that has access to full information to compute w * .', 'Rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback.', 'The feedback data in this experiment were generated by searching the n-best list for translations that are a-informative at a  {0.1, 0.5, 1.0} (with possible non-zero slack).', 'This is achieved by scanning the n-best list output for every input x t and returning the firsty t = y t that satisfies Equation (2). 5 This setting can be thought of as an idealized scenario where a user picks translations from the n-best list that are considered improvements under the optimal w * .']"
CCT141,K15-1001,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,discriminative training methods for hidden markov models theory and experiments with perceptron algorithms,['Michael Collins'],,"We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random elds (CRFs). The al-gorithms rely on Viterbi decoding of training examples, combined with sim-ple additive updates. We describe the-ory justifying the algorithms through a modication of the proof of conver-gence of the perceptron algorithm for classi cation problems. We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.","In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .","['Generalization for Online-to-Batch Conversion.', 'In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .', 'The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on unseen data we would like to bound.', 'We assume that the algorithm is fed with a sequence of examples x 1 , . . .', ', x T , and at each epoch k = 1, . . .', ', K it makes a prediction y t,k .', 'The correct label is y * t .', 'For k = 1, . . .', ', K and t = 1, . . .', ', T , let t,k = U (x t , y * t ) − U (x t , y t,k ), and denote by ∆ t,k and ξ t,k the distance at epoch k for example t, and the slack at epoch k for example t, respectively.', 'Finally, we denote by D T,K = T t=1 ∆ 2 t,K , and by w T,K the final weight vector returned after K epochs.', 'We state a condition of convergence :']",1,"['In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; #AUTHOR_TAG ) .']"
CCT142,K15-1001,A Coactive Learning View of Online Structured Prediction in Statistical Machine Translation,moses open source toolkit for statistical machine translation,"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Chris Callison-Birch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen']",experiments,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.","To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( #AUTHOR_TAG ) with dense features .","['We used the LIG corpus 3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012).', 'The corpus is a subset of the newscommentary dataset provided at WMT 4 and contains input French sentences, MT outputs, postedited outputs and English references.', 'To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( #AUTHOR_TAG ) with dense features .', 'We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en', 'data (48.65M', 'sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010).', 'perceptron cycling theorem (Block and Levin, 1970;Gelfand et al., 2010) should suffice to show a similar bound.', 'Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013).', 'In all experiments, training is started with the Moses default weights.', 'The size of the n-best list, where used, was set to 1,000.', 'Irrespective of the use of re-scaling in perceptron training, a constant learning rate of −5 was used for learning from simulated feedback, and 10 −4 for learning from surrogate translations.']",5,"['The corpus is a subset of the newscommentary dataset provided at WMT 4 and contains input French sentences, MT outputs, postedited outputs and English references.', 'To prepare SMT outputs for post-editing , the creators of the corpus used their own WMT10 system ( Potet et al. , 2010 ) , based on the Moses phrase-based decoder ( #AUTHOR_TAG ) with dense features .', 'We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en', 'sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010).', 'Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013).']"
CCT143,N01-1001,Instance-Based Natural Language Generation,forestbased statistical sentence generation,['Irene Langkilde'],,"This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent syntactic information. It also facilitates more efficient statistical ranking than a previous approach to statistical generation. An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach.","In contrast , a single statistical model allows one to maintain a single table ( #AUTHOR_TAG ) .","['In contrast , a single statistical model allows one to maintain a single table ( #AUTHOR_TAG ) .']",0,"['In contrast , a single statistical model allows one to maintain a single table ( #AUTHOR_TAG ) .']"
CCT144,N01-1001,Instance-Based Natural Language Generation,chart generation,['Martin Kay'],experiments,"This paper presents a compilation procedure which determines internal and external indices for signs in a unification based grammar to be used in improving the computational efficiency of lexicalist chart generation. The procedure takes as input a grammar and a set of feature paths indicating the position of semantic indices in a sign, and calculates the fixed-point of a set of equations derived from the grammar. The result is a set of independent constraints stating which indices in a sign can be bound to other signs within a complete sentence. Based on these constraints, two tests are formulated which reduce the search space during generation.Comment: 8 pages, Latex; to appear in 7th International Conference on   Theoretical and Methodological Issues in Machine Translation (TMI-97",IGEN uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates .,['IGEN uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates .'],0,['IGEN uses standard chart generation techniques ( #AUTHOR_TAG ) in its base generator to efficiently produce generation candidates .']
CCT145,N01-1002,Corpus-based NP Modifier Generation,can nominal expressions achieve multiple goals an empirical study,['P Jordan'],,"While previous work suggests that multiple goals can be addressed by a nominal expression, there is no systematic work describing what goals in addition to identification might be relevant and how speakers can use nominal expressions to achieve them. In this paper, we first hypothesize a number of communicative goals that could be addressed by nominal expressions in task-oriented dialogues. We then describe the intentional influences model for nominal expression generation that attempts to simultaneously address the identification goal and these additional goals with a single nominal expression. Our evaluation results show that the intentional influences model fits the nominal expressions in the COCONUT corpus as well as previous accounts that focus solely on the identification goal.","Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #AUTHOR_TAG ) .","[""Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #AUTHOR_TAG ) .""]",0,"[""Notice that it is not possible to use corpus annotation to determine the likelihood of a given property to be chosen , unless we know in advance all of the properties that can be attributed to a given object , as in the case of Jordan 's work on the COCONUT domain ( #AUTHOR_TAG ) .""]"
CCT146,N01-1002,Corpus-based NP Modifier Generation,capturing the interaction between aggregation and text planning in two generation systems,"['H Cheng', 'C Mellish']",introduction,"In natural language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text.","It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAGa ) .","['It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAGa ) .']",0,"['It has been argued that generating such modifiers is not a trivial decision because it interferes with the planning of both local and global coherence ( in the sense of ( Grosz and Sidner , 1986 ) ) ( #AUTHOR_TAGa ) .']"
CCT147,N01-1002,Corpus-based NP Modifier Generation,capturing the interaction between aggregation and text planning in two generation systems,"['H Cheng', 'C Mellish']",,"In natural language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text.","Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #AUTHOR_TAGb ) .","['Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #AUTHOR_TAGb ) .']",0,"['Using WordNet , annotating the sem feature of an adjective involves first choosing the correct sense for the adjective 2Some descriptions of int modifiers can be found in ( #AUTHOR_TAGb ) .']"
CCT148,N01-1002,Corpus-based NP Modifier Generation,can nominal expressions achieve multiple goals an empirical study,['P Jordan'],introduction,"While previous work suggests that multiple goals can be addressed by a nominal expression, there is no systematic work describing what goals in addition to identification might be relevant and how speakers can use nominal expressions to achieve them. In this paper, we first hypothesize a number of communicative goals that could be addressed by nominal expressions in task-oriented dialogues. We then describe the intentional influences model for nominal expression generation that attempts to simultaneously address the identification goal and these additional goals with a single nominal expression. Our evaluation results show that the intentional influences model fits the nominal expressions in the COCONUT corpus as well as previous accounts that focus solely on the identification goal.","In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG ) .","[""In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG ) .""]",0,"[""In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; #AUTHOR_TAG ) .""]"
CCT149,N01-1009,A corpus-based account of regular polysemy,the generative lexicon,['James Pustejovsky'],method,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.",Table 1 gives the interpretations of eight adjective-noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ) .,"['In what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v 2 is the second most likely interpretation, etc.).']",5,"['In what follows we explain the properties of the model by applying it to a small number of adjective-noun combinations taken from the lexical semantics literature.', 'Table 1 gives the interpretations of eight adjective-noun combinations discussed in #AUTHOR_TAG and Vendler ( 1968 ) .', 'Table 2 shows the five most likely interpretations for these combinations as derived by the model discussed in the previous sections (v is the most likely interpretation, v 2 is the second most likely interpretation, etc.).']"
CCT150,N01-1009,A corpus-based account of regular polysemy,the generative lexicon,['James Pustejovsky'],introduction,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #AUTHOR_TAG and the references therein ) .","['Much recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #AUTHOR_TAG and the references therein ) .']",0,"['Much recent work in lexical semantics has been concerned with accounting for regular polysemy, i.e., the regular and predictable sense alternations certain classes of words are subject to.', 'Adjectives , more than other categories , are a striking example of regular polysemy since they are able to take on different meanings depending on their context , viz. , the noun or noun class they modify ( see #AUTHOR_TAG and the references therein ) .']"
CCT151,N01-1009,A corpus-based account of regular polysemy,the generative lexicon,['James Pustejovsky'],experiments,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ) .","['We chose nine adjectives according to a set of minimal criteria and paired each adjective with 10 nouns randomly selected from the BNC.', 'We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ) .', 'From these we randomly sampled nine adjectives (difficult, easy, fast, good, hard, right, safe, slow, wrong).', 'These adjectives had to be unambiguous with respect to their part-of-speech: each adjective was unambiguously tagged as ""adjective"" 98.6% of the time, measured as the number of different part-of-speech tags assigned to the word in the BNC.', 'We identified adjective-noun pairs using Gsearch (Corley et al., 2000), a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query.', 'Gsearch was run on a lemmatized version of the BNC so as to compile a comprehensive corpus count of all nouns occurring in a modifier-head relationship with each of the nine adjectives.', 'From the syntactic analysis provided by  We used the model outlined in Section 2 to derive meanings for the 90 adjective-noun combinations.', 'We employed no threshold on the frequencies f (a, v) and f (rel, v, n).', 'In order to obtain the frequency f (a, v) the adjective was mapped to its corresponding adverb.', 'In particular, good was mapped to good and well, fast to fast, easy to easily, hard to hard, right to rightly and right, safe to safely and safe, slow to slowly and slow and wrong to wrongly and wrong.', 'The adverbial function of the adjective difficult is expressed only periphrastically (i.e., in a difficult manner, with difficulty).', 'As a result, the frequency f (difficult, v) was estimated only on the basis of infinitival constructions (see ( 17)).', 'We estimated the probability P(a, n, v, rel) for each adjective-noun pair by varying both the terms v and rel.']",5,"['We chose the adjectives as follows : we first compiled a list of all the polysemous adjectives mentioned in the lexical semantics literature ( Vendler , 1968 ; #AUTHOR_TAG ) .', 'From these we randomly sampled nine adjectives (difficult, easy, fast, good, hard, right, safe, slow, wrong).', 'Gsearch was run on a lemmatized version of the BNC so as to compile a comprehensive corpus count of all nouns occurring in a modifier-head relationship with each of the nine adjectives.', 'From the syntactic analysis provided by  We used the model outlined in Section 2 to derive meanings for the 90 adjective-noun combinations.', 'We employed no threshold on the frequencies f (a, v) and f (rel, v, n).']"
CCT152,N01-1009,A corpus-based account of regular polysemy,the generative lexicon,['James Pustejovsky'],introduction,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.",#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .,"['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'The meaning of adjective-noun combinations like those in (1) and ( 2) are usually paraphrased with a verb modified by the adjective in question or its corresponding adverb.', 'For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'In order to account for the meaning of these combinations Vendler (1968, 92) points out that ""in most cases not one verb, but a family of verbs is needed"".', 'Vendler further observes that the noun figuring in an adjective-noun combination is usually the subject or object of the paraphrasing verb.', 'Although fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpretations (see (2a,b)).', 'An easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', 'Adjectives like good allow either verb-subject or verb-object interpretations: a good cook is a cook who cooks well whereas good soup is soup that tastes good or soup that is good to eat.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'For example, the telic (purpose) role of the qualia structure for problem has a value equivalent to solve.', 'When the adjective easy is combined with problem, it predicates over the telic role of problem and consequently the adjective-noun combination receives the interpretation a problem that is easy to solve.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'Furthermore, in cases where more than one interpretations are provided (see Vendler (1968)), no information is given with respect to the likelihood of these interpretations.', 'Outof context, the number of interpretations for fast scientist is virtually unlimited, yet some interpretations are more likely than others: fast scientist is more likely to be a scientist who performs experiments quickly or who publishes quickly than a scientist who draws or drinks quickly.']",0,"['(2) a. easy problem b. difficult language c. good cook d. good soup Adjectives like fast have been extensively studied in the lexical semantics literature and their properties have been known at least since Vendler (1968).', 'For example, an easy problem is ""a problem that is easy to solve"" or ""a problem that one can solve easily"".', 'In order to account for the meaning of these combinations Vendler (1968, 92) points out that ""in most cases not one verb, but a family of verbs is needed"".', 'Although fast usually triggers a verb-subject interpretation (see (1)), easy and difficult trigger verb-object interpretations (see (2a,b)).', 'An easy problem is usually a problem that is easy to solve, whereas a difficult language is a language that is difficult to learn, speak, or write.', '#AUTHOR_TAG avoids enumerating the various senses for adjectives like fast by exploiting the semantics of the nouns they modify .', 'Pustejovsky treats nouns as having a qualia structure as part of their lexical entries, which among other things, specifies possible events associated with the entity.', 'Pustejovsky (1995) does not give an exhaustive list of the telic roles a given noun may have.', 'Furthermore, in cases where more than one interpretations are provided (see Vendler (1968)), no information is given with respect to the likelihood of these interpretations.']"
CCT153,N01-1011,A Decision Tree of Bigrams is an Accurate Predictor of Word Sense,a simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation,['T Pedersen'],conclusion,"This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results.","We have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .","['We have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .']",0,"['We have presented an ensemble approach to word sense disambiguation ( #AUTHOR_TAG ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .']"
CCT154,N01-1012,An Algorithm for Aspects of Semantic Interpretation Using an Enhanced WordNet,linking wordnet verb classes to semantic interpretation,['F Gomez'],,An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.,See ( #AUTHOR_TAG ) for a discussion .,['See ( #AUTHOR_TAG ) for a discussion .'],0,['See ( #AUTHOR_TAG ) for a discussion .']
CCT155,N01-1012,An Algorithm for Aspects of Semantic Interpretation Using an Enhanced WordNet,linking wordnet verb classes to semantic interpretation,['F Gomez'],,An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented. The selectional restrictions for the thematic roles defining the predicates are WordNet ontological categories. Thematic roles are also linked to the syntactic relations that realize them. The paper illustrates the methodology by providing a detailed analysis of some major WordNet verb classes.,Other definitions of predicates may be found in ( #AUTHOR_TAG ) .,['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .'],0,['Other definitions of predicates may be found in ( #AUTHOR_TAG ) .']
CCT156,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,generalizing case frames using a thesaurus and the mdl principle,"['H Li', 'N Abe']",,"A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as ""cuts"" in the thesaurus tree, thus reducing the generalization problem to that of estimating a ""tree cut model"" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.","This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .","['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']",1,"['This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in #AUTHOR_TAG .']"
CCT157,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,distributional clustering of english words,"['F Pereira', 'N Tishby', 'L Lee']",experiments,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.",The task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 ) .,['The task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 ) .'],1,['The task we used to compare different generalisation techniques is similar to that used by #AUTHOR_TAG and Rooth et al. ( 1999 ) .']
CCT158,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,prepositional phrase attachment through a backedoff model,"['M Collins', 'J Brooks']",,"Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events -- ignoring events which occur less than 5 times in training data reduces performance to 81.6%.","The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #AUTHOR_TAG ) .","['The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #AUTHOR_TAG ) .']",4,"['The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( #AUTHOR_TAG ) .']"
CCT159,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,accurate methods for the statistics of surprise and coincidence,['T Dunning'],Motivation,"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.","However , #AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .","['However , #AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .']",4,"['However , #AUTHOR_TAG claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .']"
CCT160,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,inducing a semantically annotated lexicon via embased clustering,"['M Rooth', 'S Riezler', 'D Prescher', 'G Carroll', 'F Beil']",experiments,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evaluated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.",The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG .,['The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG .'],1,['The task we used to compare different generalisation techniques is similar to that used by Pereira et al. ( 1993 ) and #AUTHOR_TAG .']
CCT161,N01-1013,Class-Based Probability Estimation Using a Semantic Hierarchy,accurate methods for the statistics of surprise and coincidence,['T Dunning'],experiments,"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.","The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .","['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']",1,"['The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by #AUTHOR_TAG that the G2 statistic is better suited for use in corpus-based NLP .']"
CCT162,N04-2004,A computational framework for non-lexicalist semantics,derivational minimalism,['Edward Stabler'],,"International audienceMinimalist grammars (MGs) constitute a mildly context-sensitive formalism when being equipped with a particular locality condition (LC), the shortest move condition. In this format MGs define the same class of derivable string languages as multiple context-free grammars (MCFGs). Adding another LC to MGs, the specifier island condition (SPIC), results in a proper subclass of derivable languages. It is rather straightforward to see this class is embedded within the class of languages derivable by some well-nested MCFG (MCFG wn ). In this paper we show that the embedding is even proper. We partially do so adapting the methods used in [13] to characterize the separation of MCFG wn -languages from MCFG-languages by means of a ""simple copying"" theorem. The separation of strict derivational minimalism from well-nested MCFGs is then characterized by means of a ""simple reverse copying"" theorem. Since for MGs, well-nestedness seems to be a rather ad hoc restriction, whereas for MCFGs, this holds regarding the SPIC, our result may suggest we are concerned here with a structural difference between MGs and MCFGs which cannot immediately be overcome in a non-stipulated manner","The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 ) .","['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 ) .""]",1,"[""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( #AUTHOR_TAG ; Harkema , 2000 ; Niyogi , 2001 ) .""]"
CCT163,N04-2004,A computational framework for non-lexicalist semantics,on argument structure and the lexical expression of syntactic relations,"['Kenneth Hale', 'Samuel Jay Keyser']",,,"These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .","['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .', 'They describe a cascading verb phrase analysis with multiple phonetically empty verbal projections corresponding to concepts such as inchoativity and agentivity.', 'This present framework builds on the work of Hale and Keyser, but in addition to advancing a more refined theory of verbal argument structure, I also describe a computational implementation.']",0,"['These observations and this line of reasoning has not escaped the attention of theoretical linguists : #AUTHOR_TAG propose that argument structure is , in fact , encoded syntactically .']"
CCT164,N04-2004,A computational framework for non-lexicalist semantics,the event argument and the semantics of voice,['Angelika Kratzer'],,,"It projects a functional head , voice ( #AUTHOR_TAG ) , whose specifier is the external argument .","['The light verb v DO licenses an atelic non-inchoative event, and is compatible with verbal roots expressing activity.', 'It projects a functional head , voice ( #AUTHOR_TAG ) , whose specifier is the external argument .', 'Lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a λ expression.']",0,"['It projects a functional head , voice ( #AUTHOR_TAG ) , whose specifier is the external argument .']"
CCT165,N04-2004,A computational framework for non-lexicalist semantics,a recognizer for minimalist grammars,['Henk Harkema'],,"Minimalist Grammars are a rigorous formalization of the sort of grammars proposed in the linguistic framework of Chomsky's Minimalist Program. One notable property of Minimalist Grammars is that they allow constituents to move during the derivation of a sentence, thus creating discontinuous constituents. In this paper we will present a bottom-up parsing method for Minimalist Grammars, prove its correctness, and discuss its complexity.","The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 ) .","['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 ) .""]",1,"[""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; #AUTHOR_TAG ; Niyogi , 2001 ) .""]"
CCT166,N04-2004,A computational framework for non-lexicalist semantics,distributed morphology and the pieces of inflection,"['Morris Halle', 'Alec Marantz']",introduction,"The last few years have seen the emergence of several clearly articulated alternative approaches to morphology. One such approach rests on the notion that only stems of the so-called lexical categories (N, V, A) are morpheme &quot;pieces &quot; in the traditional sense--connections between (bun-dles of) meaning (features) and (bundles of) sound (features). What look like affixes on this view are merely the by-product of morphophonological rules called word formation rules (WFRs) that are sensitive to features associated with the lexical categories, called lexemes. Such an a-morphous or affixless theory, adumbrated by Beard (1966) and Aronoff (1976), has been articulated most notably by Anderson (1992) and in major ne","In this paper , I present a computational implementation of Distributed Morphology ( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .","['In this paper , I present a computational implementation of Distributed Morphology ( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .', 'This framework leads to finer-grained semantics capable of better capturing linguistic generalizations.']",5,"['In this paper , I present a computational implementation of Distributed Morphology ( #AUTHOR_TAG ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .']"
CCT167,N04-2004,A computational framework for non-lexicalist semantics,adding semantic annotation to the penn treebank,"['Paul Kingsbury', 'Martha Palmer', 'Mitch Marcus']",introduction,"This paper presents our basic approach to creating Proposition Bank, which involves adding a layer of semantic annotation to the Penn English TreeBank. Without attempting to confirm or disconfirm any particular semantic theory, our goal is to provide consistent argument labeling that will facilitate the automatic extraction of relational data. An argument such asthe window in John broke the window and in The window brokewould receive the same label in both sentences. In order to ensure reliable human annotation, we provide our annotators with explicit guidelines for labeling all of the syntactic and semantic frames of each particular verb. We give several examples of these guidelines and discuss the inter-annotator agreement figures. We also discuss our current experiments on the automatic expansion of our verb guidelines based on verb class membership. Our current rate of progress and our consistency of annotation demonstrate the feasibility of the task.","This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( #AUTHOR_TAG ) .","['A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( #AUTHOR_TAG ) .""]",0,"[""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( Baker et al. , 1998 ) and PropBank ( #AUTHOR_TAG ) .""]"
CCT168,N04-2004,A computational framework for non-lexicalist semantics,event structure and the encoding of arguments the syntax of the english and mandarin verb phrase,['Jimmy Lin'],,"This work presents a theory of linguistic representation that attempts to capture the syntactic structure of verbs and their arguments. My framework is based on the assumption that the proper representation of argument structure is event structure. Furthermore, I develop the hypothesis that event structure is syntactic structure, and argue that verb meanings are compositionally derived in the syntax from verbalizing heads, functional elements that license eventive interpretations, and verbal roots, abstract concepts drawn from encyclopedic knowledge. The overall goal of the enterprise is to develop a theory that is able to transparently relate the structure and meaning of verbal arguments. By hypothesis, languages share the same inventory of primitive building blocks and are governed by the same set of constraints--all endowed by principles of Universal Grammar and subjected to parametric variations. Support for my theory is drawn from both Mandarin Chinese and English. In particular, the organization of the Mandarin verbal system provides strong evidence for the claim that activity and state are the only two primitive verb types in Chinese-- achievements and accomplishments are syntactically-derived complex categories. As a specific instance of complex event composition, I examine Mandarin resultative verb compounds and demonstrate that a broad range of variations can be perspicuously captured in my framework. I show that patterns of argument sharing in these verbal compounds can be analyzed as control, thus grounding argument structure in wellknown syntactic constraints such as the Minimum Distance Principle. Finally, I argue that cross-linguistic differences in the realization of verbal arguments can be reduced to variations in the way functional elements interact with verbal roots. Overall, my work not only contributes to our understanding of how events are syntactically represented, but also explicates interactions at the syntax-semantics interface, clarifying the relationship between surface form, syntactic structure, and logical form. A theory of argument structure grounded in independently-motivated syntactic constraints, on the one hand, and the semantic structure of events, on the other hand, is able to account for a wide range of empirical facts with few stipulations. Thesis Supervisor: Boris Katz Title: Principal Research Scientist","In ( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .","['In ( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework.']",2,"['In ( #AUTHOR_TAG ) , I present evidence from Mandarin Chinese that this analysis is on the right track .', 'The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework.']"
CCT169,N04-2004,A computational framework for non-lexicalist semantics,immediate head parsing for language models,['Eugene Charniak'],introduction,"We present two language models based upon an  immediate-head&quot; parser | our name for a parser that conditions all events below a constituent  c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models signicantly improve upon the trigram model base-line as well as the best previous grammar based language model. For the better of our two models these improvements are 24% and 13% respectively. We also suggest that improvement of the underlying parser should signicantly improve the model&apos;s perplexity and that even in the near term there is a lot of porential for improvement in immediate-head language models.  1 Introduction  All of the most accurate statistical parsers [2,4, 6,7,10,12] are lexicalized in the sense that they condition probabilities on the lexical content of the sentences being parsed. Furthermore, all of these parsers are wh..","Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .","['The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']",0,"['Due to advances in statistical syntactic parsing techniques ( Collins , 1997 ; #AUTHOR_TAG ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']"
CCT170,N04-2004,A computational framework for non-lexicalist semantics,the generative lexicon,['James Pustejovsky'],,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAGb ; Rappaport Hovav and Levin , 1998 ) .","['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAGb ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']",0,"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; #AUTHOR_TAGb ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']"
CCT171,N04-2004,A computational framework for non-lexicalist semantics,on argument structure and the lexical expression of syntactic relations,"['Kenneth Hale', 'Samuel Jay Keyser']",,,"With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis .","['The currently implemented system is still at the ""toy parser"" stage.', 'Although the effectiveness and coverage  of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis .', 'I believe that with a suitable lexicon (either hand crafted or automatically induced), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles.']",0,"['With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by Levin ( 1993 ) using a #AUTHOR_TAG style analysis .']"
CCT172,N04-2004,A computational framework for non-lexicalist semantics,grammaticalizing aspect and affectedness,['Carol Tenny'],,"Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Linguistics and Philosophy, 1987.","#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .","[""Activities know run believe walk Accomplishments Achievements paint a picture recognize make a chair find Under Vendler's classification, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote on-going dynamic situations."", 'Accomplishments and achievements both express a change of state, and hence are temporally bounded (telic); achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .']",0,"[""Activities know run believe walk Accomplishments Achievements paint a picture recognize make a chair find Under Vendler's classification, activities and states both depict situations that are inherently temporally unbounded (atelic); states denote static situations, whereas activities denote on-going dynamic situations."", 'Accomplishments and achievements both express a change of state, and hence are temporally bounded (telic); achievements are punctual, whereas accomplishments extend over a period of time.', '#AUTHOR_TAG observes that accomplishments differ from achievements only in terms of event duration , which is often a question of granularity .']"
CCT173,N04-2004,A computational framework for non-lexicalist semantics,english verb classes and alternations a preliminary investigation,['Beth Levin'],,"In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, ""English Verb Classes and Alternations"" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University.","With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis .","['The currently implemented system is still at the ""toy parser"" stage.', 'Although the effectiveness and coverage  of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena.', 'With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis .', 'I believe that with a suitable lexicon (either hand crafted or automatically induced), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles.']",0,"['With a minimal set of features and a small number of lexical entries , Niyogi ( 2001 ) has successfully modeled many of the argument alternations described by #AUTHOR_TAG using a Hale and Keyser ( 1993 ) style analysis .']"
CCT174,N04-2004,A computational framework for non-lexicalist semantics,the generative lexicon,['James Pustejovsky'],,"In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives. I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.","This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #AUTHOR_TAGa ) , among many others .","['Considering that the only difference between flat.ADJ and flatten.V is the suffix -en, it must be the source of inchoativity and contribute the change of state reading that distinguishes the verb from the adjective.', 'Here, we have evidence that derivational affixes affect the semantic representation of lexical items, that is, fragments of event structure are directly associated with derivational morphemes.', 'We have the following situation: In this case, the complete event structure of a word can be compositionally derived from its component morphemes.', ""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #AUTHOR_TAGa ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical item does not fully encode its associated syntactic and semantic structures.', 'Rather, meanings are composed from component morphemes.']",0,"[""This framework , where the `` semantic load '' is spread more evenly throughout the lexicon to lexical categories not typically thought to bear semantic content , is essentially the model advocated by #AUTHOR_TAGa ) , among many others ."", 'Note that such an approach is no longer lexicalist: each lexical item does not fully encode its associated syntactic and semantic structures.']"
CCT175,N04-2004,A computational framework for non-lexicalist semantics,a minimalist implementation of verb subcategorization,['Sourabh Niyogi'],,"Traditional accounts of verb subcategorization, from the classic work of Fillmore on, require either a considerable number of syntactic rules to account for diverse sentence constructions, including crosslanguage variation, or else complex linking rules mapping the thematic roles of semantic event templates with possible syntactic forms. In this paper we exhibit a third approach: we implement, via an explicit parser and lexicon, the incorporation theory of Hale and Keyser (1993, 1998) to systematically cover most patterns in English Verb Classes and Alternations (Levin 1993), typically using only 1 or 2 lexical entries per verb to subsume a large number of syntactic constructions and also most information typically contained in semantic event templates, and, further, replacing the notion of ""thematic roles"" with precise structural configurations. The implemented parser uses the merge and move operations formalized by Stabler (1997) in the minimalist framework of Chomsky (2001). As a side benefit, we extend the minimalist recognizer of Harkema (2000) to a full parsing implementation. We summarize the current compactness and coverage of our account and provide this minimalist lexicon and parser online at http://web.mit.edu/niyogi/www/minimal.htm 1 The Problem of Verb Subcategorization Why do certain verbs undergo particular certain alternations and not others? On some accounts, e.g. Levin (1993), referred to hereafter as EVCA, alternations provide insight into verb subcategorization and hence hooks to parsing, cross-language variation, machine translation, and class based verb learning. However, fully implemented accounts of the phenomena remains an open problem, with at least three alternative models, shown in Figure 1. Accounts may be solely descriptive - for example, classifying verbs as having an intransitive, a transitive, and/or ditransitive form, as is familiar. Traditional computational accounts (see 1) map these forms into individual grammar rules, (perhaps by macro expansion-like techniques) adding as many rules as necessary to account for naturally' occurring constructions (wh-movement, passive forms, etc.) For each grammatical rule, a separate semantic decomposition is required, typically labeling component phrases with one of several ""thematic roles."" A richer account provided by lexical semantics (see 2), exemplified in Jackendoff (1983, 1990) and Rappaport Hovav and Levin (1998), is one that hypothesizes semantic templates, but requires linking rules mapping syntactic frames with semantic templates governed by a particular verb. Often these semantic templates are constructed in an ad hoc manner, and the corresponding linking rules are consquently a collection of difficult-toimplement heuristics. In this paper we implement a rather different formalism (Hale and Keyser's Incorporation theory, see 3), wherein fewer lexical entries govern syntactic and semantic behavior, with no appeal to thematic roles or complex linking rules. 0. Verb Subcategorization Phenomena * Bob put. Butter was put on the bread. * Bob put butter. What was put on the bread? Bob put butter on the bread. Where was the butter put? 1. Traditional Account VP - V0 NP PPloc V0 - put VP - was VPass VPass - V0 PPloc VP/NP - V0 NP/NP PPloc VP/NP - V0 NP PPloc/NP PPloc - Ploc NP Ploc - on | in | ... PPloc/NP - Ploc NP/NP Exhaustive modelling with a considerable number of grammatical rules. Semantics separate, otherwise unspecified. 2. Lexical Semantics Account   put V NPjPPk CAUSE ( [BOB]i , GO ( [BUTTER]j , TO ( [BREAD]k )))   Syntax handled by numerous argument-fusing ""linking rules"", typically difficult to formalize. Semantic templates mirror alternation patterns, but are ad-hocly constructed. 3. Minimalist/Incorporation Account /put/ =ploc =d vcause (l(=ploc) (l(=d) (=ploc =d))) /on/ =d +k ploc (l(=d) (l(x) ((go x) (path self =d)))) // >vcause +k =d pred (l(>vcause) (l(=d) ((cause >vcause) =d))) /-ed/ >pred ++k t (l(>pred) (tense >pred past)) Small number of lexical entries handle all syntactic phenomena. Semantics directly encoded in lexical entry. Entries structurally governed by small number of rules, specifying how N/A/P are related. Figure 1: Three Different Accounts of Verb Subcategorization 2 Incorporation Theory At the heart of our new contribution to modeling verb subcategorization is the marriage of Hale and Keyser's (1993, 1998) argument structure theory with Stabler's (1997) 'minimalist' structure building rules. In the Hale and Keyser's theory, using the terminology of X-bar syntax, a particular head (labeled X), may or may or may not take a complement (labeled Y) and may or may not project a specifier (labeled S), resulting in 4 possible structural configurations: X (c) H X Y X (c)(c) H H S X (c) H X Y a (c)(c) H H S a (c) H a X X (a) -subj, +comp (V) (b) +subj, +comp (P) (c) +subj, -comp (A) (d) -subj, -comp (N) Figure 2: Four fundamental primitives in Hale and Keyser's incorporation theory The combinatorial possibilities of incorporation with X=V, A, N, P heads, plus 'head movement', is designed to yield the space of possible syntactic argument structure configurations, presumably across all languages. Notions of agent, patient, instrument, theme, goal, etc. are not 'primitives', but are derived from positions in structural configurations. In English (but not necessarily in all languages), (a) the category V takes a complement but projects no specifier; (b) the category P takes both a complement and projects a specifier; (c) the category A takes no complement but projects a specifier; (d) the category N takes neither complement nor specifier. A particular verbal entry, being of category V, may incorporate one or more of these structures as its complement, as shown in Figure 3: * Nouns incorporated directly into a verbal entry yield structures such as (a): no subject is projected by the N. The phonetic material of the noun head incorporates (undergoes head movement) into the phonetic material of the verb head, which itself may undergo further movement. Verbs such as these are intransitive by nature, generating, e.g., /The light glow -ed/ but */Bob glow -ed the light/. This argument structure typifies purely internally caused processes.","The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG ) .","['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG ) .""]",1,"['My theory of verbal argument structure can be imple- mented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing.', ""The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky 's Minimalist Program ( Stabler , 1997 ; Harkema , 2000 ; #AUTHOR_TAG ) .""]"
CCT176,N04-2004,A computational framework for non-lexicalist semantics,no escape from syntax don’t try morphological analysis in the privacy of your own lexicon,['Alec Marantz'],,"So Lexicalism claims that the syntax manipulates internally complex words, not unanalyzable atomic units. The leading idea of Lexicalism might be summarized as follows: Everyone agrees that there has to be a list of sound/meaning connections for the atomic building blocks of language (=the ""morphemes""). There also has to be a list of idiosyncratic properties associated with the building blocks. Perhaps the storage house of sound/meaning connections for building blocks and the storage house of idiosyncratic information associated with building blocks is the same house. Perhaps the distinction between this unified storage house and the computational system of syntax could be used to correlate and localize various other crucial distinctions: non-syntax vs. syntax, ""lexical"" phonological rules vs. phrasal and everywhere phonological rules, unpredictable composition vs. predictable composition ... Syntax is for the ruly, the lexicon for the unruly (see, e.g., DiSciullo and Williams 1987). The Lexicalist view of the computational lexicon may be pictured as in (3), where both the Lexicon and the Syntax connect sound and meaning by relating the sound and meaning of complex constituents systematically to the sounds and meanings of their constitutive parts.","Here , I adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .","['Following the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .', 'Verbalizing heads introduce relevant eventive interpretations in the syntax, and correspond to (assumed) universal primitives of the human cognitive system.', 'On the other hand, verbal roots represent abstract (categoryless) concepts and basically correspond to open-class items drawn from encyclopedic knowledge.', 'I assume an inventory of three verbalizing heads, each corresponding to an aforementioned primitive:']",5,"['Following the non-lexicalist tradition, these primitives are argued to occupy functional projections in the syntactic structure, as so-called light verbs.', 'Here , I adopt the model proposed by #AUTHOR_TAG and decompose lexical verbs into verbalizing heads and verbal roots .']"
CCT177,N04-2004,A computational framework for non-lexicalist semantics,the berkeley framenet project,"['Collin F Baker', 'Charles J Fillmore', 'John B Lowe']",introduction,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year #NSF IRI-9618838, #Tools for Lexicon Building&quot;#. The project&apos;s key features are #a# a commitment to corpus evidence for semantic and syntactic generalizations, and #b# the representation of the valences of its target words #mostly nouns, adjectives, and verbs# in which the semantic portion makes use of frame semantics. The resulting database will contain #a# descriptions of the semantic frames underlying the meanings of the words described, and #b# the valence representation #semantic and syntactic# of several thousand words and phrases, each accompanied by #c# a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between #frame elements&quot; and their syntactic realizations #e.g. grammatical function, phrase type, and other syntactic traits#. This report will present the project&apos;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work","This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #AUTHOR_TAG ) and PropBank ( Kingsbury et al. , 2002 ) .","['A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #AUTHOR_TAG ) and PropBank ( Kingsbury et al. , 2002 ) .""]",0,"['A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots).', 'Verbs are viewed as simple predicates over their arguments.', ""This approach has its roots in Fillmore 's Case Grammar ( 1968 ) , and serves as the foundation for two current large-scale semantic annotation projects : FrameNet ( #AUTHOR_TAG ) and PropBank ( Kingsbury et al. , 2002 ) .""]"
CCT178,N04-2004,A computational framework for non-lexicalist semantics,word meaning and,['David Dowty'],,"Reading and listening involve complex psychological processes that recruit many brain areas. The anatomy of processing English words has been studied by a variety of imaging methods. Although there is widespread agreement on the general anatomical areas involved in comprehending words, there are still disputes about the computations that go on in these areas. Examination of the time relations (circuitry) among these anatomical areas can aid in under-standing their computations. In this paper we concentrate on tasks which involve obtaining the meaning of a word in isolation or in relation to a sentence. Our current data support a finding in the literature that frontal semantic areas are active well before posterior areas. We use the subjects attention to amplify relevant brain areas involved either in semantic classification or in judging the relation of the word to a sentence in order to test the hypothesis that frontal areas are concerned with lexical semantics while posterior areas are more involved in comprehension of propositions that involve several words","There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .","['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']",0,"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( #AUTHOR_TAG ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .']"
CCT179,N04-2004,A computational framework for non-lexicalist semantics,three generative lexicalized models for statistical parsing,['Michael Collins'],introduction,"In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).","Due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .","['The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content.', 'Due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']",0,"['Due to advances in statistical syntactic parsing techniques ( #AUTHOR_TAG ; Charniak , 2001 ) , attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences .']"
CCT180,N04-2004,A computational framework for non-lexicalist semantics,semantics and cognition,['Ray Jackendoff'],,"A new perspective on cognition views cortical cell assemblies linking together knowledge about actions and perceptions not only as the vehicles of integrated action and perception processing but, furthermore, as a brain basis for a wide range of higher cortical functions, including attention, meaning and concepts, sequences, goals and intentions, and even communicative social interaction. This article explains mechanisms relevant to mechanistic action perception theory, points to concrete neuronal circuits in brains along with artificial neuronal network simulations, and summarizes recent brain imaging and other experimental data documenting the role of action perception circuits in cognition, language and communication","There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .","['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .', 'Consider the following example:']",0,"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; #AUTHOR_TAG ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .']"
CCT181,N04-2004,A computational framework for non-lexicalist semantics,a minimalist implementation of verb subcategorization,['Sourabh Niyogi'],,"Traditional accounts of verb subcategorization, from the classic work of Fillmore on, require either a considerable number of syntactic rules to account for diverse sentence constructions, including crosslanguage variation, or else complex linking rules mapping the thematic roles of semantic event templates with possible syntactic forms. In this paper we exhibit a third approach: we implement, via an explicit parser and lexicon, the incorporation theory of Hale and Keyser (1993, 1998) to systematically cover most patterns in English Verb Classes and Alternations (Levin 1993), typically using only 1 or 2 lexical entries per verb to subsume a large number of syntactic constructions and also most information typically contained in semantic event templates, and, further, replacing the notion of ""thematic roles"" with precise structural configurations. The implemented parser uses the merge and move operations formalized by Stabler (1997) in the minimalist framework of Chomsky (2001). As a side benefit, we extend the minimalist recognizer of Harkema (2000) to a full parsing implementation. We summarize the current compactness and coverage of our account and provide this minimalist lexicon and parser online at http://web.mit.edu/niyogi/www/minimal.htm 1 The Problem of Verb Subcategorization Why do certain verbs undergo particular certain alternations and not others? On some accounts, e.g. Levin (1993), referred to hereafter as EVCA, alternations provide insight into verb subcategorization and hence hooks to parsing, cross-language variation, machine translation, and class based verb learning. However, fully implemented accounts of the phenomena remains an open problem, with at least three alternative models, shown in Figure 1. Accounts may be solely descriptive - for example, classifying verbs as having an intransitive, a transitive, and/or ditransitive form, as is familiar. Traditional computational accounts (see 1) map these forms into individual grammar rules, (perhaps by macro expansion-like techniques) adding as many rules as necessary to account for naturally' occurring constructions (wh-movement, passive forms, etc.) For each grammatical rule, a separate semantic decomposition is required, typically labeling component phrases with one of several ""thematic roles."" A richer account provided by lexical semantics (see 2), exemplified in Jackendoff (1983, 1990) and Rappaport Hovav and Levin (1998), is one that hypothesizes semantic templates, but requires linking rules mapping syntactic frames with semantic templates governed by a particular verb. Often these semantic templates are constructed in an ad hoc manner, and the corresponding linking rules are consquently a collection of difficult-toimplement heuristics. In this paper we implement a rather different formalism (Hale and Keyser's Incorporation theory, see 3), wherein fewer lexical entries govern syntactic and semantic behavior, with no appeal to thematic roles or complex linking rules. 0. Verb Subcategorization Phenomena * Bob put. Butter was put on the bread. * Bob put butter. What was put on the bread? Bob put butter on the bread. Where was the butter put? 1. Traditional Account VP - V0 NP PPloc V0 - put VP - was VPass VPass - V0 PPloc VP/NP - V0 NP/NP PPloc VP/NP - V0 NP PPloc/NP PPloc - Ploc NP Ploc - on | in | ... PPloc/NP - Ploc NP/NP Exhaustive modelling with a considerable number of grammatical rules. Semantics separate, otherwise unspecified. 2. Lexical Semantics Account   put V NPjPPk CAUSE ( [BOB]i , GO ( [BUTTER]j , TO ( [BREAD]k )))   Syntax handled by numerous argument-fusing ""linking rules"", typically difficult to formalize. Semantic templates mirror alternation patterns, but are ad-hocly constructed. 3. Minimalist/Incorporation Account /put/ =ploc =d vcause (l(=ploc) (l(=d) (=ploc =d))) /on/ =d +k ploc (l(=d) (l(x) ((go x) (path self =d)))) // >vcause +k =d pred (l(>vcause) (l(=d) ((cause >vcause) =d))) /-ed/ >pred ++k t (l(>pred) (tense >pred past)) Small number of lexical entries handle all syntactic phenomena. Semantics directly encoded in lexical entry. Entries structurally governed by small number of rules, specifying how N/A/P are related. Figure 1: Three Different Accounts of Verb Subcategorization 2 Incorporation Theory At the heart of our new contribution to modeling verb subcategorization is the marriage of Hale and Keyser's (1993, 1998) argument structure theory with Stabler's (1997) 'minimalist' structure building rules. In the Hale and Keyser's theory, using the terminology of X-bar syntax, a particular head (labeled X), may or may or may not take a complement (labeled Y) and may or may not project a specifier (labeled S), resulting in 4 possible structural configurations: X (c) H X Y X (c)(c) H H S X (c) H X Y a (c)(c) H H S a (c) H a X X (a) -subj, +comp (V) (b) +subj, +comp (P) (c) +subj, -comp (A) (d) -subj, -comp (N) Figure 2: Four fundamental primitives in Hale and Keyser's incorporation theory The combinatorial possibilities of incorporation with X=V, A, N, P heads, plus 'head movement', is designed to yield the space of possible syntactic argument structure configurations, presumably across all languages. Notions of agent, patient, instrument, theme, goal, etc. are not 'primitives', but are derived from positions in structural configurations. In English (but not necessarily in all languages), (a) the category V takes a complement but projects no specifier; (b) the category P takes both a complement and projects a specifier; (c) the category A takes no complement but projects a specifier; (d) the category N takes neither complement nor specifier. A particular verbal entry, being of category V, may incorporate one or more of these structures as its complement, as shown in Figure 3: * Nouns incorporated directly into a verbal entry yield structures such as (a): no subject is projected by the N. The phonetic material of the noun head incorporates (undergoes head movement) into the phonetic material of the verb head, which itself may undergo further movement. Verbs such as these are intransitive by nature, generating, e.g., /The light glow -ed/ but */Bob glow -ed the light/. This argument structure typifies purely internally caused processes.",#AUTHOR_TAG has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .,"['#AUTHOR_TAG has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework.', 'As an example, a simplified derivation of the sentence ""The tire flattened."" is shown in Figure 1.']",2,"['#AUTHOR_TAG has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .', 'I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework.', 'As an example, a simplified derivation of the sentence ""The tire flattened."" is shown in Figure 1.']"
CCT182,N04-2004,A computational framework for non-lexicalist semantics,english verb classes and alternations a preliminary investigation,['Beth Levin'],introduction,"In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, ""English Verb Classes and Alternations"" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University.","The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .","['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']",1,"['The typical solution to the redundancy problem is to group verbs according to their argument realization patterns ( #AUTHOR_TAG ) , possibly arranged in an inheritance hierarchy .', 'The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class.', 'In addition, lexical rules could be formulated to derive certain alternations from more basic forms.']"
CCT183,N04-2004,A computational framework for non-lexicalist semantics,building verb meanings,"['Malka Rappaport Hovav', 'Beth Levin']",,,"There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .","['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .', 'Consider the following example:']",0,"['There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; Pustejovsky , 1991b ; Rappaport #AUTHOR_TAG ) .']"
CCT184,N04-2004,A computational framework for non-lexicalist semantics,verbs and times,['Zeno Vendler'],,"T HE fact that verbs have tenses indicates that considerations involving the concept of time are relevant to their use. These considerations are not limited merely to the obvious discrimination between past, present, and future; there is another, a more subtle dependence on that concept: the use of a verb may also suggest the particular way in which that verb presupposes and involves the notion of time. In a number of recent publications some attention has been paid to these finer aspects, perhaps for the first time systematically. Distinctions have been made among verbs suggesting processes, states, dispositions, occurrences, tasks, achievements, and so on. Obviously these differences cannot be explained in terms of time alone: other factors, like the presence or absence of an object, conditions, intended states of affairs, also enter the picture. Nevertheless one feels that the time element remains crucial; at least it is important enough to warrant separate treatment. Indeed, as I intend to show, if we focus our attention primarily upon the time schemata presupposed by various verbs,"" we are able to throw light on some of the obscurities which still remain in these matters. These time schemata will appear as important constituents of the concepts that prompt us to use those terms the way we consistently do. There are a few such schemata of very wide application. Once they have been discovered in some typical examples, they may be used as models of comparison in exploring and clarifying the behavior of any verb whatever. In indicating these schemata, I do not claim that they represent all possible ways in which verbs can be used correctly with respect to time determination nor that a verb exhibiting a use fairly covered by one schema cannot have divergent uses, which","A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )","['Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean.', ""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]",0,"[""A more recent approach , advocated by Rappaport Hovav and Levin ( 1998 ) , describes a basic set of event templates corresponding to Vendler 's event classes ( #AUTHOR_TAG ) : ( 3 ) a. [ x ACT <MANNER> ] ( activity ) b. [ x <STATE> ] ( state ) c. [ BECOME [ x <STATE> ] ] ( achievement ) d. [ x CAUSE [ BECOME [ x <STATE> ] ] ] ( accomplishment )""]"
CCT185,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,natural language processing for information assurance and security an overview and implementations,"['Mikhail J Atallah', 'Craig J McDonough', 'Victor Raskin', 'Sergei Nirenburg']",related work,"This research paper explores a promising interface between natural language processing (NLP) and information assurance and security (IAS). More specificall~ it is devoted to possible applications to, and further dedicated development of, the accumulated considerable resources in NLP for, IAS. The expected and partially accomplished result is in harnessing the weird, illogical ways natural languages encode meaning, the very ways that defy all the usual combinatorial approaches to mathematical--and computational--complexity and make NLP so hard, to enhance information security. The paper is of a mixed theoretical and empirical nature. Of the four possible venues of applications, (i) memorizing randomly generated passwords with the help of automatically generated funny jingles, (ii) natural language watermarking, (iii) using the available machine translation (MT) systems for (additional) encryption of text messages, and (iv) downgrading, or sanitizing classified information in networks, two venues, (i) and (iv), have been at least partially implemented and the remaining two (ii) and (iii) are being implemented to the proof-of-concept level. We must make it very clear, however, that we have done very little experimentation or evaluation at this point, though we are moving quickly in that direction. The merits of the paper, if any, are in its venture to make considerable progress achieved recently in NLE especially in knowledge representation and meaning analysis, useful for IAS needs. The NLP approach adopted here, ontological semantics, has been developed by two of the coauthors; watermarking is based on the pioneering research by another coauthor and his associates; most of the implementation of the password memorization software has been done by the fourth coauthor. All the four of us have agonized whether we should report this research now or wait till we have fully implemented all or at least some of the systems we are developing. At the end of the day, we have reached a consensus that it is important, even at this early stage, to review for the information security community what NLP can do for it and to invite feedback and further efforts and ideas on what seems likely to become a new paradigm in information security. To the body of the paper, we Mikhail J. Atallah, Craig J. McDonough, Victor Raskin Center for Education and Research in Information Assurance and Security (CERIAS, www.cerias.purdue.edu) Purdue University W. Lafayette, IN 47907 mja, raskin, mcdonoug@cerias.purdue.edu Sergei Nirenburg Computing Research Laboratory, New Mexico State University Las Cruces, NM 88003 sergei@crl.nmsu.edu have added two self-contained deliberately reference-free appendices on NLP and ontological semantics, respectively, primarily for the benefit of those IAS readers, who are interested in expanding their understanding of those fields and further exploring their possible fruitful interactions with IAS.",#AUTHOR_TAGb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006a) all belong to the syntactic transformation category.', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', '#AUTHOR_TAGb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. (2005), Meral et al. (2007, Murphy (2001), Murphy and Vogel (2007) and Topkara et al. (2006a) all belong to the syntactic transformation category.', '#AUTHOR_TAGb ) and Topkara et al. ( 2006a ) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method .']"
CCT186,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,hiding the hidden a software system for concealing ciphertext as innocuous text,"['Mark Chapman', 'George I Davida']",related work,"In this paper we present a system for protecting the privacy of cryptograms to avoid detection by censors. The system transforms ciphertext into innocuous text which can be transformed back into the original ciphertext. The expandable set of tools allows experimentation with custom dictionaries, automatic simulation of writing style, and the use of Context-Free-Grammars to control text generation. The scope of this paper is to provide an overview of the basic transformation processes and to demonstrate the quality of the generated text.",The first lexical substitution method was proposed by #AUTHOR_TAG .,"['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by #AUTHOR_TAG .', 'Later works, such as Atallah et al. (2001a), Bolshakov (2004), Taskiran et al. (2006 and Topkara et al. (2006b), further made use of part-ofspeech taggers and electronic dictionaries, such as WordNet and VerbNet, to increase the robustness of the method.', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']",0,['The first lexical substitution method was proposed by #AUTHOR_TAG .']
CCT187,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,the syntax of concealment reliable methods for plain text information hiding,"['Brian Murphy', 'Carl Vogel']",related work,"Many plain text information hiding techniques demand deep semantic processing, and so suffer in reliability. In contrast, syntactic processing is a more mature and reliable technology. Assuming a perfect parser, this paper evaluates a set of automated and reversible syntactic transforms that can hide information in plain text without changing the meaning or style of a document. A large representative collection of newspaper text is fed through a prototype system. In contrast to previous work, the output is subjected to human testing to verify that the text has not been significantly compromised by the information hiding procedure, yielding a success rate of 96% and bandwidth of 0.3 bits per sentence.","Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , #AUTHOR_TAG and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , #AUTHOR_TAG and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , #AUTHOR_TAG and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"
CCT188,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,natural language processing for information assurance and security an overview and implementations,"['Mikhail J Atallah', 'Craig J McDonough', 'Victor Raskin', 'Sergei Nirenburg']",related work,"This research paper explores a promising interface between natural language processing (NLP) and information assurance and security (IAS). More specificall~ it is devoted to possible applications to, and further dedicated development of, the accumulated considerable resources in NLP for, IAS. The expected and partially accomplished result is in harnessing the weird, illogical ways natural languages encode meaning, the very ways that defy all the usual combinatorial approaches to mathematical--and computational--complexity and make NLP so hard, to enhance information security. The paper is of a mixed theoretical and empirical nature. Of the four possible venues of applications, (i) memorizing randomly generated passwords with the help of automatically generated funny jingles, (ii) natural language watermarking, (iii) using the available machine translation (MT) systems for (additional) encryption of text messages, and (iv) downgrading, or sanitizing classified information in networks, two venues, (i) and (iv), have been at least partially implemented and the remaining two (ii) and (iii) are being implemented to the proof-of-concept level. We must make it very clear, however, that we have done very little experimentation or evaluation at this point, though we are moving quickly in that direction. The merits of the paper, if any, are in its venture to make considerable progress achieved recently in NLE especially in knowledge representation and meaning analysis, useful for IAS needs. The NLP approach adopted here, ontological semantics, has been developed by two of the coauthors; watermarking is based on the pioneering research by another coauthor and his associates; most of the implementation of the password memorization software has been done by the fourth coauthor. All the four of us have agonized whether we should report this research now or wait till we have fully implemented all or at least some of the systems we are developing. At the end of the day, we have reached a consensus that it is important, even at this early stage, to review for the information security community what NLP can do for it and to invite feedback and further efforts and ideas on what seems likely to become a new paradigm in information security. To the body of the paper, we Mikhail J. Atallah, Craig J. McDonough, Victor Raskin Center for Education and Research in Information Assurance and Security (CERIAS, www.cerias.purdue.edu) Purdue University W. Lafayette, IN 47907 mja, raskin, mcdonoug@cerias.purdue.edu Sergei Nirenburg Computing Research Laboratory, New Mexico State University Las Cruces, NM 88003 sergei@crl.nmsu.edu have added two self-contained deliberately reference-free appendices on NLP and ontological semantics, respectively, primarily for the benefit of those IAS readers, who are interested in expanding their understanding of those fields and further exploring their possible fruitful interactions with IAS.","Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an n- gram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']",0,"['The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as #AUTHOR_TAGa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']"
CCT189,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,words are not enough sentence level natural language watermarking,"['Mercan Topkara', 'Umut Topkara', 'Mikhail J Atallah']",related work,"Compared to other media, natural language text presents unique challenges for information hiding. These challenges require the design of a robust algorithm that can work under following constraints: (i) low embedding bandwidth, i.e., number of sentences is comparable with message length, (ii) not all transformations can be applied to a given sentence (iii) the number of alternative forms for a sentence is relatively small, a limitation governed by the grammar and vocabulary of the natural language, as well as the requirement to preserve the style and fluency of the document. The adversary can carry out all the transformations used for embedding to remove the embedded message. In addition, the adversary can also permute the sentences, select and use a subset of sentences, and insert new sentences. We give a scheme that overcomes these challenges, together with a partial implementation and its evaluation for the English language. The present application of this scheme works at the sentence level while also using a word-level watermarking technique that was recently designed and built into a fully automatic system (""Equimark""). Unlike Equimark, whose resilience relied on the introduction of ambiguities, the present paper's sentence-level technique is more tuned to situations where very little change to the text is allowable (i.e., when style is important). Secondarily, this paper shows how to use lower-level (in this case word-level) marking to improve the resilience and embedding properties of higher level (in this case sentence level) schemes. We achieve this by using the word-based methods as a separate channel from the sentence-based methods, thereby improving the results of either one alone. The sentence level watermarking technique we introduce is novel and powerful, as it relies on multiple features of each sentence and exploits the notion of orthogonality between features.","Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']",0,"['Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and #AUTHOR_TAGb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .']"
CCT190,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,syntactic information hiding in plain text masters thesis trinity college dublin,['Brian Murphy'],related work,,"Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , #AUTHOR_TAG , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , #AUTHOR_TAG , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , #AUTHOR_TAG , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .']"
CCT191,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,a method of linguistic steganography based on coladdressallyverified synonym,['Igor A Bolshakov'],related work,,"Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .","['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']",0,"['The simplest and most straightforward subliminal modification of text is to substitute selected words with their synonyms.', 'The first lexical substitution method was proposed by Chapman and Davida (1997).', 'Later works , such as Atallah et al. ( 2001a ) , #AUTHOR_TAG , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .', 'Taskiran et al. (2006) attempt to use context by prioritizing the alternatives using an ngram language model; that is, rather than randomly choose an option from the synonym set, the system relies on the language model to select the synonym.', 'Topkara et al. (2005) and Topkara et al. (2006b) report an average embedding capacity of 0.67 bits per sentence for the synonym substitution method.']"
CCT192,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,webscale ngram models for lexical disambiguation,"['Shane Bergsma', 'Dekang Lin', 'Randy Goebel']",experiments,,"The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .","['The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .', 'The striking feature of the n-gram corpus is the large number of n-grams and the size of the counts, since the counts were extracted from over 1 trillion word tokens of English text on publicly accessible Web pages collected in January 2006.', 'For example, the 5-gram phrase the part that you were has a count of 103.', 'The compressed data is around 24 GB on disk.']",0,"['The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( #AUTHOR_TAG ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .']"
CCT193,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,steganography in digital media principles algorithms and applications,['Jessica Fridrich'],introduction,"Steganography, the art of hiding of information in apparently innocuous objects or images, is a field with a rich heritage, and an area of rapid current development. This clear, self-contained guide shows you how to understand the building blocks of covert communication in digital media files and how to apply the techniques in practice, including those of steganalysis, the detection of steganography. Assuming only a basic knowledge in calculus and statistics, the book blends the various strands of steganography, including information theory, coding, signal estimation and detection, and statistical signal processing. Experiments on real media files demonstrate the performance of the techniques in real life, and most techniques are supplied with pseudo-code, making it easy to implement the algorithms. The book is ideal for students taking courses on steganography and information hiding, and is also a useful reference for engineers and practitioners working in media security and information assurance.","Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ) .","['Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ) .', 'The covert communication is such that the very act of communication is to be kept secret from outside observers.', 'A related area is Watermarking, in which modifications are made to a cover medium in order to identify it, for example for the purposes of copyright.', 'Here the changes may be known to an observer, and the task is to make the changes in such a way that the watermark cannot easily be removed.']",0,"['Steganography is concerned with hiding information in some cover medium , by manipulating properties of the medium in such a way that the hidden information is not easily detectable by an observer ( #AUTHOR_TAG ) .']"
CCT194,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,words are not enough sentence level natural language watermarking,"['Mercan Topkara', 'Umut Topkara', 'Mikhail J Atallah']",related work,"Compared to other media, natural language text presents unique challenges for information hiding. These challenges require the design of a robust algorithm that can work under following constraints: (i) low embedding bandwidth, i.e., number of sentences is comparable with message length, (ii) not all transformations can be applied to a given sentence (iii) the number of alternative forms for a sentence is relatively small, a limitation governed by the grammar and vocabulary of the natural language, as well as the requirement to preserve the style and fluency of the document. The adversary can carry out all the transformations used for embedding to remove the embedded message. In addition, the adversary can also permute the sentences, select and use a subset of sentences, and insert new sentences. We give a scheme that overcomes these challenges, together with a partial implementation and its evaluation for the English language. The present application of this scheme works at the sentence level while also using a word-level watermarking technique that was recently designed and built into a fully automatic system (""Equimark""). Unlike Equimark, whose resilience relied on the introduction of ambiguities, the present paper's sentence-level technique is more tuned to situations where very little change to the text is allowable (i.e., when style is important). Secondarily, this paper shows how to use lower-level (in this case word-level) marking to improve the resilience and embedding properties of higher level (in this case sentence level) schemes. We achieve this by using the word-based methods as a separate channel from the sentence-based methods, thereby improving the results of either one alone. The sentence level watermarking technique we introduce is novel and powerful, as it relies on multiple features of each sentence and exploits the notion of orthogonality between features.","Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and #AUTHOR_TAGa ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and #AUTHOR_TAGa ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and #AUTHOR_TAGa ) all belong to the syntactic transformation category .']"
CCT195,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,a method of text watermarking using presuppositions,"['M Olga Vybornova', 'Benoit Macq']",related work,"We propose a method for watermarking texts of arbitrary length using natural-language semantic structures. For the key of our approach we use the linguistic semantic phenomenon of presuppositions. Presupposition is the implicit information considered as well-known or which readers of the text are supposed to treat as well-known; this information is a semantic component of certain linguistic expressions (lexical items and syntactical constructions called presupposition triggers). The same sentence can be used with or without presupposition, or with a different presupposition trigger, provided that all the relations between subjects, objects and other discourse referents are preserved - such transformations will not change the meaning of the sentence. We define the distinct rules for presupposition identification for each trigger and regular transformation rules for using/non-using the presupposition in a given sentence (one bit per sentence in this case). Isolated sentences can carry the proposed watermarks. However, the longer is the text, the more efficient is the watermark. The proposed approach is resilient to main types of random transformations, like passivization, topicalization, extraposition, preposing, etc. The web of resolved presupposed information in the text will hold the watermark of the text (e.g. integrity watermark, or prove of ownership), introducing ""secret ordering"" into the text structure to make it resilient to ""data loss"" attacks and ""data altering"" attacks.Anglai","#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .","['The semantic transformation method is the most sophisticated approach for linguistic steganography, and perhaps impractical given the current state-ofthe-art for NLP technology.', 'It requires some sophisticated tools and knowledge to model natural language semantics.', 'Atallah et al. (2002) used semantic transformations and embed information in textmeaning representation (TMR) trees of the text by either pruning, grafting or substituting the tree structure with information available from ontological semantic resources.', '#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .']",0,"['#AUTHOR_TAG aimed to embed information by exploiting the linguistic phenomenon of presupposition , with the idea that some presuppositional information can be removed without changing the meaning of a sentence .']"
CCT196,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,extracting paraphrases from a parallel corpus,"['Regina Barzilay', 'Kathleen R McKeown']",experiments,"While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.",#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .,"['Table 1 gives summary statistics of the paraphrase dictionary and its coverage on Section 00 of the Penn Treebank.', 'The length of the extracted n-gram phrases ranges from unigrams to five-grams.', 'The coverage figure gives the percentage of sentences which have at least one phrase in the dictionary.', 'The coverage is important for us because it determines the payload capacity of the embedding method described in Section 5.  Original phrase Paraphrases the end of this year later this year the end of the year year end a number of people some of my colleagues differences the European peoples party the PPE group dictionary is a mapping from phrases to sets of possible paraphrases.', 'Each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'The examples show that, while some of the paraphrases are of a high quality, some are not.', 'For example, differences is unlikely to be a suitable paraphrase for a number of people in any context.', 'Moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', 'For example, year end is an unsuitable paraphrase for the end of this year in the sentence The chart compares the gold price at the end of last year with the end of this year.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', 'Section 4 describes our method for determining if a paraphrase is suitable in a given context.']",0,"['Table 1 gives summary statistics of the paraphrase dictionary and its coverage on Section 00 of the Penn Treebank.', 'Each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here.', 'The examples show that, while some of the paraphrases are of a high quality, some are not.', 'Moreover, there are some phrase, paraphrase pairs which are only suitable in particular contexts.', '#AUTHOR_TAG also note that the applicability of paraphrases is strongly influenced by context .', 'Section 4 describes our method for determining if a paraphrase is suitable in a given context.']"
CCT197,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,syntactic constraints on paraphrases extracted from parallel corpora,['Chris Callison-Burch'],experiments,"ccb cs jhu edu We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.","The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation .","['The cover text used for our experiments consists of newspaper sentences from Section 00 of the Penn Treebank (Marcus et al., 1993).', 'Hence we require possible paraphrases for phrases that occur in Section 00.', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation .']",5,"['Hence we require possible paraphrases for phrases that occur in Section 00.', 'The paraphrase dictionary that we use was generated for us by Chris Callison-Burch , using the technique described in #AUTHOR_TAG , which exploits a parallel corpus and methods developed for statistical machine translation .']"
CCT198,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,widecoverage efficient statistical parsing with ccg and loglinear models,"['Stephen Clark', 'James R Curran']",introduction,,"We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned the same CCG lexical categories by the parser.","['In order to test the grammaticality and meaning preserving nature of a paraphrase, we employ a simple technique based on checking whether the contexts containing the paraphrase are in the Google ngram corpus.', 'This technique is based on the simple hypothesis that, if the paraphrase in context has been used many times before on the web, then it is an appropriate use.', 'We test our n-gram-based system against some human judgements of the grammaticality of paraphrases in context.', 'We find that using larger contexts leads to a high precision system (100% when using 5-grams), but at the cost of a reduced recall.', 'This precision-recall tradeoff reflects the inherent tradeoff between imperceptibility and payload in a Linguistic Steganography system.', 'We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned the same CCG lexical categories by the parser.', 'This method increases the precision of the Google n-gram check with a slight loss in recall.']",5,"['We also experiment with a CCG parser (Clark and Curran, 2007), requiring that the contexts surrounding the original phrase and paraphrase are assigned the same CCG lexical categories by the parser.']"
CCT199,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,a natural language watermarking based on chinese syntax,"['Yuling Liu', 'Xingming Sun', 'Yong Wu']",related work,"A novel text watermarking algorithm is presented. It combines natural language watermarking and Chinese syntax based on BP neural networks. Since the watermarking signals are embedded into some Chinese syntactic structure rather than the appearance of text elements, the algorithm is totally based on the content that can prove to be very resilient. It will play an important role in protecting the security of Chinese documents over Internet.","#AUTHOR_TAG , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', '#AUTHOR_TAG , Meral et al. ( 2007 ) , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']"
CCT200,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,whitesteg a new scheme in information hiding using text steganography,"['Lip Y Por', 'Ang T Fong', 'B Delina']",introduction,"Abstract:- Sending encrypted messages frequently will draw the attention of third parties, i.e. crackers and hackers, perhaps causing attempts to break and reveal the original messages. In this digital world, steganography is introduced to hide the existence of the communication by concealing a secret message inside another unsuspicious message. The hidden message maybe plaintext, or any data that can be represented as a stream of bits. Steganography is often being used together with cryptography and offers an acceptable amount of privacy and security over the communication channel. This paper presents an overview of text steganography and a brief history of steganography along with various existing techniques of text steganography. Highlighted are some of the problems inherent in text steganography as well as issues with existing solutions. A new approach, named WhiteSteg is proposed in information hiding using inter-word spacing and inter-paragraph spacing as a hybrid method to reduce the visible detection of the embedded messages. WhiteSteg offers dynamic generated cover-text with six options of maximum capacity according to the length of the secret message. Besides, the advantage of exploiting whitespaces in information hiding is discussed. This paper also analyzes the significant drawbacks of each existing method and how WhiteSteg could be recommended as a solution","Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #AUTHOR_TAG ) .","['Section 2 describes some of the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #AUTHOR_TAG ) .', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits.', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.']",1,"['Section 2 describes some of the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( #AUTHOR_TAG ) .']"
CCT201,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,syntactic constraints on paraphrases extracted from parallel corpora,['Chris Callison-Burch'],,"ccb cs jhu edu We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.","Our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .","['Section 2 describes some of the previous transformations used in Linguistic Steganography.', 'Note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words (Por et al., 2008).', 'Our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.']",5,"['Our proposed method is based on the automatically acquired paraphrase dictionary described in #AUTHOR_TAG , in which the application of paraphrases from the dictionary encodes secret bits .', 'One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts.', 'Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.']"
CCT202,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,a comprehensive bibliography of linguistic steganography,['Richard Bergmair'],introduction,"In this paper, we will attempt to give a comprehensive bibliographic account of the work in linguistic steganography published up to date. As the field is still in its infancy there is no widely accepted publication venue. Relevant work on the subject is scattered throughout the literature on information security, information hiding, imaging and watermarking, cryptology, and natural language processing. Bibliographic references within the field are very sparse. This makes literature research on linguistic steganography a tedious task and a comprehensive bibliography a valuable aid to the researcher.","However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #AUTHOR_TAG ) .","['There is a large literature on image steganography and watermarking, in which images are modified to encode a hidden message or watermark.', 'Image stegosystems exploit the redundancy in an image representation together with limitations of the human visual system.', 'For example, a standard image stegosystem uses the least-significant-bit (LSB) substitution technique.', 'Since the difference between 11111111 and 11111110 in the value for red/green/blue intensity is likely to be undetectable by the human eye, the LSB can be used to hide information other than colour, without being perceptable by a human observer. 1', ' key question for any steganography system is the choice of cover medium.', 'Given the ubiquitous nature of natural languages and electronic text, text is an obvious medium to consider.', 'However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #AUTHOR_TAG ) .', 'The likely reason is that it is easier to make changes to images and other nonlinguistic media which are undetectable by an observer.', 'Language has the property that even small local changes to a text, e.g.', 'replacing a word by a word with similar meaning, may result in text which is anomalous at the document level, or anomalous with respect to the state of the world.', 'Hence finding linguistic transformations which can be applied reliably and often is a challenging problem for Linguistic Steganography.']",0,"['However , the literature on Linguistic Steganography , in which linguistic properties of a text are modified to hide information , is small compared with other media ( #AUTHOR_TAG ) .']"
CCT203,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,widecoverage efficient statistical parsing with ccg and loglinear models,"['Stephen Clark', 'James R Curran']",method,,We use the #AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .,"['In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the #AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', 'Combinatory Categorial Grammar (CCG) is a lexicalised grammar formalism, in which CCG lexical categories -typically expressing subcategorisation information -are assigned to each word in a sentence.', 'The grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing.', 'If there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical.', 'Hence the grammar check is at the word, rather than derivation, level; however, CCG lexical categories contain a large amount of syntactic information which this method is able to exploit.']",5,"['In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method.', 'We use the #AUTHOR_TAG CCG parser to analyse the sentence before and after paraphrasing .', 'Combinatory Categorial Grammar (CCG) is a lexicalised grammar formalism, in which CCG lexical categories -typically expressing subcategorisation information -are assigned to each word in a sentence.', 'Hence the grammar check is at the word, rather than derivation, level; however, CCG lexical categories contain a large amount of syntactic information which this method is able to exploit.']"
CCT204,N10-1084,Practical Linguistic Steganography using Contextual Synonym Substitution and a Novel Vertex Coding Method,syntactic tools for text watermarking,"['Hasan M Meral', 'Emre Sevinc', 'Ersin Unkar', 'Bulent Sankur', 'A Sumru Ozsoy', 'Tunga Gungor']",related work,"This paper explores the morphosyntactic tools for text watermarking and develops a syntax-based natural language watermarking scheme. Turkish, an agglutinative language, provides a good ground for the syntax-based natural language watermarking with its relatively free word order possibilities and rich repertoire of morphosyntactic structures. The unmarked text is first transformed into a syntactic tree diagram in which the syntactic hierarchies and the functional dependencies are coded. The watermarking software then operates on the sentences in syntax tree format and executes binary changes under control of Wordnet to avoid semantic drops. The key-controlled randomization of morphosyntactic tool order and the insertion of void watermark provide a certain level of security. The embedding capacity is calculated statistically, and the imperceptibility is measured using edit hit counts.","Liu et al. ( 2005 ) , #AUTHOR_TAG , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .","['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'This method is based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization and clefting.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'Later, Atallah et al. (2001b) embedded information in the tree structure of the text by adjusting the structural properties of intermediate representations of sentences.', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , #AUTHOR_TAG , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .', 'After embedding the secret message, modified deep structure forms are converted into the surface structure format via language generation tools.', 'Atallah et al. (2001b) and Topkara et al. (2006a) attained the embedding capacity of 0.5 bits per sentence with the syntactic transformation method.']",0,"['The second and the most widely used manipulations for linguistic steganography are syntactic transformations.', 'The first syntactic transformation method is presented by Atallah et al. (2001a).', 'In other words, instead of performing lexical substitution directly to the text, the secret message is embedded into syntactic parse trees of the sentences.', 'Liu et al. ( 2005 ) , #AUTHOR_TAG , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .']"
CCT205,N13-1036,Unsupervised Arabic dialect segmentation for machine translation,dialectal to standard arabic paraphrasing to improve arabicenglish statistical machine translation,"['Wael Salloum', 'Nizar Habash']",related work,"This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words.","In our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .","['In our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', 'We did not use a language model to pick the best path; instead we kept the ambiguity in the lattice and passed it to our SMT system.', 'In contrast, in this paper, we run ELISSA on untokenized Arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated MSA lattice through a language model.', ""Certain aspects of our approach are similar to Riesa and Yarowsky (2006)'s, in that we use morphological analysis for DA to help DA-English MT; but unlike them, we use a rule-based approach to model DA morphology.""]",1,"['In our previous work ( #AUTHOR_TAG ; Salloum and Habash , 2012 ) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only .', 'In contrast, in this paper, we run ELISSA on untokenized Arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated MSA lattice through a language model.']"
CCT206,N13-1036,Unsupervised Arabic dialect segmentation for machine translation,a systematic comparison of various statistical alignment models,"['F J Och', 'H Ney']",,"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.",The parallel corpus is word-aligned using GIZA + + ( #AUTHOR_TAG ) .,"['We use the open-source Moses toolkit (Koehn et al., 2007) to build a phrase-based SMT system trained on mostly MSA data (64M words on the Arabic side) obtained from several LDC corpora including some limited DA data.', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + + ( #AUTHOR_TAG ) .', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']",5,"['Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA + + ( #AUTHOR_TAG ) .', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic text is also Alif/Ya normalized.']"
CCT207,N13-1036,Unsupervised Arabic dialect segmentation for machine translation,dialectal to standard arabic paraphrasing to improve arabicenglish statistical machine translation,"['Wael Salloum', 'Nizar Habash']",,"This paper is about improving the quality of Arabic-English statistical machine translation (SMT) on dialectal Arabic text using morphological knowledge. We present a light-weight rule-based approach to producing Modern Standard Arabic (MSA) paraphrases of dialectal Arabic out-of-vocabulary (OOV) words and low frequency words. Our approach extends an existing MSA analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrase-based SMT system. This approach improves BLEU scores on a blind test set by 0.56 absolute BLEU (or 1.5% relative). A manual error analysis of translated dialectal words shows that our system produces correct translations in 74% of the time for OOVs and 60% of the time for low frequency words.",This is a similar conclusion to our previous work in #AUTHOR_TAG .,"['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'Phrase-based trans-  lation is also added to word-based translation.', 'Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection.', 'The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']",1,"['In the last system group, phrase+word-based selection, phrase-based selection is used to select phrases and add them on top of the best performers of the previous two groups.', 'Phrase-based trans-  lation is also added to word-based translation.', 'Results show that selecting and translating phrases improve the three best performers of word-based selection.', 'The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection.', 'The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries.', 'Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation.', 'This is a similar conclusion to our previous work in #AUTHOR_TAG .']"
CCT208,N13-1036,Unsupervised Arabic dialect segmentation for machine translation,moses open source toolkit for statistical machine translation,"['Philipp Koehn', 'Hieu Hoang', 'Alexandra Birch', 'Christopher Callison-Burch', 'Marcello Federico', 'Nicola Bertoldi', 'Brooke Cowan', 'Wade Shen', 'Christine Moran', 'Richard Zens', 'Christopher Dyer', 'Ondrej Bojar', 'Alexandra Constantin', 'Evan Herbst']",,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.",We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .,"['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .', 'Our system uses a standard phrase-based architecture.', 'The parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003).', 'Phrase translations of up to 10 words are extracted in the Moses phrase table.', 'The language model for our system is trained on the English side of the bitext augmented with English Gigaword (Graff and Cieri, 2003).', 'We use a 5-gram language model with modified Kneser-Ney smoothing.', 'Feature weights are tuned to maximize BLEU on the NIST MTEval 2006 test set using Minimum Error Rate Training (Och, 2003).', 'This is only done on the baseline systems.', 'The English data is tokenized using simple punctuation-based rules.', 'The Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Habash and Rambow, 2005;Roth et al., 2008).', 'The Arabic text is also Alif/Ya normalized.', 'MADA-produced Arabic lemmas are used for word alignment.']",5,['We use the open-source Moses toolkit ( #AUTHOR_TAG ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .']
CCT209,P00-1001,Processes that shape conversation and their implications for computational linguistics,deterministic parsing of syntactic nonfluencies,['D Hindle'],introduction,,"Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .","[""The implicit assumptions of psychological and computational theories that ignore disfluencies must be either that people aren't disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing."", 'The first assumption is clearly false; disfluency rates in spontaneous speech are estimated by Fox Tree (1995) and by Bortfeld, Leon, Bloom, Schober, and Brennan (2000) to be about 6 disfluencies per 100 words, not including silent pauses.', 'The rate is lower for speech to machines (Oviatt, 1995;Shriberg, 1996), due in part to utterance length; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative.', 'The average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully.', 'The good news is that speakers can adapt to machines; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused on the task itself.', 'As for the second assumption, if the goal is to eventually process unrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky.', 'In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g., Tanenhaus et al. 1995).', 'This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance.', 'Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .']",0,"['Few approaches to parsing have tried to handle disfluent utterances ( notable exceptions are Core & Schubert , 1999 ; #AUTHOR_TAG ; Nakatani & Hirschberg , 1994 ; Shriberg , Bear , & Dowding , 1992 ) .']"
CCT210,P06-1012,Estimating class priors in domain adaptation for word sense disambiguation,an empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation,"['Yoong Keok Lee', 'Hwee Tou Ng']",experiments,"In this paper, we evaluate a variety  of knowledge sources and supervised  learning algorithms for word sense  disambiguation on SENSEVAL-2 and  SENSEVAL-1 data. Our knowledge  sources include the part-of-speech of  neighboring words, single words in the  surrounding context, local collocations,  and syntactic relations. The learning algorithms  evaluated include Support Vector  Machines (SVM), Naive Bayes, AdaBoost,  and decision tree algorithms. We  present empirical results showing the relative  contribution of the component knowledge  sources and the different learning  algorithms. In particular, using all of  these knowledge sources and SVM (i.e.,  a single learning algorithm) achieves accuracy  higher than the best official scores  on both SENSEVAL-2 and SENSEVAL-1  test data","Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .","['Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', 'Knowledge sources used include partsof-speech, surrounding words, and local collocations.', 'This approach achieves state-of-the-art accuracy.', 'All accuracies reported in our experiments are micro-averages over all test examples.']",5,"['Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( #AUTHOR_TAG ) for our experiments , using the naive Bayes algorithm as our classifier .', 'Knowledge sources used include partsof-speech, surrounding words, and local collocations.']"
CCT211,P08-1101,"Regularized Structured Perceptron: A Case Study on Chinese Word Segmentation, POS Tagging and Parsing",chinese partofspeech tagging oneatatime or allatonce wordbased or characterbased,"['Hwee Tou Ng', 'Jin Kiat Low']",experiments,,We chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison .,"['The overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007).', 'Despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy.', 'One likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven.', 'However, the comparison is indirect because our partitions of the CTB corpus are different.', 'Shi and Wang (2007) also chunked the sentences before doing 10-fold cross validation, but used an uneven split.', 'We chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison .']",5,"['The overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007).', 'One likely reason is that Shi and Wang (2007) included knowledge about special characters and semantic knowledge from web corpora (which may explain the higher baseline accuracy), while our system is completely data-driven.', 'We chose to follow #AUTHOR_TAG and split the sentences evenly to facilitate further comparison .']"
CCT212,P08-1101,"Regularized Structured Perceptron: A Case Study on Chinese Word Segmentation, POS Tagging and Parsing",chinese segmentation with a wordbased perceptron algorithm,"['Yue Zhang', 'Stephen Clark']",experiments,,"We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .","['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .', 'We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).', 'The features used by the baseline segmentor are shown in Table 1.', 'The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.']",2,"['We built a two-stage baseline system , using the perceptron segmentation model from our previous work ( #AUTHOR_TAG ) and the perceptron POS tagging model from Collins ( 2002 ) .']"
CCT213,P10-1143,Unsupervised Event Coreference Resolution with Rich Linguistic Features,unsupervised coreference resolution in a nonparametric bayesian model,"['Aria Haghighi', 'Dan Klein']",method,"We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 MUC F1 measure on the MUC-6 test set, broadly in the range of some recent supervised results.",Our HDP extension is also inspired from the Bayesian model proposed by #AUTHOR_TAG .,"['We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L.', 'Our HDP extension is also inspired from the Bayesian model proposed by #AUTHOR_TAG .', 'However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008).']",4,"['Our HDP extension is also inspired from the Bayesian model proposed by #AUTHOR_TAG .', 'However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008).']"
CCT214,P10-2002,A Joint Rule Selection Model for Hierarchical Phrase-based Translation *,soft syntactic constraints for hierarchical phrasedbased translation,"['Yuval Marton', 'Philip Resnik']",method,"In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then nding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English.","These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .","['ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']",4,"['These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; #AUTHOR_TAG ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']"
CCT215,P10-2002,A Joint Rule Selection Model for Hierarchical Phrase-based Translation *,rich sourceside context for statistical machine translation,"['Kevin Gimpel', 'Noah A Smith']",method,"We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated. This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007). The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrase's translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chinese-to-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach.","These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .","['ME approach has the merit of easily combining different features to predict the probability of each class.', 'We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']",4,"['We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM.', 'These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; #AUTHOR_TAG ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .']"
CCT216,P10-2005,Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages,confidence measure for word alignment,['Fei Huang'],introduction,"In this paper we present a confidence mea-sure for word alignment based on the posterior probability of alignment links. We introduce sentence alignment confi-dence measure and alignment link con-fidence measure. Based on these mea-sures, we improve the alignment qual-ity by selecting high confidence sentence alignments and alignment links from mul-tiple word alignments of the same sen-tence pair. Additionally, we remove low confidence alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and sig-nificantly reduces the phrase translation table size.","More recently , an alignment selection approach was proposed in ( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .","['More recently , an alignment selection approach was proposed in ( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model).', 'In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function.', 'There is no need for a pre-determined threshold as used in (Huang, 2009).', 'Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners.', 'Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.']",1,"['More recently , an alignment selection approach was proposed in ( #AUTHOR_TAG ) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold .', 'Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.']"
CCT217,P10-2019,Chinese semantic role labeling with shallow parsing,accurate unlexicalized parsing,"['Dan Klein', 'Christopher D Manning']",,"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.","Inspired by ( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent .","['We introduce two types of chunks.', 'The first is simply the phrase type, such as NP, PP, of current chunk.', 'The column CHUNK 1 illustrates this kind of chunk type definition.', 'The second is more complicated.', ""Inspired by ( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", 'For example, an NP immediately dominated by a S, will be substituted by NPˆS.', 'This strategy severely increases the number of chunk types and make it hard to train chunking models.', 'To shrink this number, we linguistically use a cluster of CTB phrasal types, which was introduced in .', 'The column CHUNK 2 illustrates this definition.', 'E.g., NPˆS implicitly represents Subject while NPˆVP represents Object.']",4,"['We introduce two types of chunks.', ""Inspired by ( #AUTHOR_TAG ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent ."", 'This strategy severely increases the number of chunk types and make it hard to train chunking models.', 'To shrink this number, we linguistically use a cluster of CTB phrasal types, which was introduced in .']"
CCT218,P10-2026,Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation,comparison of extended lexicon models in search and rescoring for smt,"['Saˇsa Hasan', 'Hermann Ney']",related work,We show how the integration of an extended lexicon model into the decoder can improve translation performance. The model is based on lexical triggers that capture long-distance dependencies on the sentence level. The results are compared to variants of the model that are applied in reranking of n-best lists. We present how a combined application of these models in search and rescoring gives promising results. Experiments are reported on the GALE Chinese-English task with improvements of up to +0.9% BLEU and -1.5% TER absolute on a competitive baseline.,The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #AUTHOR_TAG ) .,"['The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #AUTHOR_TAG ) .', 'Hasan and Ney ( 2009) introduced a second word to trigger the target word without considering any linguistic information.', 'Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'However, as the size of the corpus increases, the maximum entropy model will become larger.', 'Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection.', 'Taking the dependency edge as condition, our approach is very different from previous approaches of exploring context information.']",4,"['The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( #AUTHOR_TAG ) .', 'Furthermore, since the second word can come from any part of the sentence, there may be a prohibitively large number of parameters involved.', 'Besides, He et al. (2008) built a maximum entropy model which combines rich context information for selecting translation rules during decoding.', 'Similarly, In (Shen et al., 2009), context language model is proposed for better rule selection.']"
CCT219,W06-1639,Get out the vote,mining newsgroups using networks arising from social behavior,"['R Agrawal', 'S Rajagopalan', 'R Srikant', 'Y Xu']",related work,"Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to "" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text","Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .","['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']",0,"['Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks ( #AUTHOR_TAG ) .']"
CCT220,W06-1639,Get out the vote,extracting policy positions from political texts using words as data american political science review,"['M Laver', 'K Benoit', 'J Garry']",related work,,"There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( #AUTHOR_TAG ; Efron , 2004 ; Mullen and Malouf , 2006 ) .']"
CCT221,W06-1639,Get out the vote,maxmargin markov networks,"['B Taskar', 'C Guestrin', 'D Koller']",related work,"In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"
CCT222,W06-1639,Get out the vote,cultural orientation classifying subjective documents by cociation sic analysis,['M Efron'],related work,"This paper introduces a simple method for estimating cultural orientation, the affiliations of hypertext documents in a polarized field of discourse. Using a probabilistic model based on cocitation information, two experiments are reported. The first experiment tests the modeli? 1/2 s ability to discriminate between left- and right-wing documents about politics. In this context the model is tested on two sets of data, 695 partisan web documents, and 162 political weblogs. Accuracy above 90% is obtained from the cocitation model, outperforming lexically based classifiers at statistically significant levels. In the second experiment, the proposed method is used to classify the home pages of musical artists with respect to their mainstream or ""alternative"" appeal. For musical artists the model is tested on a set of 515 artist home pages, achieving 88% accuracy.","There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; #AUTHOR_TAG ; Mullen and Malouf , 2006 ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']"
CCT223,W06-1639,Get out the vote,transductive learning via spectral graph partitioning,['T Joachims'],related work,"We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case.","Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and #AUTHOR_TAG .","['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and #AUTHOR_TAG .', 'Zhu (2005) maintains a survey of this area.']",0,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and #AUTHOR_TAG .']"
CCT224,W06-1639,Get out the vote,iterative classification in relational data,"['J Neville', 'D Jensen']",related work,"Relational data offer a unique opportunity for improving the classification accuracy of statistical models. If two objects are related, inferring something about one object can aid inferences about the other. We present an iterative classification procedure that exploits this characteristic of relational data. This approach uses simple Bayesian classifiers in an iterative fashion, dynamically updating the attributes of some objects as inferences are made about related objects. Inferences made with high confidence in initial iterations are fed back into the data and are used to strengthen subsequent inferences about related objects. We evaluate the performance of iterative classification on a corporate dataset, using a binary classification task. Experiments indicate that iterative classification significantly increases accuracy when compared to a single-pass approach. 1","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( #AUTHOR_TAG ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .']"
CCT225,W06-1639,Get out the vote,detection of agreement vs disagreement in meetings training with unlabeled data,"['D Hillard', 'M Ostendorf', 'E Shriberg']",related work,"To support summarization of automatically transcribed meetings, we introduce a classifier to recognize agreement or disagreement utterances, utilizing both word-based and prosodic cues. We show that hand-labeling efforts can be minimized by using unsupervised training on a large unlabeled data set combined with supervised training on a small amount of data. For ASR transcripts with over 45% WER, the system recovers nearly 80% of agree/disagree utterances with a confusion rate of only 3%.","More sophisticated approaches have been proposed ( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .","['More sophisticated approaches have been proposed ( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .', 'Also relevant is work on the gen- eral problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002).']",0,"['More sophisticated approaches have been proposed ( #AUTHOR_TAG ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( Galley et al. , 2004 ) .', 'Also relevant is work on the gen- eral problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002).']"
CCT226,W06-1639,Get out the vote,the theory and practice of discourse parsing and summarization,['D Marcu'],related work,"From the Publisher:  Until now, most discourse researchers have assumed that full semantic understanding is necessary to derive the discourse structure of texts. This book documents the first serious attempt to construct automatically and use nonsemantic computational structures for text summarization. Daniel Marcu develops a semantics-free theoretical framework that is both general enough to be applicable to naturally occurring texts and concise enough to facilitate an algorithmic approach to discourse analysis. He presents and evaluates two discourse parsing methods: one uses manually written rules that reflect common patterns of usage of cue phrases such as ""however"" and ""in addition to""; the other uses rules that are learned automatically from a corpus of discourse structures. By means of a psycholinguistic experiment, Marcu demonstrates how a discourse-based summarizer identifies the most important parts of texts at levels of performance that are close to those of humans.  Marcu also discusses how the automatic derivation of discourse structures may be used to improve the performance of current natural language generation, machine translation, summarization, question answering, and information retrieval systems.","Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 ) .","['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 ) .']",0,"['Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( #AUTHOR_TAG ; Teufel and Moens , 2002 ) .']"
CCT227,W06-1639,Get out the vote,multidimensional text analysis for erulemaking,"['N Kwon', 'S Shulman', 'E Hovy']",related work,"To support rule-writers, we are developing techniques to automatically analyze large number of public comments on proposed regulations. A document is analyzed in various ways including argument structure, topics, and opinions. The individual results are integrated into a unified output. The experiments reported here were performed on comments submitted to the Environmental Protection Agency in response to their proposed rule for mercury regulation.","Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; Cardie et al. , 2006 ; #AUTHOR_TAG ) .']"
CCT228,W06-1639,Get out the vote,mining newsgroups using networks arising from social behavior,"['R Agrawal', 'S Rajagopalan', 'R Srikant', 'Y Xu']",introduction,"Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to "" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text","For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cfXXX #AUTHOR_TAG.","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document in- dependently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented docu- ments, however, can be linked through certain re- lationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cfXXX #AUTHOR_TAG.', 'Agreement evidence can be a powerful aid in our classification task: for ex- ample, we can easily categorize a complicated (or overly terse) document if we find within it indica- tions of agreement with a clearly positive text.']",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document in- dependently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented docu- ments, however, can be linked through certain re- lationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement be-tween two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cfXXX #AUTHOR_TAG.', 'Agreement evidence can be a powerful aid in our classification task: for ex- ample, we can easily categorize a complicated (or overly terse) document if we find within it indica- tions of agreement with a clearly positive text.']"
CCT229,W06-1639,Get out the vote,a preliminary investigation into sentiment analysis of informal political discourse,"['T Mullen', 'R Malouf']",related work,"With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each","There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) ( Laver et al. , 2003 ; Efron , 2004 ; #AUTHOR_TAG ) .']"
CCT230,W06-1639,Get out the vote,thumbs up sentiment classification using machine learning techniques,"['B Pang', 'L Lee', 'S Vaithyanathan']",method,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.","Fol- lowing standard practice in sentiment analysis ( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.","['In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5', 'Fol- lowing standard practice in sentiment analysis ( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.', 'The ind value for each speech segment s was based on the signed distanceds fromthevectorrepresentingstothe trained SVM decision plane: \x0e � ds \x0e23�4s� d s �23�4s� ds ��23�4s def ds = \x0e+ 23�4s 2 ind s;Y where 3�4s is the standard deviation of d s over all speech segments s in the debate in question, and def ind s;N = \x0e�ind s;Y .']",5,"['Fol- lowing standard practice in sentiment analysis ( #AUTHOR_TAG ) , the input to SVMlight con- sisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.']"
CCT231,W06-1639,Get out the vote,automated classification of congressional legislation,"['S Purpura', 'D Hillard']",related work,"For social science researchers, content analysis and classification of United States Congressional legislative activities have been time consuming and costly. The Library of Congress THOMAS system provides detailed information about bills and laws, but its classification system, the Legislative Indexing Vocabulary (LIV), is geared toward information retrieval instead of the pattern or historical trend recognition that social scientists value. The same event (a bill) may be coded with many subjects at the same time, with little indication of its primary emphasis. In addition, because the LIV system has not been applied to other activities, it cannot be used to compare (for example) legislative issue attention to executive, media, or public issue attention.This paper presents the Congressional Bills Project's (www.congressionalbills.org) automated classification system. This system applies a topic spotting classification algorithm to the task of coding legislative activities into one of 226 subtopic areas. The algorithm uses a traditional bag-of-words document representation, an extensive set of human coded examples, and an exhaustive topic coding system developed for use by the Congressional Bills Project and the Policy Agendas Project (www.policyagendas.org). Experimental results demonstrate that the automated system is about as effective as human assessors, but with significant time and cost savings. The paper concludes by discussing challenges to moving the system into operational use.","Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG ) .","['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG ) .']",0,"['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( Yang and Callan , 2005 ; #AUTHOR_TAG ) .']"
CCT232,W06-1639,Get out the vote,conditional models of identity uncertainty with application to noun coreference,"['A McCallum', 'B Wellner']",related work,"Coreference analysis, also known as record linkage or identity uncer-tainty, is a difficult and important problem in natural language process-ing, databases, citation matching and many other tasks. This paper intro-duces several discriminative, conditional-probability models for coref-erence analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational--they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies--paralleling the advantages of con-ditional random fields over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; #AUTHOR_TAG ) .']"
CCT233,W06-1639,Get out the vote,thumbs up or thumbs down semantic orientation applied to unsupervised classification of reviews,['P Turney'],introduction,"This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., ""subtle nuances"") and a negative semantic orientation when it has bad associations (e.g., ""very cavalier""). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word ""excellent"" minus the mutual information between the given phrase and the word ""poor"". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.","In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 ) .""]",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; #AUTHOR_TAG ; Dave et al. , 2003 ) .""]"
CCT234,W06-1639,Get out the vote,sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified,"['A Agarwal', 'P Bhattacharyya']",related work,"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given.","Previous sentiment-analysis work in different domains has considered inter-document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links (Agrawal et al., 2003).","['Previous sentiment-analysis work in different domains has considered inter-document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links (Agrawal et al., 2003).']",0,"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyper- links (Agrawal et al., 2003).']"
CCT235,W06-1639,Get out the vote,conditional random fields probabilistic models for segmenting and labeling sequence data,"['J Lafferty', 'A McCallum', 'F Pereira']",related work,"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be inter- esting to investigate the application of such meth- ods to our problem.', 'However, we also believe that our approach has important advantages, in- cluding conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; #AUTHOR_TAG ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .']"
CCT236,W06-1639,Get out the vote,evaluation of machine learning methods for natural language processing tasks,"['W Daelemans', 'V Hoste']",method,"We show that the methodology currently in use for comparing symbolic supervised learning methods applied to human language technology tasks is unreliable. We show that the interaction between algorithm parameter settings and feature selection within a single algorithm often accounts for a higher variation in results than differences between different algorithms or information sources. We illustrate this with experiments on a number of linguistic datasets. The consequences of this phenomenon are far-reaching, and we discuss possible solutions to this methodological problem.","Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 ) .","['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 ) .']",3,"['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( #AUTHOR_TAG ; Munson et al. , 2005 ) .']"
CCT237,W06-1639,Get out the vote,analyzing research papers using citation sentences,"['W Lehnert', 'C Cardie', 'E Riloff']",related work,,"Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( #AUTHOR_TAG ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .","['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( #AUTHOR_TAG ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .']",0,"['Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( #AUTHOR_TAG ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .']"
CCT238,W06-1639,Get out the vote,seeing stars exploiting class relationships for sentiment categorization with respect to rating scales,"['B Pang', 'L Lee']",introduction,"We address the rating-inference problem, wherein rather than simply decide whether a review is ""thumbs up"" or ""thumbs down"", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five ""stars""). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, ""three stars"" is intuitively closer to ""four stars"" than to ""one star"".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.","A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.']"
CCT239,W06-1639,Get out the vote,sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified,"['A Agarwal', 'P Bhattacharyya']",introduction,"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given.","A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( #AUTHOR_TAG ; Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .']"
CCT240,W06-1639,Get out the vote,seeing stars when there aren’t many stars graphbased semisupervised learning for sentiment categorization,"['A B Goldberg', 'J Zhu']",introduction,"We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., ""4 stars""), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.","A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.', 'For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data.', ""Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available.""]",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) .']"
CCT241,W06-1639,Get out the vote,learning associative markov networks,"['B Taskar', 'V Chatalbashev', 'D Koller']",related work,"Markov networks are extensively used to model complex sequential, spatial, and relational interactions in fields as diverse as image processing, natural language analysis, and bioinformatics. However, inference and learning in general Markov networks is intractable. In this paper, we focus on learning a large subclass of such models (called associative Markov networks) that are tractable or closely approximable. This subclass contains networks of discrete variables with K labels each and clique potentials that favor the same labels for all variables in the clique. Such networks capture the ""guilt by association "" pattern of reasoning present in many domains, in which connected (""associated"") variables tend to have the same label. Our approach exploits a linear programming relaxation for the task of finding the best joint assignment in such networks, which provides an approximate quadratic program (QP) for the problem of learning a marginmaximizing Markov network. We show that for associative Markov network over binary-valued variables, this approximate QP is guaranteed to return an optimal parameterization for Markov networks of arbitrary topology. For the nonbinary case, optimality is not guaranteed, but the relaxation produces good solutions in practice. Experimental results with hypertext and newswire classification show significant advantages over standard approaches. 1","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; #AUTHOR_TAG ; McCallum and Wellner , 2004 ) .']"
CCT242,W06-1639,Get out the vote,identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies,"['M Galley', 'K McKeown', 'J Hirschberg', 'E Shriberg']",related work,"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.","More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ) .","['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ) .', 'Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002).']",0,"['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed ( Hillard et al. , 2003 ) , including an extension that , in an interesting reversal of our problem , makes use of sentimentpolarity indicators within speech segments ( #AUTHOR_TAG ) .', 'Also relevant is work on the general problems of dialog-act tagging (Stolcke et al., 2000), citation analysis (Lehnert et al., 1990), and computational rhetorical analysis (Marcu, 2000;Teufel and Moens, 2002).']"
CCT243,W06-1639,Get out the vote,a sentimental education sentiment analysis using subjectivity summarization based on minimum cuts,"['B Pang', 'L Lee']",method,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.","As has been previously observed and exploited in the NLP literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","['As has been previously observed and exploited in the NLP literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']",1,"['As has been previously observed and exploited in the NLP literature ( #AUTHOR_TAG ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"
CCT244,W06-1639,Get out the vote,discriminative probabilistic models for relational data,"['B Taskar', 'P Abbeel', 'D Koller']",related work,"In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; #AUTHOR_TAG ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.']"
CCT245,W06-1639,Get out the vote,electronic rulemaking new frontiers in public participation prepared for the annual meeting of the american political science association,"['S Shulman', 'D Schlosberg']",introduction,,"In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #AUTHOR_TAG ) .","[""In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #AUTHOR_TAG ) ."", 'Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics 2 , or at least on political journalism 3 .', 'Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process.']",0,"[""In the United States , for example , governmental bodies are providing and soliciting political documents via the Internet , with lofty goals in mind : electronic rulemaking ( eRulemaking ) initiatives involving the `` electronic collection , distribution , synthesis , and analysis of public commentary in the regulatory rulemaking process '' , may `` [ alter ] the citizen-government relationship '' ( #AUTHOR_TAG ) .""]"
CCT246,W06-1639,Get out the vote,yahoo for amazon extracting market sentiment from stock message boards,"['S Das', 'M Chen']",introduction,,"In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .']",0,"['In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( #AUTHOR_TAG ; Pang et al. , 2002 ; Turney , 2002 ; Dave et al. , 2003 ) .']"
CCT247,W06-1639,Get out the vote,semisupervised learning literature survey computer sciences,['J Zhu'],related work,,#AUTHOR_TAG maintains a survey of this area .,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor andLafferty (2002), andJoachims (2003).', '#AUTHOR_TAG maintains a survey of this area .']",0,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al. (2002), Kondor andLafferty (2002), andJoachims (2003).', '#AUTHOR_TAG maintains a survey of this area .']"
CCT248,W06-1639,Get out the vote,seeing stars exploiting class relationships for sentiment categorization with respect to rating scales,"['B Pang', 'L Lee']",related work,"We address the rating-inference problem, wherein rather than simply decide whether a review is ""thumbs up"" or ""thumbs down"", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five ""stars""). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, ""three stars"" is intuitively closer to ""four stars"" than to ""one star"".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.","Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).","['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']",0,"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ; Goldberg and Zhu , 2006 ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']"
CCT249,W06-1639,Get out the vote,on the computation of point of view,['W Sack'],introduction,"Previous work in AI story understanding has largely been used to build tools which can summarize stories and categorize them according to the events they describe (e.g., the technologies developed for the Message Understanding Conferences). These sorts of technologies are built around the assumptions that (1) events reported as facts in news stories should be ""understood"" as facts; (2) the style of a story, i.e., the way in which a story is told, is not of interest; and, (3) the source of a story should not influence its analysis. These assumptions are obviously unrealistic. Everyone knows that one should not believe everything in the news. But, by making these simplifying assumptions most existing story understanding systems function as gullible ""readers."" The focus of my current research is to build a less gullible story understander by encoding in it a means to recognize point of view. The techniques that I am developing will be useful, not only for information retrieval tasks which demand a search for credible stories, but also in future entertainment technologies which will be capable of finding and then assembling together into a unified presentation a set of texts or video clips to tell a story from an ensemble of points of view.","Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , #AUTHOR_TAG , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , #AUTHOR_TAG , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , #AUTHOR_TAG , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']"
CCT250,W06-1639,Get out the vote,sentiment analysis a new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified,"['A Agarwal', 'P Bhattacharyya']",method,"Sentiment Analysis aims at determining the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based approach to this problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet1 synonymy graphs. A method to improve the accuracy of classification over a set of test documents is finally given.","As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']",1,"['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; #AUTHOR_TAG ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"
CCT251,W06-1639,Get out the vote,learning from labeled and unlabeled data using graph mincuts,"['A Blum', 'S Chawla']",method,"Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.","Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .","['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .', 'In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.']",5,"['The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation.', 'Our classification framework , directly inspired by #AUTHOR_TAG , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .']"
CCT252,W06-1639,Get out the vote,nearduplicate detection for erulemaking,"['H Yang', 'J Callan']",related work,,"Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 ) .","['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 ) .']",0,"['Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( #AUTHOR_TAG ; Purpura and Hillard , 2006 ) .']"
CCT253,W06-1639,Get out the vote,tracking point of view in narrative,['J M Wiebe'],introduction,"Third-person fictional narrative text is composed not only of passages that objectively narrate events, but also of passages that present characters' thoughts, perceptions, and inner states. Such passages take a character's psychological point of view. A language understander must determine the current psychological point of view in order to distinguish the beliefs of the characters from the facts of the story, to correctly attribute beliefs and other attitudes to their sources, and to understand the discourse relations among sentences. Tracking the psychological point of view is not a trivial problem, because many sentences are not explicitly marked for point of view, and whether the point of view of a sentence is objective or that of a character (and if the latter, which character it is) often depends on the context in which the sentence appears. Tracking the psychological point of view is the problem addressed in this work. The approach is to seek, by extensive examinations of naturally occurring narrative, regularities in the ways that authors manipulate point of view, and to develop an algorithm that tracks point of view on the basis of the regularities found. This paper presents this algorithm, gives demonstrations of an implemented system, and describes the results of some preliminary empirical studies, which lend support to the algorithm.","Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , Hearst ( 1992 ) , Sack ( 1994 ) , and #AUTHOR_TAG ; see Esuli ( 2006 ) for an active bibliography ) .']"
CCT254,W06-1639,Get out the vote,learning from labeled and unlabeled data using graph mincuts,"['A Blum', 'S Chawla']",related work,"Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.","Notable early papers on graph-based semisupervised learning include #AUTHOR_TAG , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .","['Notable early papers on graph-based semisupervised learning include #AUTHOR_TAG , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']",0,"['Notable early papers on graph-based semisupervised learning include #AUTHOR_TAG , Bansal et al. ( 2002 ) , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .']"
CCT255,W06-1639,Get out the vote,on the collective classification of email “speech acts”,"['V Carvalho', 'W W Cohen']",related work,"We consider classification of email messages as to whether or not they contain certain ""email acts"", such as a request or a commitment. We show that exploiting the sequential correlation among email messages in the same thread can improve email-act classification. More specifically, we describe a new text-classification algorithm based on a dependency-network based collective classification method, in which the local classifiers are maximum entropy models based on words and certain relational features. We show that statistically significant improvements over a bag-of-words baseline classifier can be obtained for some, but not all, email-act classes. Performance improvements obtained by collective classification appears to be consistent across many email acts suggested by prior speech-act theory.","Relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .","['We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work.', 'Relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .']",0,"['Relationships between the unlabeled items #AUTHOR_TAG consider sequential relations between different types of emails ( e.g. , between requests and satisfactions thereof ) to classify messages , and thus also explicitly exploit the structure of conversations .']"
CCT256,W06-1639,Get out the vote,the american congress,"['S S Smith', 'J M Roberts', 'R J Vander Wielen']",introduction,,"People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ) .","[""Evaluative and persuasive documents, such as a politician's speech regarding a bill or a blogger's commentary on a legislative proposal, form a particularly interesting type of politically oriented text."", 'People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ) .', 'Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist.']",0,"['People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( #AUTHOR_TAG ) .']"
CCT257,W06-1639,Get out the vote,collective content selection for concepttotext generation,"['R Barzilay', 'M Lapata']",method,"A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.","As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .","['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']",1,"['As has been previously observed and exploited in the NLP literature ( Pang and Lee , 2004 ; Agarwal and Bhattacharyya , 2005 ; #AUTHOR_TAG ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .', 'In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.']"
CCT258,W06-1639,Get out the vote,thumbs up sentiment classification using machine learning techniques,"['B Pang', 'L Lee', 'S Vaithyanathan']",introduction,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.","In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 ) .""]",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; #AUTHOR_TAG ; Turney , 2002 ; Dave et al. , 2003 ) .""]"
CCT259,W06-1639,Get out the vote,correlation clustering,"['N Bansal', 'A Blum', 'S Chawla']",related work,"In this paper, we introduce and study the Robust-Correlation-Clustering problem: given a graph G = (V,E) where every edge is either labeled + or - (denoting similar or dissimilar pairs of vertices), and a parameter m, the goal is to delete a set D of m vertices, and partition the remaining vertices V  D into clusters to minimize the cost of the clustering, which is the sum of the number of + edges with end-points in different clusters and the number of - edges with end-points in the same cluster. This generalizes the classical Correlation-Clustering problem which is the special case when m = 0. Correlation clustering is useful when we have (only) qualitative information about the similarity or dissimilarity of pairs of points, and Robust-Correlation-Clustering equips this model with the capability to handle noise in datasets. In this work, we present a constant-factor bi-criteria algorithm for Robust-Correlation-Clustering on complete graphs (where our solution is O(1)-approximate w.r.t the cost while however discarding O(1) m points as outliers), and also complement this by showing that no finite approximation is possible if we do not violate the outlier budget. Our algorithm is very simple in that it first does a simple LP-based pre-processing to delete O(m) vertices, and subsequently runs a particular Correlation-Clustering algorithm ACNAlg [Ailon et al., 2005] on the residual instance. We then consider general graphs, and show (O(log n), O(log^2 n)) bi-criteria algorithms while also showing a hardness of alpha_MC on both the cost and the outlier violation, where alpha_MC is the lower bound for the Minimum-Multicut problem","Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , #AUTHOR_TAG , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .","['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , #AUTHOR_TAG , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']",0,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , #AUTHOR_TAG , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']"
CCT260,W06-1639,Get out the vote,mining the peanut gallery opinion extraction and semantic classification of product reviews,"['K Dave', 'S Lawrence', 'D M Pennock']",introduction,"The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.","In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG ) .""]",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography).', ""In particular , since we treat each individual speech within a debate as a single `` document '' , we are considering a version of document-level sentiment-polarity classification , namely , automatically distinguishing between positive and negative documents ( Das and Chen , 2001 ; Pang et al. , 2002 ; Turney , 2002 ; #AUTHOR_TAG ) .""]"
CCT261,W06-1639,Get out the vote,using natural language processing to improve erulemaking,"['C Cardie', 'C Farina', 'T Bruce', 'E Wagner']",related work,"This paper describes in brief Cornell's interdisciplinary eRulemaking project that was recently funded (December, 2005) by the National Science Foundation.","Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( Shulman et al. , 2005 ; #AUTHOR_TAG ; Kwon et al. , 2006 ) .']"
CCT262,W06-1639,Get out the vote,a computational theory of perspective and reference in narrative,"['J M Wiebe', 'W J Rapaport']",introduction,"William J. end Shapiro, Stuart C. (1984), &quot;Quasi-lndexical Reference in Propositional Semantic Networks, &quot; Proceedings of the loth International Conference on Computational Linguistics ( COLING-84 ; Stanford Univ.) (Morristown, NJ: Assoc. for Computational Linguistics): 65-70.  Rapaport, William J. (1986), &quot;Logical Foundations for Belief Representation,&quot; Cognitiv e Science 10: 371-422.  Reiser, Brian J. (1981), &quot;Character Tracking and the Understanding of Narrative,&quot; Proceedings of the 7th International  Joint Conference on Artificial Intelligence (IJCAI-81; Van. couver) (Los Altos, CA: Morgen Kanhmmn): 209-211.  Roach, Eleanor and Lloyd, B.B. (1978). Cognition and Categorization (Hillsdale, NJ: Lawrence Erlbaum Associ- ates).  Shapiro, Smart C. (1979). &quot;The SNePS Sementic Network Processing System,&quot; in N.V. Findlet (ed.), Associative Network. v (New York: Academic): 179-203.  Shapiro, Stuart C. end Rapaport, William J. (1987). &quot;SNePS Considered as a Fully Intensional Propositional","Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #AUTHOR_TAG , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #AUTHOR_TAG , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes #AUTHOR_TAG , Hearst ( 1992 ) , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']"
CCT263,W06-1639,Get out the vote,learning probabilistic models of relational structure,"['L Getoor', 'N Friedman', 'D Koller', 'B Taskar']",related work,"Most real-world data is stored in relational form. In contrast, most statistical learning methods work with ""flat"" data representations, forcing us to convert our data into a form that loses much of the relational structure. The recently introduced framework of probabilistic relational models (PRMs) allows us to represent probabilistic models over multiple entities that utilize the relations between them. In this paper, we propose the use of probabilistic models not only for the attributes in a relational model, but for the relational structure itself. We propose two mechanisms for modeling structural uncertainty: reference uncertainty and existence uncertainty. We describe the appropriate conditions for using each model and present learning algorithms for each. We present experimental results showing that the learned models can be used to predict relational structure and, moreover, the observed relational structure can be used to provide better predictions for the attributes in the model.","Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .","['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']",0,"['Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( Neville and Jensen , 2000 ; Lafferty et al. , 2001 ; #AUTHOR_TAG ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .', 'It would be interesting to investigate the application of such methods to our problem.', 'However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve.']"
CCT264,W06-1639,Get out the vote,diffusion kernels on graphs and other discrete input spaces,"['R I Kondor', 'J D Lafferty']",related work,"The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space.","Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , #AUTHOR_TAG , and Joachims ( 2003 ) .","['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , #AUTHOR_TAG , and Joachims ( 2003 ) .', 'Zhu (2005) maintains a survey of this area.']",0,"['Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , Bansal et al. ( 2002 ) , #AUTHOR_TAG , and Joachims ( 2003 ) .']"
CCT265,W06-1639,Get out the vote,summarizing scientific articles experiments with relevance and rhetorical status,"['S Teufel', 'M Moens']",related work,"In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work. We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges' agreement on these annotations. We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field.","Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG ) .","['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'More sophisticated approaches have been proposed (Hillard et al., 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al., 2004).', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG ) .']",0,"['We used a simple method to learn to identify cross-speaker references indicating agreement.', 'Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; #AUTHOR_TAG ) .']"
CCT266,W06-1639,Get out the vote,coupling niche browsers and affect analysis for an opinion mining application,"['G Grefenstette', 'Y Qu', 'J G Shanahan', 'D A Evans']",related work,"Newspapers generally attempt to present the news objectively. But textual affect analysis shows that many words carry positive or negative emotional charge. In this article, we show that coupling niche browsing technology and affect analysis technology allows us to create a new application that measures the slant in opinion given to public figures in the popular press.","An exception is #AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is #AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005;Cardie et al., 2006;Kwon et al., 2006).', 'An exception is #AUTHOR_TAG , who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site .']"
CCT267,W06-1639,Get out the vote,optimizing to arbitrary nlp metrics using ensemble selection,"['A Munson', 'C Cardie', 'R Caruana']",method,"While there have been many successful applications of machine learning methods to tasks in NLP, learning algorithms are not typically designed to optimize NLP performance metrics. This paper evaluates an ensemble selection framework designed to optimize arbitrary metrics and automate the process of algorithm selection and parameter tuning. We report the results of experiments that instantiate the framework for three NLP tasks, using six learning algorithms, a wide variety of parameterizations, and 15 performance metrics. Based on our results, we make recommendations for subsequent machine-learning-based research for natural language learning.","Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .","['SVMlight is available at svmlight.joachims.org.', 'Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']",3,"['Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( Daelemans and Hoste , 2002 ; #AUTHOR_TAG ) .']"
CCT268,W06-1639,Get out the vote,language processing technologies for electronic rulemaking a project highlight,"['S Shulman', 'J Callan', 'E Hovy', 'S Zavestoski']",related work,"In this project, we are developing new text processing tools that help people perform advanced analysis of large collections of text commentary. This problem is increasingly faced by the United States federal government's regulation writers who formulate the rules and regulations that define the details of laws enacted by Congress. Our research focuses on text clustering, text searching using information retrieval, near-duplicate detection, opinion identification, stakeholder characterization, and extractive summarization, as well as the impact of such tools on the process of rulemaking itself. Versions of a Rule-Writer's Workbench will be built by Computer Science researchers at ISI and CMU, deployed annually for experimental use by our government partners, and evaluated by social science researchers from the Library and Information Science and Sociology departments at the Universities of Pittsburgh and San Francisco respectively. This three-year project started in October 2004 and is funded under the National Science Foundation's Digital Government program.","Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .","['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .', 'There has also been work focused upon determining the political leaning (e.g., ""liberal"" vs. ""conservative"") of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the ""unlabeled"" texts) (Laver et al., 2003;Efron, 2004;Mullen and Malouf, 2006).', 'An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.']",0,"['Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking , allowing the automatic analysis of the opinions that people submit ( #AUTHOR_TAG ; Cardie et al. , 2006 ; Kwon et al. , 2006 ) .']"
CCT269,W06-1639,Get out the vote,a preliminary investigation into sentiment analysis of informal political discourse,"['T Mullen', 'R Malouf']",introduction,"With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each","For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .","['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006).', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .', 'Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.']",0,"['Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.', 'Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions.', 'For example, we may find textual4 evidence of a high likelihood of agreement between two speakers, such as explicit assertions (�I second that!�) or quotation of messages in emails or postings ( see #AUTHOR_TAG but cfXXX Agrawal et al. ( 2003 ) ) .']"
CCT270,W06-1639,Get out the vote,seeing stars when there aren’t many stars graphbased semisupervised learning for sentiment categorization,"['A B Goldberg', 'J Zhu']",related work,"We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., ""4 stars""), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.","Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).","['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']",0,"['Previous sentiment-analysis work in different domains has considered inter-document similarity ( Agarwal and Bhattacharyya , 2005 ; Pang and Lee , 2005 ; #AUTHOR_TAG ) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).']"
CCT271,W06-1639,Get out the vote,directionbased text interpretation as an information access refinement,['M Hearst'],introduction,,"Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .","['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .', 'In particular, since we treat each individual speech within a debate as a single ""document"", we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001;Pang et al., 2002;Turney, 2002;Dave et al., 2003).']",0,"['Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis , an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language ( early work includes Wiebe and Rapaport ( 1988 ) , #AUTHOR_TAG , Sack ( 1994 ) , and Wiebe ( 1994 ) ; see Esuli ( 2006 ) for an active bibliography ) .']"
CCT272,W10-1758,From “disciplined subjectivity” to “taming wild thoughts”: Bion's elaboration of the analysing instrument,online largemargin training of syntactic and structural translation features,"['David Chiang', 'Yuval Marton', 'Philis Resnik']",conclusion,"Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train a large number of Marton and Resnik's soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 Bleu on a subset of the NIST 2006 Arabic-English evaluation data.",Our plan is to implement a windowed or moving-average version of BLEU as in ( #AUTHOR_TAG ) .,"['We have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'When the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'For example, when using BLEU, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single-sentence objective functions.', 'Our plan is to implement a windowed or moving-average version of BLEU as in ( #AUTHOR_TAG ) .']",3,"['We have thus far implemented two objective functions which operate on individual sentences without regard for choices made on other sentences.', 'When the final evaluation metric incorporates global statistics, however, an objective function which takes them into account is desirable.', 'For example, when using BLEU, it makes a big difference whether individual sentences are both longer and shorter than the reference or systematically shorter than the reference, but these two cases can not be distinguished by single-sentence objective functions.', 'Our plan is to implement a windowed or moving-average version of BLEU as in ( #AUTHOR_TAG ) .']"
CCT273,W10-2910,The effect of syntactic representation on semantic role labeling,effective use of wordnet semantics via kernelbased learning,"['Roberto Basili', 'Marco Cammisa', 'Alessandro Moschitti']",conclusion,"Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.    In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available.","The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .","['The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', '(Basili et al., 2005;Bloehdorn et al., 2006).', 'However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a;Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy.', '(ii) The latter can be further boosted by studying complex structural kernels, e.g.', '(Moschitti, 2008;Nguyen et al., 2009;Dinarelli et al., 2009).', '(iii) More specific predicate argument structures such those proposed in FrameNet, e.g.', '(Baker et al., 1998;Giuglea and Moschitti, 2004;Giuglea and Moschitti, 2006;Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context.', 'Finally, while the strategy based on reranking resulted in a significant performance boost, it remains to be seen whether a higher accuracy can be achieved by developing a more sophisticated inference algorithm based on dynamic programming.', 'However, while the development of such an algorithm is an interesting problem, it will not necessarily result in a more usable system -when using a reranker, it is easy to trade accuracy for efficiency.']",0,"['The flexible architecture we have presented enables interesting future research : ( i ) a straightforward improvement is the use of lexical similarity to reduce data sparseness , e.g. ( #AUTHOR_TAG ; Basili et al. , 2006 ; Bloehdorn et al. , 2006 ) .', 'However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy.', 'Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a;Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy.', '(ii) The latter can be further boosted by studying complex structural kernels, e.g.']"
CCT274,W10-4005,Identifying word translations in non-parallel texts,utilizing citations of foreign words in corpusbased dictionary generation,"['Reinhard Rapp', 'Michael Zock']",experiments,"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations.",This contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .,"['The right column in Table 1 shows the scores if (using the product-of-ranks algorithm) four source languages are taken into account in parallel.', 'As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal.', 'This contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .', 'So this casts some doubt on these.', 'However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement.', 'This is not the case here, where we try to improve on a score of around 50 for English.', 'Remember that this is a somewhat conservative score as we count correct but alternative translations, as errors.', 'As this is already a performance much closer to the optimum, making further performance gains is more difficult.', 'Therefore, perhaps we should take it as a success that the product-of-ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non-English languages was probably mostly detrimental.']",1,['This contrasts with the findings described in #AUTHOR_TAG where significant improvements could be achieved by increasing the number of source languages .']
CCT275,W10-4005,Identifying word translations in non-parallel texts,utilizing citations of foreign words in corpusbased dictionary generation,"['Reinhard Rapp', 'Michael Zock']",introduction,"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations.","As suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .","['So far, we always computed translations to single source words.', 'However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm.', 'As suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'So for each candidate we obtain a product of ranks.', 'We then assume that the candidate with the smallest product will be the best translation. 3', 'et us illustrate this by an example: If the given words are the variants of the word nervous in English, French, German, and Spanish, i.e. nervous, nerveux, nervös, and nervioso, and if we want to find out their translation into Italian, we would look at the association vectors of each word in our Italian target vocabulary.', 'The association strengths in these vectors need to be inversely sorted, and in each of them we will look up the positions of our four given words.', 'Then for each vector we compute the product of the four ranks, and finally sort the Italian vocabulary according to these products.', 'We would then expect that the correct Italian translation, namely nervoso, ends up in the first position, i.e. has the smallest value for its product of ranks.']",4,"['So far, we always computed translations to single source words.', 'As suggested in #AUTHOR_TAG this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .', 'We then assume that the candidate with the smallest product will be the best translation. 3']"
CCT276,W10-4005,Identifying word translations in non-parallel texts,utilizing citations of foreign words in corpusbased dictionary generation,"['Reinhard Rapp', 'Michael Zock']",conclusion,"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations.","Whereas #AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .","['Whereas #AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .', 'We were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair.', 'For example, it is more promising to look at occurrences of English words in a German corpus rather than the other way around.', 'Because of the special status of English it is also advisable to use it as a pivot wherever possible.']",1,"['Whereas #AUTHOR_TAG dealt only with an English corpus , the current work shows that this methodology is applicable to a wide range of languages and corpora .']"
CCT277,W10-4215,Concept Type Prediction and Responsive Adaptation in a Dialogue System,extracting paraphrases from a parallel corpus,"['R Barzilay', 'K McKeown']",conclusion,"While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.",For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG .,"['The current work has focussed on high-level mapping rules which can be used both for generation from databases and knowledge representations and also for generation from text.', 'In future work, we will focus on mapping text (in monologue form) to dialogue.', 'For this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form.', 'For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG .', 'An important component of our future effort will be to evaluate whether automatically generating dialogues from naturally-occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue.']",3,['For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in #AUTHOR_TAG .']
CCT278,W11-0218,Generalising semantic category disambiguation with large lexical resources for fun and profit,boosting precision and recall of dictionarybased protein name recognition,"['Y Tsuruoka', 'J Tsujii']",conclusion,"Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GE-NIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score.",A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .,"['While we expected to see clear benefits from both using Gazetteers and SimString features, our exper- iments returned negative results for the majority of the corpora.', 'For NLPBA, GENIA and ID we are aware that most of the instances are either proteins or belong to event trigger classes for which we may not have had adequate lexical resources for disambiguation.', 'By contrast, for Super GREC there are several distinct classes for which we expected lexical resources to have fair coverage for SimString and Gazetteer features.', 'While an advantage over Internal was observed for Super GREC, SimString features showed no benefit over Gazetteer features.', 'The methods exhibited the expected result on only one of the six corpora, CALBC CII, where there is a clear advantage for Gazetteer over Internal and a further clear advantage for SimString over Gazetteer.', 'Disappointingly, we did not succeed in establishing a clear improvement for more than one of the six corpora.', 'Although we have not been successful in Figure 6: Learning curve for Super GREC Figure 7: Learning curve for EPI proving our initial hypothesis we argue that our results calls for further study due to several concerns raised by the results remaining unanswered.', 'It may be that our notion of distance to lexical resource entries is too naive.', 'A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']",3,"['A possible future direction would be to compare the query string to retrieved results using a method similar to that of #AUTHOR_TAG .', 'This would enable us to retain the advantage of fast approximate string matching, thus being able to utilise larger lexical resources than if we were to calculate sophisticated alignments for each lexical entry.']"
CCT279,W14-1704,The Illinois-Columbia System in the CoNLL-2014 Shared Task,the ui system in the hoo 2012 shared task on error correction,"['A Rozovskaya', 'M Sammons', 'D Roth']",experiments,"We describe the University of Illinois (UI) system that participated in the Helping Our Own (HOO) 2012 shared task, which focuses on correcting preposition and determiner errors made by non-native English speakers. The task consisted of three metrics: Detection, Recognition, and Correction, and measured performance before and after additional revisions to the test data were made. Out of 14 teams that participated, our system scored first in Detection and Recognition and second in Correction before the revisions; and first in Detection and second in the other metrics after revisions. We describe our underlying approach, which relates to our previous work in this area, and propose an improvement to the earlier method, error inflation, which results in significant gains in performance.",The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .,"['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .', 'The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.', 'The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles).']",5,['The article classifier is a discriminative model that draws on the state-of-the-art approach described in #AUTHOR_TAG .']
CCT280,W14-1704,The Illinois-Columbia System in the CoNLL-2014 Shared Task,algorithm selection and model adaptation for esl correction tasks,"['A Rozovskaya', 'D Roth']",experiments,"We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essential to making progress in ESL error correction - algorithm selection and model adaptation to the first language of the ESL learner.    A variety of learning algorithms have been applied to correct ESL mistakes, but often comparisons were made between incomparable data sets. We conduct an extensive, fair comparison of four popular learning methods for the task, reversing conclusions from earlier evaluations. Our results hold for different training sets, genres, and feature sets.    A second key issue in ESL error correction is the adaptation of a model to the first language of the writer. Errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the non-native writers. We propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods.",The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #AUTHOR_TAG ) .,"['The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #AUTHOR_TAG ) .', 'Thus, the classifiers trained on the learner data make use of a discriminative model.', 'Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)).']",4,['The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( #AUTHOR_TAG ) .']
CCT281,W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies,"['Michel Galley', 'Kathleen McKeown', 'Julia Hirschberg', 'Elizabeth Shriberg']",related work,"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.","Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .","['Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']",1,"['Another line of research that is correlated with ours is recognition of agreement/disagreement ( Misra and Walker , 2013 ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; #AUTHOR_TAG ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']"
CCT282,W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,topic independent identification of agreement and disagreement in social media dialogue,"['Amita Misra', 'Marilyn A Walker']",,"Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community's ability to develop better theories, as well as good off the shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%.Comment: @inproceedings{Misra2013TopicII, title={Topic Independent   Identification of Agreement and Disagreement in Social Media Dialogue},   author={Amita Misra and Marilyn A. Walker}, booktitle={SIGDIAL Conference},   year={2013}","In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG ) .","['The first row of the Table 6 represents the baseline results.', 'Though the precision is high for agreement category, the recall is quite low and that results in a poor overall F1 measure.', ""This shows that even though markers like 'agree' or 'disagree'  For the next set of experiments we used a supervised machine learning approach for the two-way classification (Agree/Disagree)."", 'We use Support Vector Machines (SVM) as our machine-learning algorithm for classification as implemented in Weka (Hall et al., 2009) and ran 10-fold cross validation.', 'As a SVM baseline, we first use all unigrams in Callout and Target as features (Table 6, Row 2).', 'We notice that the recall improves significantly when compared with the rule-based method.', 'To further improve the classification accuracy, we use Mutual Information (MI) to select the words in the Callouts and Targets that are likely to be associated with the categories Agree and Disagree, respectively.', 'Specifically, we sort each word based on its MI value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'The feature vector includes only words present in the MI list.', 'Compared to the all unigrams baseline, the MI-based unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6).', 'The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG ) .']",4,"['In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; #AUTHOR_TAG ) .']"
CCT283,W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,topic independent identification of agreement and disagreement in social media dialogue,"['Amita Misra', 'Marilyn A Walker']",related work,"Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community's ability to develop better theories, as well as good off the shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%.Comment: @inproceedings{Misra2013TopicII, title={Topic Independent   Identification of Agreement and Disagreement in Social Media Dialogue},   author={Amita Misra and Marilyn A. Walker}, booktitle={SIGDIAL Conference},   year={2013}","Another line of research that is correlated with ours is recognition of agreement/disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .","['Another line of research that is correlated with ours is recognition of agreement/disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']",1,"['Another line of research that is correlated with ours is recognition of agreement/disagreement ( #AUTHOR_TAG ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .', 'For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010;Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.']"
CCT284,W14-2106,Analyzing Argumentative Discourse Units in Online Interactions,identifying agreement and disagreement in conversational speech use of bayesian networks to model pragmatic dependencies,"['Michel Galley', 'Kathleen McKeown', 'Julia Hirschberg', 'Elizabeth Shriberg']",,"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.","In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 ) .","['The first row of the Table 6 represents the baseline results.', 'Though the precision is high for agreement category, the recall is quite low and that results in a poor overall F1 measure.', ""This shows that even though markers like 'agree' or 'disagree'  For the next set of experiments we used a supervised machine learning approach for the two-way classification (Agree/Disagree)."", 'We use Support Vector Machines (SVM) as our machine-learning algorithm for classification as implemented in Weka (Hall et al., 2009) and ran 10-fold cross validation.', 'As a SVM baseline, we first use all unigrams in Callout and Target as features (Table 6, Row 2).', 'We notice that the recall improves significantly when compared with the rule-based method.', 'To further improve the classification accuracy, we use Mutual Information (MI) to select the words in the Callouts and Targets that are likely to be associated with the categories Agree and Disagree, respectively.', 'Specifically, we sort each word based on its MI value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words.', 'The feature vector includes only words present in the MI list.', 'Compared to the all unigrams baseline, the MI-based unigrams improve the F1 by 4% (Agree) and 2% (Disagree) (Table 6).', 'The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification.', 'In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 ) .']",4,"['In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( #AUTHOR_TAG ; Misra and Walker , 2013 ) .']"
